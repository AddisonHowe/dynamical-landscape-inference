Args:
Namespace(name='model_phi1_4a_distortion_v1_5_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_5/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_5/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.054671332240104675, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2183631254

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.650453560876984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.650453560876984 | validation: 7.567041641544126]
	TIME [epoch: 168 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.761763998751603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.761763998751603 | validation: 7.387824366750863]
	TIME [epoch: 0.765 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.444221049017623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.444221049017623 | validation: 7.278233894884673]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.345986827539246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.345986827539246 | validation: 7.253733704797942]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.199195011731628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.199195011731628 | validation: 7.025581206818103]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.0634952139508504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.0634952139508504 | validation: 6.681259464490252]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.9939715574616335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.9939715574616335 | validation: 6.466622541151605]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.70580040793145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.70580040793145 | validation: 6.181239860854802]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.582122737949053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.582122737949053 | validation: 6.594130593578888]
	TIME [epoch: 0.698 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.322852813357558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.322852813357558 | validation: 6.273724115014767]
	TIME [epoch: 0.699 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.925558045181664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.925558045181664 | validation: 5.888199905617241]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.286480836795518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.286480836795518 | validation: 5.526820447711789]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.987521265415504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.987521265415504 | validation: 6.9971195619704645]
	TIME [epoch: 0.701 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.094366811250811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.094366811250811 | validation: 5.5949132102174275]
	TIME [epoch: 0.698 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.707269453810416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.707269453810416 | validation: 5.426547919480782]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.622002729476673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.622002729476673 | validation: 5.865053467716698]
	TIME [epoch: 0.7 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.42839607264498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.42839607264498 | validation: 5.899896527702495]
	TIME [epoch: 0.698 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.074976707451742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.074976707451742 | validation: 5.782339461803088]
	TIME [epoch: 0.699 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.091149077604313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.091149077604313 | validation: 5.298844296469184]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.358917323300867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.358917323300867 | validation: 5.526226009126559]
	TIME [epoch: 0.695 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.20219237327893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.20219237327893 | validation: 5.7828746302065]
	TIME [epoch: 0.693 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.528876515628642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.528876515628642 | validation: 5.7588414360704805]
	TIME [epoch: 0.694 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.644826209421583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.644826209421583 | validation: 5.637941573691403]
	TIME [epoch: 0.695 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.221196506148999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.221196506148999 | validation: 5.111754711880159]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.503601463281144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.503601463281144 | validation: 5.180114334795121]
	TIME [epoch: 0.698 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.272080257297946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.272080257297946 | validation: 5.223825855445052]
	TIME [epoch: 0.697 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.081948769164355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.081948769164355 | validation: 5.548781062939611]
	TIME [epoch: 0.698 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.437003285486038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.437003285486038 | validation: 5.539517736203198]
	TIME [epoch: 0.697 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.8704725852809245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.8704725852809245 | validation: 4.961914087356878]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.931967384198432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.931967384198432 | validation: 4.821654427263573]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9458932284853665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9458932284853665 | validation: 5.2859037604472]
	TIME [epoch: 0.699 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.7094604765196655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7094604765196655 | validation: 4.581084396251717]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6480387913084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6480387913084 | validation: 4.9375951524387345]
	TIME [epoch: 0.699 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.236107271489236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.236107271489236 | validation: 5.025689200602726]
	TIME [epoch: 0.697 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.342039136968779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.342039136968779 | validation: 4.694469706757805]
	TIME [epoch: 0.7 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7879814695727205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7879814695727205 | validation: 5.101301995965439]
	TIME [epoch: 0.698 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.2851968686002495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2851968686002495 | validation: 4.9210899235378305]
	TIME [epoch: 0.697 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.039117736550148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.039117736550148 | validation: 4.899260979938557]
	TIME [epoch: 0.697 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0055451076088495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0055451076088495 | validation: 4.611124352186401]
	TIME [epoch: 0.698 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7385688752133706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7385688752133706 | validation: 4.7248677227717]
	TIME [epoch: 0.698 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8071338232730048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8071338232730048 | validation: 4.762262282485699]
	TIME [epoch: 0.697 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.004403472885989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.004403472885989 | validation: 4.6440840006858375]
	TIME [epoch: 0.697 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9608098687967845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9608098687967845 | validation: 4.294148328948911]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5348375566799497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5348375566799497 | validation: 4.4002125380694705]
	TIME [epoch: 0.698 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.56553183153913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.56553183153913 | validation: 4.220047937081335]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5232156784441115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5232156784441115 | validation: 4.2068745170021495]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5358988943586507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5358988943586507 | validation: 4.41990103493536]
	TIME [epoch: 0.699 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8146034045415966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8146034045415966 | validation: 4.875431087057628]
	TIME [epoch: 0.697 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8895892259021565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8895892259021565 | validation: 4.127883210142426]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.400277665379516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.400277665379516 | validation: 4.1062577760803265]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.458358785950793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.458358785950793 | validation: 3.8659518788909084]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.131494158441596		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.131494158441596 | validation: 3.7597723687607316]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1599316408908327		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 3.1599316408908327 | validation: 4.297247751400196]
	TIME [epoch: 0.696 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7627054683084324		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 3.7627054683084324 | validation: 4.227772835227478]
	TIME [epoch: 0.694 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.516579477024975		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 3.516579477024975 | validation: 3.6996403295269418]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1337722984161736		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 3.1337722984161736 | validation: 3.501535040761027]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2984070762980653		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 3.2984070762980653 | validation: 3.4680214190443923]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9143821090604063		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 2.9143821090604063 | validation: 3.1144188865776012]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.782581014872725		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 2.782581014872725 | validation: 3.257710768143222]
	TIME [epoch: 0.695 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.930123519089232		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 2.930123519089232 | validation: 3.3925668793166466]
	TIME [epoch: 0.694 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.37305760224531		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 3.37305760224531 | validation: 3.9822968141177757]
	TIME [epoch: 0.695 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9808470851131323		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 2.9808470851131323 | validation: 2.926518261128961]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.681419048783763		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 2.681419048783763 | validation: 2.8718066131398414]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.739578342905223		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 2.739578342905223 | validation: 2.349511904674059]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2360360417023517		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 2.2360360417023517 | validation: 2.180633844929255]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0891193327439233		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 2.0891193327439233 | validation: 2.191979778105541]
	TIME [epoch: 0.697 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5763299703543043		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 2.5763299703543043 | validation: 4.1638768668436965]
	TIME [epoch: 0.697 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4816362733369375		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 3.4816362733369375 | validation: 1.915685519554321]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8099216647057825		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.8099216647057825 | validation: 2.460648889753128]
	TIME [epoch: 0.696 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.441708305968987		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 2.441708305968987 | validation: 3.413668388479026]
	TIME [epoch: 0.695 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6197814930898837		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 2.6197814930898837 | validation: 2.0530786901587006]
	TIME [epoch: 0.694 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1827819434652724		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 2.1827819434652724 | validation: 2.645543984452253]
	TIME [epoch: 0.694 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.683887236784577		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 2.683887236784577 | validation: 1.72308061284268]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.785916930924142		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.785916930924142 | validation: 1.6072759565937387]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6866102608240607		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.6866102608240607 | validation: 1.7636460902801385]
	TIME [epoch: 0.7 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.938817382204943		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.938817382204943 | validation: 1.9996619943499263]
	TIME [epoch: 0.696 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2572773599563014		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 2.2572773599563014 | validation: 1.6146121239596873]
	TIME [epoch: 0.696 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8690337660614862		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.8690337660614862 | validation: 1.476820766456319]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.628710829238097		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.628710829238097 | validation: 1.3894867475522599]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6192839119312634		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.6192839119312634 | validation: 1.2645616322273785]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7414114599751358		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.7414114599751358 | validation: 1.5934717340584894]
	TIME [epoch: 0.699 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.795931159046711		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.795931159046711 | validation: 1.3288811276523644]
	TIME [epoch: 0.699 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7654721089805594		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.7654721089805594 | validation: 3.2003744861701353]
	TIME [epoch: 0.697 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4971546093977817		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 2.4971546093977817 | validation: 1.1544442451377286]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5179470648074238		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.5179470648074238 | validation: 1.4540668155021048]
	TIME [epoch: 0.698 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.732602955520272		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.732602955520272 | validation: 2.3296225107540045]
	TIME [epoch: 0.697 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9991368382015458		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.9991368382015458 | validation: 1.1344679583109767]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5814262728859516		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.5814262728859516 | validation: 1.6193776925935652]
	TIME [epoch: 0.699 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5754549554135953		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.5754549554135953 | validation: 1.2545084927410697]
	TIME [epoch: 0.696 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8069858617764645		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.8069858617764645 | validation: 2.000791631780116]
	TIME [epoch: 0.698 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.831945316618397		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.831945316618397 | validation: 1.2483004770566288]
	TIME [epoch: 0.696 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5498860816083515		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.5498860816083515 | validation: 1.4366791296681907]
	TIME [epoch: 0.697 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.482736001965911		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.482736001965911 | validation: 1.1198296378775325]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.476342829240723		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.476342829240723 | validation: 1.3902856492485383]
	TIME [epoch: 0.696 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5305372177000527		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.5305372177000527 | validation: 1.29284406288553]
	TIME [epoch: 0.697 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6449602240675731		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.6449602240675731 | validation: 1.6391871942761285]
	TIME [epoch: 0.696 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7064877302261343		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.7064877302261343 | validation: 1.206381532241904]
	TIME [epoch: 0.696 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5466064282516379		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.5466064282516379 | validation: 1.23692321826478]
	TIME [epoch: 0.696 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4351131228764014		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.4351131228764014 | validation: 1.0821211720795234]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3889858676011608		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.3889858676011608 | validation: 1.349931418613126]
	TIME [epoch: 0.696 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4415398244301418		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.4415398244301418 | validation: 1.2009036179191204]
	TIME [epoch: 0.697 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5441235699752596		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.5441235699752596 | validation: 1.9938646774993765]
	TIME [epoch: 0.697 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7404188673186776		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.7404188673186776 | validation: 1.0195094221499967]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6652299502507526		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.6652299502507526 | validation: 1.753669120747773]
	TIME [epoch: 0.698 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7414954644103555		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.7414954644103555 | validation: 1.0877998199110674]
	TIME [epoch: 0.7 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5290636446830939		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.5290636446830939 | validation: 1.3406003800790787]
	TIME [epoch: 0.697 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4135571786101724		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.4135571786101724 | validation: 1.0329046824861832]
	TIME [epoch: 0.698 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3993623413906577		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.3993623413906577 | validation: 1.286298175815639]
	TIME [epoch: 0.699 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4147235251044492		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.4147235251044492 | validation: 1.0676947822623821]
	TIME [epoch: 0.697 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4713559342037206		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.4713559342037206 | validation: 1.7352998240883188]
	TIME [epoch: 0.697 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6032031714474066		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.6032031714474066 | validation: 1.1789145492256725]
	TIME [epoch: 0.698 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6768045676343628		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.6768045676343628 | validation: 1.5376584175249581]
	TIME [epoch: 0.698 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5040211080264598		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.5040211080264598 | validation: 1.0107600162423613]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3919555141738458		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.3919555141738458 | validation: 1.2871151863462142]
	TIME [epoch: 0.699 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.426331479304965		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.426331479304965 | validation: 0.9873481698121428]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4881584824290992		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.4881584824290992 | validation: 1.2930659689587964]
	TIME [epoch: 0.695 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4670728305440792		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.4670728305440792 | validation: 1.066503214776297]
	TIME [epoch: 0.695 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.39693929676311		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.39693929676311 | validation: 1.1229062597501513]
	TIME [epoch: 0.696 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3985738774421574		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.3985738774421574 | validation: 1.4696021950345484]
	TIME [epoch: 0.694 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5255541471806475		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.5255541471806475 | validation: 1.401479381939225]
	TIME [epoch: 0.695 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.757154324009954		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.757154324009954 | validation: 1.7179829224176235]
	TIME [epoch: 0.696 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.619045660600725		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.619045660600725 | validation: 0.9665785725304851]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4597554321910453		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.4597554321910453 | validation: 1.4196074717464935]
	TIME [epoch: 0.696 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4699746631831658		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.4699746631831658 | validation: 0.9964371414837377]
	TIME [epoch: 0.695 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4562553260834181		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.4562553260834181 | validation: 1.401720971092764]
	TIME [epoch: 0.696 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4195867678562155		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.4195867678562155 | validation: 0.9738540430120513]
	TIME [epoch: 0.697 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.442298265173298		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.442298265173298 | validation: 1.348961000992078]
	TIME [epoch: 0.695 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4090443243023643		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.4090443243023643 | validation: 1.0520703294984106]
	TIME [epoch: 0.695 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4300720050194957		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.4300720050194957 | validation: 1.3495248960101134]
	TIME [epoch: 0.696 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4584533133657502		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.4584533133657502 | validation: 1.1236267225030354]
	TIME [epoch: 0.696 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4991740238682918		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.4991740238682918 | validation: 1.2969381437738976]
	TIME [epoch: 0.695 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4747348520965158		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.4747348520965158 | validation: 1.1197453291069568]
	TIME [epoch: 0.695 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4072905551543737		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.4072905551543737 | validation: 1.1844385624461402]
	TIME [epoch: 0.695 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3920905611783685		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.3920905611783685 | validation: 1.0032627993184169]
	TIME [epoch: 0.695 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.378201503002515		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.378201503002515 | validation: 1.6023889616854363]
	TIME [epoch: 0.695 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5176659249198765		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.5176659249198765 | validation: 1.075500705654034]
	TIME [epoch: 0.696 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6409338040572172		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.6409338040572172 | validation: 1.4434941116416378]
	TIME [epoch: 0.696 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4726496231932373		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.4726496231932373 | validation: 0.9942746249322891]
	TIME [epoch: 0.695 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3653015851494288		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.3653015851494288 | validation: 1.050499720132326]
	TIME [epoch: 0.695 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3215521850395842		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.3215521850395842 | validation: 1.1396790237506522]
	TIME [epoch: 0.696 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3421114561442757		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.3421114561442757 | validation: 0.9804017190855113]
	TIME [epoch: 0.697 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4160924679510165		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.4160924679510165 | validation: 1.4903234314179525]
	TIME [epoch: 0.696 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4786856936342168		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.4786856936342168 | validation: 1.084528382322994]
	TIME [epoch: 0.695 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.46966986812017		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.46966986812017 | validation: 1.3530464249875853]
	TIME [epoch: 0.697 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4373265436891993		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.4373265436891993 | validation: 1.0535230949033794]
	TIME [epoch: 0.697 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4243277892295831		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.4243277892295831 | validation: 1.092106055018967]
	TIME [epoch: 0.695 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3862797672513492		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.3862797672513492 | validation: 1.130341951894741]
	TIME [epoch: 0.695 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.380161903724225		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.380161903724225 | validation: 1.0478642327485177]
	TIME [epoch: 0.696 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3848812667454002		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.3848812667454002 | validation: 1.108620734436669]
	TIME [epoch: 0.696 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.36151481184498		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.36151481184498 | validation: 1.1270423952553152]
	TIME [epoch: 0.695 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3486338426605877		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.3486338426605877 | validation: 1.0347591527748718]
	TIME [epoch: 0.695 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3974540314585653		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.3974540314585653 | validation: 1.7999376732520902]
	TIME [epoch: 0.697 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6259458266898912		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.6259458266898912 | validation: 0.9454303003060833]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_153.pth
	Model improved!!!
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6010630412906028		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.6010630412906028 | validation: 1.3653830962276055]
	TIME [epoch: 0.695 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.417786087926852		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.417786087926852 | validation: 0.9940713323441243]
	TIME [epoch: 0.705 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.341275579450073		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.341275579450073 | validation: 1.0607424923006261]
	TIME [epoch: 0.696 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3032334326534414		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.3032334326534414 | validation: 1.0953910801755036]
	TIME [epoch: 0.695 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2996861021601365		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.2996861021601365 | validation: 0.9749914816465854]
	TIME [epoch: 0.695 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3091955583715937		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.3091955583715937 | validation: 1.2326342176409066]
	TIME [epoch: 0.697 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3263011315600397		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.3263011315600397 | validation: 0.9189909658457165]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3930185277542262		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.3930185277542262 | validation: 1.5377222784537556]
	TIME [epoch: 0.695 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4690307305759691		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.4690307305759691 | validation: 1.1464800498358112]
	TIME [epoch: 0.7 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5479794857230744		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.5479794857230744 | validation: 1.4039498626683171]
	TIME [epoch: 0.695 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.514786558870333		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.514786558870333 | validation: 1.0465448083995488]
	TIME [epoch: 0.696 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3716971804751816		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.3716971804751816 | validation: 0.9610378110527681]
	TIME [epoch: 0.695 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3047022853936412		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.3047022853936412 | validation: 1.1129546259593415]
	TIME [epoch: 0.696 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3036271976150835		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.3036271976150835 | validation: 0.9711191878385059]
	TIME [epoch: 0.695 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3162045030721055		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.3162045030721055 | validation: 1.2056383515340974]
	TIME [epoch: 0.694 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3495742553915482		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.3495742553915482 | validation: 1.051627797975305]
	TIME [epoch: 0.695 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3858254223308872		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.3858254223308872 | validation: 1.5775837813852993]
	TIME [epoch: 0.696 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.493109591798082		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.493109591798082 | validation: 0.936774849097375]
	TIME [epoch: 0.696 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4837327395807227		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.4837327395807227 | validation: 1.2681472340579203]
	TIME [epoch: 0.697 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3569068397665474		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.3569068397665474 | validation: 1.0101478814201368]
	TIME [epoch: 0.695 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3108594136846567		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.3108594136846567 | validation: 0.9946538305887958]
	TIME [epoch: 0.695 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3069800868863497		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.3069800868863497 | validation: 1.1991482014623411]
	TIME [epoch: 0.695 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3231894350260873		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.3231894350260873 | validation: 1.0255863622286518]
	TIME [epoch: 0.694 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.33775268794028		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.33775268794028 | validation: 1.3087556162065046]
	TIME [epoch: 0.695 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3761909389565046		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.3761909389565046 | validation: 0.9868043676513232]
	TIME [epoch: 0.696 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3688700020146924		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.3688700020146924 | validation: 1.3025707041433674]
	TIME [epoch: 0.695 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3609577524503282		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.3609577524503282 | validation: 0.957647096788723]
	TIME [epoch: 0.695 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3286949063358071		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.3286949063358071 | validation: 1.092426815563065]
	TIME [epoch: 0.695 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.290506173452693		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.290506173452693 | validation: 0.9825232101092496]
	TIME [epoch: 0.695 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2803937712019948		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.2803937712019948 | validation: 0.9985858277923909]
	TIME [epoch: 0.695 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2735276528264823		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.2735276528264823 | validation: 1.0863063075255301]
	TIME [epoch: 0.695 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2805065030318958		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.2805065030318958 | validation: 0.9386918299831338]
	TIME [epoch: 0.696 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3642205439627761		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.3642205439627761 | validation: 1.2927609972798992]
	TIME [epoch: 0.696 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4086819773607433		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.4086819773607433 | validation: 1.0548949557560492]
	TIME [epoch: 0.694 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3355738631425766		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.3355738631425766 | validation: 1.0081352669680406]
	TIME [epoch: 0.694 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3445488390302365		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.3445488390302365 | validation: 1.7656984738557686]
	TIME [epoch: 0.695 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5957647883399364		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.5957647883399364 | validation: 0.9217615154628143]
	TIME [epoch: 0.694 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4907779460483346		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.4907779460483346 | validation: 1.0391292637231666]
	TIME [epoch: 0.695 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.283822383932459		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.283822383932459 | validation: 1.1898013717587703]
	TIME [epoch: 0.694 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.30315379852955		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.30315379852955 | validation: 0.9408759223513887]
	TIME [epoch: 0.694 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3377203903332906		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.3377203903332906 | validation: 1.260327566325485]
	TIME [epoch: 0.695 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.339144787869819		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.339144787869819 | validation: 0.9688026250481926]
	TIME [epoch: 0.694 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.28203717245512		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.28203717245512 | validation: 1.061734236758434]
	TIME [epoch: 0.694 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2778714931066517		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.2778714931066517 | validation: 1.009873806684275]
	TIME [epoch: 0.702 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2622775554422934		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.2622775554422934 | validation: 0.9154828764209239]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2661494640931081		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.2661494640931081 | validation: 1.105075912053446]
	TIME [epoch: 0.696 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2648595730172705		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.2648595730172705 | validation: 0.9090532027729271]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2922671289589664		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.2922671289589664 | validation: 1.4299965811904363]
	TIME [epoch: 178 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3972363243707122		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.3972363243707122 | validation: 1.053432226968044]
	TIME [epoch: 1.38 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.474710834001292		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.474710834001292 | validation: 1.0975742275995535]
	TIME [epoch: 1.36 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3119058156758445		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.3119058156758445 | validation: 1.0411621775069706]
	TIME [epoch: 1.36 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2740379165688085		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.2740379165688085 | validation: 0.8899772696975266]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2796111489350641		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.2796111489350641 | validation: 1.1531593442886674]
	TIME [epoch: 1.36 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.27607947507508		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.27607947507508 | validation: 0.8854496776656016]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_207.pth
	Model improved!!!
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2879084473530384		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.2879084473530384 | validation: 1.1728610552167318]
	TIME [epoch: 1.36 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.283707001188343		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.283707001188343 | validation: 0.9645916158710256]
	TIME [epoch: 1.36 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2844306817067146		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.2844306817067146 | validation: 1.2427269159195307]
	TIME [epoch: 1.36 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.366528867323121		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.366528867323121 | validation: 0.9970414253384885]
	TIME [epoch: 1.36 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4116381624817735		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.4116381624817735 | validation: 1.2457760811762078]
	TIME [epoch: 1.36 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.308475454336346		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 1.308475454336346 | validation: 0.8996747407536518]
	TIME [epoch: 1.36 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2526895118612587		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.2526895118612587 | validation: 1.003446143383852]
	TIME [epoch: 1.36 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2312518759393423		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 1.2312518759393423 | validation: 0.9707291962732371]
	TIME [epoch: 1.36 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.238187186343593		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.238187186343593 | validation: 1.002672248761191]
	TIME [epoch: 1.36 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2440283060726611		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.2440283060726611 | validation: 1.0850435494780672]
	TIME [epoch: 1.36 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2671489022069788		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.2671489022069788 | validation: 0.9832098003743456]
	TIME [epoch: 1.36 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3594602434644458		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.3594602434644458 | validation: 1.3835678490495527]
	TIME [epoch: 1.36 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3897067057879076		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.3897067057879076 | validation: 0.8656728406149089]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3069745395599244		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 1.3069745395599244 | validation: 1.0535680683100397]
	TIME [epoch: 1.37 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.241450456637607		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.241450456637607 | validation: 0.8167288002270567]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_222.pth
	Model improved!!!
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2712935544258581		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 1.2712935544258581 | validation: 1.2062273790466773]
	TIME [epoch: 1.36 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2890996577246199		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.2890996577246199 | validation: 0.844526581380348]
	TIME [epoch: 1.36 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2826516549099518		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 1.2826516549099518 | validation: 1.0253833496285816]
	TIME [epoch: 1.36 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.236841138424654		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 1.236841138424654 | validation: 0.9885461383264771]
	TIME [epoch: 1.36 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2403314372818282		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.2403314372818282 | validation: 1.057555646056892]
	TIME [epoch: 1.36 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2660093228095928		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.2660093228095928 | validation: 0.9880127038313383]
	TIME [epoch: 1.36 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3525473797000265		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 1.3525473797000265 | validation: 1.2587528369097833]
	TIME [epoch: 1.36 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.335422572493094		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.335422572493094 | validation: 0.9324368839520475]
	TIME [epoch: 1.36 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2550920507965533		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 1.2550920507965533 | validation: 0.913981745693695]
	TIME [epoch: 1.36 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.23087713498238		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.23087713498238 | validation: 1.04463722104885]
	TIME [epoch: 1.36 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2265072779071684		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 1.2265072779071684 | validation: 0.9406530056872552]
	TIME [epoch: 1.36 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.200867132912757		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.200867132912757 | validation: 1.0250584025925946]
	TIME [epoch: 1.36 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2104849693561393		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 1.2104849693561393 | validation: 0.9183217131645547]
	TIME [epoch: 1.36 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2253163075413402		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.2253163075413402 | validation: 1.0732002095542494]
	TIME [epoch: 1.36 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2471309291365202		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 1.2471309291365202 | validation: 1.071735597292475]
	TIME [epoch: 1.36 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3329409969110035		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 1.3329409969110035 | validation: 1.5591845495475767]
	TIME [epoch: 1.36 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4847130471608068		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 1.4847130471608068 | validation: 0.7946812475637931]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3550077360513357		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 1.3550077360513357 | validation: 0.9690846076234237]
	TIME [epoch: 1.36 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2283778442025868		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 1.2283778442025868 | validation: 1.0542355446927885]
	TIME [epoch: 1.36 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2605198710283818		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 1.2605198710283818 | validation: 0.9071856804250017]
	TIME [epoch: 1.37 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.239708134838671		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 1.239708134838671 | validation: 1.0715232336275926]
	TIME [epoch: 1.36 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2302748914794592		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 1.2302748914794592 | validation: 0.9285886182648856]
	TIME [epoch: 1.36 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2145204105438183		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 1.2145204105438183 | validation: 0.9222687014736533]
	TIME [epoch: 1.36 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1975332297484997		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.1975332297484997 | validation: 0.9810186810722558]
	TIME [epoch: 1.36 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.206465699435597		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 1.206465699435597 | validation: 0.8954102034862106]
	TIME [epoch: 1.36 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2175282619596943		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 1.2175282619596943 | validation: 1.1295501419967238]
	TIME [epoch: 1.36 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.227587538466122		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 1.227587538466122 | validation: 0.8890462724269264]
	TIME [epoch: 1.36 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2620785935560772		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 1.2620785935560772 | validation: 1.3032090069075897]
	TIME [epoch: 1.36 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2925516630695248		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 1.2925516630695248 | validation: 0.8345196370269751]
	TIME [epoch: 1.35 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2585625917310481		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 1.2585625917310481 | validation: 1.1531529231516646]
	TIME [epoch: 1.36 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.238377880670272		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 1.238377880670272 | validation: 0.8642382635098222]
	TIME [epoch: 1.36 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2129954242891317		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 1.2129954242891317 | validation: 0.9493872968526414]
	TIME [epoch: 1.36 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2011498421054885		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 1.2011498421054885 | validation: 1.023553596932025]
	TIME [epoch: 1.36 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2164498816192661		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 1.2164498816192661 | validation: 0.8811004376751446]
	TIME [epoch: 1.36 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2194426674298167		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 1.2194426674298167 | validation: 0.9738100570879097]
	TIME [epoch: 1.36 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1979821741062346		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 1.1979821741062346 | validation: 0.9914524633144787]
	TIME [epoch: 1.36 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.188603869725557		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 1.188603869725557 | validation: 0.8739532651070798]
	TIME [epoch: 1.35 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1973233356828952		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 1.1973233356828952 | validation: 1.206051737698604]
	TIME [epoch: 1.36 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2702124565709472		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 1.2702124565709472 | validation: 0.9884169340296917]
	TIME [epoch: 1.36 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3723225324271011		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 1.3723225324271011 | validation: 1.1349472211099432]
	TIME [epoch: 1.36 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2442267448985456		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 1.2442267448985456 | validation: 0.9249356915315705]
	TIME [epoch: 1.36 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1862081536545988		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 1.1862081536545988 | validation: 0.8391040775465491]
	TIME [epoch: 1.36 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1922922047680922		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 1.1922922047680922 | validation: 1.0586887028859782]
	TIME [epoch: 1.36 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.205173496791343		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 1.205173496791343 | validation: 0.8163845765971431]
	TIME [epoch: 1.36 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1940570596096536		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 1.1940570596096536 | validation: 1.0830417309943379]
	TIME [epoch: 1.36 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.206073022967179		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 1.206073022967179 | validation: 0.8480944797620036]
	TIME [epoch: 1.36 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2341786454691934		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 1.2341786454691934 | validation: 1.141831502381048]
	TIME [epoch: 1.36 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2433085016758971		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 1.2433085016758971 | validation: 0.926111286680148]
	TIME [epoch: 1.36 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.222702660003686		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 1.222702660003686 | validation: 0.9882118167014536]
	TIME [epoch: 1.36 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1986223918162071		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 1.1986223918162071 | validation: 0.9346967111645115]
	TIME [epoch: 1.36 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.173474654065846		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 1.173474654065846 | validation: 0.8759919326033764]
	TIME [epoch: 1.36 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1643611315276312		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 1.1643611315276312 | validation: 0.9334166843266026]
	TIME [epoch: 1.36 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1660658833905324		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 1.1660658833905324 | validation: 0.8892777968500308]
	TIME [epoch: 1.36 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1635412829850247		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 1.1635412829850247 | validation: 0.9321468599915184]
	TIME [epoch: 1.36 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1660376598020343		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 1.1660376598020343 | validation: 0.8604651630823863]
	TIME [epoch: 1.36 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1795837847966866		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 1.1795837847966866 | validation: 1.2080822144936765]
	TIME [epoch: 1.36 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.250952042969837		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 1.250952042969837 | validation: 0.9560206170969336]
	TIME [epoch: 1.36 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3048894476877908		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 1.3048894476877908 | validation: 1.3573384937194128]
	TIME [epoch: 1.36 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3682881449033668		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 1.3682881449033668 | validation: 0.8798822925935301]
	TIME [epoch: 1.36 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2138790934290395		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 1.2138790934290395 | validation: 0.8089940899631158]
	TIME [epoch: 1.36 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1696276743380707		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 1.1696276743380707 | validation: 1.0258017680914946]
	TIME [epoch: 1.36 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1874197479827702		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 1.1874197479827702 | validation: 0.8843891982362528]
	TIME [epoch: 1.36 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1636076026585367		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 1.1636076026585367 | validation: 0.8469636326027156]
	TIME [epoch: 1.36 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1528218971165256		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 1.1528218971165256 | validation: 0.8919116954671618]
	TIME [epoch: 1.36 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1574698637513623		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 1.1574698637513623 | validation: 0.9564413613609969]
	TIME [epoch: 1.36 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1500469171297192		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 1.1500469171297192 | validation: 0.8209463148351408]
	TIME [epoch: 1.36 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.154002756100672		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 1.154002756100672 | validation: 0.9273498791900858]
	TIME [epoch: 1.36 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1493781400507364		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 1.1493781400507364 | validation: 0.8517033495266717]
	TIME [epoch: 1.36 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1460360199875512		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 1.1460360199875512 | validation: 0.9054314859946356]
	TIME [epoch: 1.36 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1456301750114388		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 1.1456301750114388 | validation: 0.8483225216897744]
	TIME [epoch: 1.36 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1570526098602465		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 1.1570526098602465 | validation: 0.9512416154471831]
	TIME [epoch: 1.36 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.17483850816044		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 1.17483850816044 | validation: 1.135959354760615]
	TIME [epoch: 1.36 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.274009016826655		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 1.274009016826655 | validation: 0.9505746909921116]
	TIME [epoch: 1.36 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2807598399918672		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 1.2807598399918672 | validation: 1.1060520665719864]
	TIME [epoch: 1.36 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2041342004034445		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 1.2041342004034445 | validation: 0.7898561874127785]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1749275664773933		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 1.1749275664773933 | validation: 0.8487271979302469]
	TIME [epoch: 1.36 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.135582664259789		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 1.135582664259789 | validation: 0.9780943276036537]
	TIME [epoch: 1.36 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.155332716529555		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 1.155332716529555 | validation: 0.8438808815924042]
	TIME [epoch: 1.36 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1473109507602943		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 1.1473109507602943 | validation: 1.0091788320178883]
	TIME [epoch: 1.36 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1672976101305428		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 1.1672976101305428 | validation: 0.917121076804341]
	TIME [epoch: 1.36 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.16399068745519		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 1.16399068745519 | validation: 0.9207529143280381]
	TIME [epoch: 1.36 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.162563159029973		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 1.162563159029973 | validation: 0.9072109846818569]
	TIME [epoch: 1.36 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1443376433377597		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 1.1443376433377597 | validation: 0.8800711161056051]
	TIME [epoch: 1.36 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1321721299741694		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 1.1321721299741694 | validation: 0.7775954094721325]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_306.pth
	Model improved!!!
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1520583960295434		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 1.1520583960295434 | validation: 0.9953211167688493]
	TIME [epoch: 1.36 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.154172642270281		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 1.154172642270281 | validation: 0.8667365979618137]
	TIME [epoch: 1.36 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1645253928339603		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 1.1645253928339603 | validation: 1.0536452619823942]
	TIME [epoch: 1.36 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.213991050167908		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 1.213991050167908 | validation: 0.8764682085945886]
	TIME [epoch: 1.36 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.181521911660397		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 1.181521911660397 | validation: 1.0349867926583385]
	TIME [epoch: 1.36 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.16282852274553		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 1.16282852274553 | validation: 0.7538144177916218]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1460959043938586		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 1.1460959043938586 | validation: 0.9393575921990558]
	TIME [epoch: 1.36 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1337641776697303		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 1.1337641776697303 | validation: 0.8251535190485758]
	TIME [epoch: 1.36 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1170681562899711		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 1.1170681562899711 | validation: 0.9038937272573602]
	TIME [epoch: 1.36 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1143927297670002		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 1.1143927297670002 | validation: 0.8372704399865027]
	TIME [epoch: 1.36 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1193284070575458		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 1.1193284070575458 | validation: 0.8594299557365093]
	TIME [epoch: 1.36 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1181268565531384		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 1.1181268565531384 | validation: 0.9376232665325412]
	TIME [epoch: 1.36 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.131841181555684		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 1.131841181555684 | validation: 0.8043974482704388]
	TIME [epoch: 1.36 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.140563948560054		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 1.140563948560054 | validation: 0.8730198359639515]
	TIME [epoch: 1.36 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1327899838251412		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 1.1327899838251412 | validation: 1.0256314317068775]
	TIME [epoch: 1.36 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1738297527820294		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 1.1738297527820294 | validation: 0.8358543768119143]
	TIME [epoch: 1.36 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2313254723151392		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 1.2313254723151392 | validation: 1.230361115329391]
	TIME [epoch: 1.36 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2623675659561102		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 1.2623675659561102 | validation: 0.8356006485635764]
	TIME [epoch: 1.36 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1761106441757518		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 1.1761106441757518 | validation: 0.7985631897694598]
	TIME [epoch: 1.36 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1005257923318497		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 1.1005257923318497 | validation: 0.9422413546672399]
	TIME [epoch: 1.36 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1341728300454486		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 1.1341728300454486 | validation: 0.8348644366023544]
	TIME [epoch: 1.36 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.115602588231158		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 1.115602588231158 | validation: 0.8533630239951565]
	TIME [epoch: 1.37 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1017867340257517		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 1.1017867340257517 | validation: 0.8863965617050726]
	TIME [epoch: 1.36 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1041351874661376		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 1.1041351874661376 | validation: 0.793021237740382]
	TIME [epoch: 1.36 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1072101151585694		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 1.1072101151585694 | validation: 0.8590924745504787]
	TIME [epoch: 1.36 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1046867638274709		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 1.1046867638274709 | validation: 0.8513651926151873]
	TIME [epoch: 1.36 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.099449494051197		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 1.099449494051197 | validation: 0.8611048236496783]
	TIME [epoch: 1.36 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0870108018819375		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 1.0870108018819375 | validation: 0.7794488664214164]
	TIME [epoch: 1.36 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1031324076710254		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 1.1031324076710254 | validation: 0.9202180940230591]
	TIME [epoch: 1.36 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.110879509388322		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 1.110879509388322 | validation: 0.9215697592338788]
	TIME [epoch: 1.36 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.236991902573423		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 1.236991902573423 | validation: 1.1999512415991231]
	TIME [epoch: 1.36 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2365586142677156		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 1.2365586142677156 | validation: 0.7429159961193585]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_338.pth
	Model improved!!!
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.104480906685138		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 1.104480906685138 | validation: 0.8660875020478711]
	TIME [epoch: 1.36 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0974073567649432		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 1.0974073567649432 | validation: 0.8864078477243744]
	TIME [epoch: 1.36 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0979579677781524		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 1.0979579677781524 | validation: 0.7253299136278127]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_341.pth
	Model improved!!!
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1132187075430187		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 1.1132187075430187 | validation: 0.9339916834670685]
	TIME [epoch: 1.36 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0995471625323705		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 1.0995471625323705 | validation: 0.7590259717077377]
	TIME [epoch: 1.36 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1005896352748195		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 1.1005896352748195 | validation: 0.8343379616228046]
	TIME [epoch: 1.36 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0879257800277664		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 1.0879257800277664 | validation: 0.8044404397574543]
	TIME [epoch: 1.36 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0892834350397866		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 1.0892834350397866 | validation: 0.8362493415412081]
	TIME [epoch: 1.36 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0872403242958841		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 1.0872403242958841 | validation: 0.8196963284867765]
	TIME [epoch: 1.36 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0911722055863753		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 1.0911722055863753 | validation: 0.9277264288382829]
	TIME [epoch: 1.36 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1020799447413046		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 1.1020799447413046 | validation: 0.8617852623107534]
	TIME [epoch: 1.37 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1845666708916727		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 1.1845666708916727 | validation: 1.0190418059772925]
	TIME [epoch: 1.36 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1447165214402963		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 1.1447165214402963 | validation: 0.7905125272354047]
	TIME [epoch: 1.36 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.084586866609343		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 1.084586866609343 | validation: 0.7572948785485086]
	TIME [epoch: 1.36 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0706369884991047		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 1.0706369884991047 | validation: 0.8635718903041177]
	TIME [epoch: 1.36 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0770061695682858		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 1.0770061695682858 | validation: 0.7758882062371648]
	TIME [epoch: 1.36 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0626818115383112		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 1.0626818115383112 | validation: 0.8448998174133946]
	TIME [epoch: 1.36 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0749927718277668		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 1.0749927718277668 | validation: 0.7457022772665853]
	TIME [epoch: 1.36 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0961375616346698		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 1.0961375616346698 | validation: 0.9078598454269532]
	TIME [epoch: 1.36 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0852570132245585		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 1.0852570132245585 | validation: 0.7198582527696727]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_358.pth
	Model improved!!!
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0789819447269695		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 1.0789819447269695 | validation: 0.9608224765128561]
	TIME [epoch: 1.36 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.085038192152401		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 1.085038192152401 | validation: 0.76894782101524]
	TIME [epoch: 1.36 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1231109063720508		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 1.1231109063720508 | validation: 1.0502037973791762]
	TIME [epoch: 1.36 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1708174287102724		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 1.1708174287102724 | validation: 0.8953994035602222]
	TIME [epoch: 1.36 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0872112397785756		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 1.0872112397785756 | validation: 0.7422271751013702]
	TIME [epoch: 1.36 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0429637866232495		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 1.0429637866232495 | validation: 0.794732659015291]
	TIME [epoch: 1.36 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0387680440071523		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 1.0387680440071523 | validation: 0.7934781237138764]
	TIME [epoch: 1.36 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0332062686962242		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 1.0332062686962242 | validation: 0.7504232306415433]
	TIME [epoch: 1.36 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0177096395535943		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 1.0177096395535943 | validation: 0.7742954563544928]
	TIME [epoch: 1.36 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.038770283332329		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 1.038770283332329 | validation: 0.8371424308779631]
	TIME [epoch: 1.36 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0779133308247548		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 1.0779133308247548 | validation: 0.9900556842095671]
	TIME [epoch: 1.36 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1502522827279147		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 1.1502522827279147 | validation: 0.9022469176861106]
	TIME [epoch: 1.37 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0821174498767137		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 1.0821174498767137 | validation: 0.7769254189380117]
	TIME [epoch: 1.36 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0344330288674002		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 1.0344330288674002 | validation: 0.7387199814604024]
	TIME [epoch: 1.36 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0163359584018437		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 1.0163359584018437 | validation: 0.7832835189284618]
	TIME [epoch: 1.36 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0212723649033248		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 1.0212723649033248 | validation: 0.7751526319472116]
	TIME [epoch: 1.36 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0139976692681294		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 1.0139976692681294 | validation: 0.7167250045007437]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_375.pth
	Model improved!!!
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.006943565244758		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 1.006943565244758 | validation: 0.8764149338790042]
	TIME [epoch: 1.36 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0449562979767284		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 1.0449562979767284 | validation: 0.8199404996126994]
	TIME [epoch: 1.36 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0624633484043366		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 1.0624633484043366 | validation: 0.8017508367085798]
	TIME [epoch: 1.36 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0561433798231434		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 1.0561433798231434 | validation: 0.7900478364132927]
	TIME [epoch: 1.36 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0046839879378089		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 1.0046839879378089 | validation: 0.649543926831319]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_380.pth
	Model improved!!!
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0221590521473514		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 1.0221590521473514 | validation: 0.8259292884517775]
	TIME [epoch: 1.36 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0260795797346192		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 1.0260795797346192 | validation: 0.7025348718374926]
	TIME [epoch: 1.36 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0088646832542754		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 1.0088646832542754 | validation: 0.823575758863316]
	TIME [epoch: 1.36 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0121161314918994		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 1.0121161314918994 | validation: 0.6646775515035049]
	TIME [epoch: 1.36 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9997198173537782		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.9997198173537782 | validation: 0.7849423597669514]
	TIME [epoch: 1.36 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9722721045360347		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.9722721045360347 | validation: 0.7068909538176379]
	TIME [epoch: 1.36 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9840558864910915		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.9840558864910915 | validation: 0.838674140122231]
	TIME [epoch: 1.36 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0256669607604183		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 1.0256669607604183 | validation: 0.7306647883234656]
	TIME [epoch: 1.36 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9830070716743236		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.9830070716743236 | validation: 0.7652145407843944]
	TIME [epoch: 1.36 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9974779162939099		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.9974779162939099 | validation: 0.8339953488647645]
	TIME [epoch: 1.36 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0434705443446968		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 1.0434705443446968 | validation: 0.7610240591161924]
	TIME [epoch: 1.36 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.021680583843592		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 1.021680583843592 | validation: 0.721377990421066]
	TIME [epoch: 1.36 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9600514644587304		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.9600514644587304 | validation: 0.6550589580395798]
	TIME [epoch: 1.36 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9567035833393932		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.9567035833393932 | validation: 0.7326953577016607]
	TIME [epoch: 1.36 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9503925049164761		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.9503925049164761 | validation: 0.6566504615034298]
	TIME [epoch: 1.36 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.969619088760707		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.969619088760707 | validation: 0.8072988898035742]
	TIME [epoch: 1.36 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9878172969136018		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.9878172969136018 | validation: 0.7313322173047955]
	TIME [epoch: 1.36 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0009127305434615		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 1.0009127305434615 | validation: 0.7645940737224417]
	TIME [epoch: 1.36 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9947215102879995		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.9947215102879995 | validation: 0.7926307719210656]
	TIME [epoch: 1.36 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.959339833124714		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.959339833124714 | validation: 0.5766440326531845]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_400.pth
	Model improved!!!
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9453537536853912		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.9453537536853912 | validation: 0.7044007687409538]
	TIME [epoch: 1.36 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9223674754658514		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.9223674754658514 | validation: 0.6466176910380216]
	TIME [epoch: 1.36 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.899725089930207		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.899725089930207 | validation: 0.7011105793177694]
	TIME [epoch: 1.36 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9590120043366528		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.9590120043366528 | validation: 0.7499959303428176]
	TIME [epoch: 1.36 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9697669431630943		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.9697669431630943 | validation: 0.6490197905215997]
	TIME [epoch: 1.36 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9114799829875497		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.9114799829875497 | validation: 0.6062515153920447]
	TIME [epoch: 1.37 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8553481709965288		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.8553481709965288 | validation: 0.6133066651829647]
	TIME [epoch: 1.37 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8362653360977969		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.8362653360977969 | validation: 0.583431330226931]
	TIME [epoch: 1.36 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8131358799028925		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.8131358799028925 | validation: 0.6858012498979229]
	TIME [epoch: 1.36 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8340806259624143		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.8340806259624143 | validation: 0.6072823208448441]
	TIME [epoch: 1.36 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7981210178602219		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.7981210178602219 | validation: 0.6071973646585676]
	TIME [epoch: 1.36 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8788727293463343		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.8788727293463343 | validation: 0.7294742269417603]
	TIME [epoch: 1.37 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9539424194098691		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.9539424194098691 | validation: 0.6079867311647503]
	TIME [epoch: 1.36 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7683074707233137		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.7683074707233137 | validation: 0.5123563938462601]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_414.pth
	Model improved!!!
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8484009942972784		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.8484009942972784 | validation: 0.6047245180143903]
	TIME [epoch: 1.36 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7706762961190173		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.7706762961190173 | validation: 0.5361829550762982]
	TIME [epoch: 1.36 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6986961423443111		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.6986961423443111 | validation: 0.5648949162036049]
	TIME [epoch: 1.36 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.762036797494472		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.762036797494472 | validation: 0.4865981845969479]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_418.pth
	Model improved!!!
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6936690730124235		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.6936690730124235 | validation: 0.530639673545721]
	TIME [epoch: 1.36 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.708275929662188		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.708275929662188 | validation: 0.49245845772694513]
	TIME [epoch: 1.36 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6884558554693808		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.6884558554693808 | validation: 0.47510145087674804]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_421.pth
	Model improved!!!
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6694547039213778		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.6694547039213778 | validation: 0.49673191840486136]
	TIME [epoch: 1.36 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.713503618885041		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.713503618885041 | validation: 0.5600913947697056]
	TIME [epoch: 1.36 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7116709702893133		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.7116709702893133 | validation: 0.6065053542791043]
	TIME [epoch: 1.36 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8675607560250371		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.8675607560250371 | validation: 0.47156871119781274]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_425.pth
	Model improved!!!
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6786234453763885		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.6786234453763885 | validation: 0.4429845673051358]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_426.pth
	Model improved!!!
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6796688411621198		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.6796688411621198 | validation: 0.41330701143460413]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_427.pth
	Model improved!!!
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6425547478934166		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.6425547478934166 | validation: 0.4682147916709779]
	TIME [epoch: 1.36 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6327201103148931		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.6327201103148931 | validation: 0.4292266628824646]
	TIME [epoch: 1.36 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.623441876534586		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.623441876534586 | validation: 0.4425865680123849]
	TIME [epoch: 1.36 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6451744635025952		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.6451744635025952 | validation: 0.6470761254003414]
	TIME [epoch: 1.36 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8828890140332917		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.8828890140332917 | validation: 0.5392618991538198]
	TIME [epoch: 1.37 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6782020113492838		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.6782020113492838 | validation: 0.4335083501925054]
	TIME [epoch: 1.36 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6531018817993547		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.6531018817993547 | validation: 0.3532837866442315]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_434.pth
	Model improved!!!
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6061390124863951		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.6061390124863951 | validation: 0.44668784210797413]
	TIME [epoch: 1.36 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5852094497401704		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.5852094497401704 | validation: 0.4300058534762199]
	TIME [epoch: 1.36 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5864149093922488		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.5864149093922488 | validation: 0.593581727731098]
	TIME [epoch: 1.36 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6816467861211181		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.6816467861211181 | validation: 0.5198760317077674]
	TIME [epoch: 1.36 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6636003902398349		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.6636003902398349 | validation: 0.4695458153291838]
	TIME [epoch: 1.36 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6387880866185119		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.6387880866185119 | validation: 0.4806744207894609]
	TIME [epoch: 1.36 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6038732514330015		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.6038732514330015 | validation: 0.3922848080431053]
	TIME [epoch: 1.36 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6097504838466873		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.6097504838466873 | validation: 0.48643424716949024]
	TIME [epoch: 1.36 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.638896554126729		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.638896554126729 | validation: 0.38256992406063156]
	TIME [epoch: 1.36 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6358807447121622		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.6358807447121622 | validation: 0.4384149903649557]
	TIME [epoch: 1.36 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5752957399815086		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.5752957399815086 | validation: 0.36992716051450764]
	TIME [epoch: 1.36 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5542291872143482		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.5542291872143482 | validation: 0.38734284932676916]
	TIME [epoch: 1.36 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5503249495635295		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.5503249495635295 | validation: 0.3658622656540673]
	TIME [epoch: 1.36 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5356854162732085		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.5356854162732085 | validation: 0.8486061868680252]
	TIME [epoch: 1.36 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0702268723594406		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 1.0702268723594406 | validation: 0.6993004147496804]
	TIME [epoch: 1.36 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8974088660001303		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.8974088660001303 | validation: 0.5531003552141458]
	TIME [epoch: 1.36 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7143516640574488		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.7143516640574488 | validation: 0.31460423997893383]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_451.pth
	Model improved!!!
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6130693076488057		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.6130693076488057 | validation: 0.38640437065352473]
	TIME [epoch: 1.37 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5420442352530075		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.5420442352530075 | validation: 0.41770976020159445]
	TIME [epoch: 1.37 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5578148624016062		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.5578148624016062 | validation: 0.40851232188238173]
	TIME [epoch: 1.37 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5278297567667375		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.5278297567667375 | validation: 0.32161806367769613]
	TIME [epoch: 1.37 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5375210499842935		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.5375210499842935 | validation: 0.37502678972438125]
	TIME [epoch: 1.36 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5398662016814888		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.5398662016814888 | validation: 0.46002198432360397]
	TIME [epoch: 1.37 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5328243966324373		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.5328243966324373 | validation: 0.32468638561015545]
	TIME [epoch: 1.37 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.522572544518306		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.522572544518306 | validation: 0.3995680183336624]
	TIME [epoch: 1.36 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5247811049302735		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.5247811049302735 | validation: 0.34569189424313607]
	TIME [epoch: 1.36 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5100549575553794		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.5100549575553794 | validation: 0.3387628525136714]
	TIME [epoch: 1.36 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49293218189330334		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.49293218189330334 | validation: 0.3977664630052902]
	TIME [epoch: 1.36 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5901468270832387		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.5901468270832387 | validation: 0.5607359879935542]
	TIME [epoch: 1.36 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6803291689573621		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.6803291689573621 | validation: 0.34895100146957647]
	TIME [epoch: 1.36 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5929727793331317		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.5929727793331317 | validation: 0.39109825432700185]
	TIME [epoch: 1.36 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.514759007307264		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.514759007307264 | validation: 0.34305847383067334]
	TIME [epoch: 1.36 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5049095554898946		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.5049095554898946 | validation: 0.3639200533233809]
	TIME [epoch: 1.36 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4803576452965794		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.4803576452965794 | validation: 0.33682137433268783]
	TIME [epoch: 1.36 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4795682343290541		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.4795682343290541 | validation: 0.31886616378570337]
	TIME [epoch: 1.36 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4896457914355887		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.4896457914355887 | validation: 0.4718884317812238]
	TIME [epoch: 1.36 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5422930435870394		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.5422930435870394 | validation: 0.31458958595466235]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_471.pth
	Model improved!!!
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5128709043953568		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.5128709043953568 | validation: 0.40127901149820855]
	TIME [epoch: 1.36 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5097268692917788		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.5097268692917788 | validation: 0.33615408981302536]
	TIME [epoch: 1.36 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5274019289331107		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.5274019289331107 | validation: 0.4156709762143007]
	TIME [epoch: 1.37 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5176959182288081		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.5176959182288081 | validation: 0.3160143499706374]
	TIME [epoch: 1.36 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.505918351181038		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.505918351181038 | validation: 0.36149191646162404]
	TIME [epoch: 1.36 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4711655844791723		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.4711655844791723 | validation: 0.2907433123207337]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_477.pth
	Model improved!!!
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43799952140620235		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.43799952140620235 | validation: 0.8744643300105666]
	TIME [epoch: 1.36 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1668331561097918		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 1.1668331561097918 | validation: 0.9362543191268493]
	TIME [epoch: 1.36 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0554859780353847		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 1.0554859780353847 | validation: 0.9318680882669697]
	TIME [epoch: 1.36 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0091025752112859		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 1.0091025752112859 | validation: 0.5832852966190345]
	TIME [epoch: 1.36 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8147519870355459		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.8147519870355459 | validation: 0.3772308081410187]
	TIME [epoch: 1.36 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5624091069356054		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.5624091069356054 | validation: 0.3002979387441847]
	TIME [epoch: 1.36 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.470892068949061		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.470892068949061 | validation: 0.3340224393996007]
	TIME [epoch: 1.36 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47651090577647837		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.47651090577647837 | validation: 0.36280335657117685]
	TIME [epoch: 1.36 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46966801483140574		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.46966801483140574 | validation: 0.30229204361075745]
	TIME [epoch: 1.37 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4533833973591249		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.4533833973591249 | validation: 0.2800084792227038]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_487.pth
	Model improved!!!
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45984612767477046		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.45984612767477046 | validation: 0.29242964837354485]
	TIME [epoch: 1.36 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4396026905378217		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.4396026905378217 | validation: 0.30118054174910086]
	TIME [epoch: 1.36 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43179777320592466		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.43179777320592466 | validation: 0.29617484608952094]
	TIME [epoch: 1.36 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4309103788490498		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.4309103788490498 | validation: 0.26770087761596945]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_491.pth
	Model improved!!!
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4413214913211425		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.4413214913211425 | validation: 0.4002501233273578]
	TIME [epoch: 1.36 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4845961139891966		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.4845961139891966 | validation: 0.5758770671113342]
	TIME [epoch: 1.36 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9236017350051018		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.9236017350051018 | validation: 0.39078820848950746]
	TIME [epoch: 1.37 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5250344712573307		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.5250344712573307 | validation: 0.5928253275985143]
	TIME [epoch: 1.36 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.626000323352293		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.626000323352293 | validation: 0.2762422845817862]
	TIME [epoch: 1.36 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4981475068631241		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.4981475068631241 | validation: 0.3104241138999919]
	TIME [epoch: 1.36 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49306331551216714		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.49306331551216714 | validation: 0.38445481280475824]
	TIME [epoch: 1.36 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4599192285953286		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.4599192285953286 | validation: 0.2857182634232703]
	TIME [epoch: 1.36 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42162246852044843		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.42162246852044843 | validation: 0.27808283812893286]
	TIME [epoch: 1.36 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42885582607651557		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.42885582607651557 | validation: 0.3272342591018823]
	TIME [epoch: 181 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.439066450809125		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.439066450809125 | validation: 0.2917039446593018]
	TIME [epoch: 2.72 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4301068470848482		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.4301068470848482 | validation: 0.3243680751871567]
	TIME [epoch: 2.71 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4359585185211726		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.4359585185211726 | validation: 0.3717011383662113]
	TIME [epoch: 2.71 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5802117436658939		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.5802117436658939 | validation: 0.3553211552444704]
	TIME [epoch: 2.71 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4510649649495241		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.4510649649495241 | validation: 0.33931855422002166]
	TIME [epoch: 2.71 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43137980400262993		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.43137980400262993 | validation: 0.2553068959710969]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_507.pth
	Model improved!!!
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42737320416399543		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.42737320416399543 | validation: 0.3008449676770126]
	TIME [epoch: 2.71 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41560050495521905		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.41560050495521905 | validation: 0.30865268413183694]
	TIME [epoch: 2.72 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4159453328397564		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.4159453328397564 | validation: 0.33584690310094]
	TIME [epoch: 2.71 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4183006456876227		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.4183006456876227 | validation: 0.26161525973200755]
	TIME [epoch: 2.71 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4221625507753868		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.4221625507753868 | validation: 0.3304977498760377]
	TIME [epoch: 2.71 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41431543536805704		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.41431543536805704 | validation: 0.27881428740484865]
	TIME [epoch: 2.71 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4123955519125736		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.4123955519125736 | validation: 0.3450310005021523]
	TIME [epoch: 2.71 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40130912097369575		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.40130912097369575 | validation: 0.2515097524104339]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_515.pth
	Model improved!!!
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.408091458025923		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.408091458025923 | validation: 0.3482288808929923]
	TIME [epoch: 2.71 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4194062212740046		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.4194062212740046 | validation: 0.5347544964549695]
	TIME [epoch: 2.71 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8289179849373447		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.8289179849373447 | validation: 0.4834259613493864]
	TIME [epoch: 2.71 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6917961103305638		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.6917961103305638 | validation: 0.4134525798097155]
	TIME [epoch: 2.72 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5189957720452618		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.5189957720452618 | validation: 0.27531567487896674]
	TIME [epoch: 2.71 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42419614208594497		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.42419614208594497 | validation: 0.2311697160823552]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_521.pth
	Model improved!!!
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3840601254109426		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.3840601254109426 | validation: 0.2994510599869228]
	TIME [epoch: 2.71 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3995910345119548		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.3995910345119548 | validation: 0.26587720581913477]
	TIME [epoch: 2.71 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4109840033897549		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.4109840033897549 | validation: 0.31762998802326187]
	TIME [epoch: 2.71 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4182214010209221		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.4182214010209221 | validation: 0.2641385825966071]
	TIME [epoch: 2.71 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40284068321954103		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.40284068321954103 | validation: 0.31946494897169875]
	TIME [epoch: 2.71 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.404677326106266		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.404677326106266 | validation: 0.27062363497192704]
	TIME [epoch: 2.71 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4236353595766192		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.4236353595766192 | validation: 0.3227211668585746]
	TIME [epoch: 2.7 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.376982008255585		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.376982008255585 | validation: 0.2471521577890116]
	TIME [epoch: 2.71 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3648683015282649		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.3648683015282649 | validation: 0.24342629281288236]
	TIME [epoch: 2.71 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3602873542992262		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.3602873542992262 | validation: 0.255425674125096]
	TIME [epoch: 2.7 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3612786360351343		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.3612786360351343 | validation: 0.2426422174910865]
	TIME [epoch: 2.69 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34906507482620497		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.34906507482620497 | validation: 0.25991358629020533]
	TIME [epoch: 2.69 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.353361904786376		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.353361904786376 | validation: 0.23342220091770527]
	TIME [epoch: 2.69 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3514140990073328		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.3514140990073328 | validation: 0.298032066112871]
	TIME [epoch: 2.7 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3884399516697113		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.3884399516697113 | validation: 0.2810387725845489]
	TIME [epoch: 2.69 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4613052911402401		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.4613052911402401 | validation: 0.4491436548063217]
	TIME [epoch: 2.69 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.511315460921744		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.511315460921744 | validation: 0.248654649240426]
	TIME [epoch: 2.69 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3696649243737312		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.3696649243737312 | validation: 0.23305692763711283]
	TIME [epoch: 2.69 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3411929781532392		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.3411929781532392 | validation: 0.25661859613332]
	TIME [epoch: 2.69 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34425694868114703		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.34425694868114703 | validation: 0.23515452151394936]
	TIME [epoch: 2.7 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3508263783612767		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.3508263783612767 | validation: 0.29375820006990766]
	TIME [epoch: 2.69 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3518576583440892		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.3518576583440892 | validation: 0.24459198106322538]
	TIME [epoch: 2.69 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3560485932812712		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.3560485932812712 | validation: 0.2920681843722844]
	TIME [epoch: 2.69 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36129356001904384		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.36129356001904384 | validation: 0.23971431221503128]
	TIME [epoch: 2.69 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35155378726700975		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.35155378726700975 | validation: 0.2961091183820223]
	TIME [epoch: 2.69 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3650915504547207		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.3650915504547207 | validation: 0.23745254832603493]
	TIME [epoch: 2.69 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34694358040922213		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.34694358040922213 | validation: 0.2868038974162209]
	TIME [epoch: 2.69 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38379884763327843		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.38379884763327843 | validation: 0.21613822318030929]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_549.pth
	Model improved!!!
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36608425843399983		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.36608425843399983 | validation: 0.28545330847476463]
	TIME [epoch: 2.69 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34974521175893686		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.34974521175893686 | validation: 0.23025984133729002]
	TIME [epoch: 2.69 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.351167315976221		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.351167315976221 | validation: 0.3047476285098248]
	TIME [epoch: 2.7 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35039374134313006		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.35039374134313006 | validation: 0.23308059367107817]
	TIME [epoch: 2.69 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32948483853611765		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.32948483853611765 | validation: 0.2769967540973843]
	TIME [epoch: 2.69 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.323843735647901		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.323843735647901 | validation: 0.2208478538539536]
	TIME [epoch: 2.69 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31746324639331264		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.31746324639331264 | validation: 0.2468322158199806]
	TIME [epoch: 2.69 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3340439224044268		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.3340439224044268 | validation: 0.24321729266134043]
	TIME [epoch: 2.69 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39836665257679765		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.39836665257679765 | validation: 0.3299916184551835]
	TIME [epoch: 2.69 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38733645980290277		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.38733645980290277 | validation: 0.19676587254621045]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_559.pth
	Model improved!!!
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3321323185449728		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.3321323185449728 | validation: 0.258554560628567]
	TIME [epoch: 2.69 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3173952074825128		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.3173952074825128 | validation: 0.23260566824034168]
	TIME [epoch: 2.69 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3109977807097535		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.3109977807097535 | validation: 0.22730885454453037]
	TIME [epoch: 2.69 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3168081349295684		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.3168081349295684 | validation: 0.25159039961799623]
	TIME [epoch: 2.7 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3040823692229431		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.3040823692229431 | validation: 0.204874606993936]
	TIME [epoch: 2.69 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3000383857832135		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.3000383857832135 | validation: 0.3010140358831532]
	TIME [epoch: 2.69 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43984509361250534		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.43984509361250534 | validation: 0.41265209183847285]
	TIME [epoch: 2.69 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.592623561438259		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.592623561438259 | validation: 0.4030732341270329]
	TIME [epoch: 2.69 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43057151090880313		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.43057151090880313 | validation: 0.2534600161106836]
	TIME [epoch: 2.69 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35854833896956534		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.35854833896956534 | validation: 0.228672842486052]
	TIME [epoch: 2.69 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29696986993689933		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.29696986993689933 | validation: 0.24892899526783846]
	TIME [epoch: 2.69 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30088996742150936		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.30088996742150936 | validation: 0.20290540389803935]
	TIME [epoch: 2.69 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3055875993858157		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.3055875993858157 | validation: 0.23975822926880658]
	TIME [epoch: 2.69 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3559295220992854		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.3559295220992854 | validation: 0.23173383870383646]
	TIME [epoch: 2.69 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36634169983363307		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.36634169983363307 | validation: 0.30430766217959593]
	TIME [epoch: 2.7 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35792579725311124		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.35792579725311124 | validation: 0.20438959552609576]
	TIME [epoch: 2.69 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3208067015846167		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.3208067015846167 | validation: 0.22439672219664983]
	TIME [epoch: 2.69 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2832262002304753		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.2832262002304753 | validation: 0.2002202527175069]
	TIME [epoch: 2.69 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2863998367065713		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.2863998367065713 | validation: 0.2632195846285006]
	TIME [epoch: 2.69 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3008003618343465		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.3008003618343465 | validation: 0.19894205044520177]
	TIME [epoch: 2.69 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2960737651376131		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.2960737651376131 | validation: 0.23833038553320743]
	TIME [epoch: 2.69 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29149626644344145		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.29149626644344145 | validation: 0.1957385045749064]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_581.pth
	Model improved!!!
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28451742331941376		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.28451742331941376 | validation: 0.2336643298957907]
	TIME [epoch: 2.69 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28957198351614005		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.28957198351614005 | validation: 0.19653317660405223]
	TIME [epoch: 2.69 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2905592353059956		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.2905592353059956 | validation: 0.2569982515842472]
	TIME [epoch: 2.69 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3017103844608336		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.3017103844608336 | validation: 0.2275244242510268]
	TIME [epoch: 2.7 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34924415488675054		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.34924415488675054 | validation: 0.29895034838069867]
	TIME [epoch: 2.69 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31295408351924237		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.31295408351924237 | validation: 0.219192359179113]
	TIME [epoch: 2.69 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.366789895587375		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.366789895587375 | validation: 0.23589995912313497]
	TIME [epoch: 2.69 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28376901336624377		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.28376901336624377 | validation: 0.22318487997456568]
	TIME [epoch: 2.69 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27935152495859605		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.27935152495859605 | validation: 0.18654670171555518]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_590.pth
	Model improved!!!
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27021390270866613		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.27021390270866613 | validation: 0.2290129246452624]
	TIME [epoch: 2.69 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27022556592292357		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.27022556592292357 | validation: 0.17961847759469118]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_592.pth
	Model improved!!!
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26999932220112627		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.26999932220112627 | validation: 0.24266997225008718]
	TIME [epoch: 2.69 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28480985923401436		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.28480985923401436 | validation: 0.22400201192814398]
	TIME [epoch: 2.69 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3111315102317539		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.3111315102317539 | validation: 0.29258748117369254]
	TIME [epoch: 2.7 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3257440796641329		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.3257440796641329 | validation: 0.18097117082055256]
	TIME [epoch: 2.69 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2833536566050534		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.2833536566050534 | validation: 0.23490224349045002]
	TIME [epoch: 2.69 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2639790443982274		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.2639790443982274 | validation: 0.18863106251991457]
	TIME [epoch: 2.69 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2595226019097747		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.2595226019097747 | validation: 0.6909283847839685]
	TIME [epoch: 2.69 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0479564488286586		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 1.0479564488286586 | validation: 0.7717107352745063]
	TIME [epoch: 2.69 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0609100616398972		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 1.0609100616398972 | validation: 0.6676064556203573]
	TIME [epoch: 2.69 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8314104942362818		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.8314104942362818 | validation: 0.6262890843803588]
	TIME [epoch: 2.69 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7786121512252888		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.7786121512252888 | validation: 0.5476047017282435]
	TIME [epoch: 2.68 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.725921981511434		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.725921981511434 | validation: 0.4235710133976285]
	TIME [epoch: 2.69 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6402196797298468		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.6402196797298468 | validation: 0.2828038046335607]
	TIME [epoch: 2.69 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45172056901424923		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.45172056901424923 | validation: 0.26780005472225543]
	TIME [epoch: 2.7 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3285192875553991		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.3285192875553991 | validation: 0.2014232598515359]
	TIME [epoch: 2.69 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2754493463863891		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.2754493463863891 | validation: 0.20715733045015816]
	TIME [epoch: 2.69 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25175345094850116		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.25175345094850116 | validation: 0.1761660623442348]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_609.pth
	Model improved!!!
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2534676965892798		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.2534676965892798 | validation: 0.17396300874954562]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_610.pth
	Model improved!!!
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25752334882011535		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.25752334882011535 | validation: 0.17414915877469558]
	TIME [epoch: 2.69 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25839053496030084		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.25839053496030084 | validation: 0.1712718671464841]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_612.pth
	Model improved!!!
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2535261242342955		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.2535261242342955 | validation: 0.18801005781871005]
	TIME [epoch: 2.68 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24833074333493485		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.24833074333493485 | validation: 0.17955064984937896]
	TIME [epoch: 2.68 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2469275159147133		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.2469275159147133 | validation: 0.21338784434011127]
	TIME [epoch: 2.68 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.253293036011356		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.253293036011356 | validation: 0.1770337541644294]
	TIME [epoch: 2.68 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2548238934621105		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.2548238934621105 | validation: 0.21342327699185892]
	TIME [epoch: 2.7 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2534447307212127		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.2534447307212127 | validation: 0.2005900971417697]
	TIME [epoch: 2.68 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31193893180971616		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.31193893180971616 | validation: 0.25213868143139023]
	TIME [epoch: 2.69 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26633290522029546		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.26633290522029546 | validation: 0.189697942851571]
	TIME [epoch: 2.69 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24495686084867604		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.24495686084867604 | validation: 0.17689138922850667]
	TIME [epoch: 2.69 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2699992627052223		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.2699992627052223 | validation: 0.2527364627276069]
	TIME [epoch: 2.69 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2691650278890851		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.2691650278890851 | validation: 0.18387691228208802]
	TIME [epoch: 2.69 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24980752090959285		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.24980752090959285 | validation: 0.17787576829890284]
	TIME [epoch: 2.69 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2393837414992094		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.2393837414992094 | validation: 0.23535314322684578]
	TIME [epoch: 2.69 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3510192919947069		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.3510192919947069 | validation: 0.24455479310671252]
	TIME [epoch: 2.69 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27692158586615434		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.27692158586615434 | validation: 0.2045297163384544]
	TIME [epoch: 2.69 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2632697841124271		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.2632697841124271 | validation: 0.17973250173849542]
	TIME [epoch: 2.7 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24888708918792518		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.24888708918792518 | validation: 0.2002598495455472]
	TIME [epoch: 2.69 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23326894033262283		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.23326894033262283 | validation: 0.2781671925177024]
	TIME [epoch: 2.69 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4042204026218469		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.4042204026218469 | validation: 0.131579742713677]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_631.pth
	Model improved!!!
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3443725426385953		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.3443725426385953 | validation: 0.17660051852315523]
	TIME [epoch: 2.68 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30593500572761584		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.30593500572761584 | validation: 0.23359173846984002]
	TIME [epoch: 2.68 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3066191019615218		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.3066191019615218 | validation: 0.23286038264111042]
	TIME [epoch: 2.68 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25221251389121496		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.25221251389121496 | validation: 0.1888489336995648]
	TIME [epoch: 2.68 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24446972196902272		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.24446972196902272 | validation: 0.1689407714670631]
	TIME [epoch: 2.68 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23653223226351794		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.23653223226351794 | validation: 0.1755296679475]
	TIME [epoch: 2.68 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.228510376650772		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.228510376650772 | validation: 0.17969106158286335]
	TIME [epoch: 2.69 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22718964664136182		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.22718964664136182 | validation: 0.17209829306510752]
	TIME [epoch: 2.68 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22926662689993585		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.22926662689993585 | validation: 0.1869261709140146]
	TIME [epoch: 2.69 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2344358081127011		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.2344358081127011 | validation: 0.16768571066183013]
	TIME [epoch: 2.69 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23323088308254408		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.23323088308254408 | validation: 0.20970783624377118]
	TIME [epoch: 2.69 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23734327361924756		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.23734327361924756 | validation: 0.18467006894141094]
	TIME [epoch: 2.69 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2519731264454772		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.2519731264454772 | validation: 0.24636767586181219]
	TIME [epoch: 2.69 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25203447988375904		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.25203447988375904 | validation: 0.17017348623199086]
	TIME [epoch: 2.69 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22914949599369755		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.22914949599369755 | validation: 0.18642947992398717]
	TIME [epoch: 2.69 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2261814104805632		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.2261814104805632 | validation: 0.17117839970193627]
	TIME [epoch: 2.69 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22407380428680534		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.22407380428680534 | validation: 0.19364217974790543]
	TIME [epoch: 2.69 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23038690786155738		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.23038690786155738 | validation: 0.16117140325475535]
	TIME [epoch: 2.69 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22815886106889213		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.22815886106889213 | validation: 0.21374826760713594]
	TIME [epoch: 2.69 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24234994799584414		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.24234994799584414 | validation: 0.1837833217598832]
	TIME [epoch: 2.69 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25655133305096334		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.25655133305096334 | validation: 0.25383817275266657]
	TIME [epoch: 2.69 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2531363358555537		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.2531363358555537 | validation: 0.17204537031239625]
	TIME [epoch: 2.69 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23567714651611305		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.23567714651611305 | validation: 0.1854470575407245]
	TIME [epoch: 2.69 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21986127737497074		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.21986127737497074 | validation: 0.18290603649492856]
	TIME [epoch: 2.69 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22350830913145928		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.22350830913145928 | validation: 0.1539074061267651]
	TIME [epoch: 2.69 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2255155027094723		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.2255155027094723 | validation: 0.21598352008889174]
	TIME [epoch: 2.69 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2593048979025903		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.2593048979025903 | validation: 0.1563556619364426]
	TIME [epoch: 2.69 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2451688284268152		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.2451688284268152 | validation: 0.19212126248875977]
	TIME [epoch: 2.69 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24742732946294144		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.24742732946294144 | validation: 0.23322890764801488]
	TIME [epoch: 2.69 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3507207583344654		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.3507207583344654 | validation: 0.21721067077842823]
	TIME [epoch: 2.69 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24344722662535276		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.24344722662535276 | validation: 0.22327199978285553]
	TIME [epoch: 2.69 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25138356835492137		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.25138356835492137 | validation: 0.15039858432238196]
	TIME [epoch: 2.69 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22286462278940528		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.22286462278940528 | validation: 0.17591370100913967]
	TIME [epoch: 2.69 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21787554643336746		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.21787554643336746 | validation: 0.17463119997012128]
	TIME [epoch: 2.69 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22683333856664964		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.22683333856664964 | validation: 0.1462778043901039]
	TIME [epoch: 2.69 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22107139247447152		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.22107139247447152 | validation: 0.17043608593670523]
	TIME [epoch: 2.69 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21533517967886773		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.21533517967886773 | validation: 0.1874187179841738]
	TIME [epoch: 2.69 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22123253406704152		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.22123253406704152 | validation: 0.14755585803493912]
	TIME [epoch: 2.69 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21410573146026335		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.21410573146026335 | validation: 0.4222924118119325]
	TIME [epoch: 2.69 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6620502756902306		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.6620502756902306 | validation: 0.24610477463024305]
	TIME [epoch: 2.69 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4144236086209696		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.4144236086209696 | validation: 0.361045344708867]
	TIME [epoch: 2.7 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3391889660024032		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.3391889660024032 | validation: 0.22936473971533664]
	TIME [epoch: 2.69 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2559703563793297		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.2559703563793297 | validation: 0.1690571091709097]
	TIME [epoch: 2.69 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27149138171357867		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.27149138171357867 | validation: 0.20331207340532098]
	TIME [epoch: 2.69 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2305077663394961		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.2305077663394961 | validation: 0.19354584054689536]
	TIME [epoch: 2.69 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21905868679225624		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.21905868679225624 | validation: 0.1660866549279296]
	TIME [epoch: 2.69 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21135258257981565		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.21135258257981565 | validation: 0.15801138063104803]
	TIME [epoch: 2.69 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21139077773792697		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.21139077773792697 | validation: 0.1470158517635067]
	TIME [epoch: 2.69 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21048601892667385		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.21048601892667385 | validation: 0.16040173061371213]
	TIME [epoch: 2.69 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21213445699202774		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.21213445699202774 | validation: 0.1566010516263571]
	TIME [epoch: 2.69 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21057747710942523		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.21057747710942523 | validation: 0.17967385134143882]
	TIME [epoch: 2.69 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20613360543548637		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.20613360543548637 | validation: 0.166575536383138]
	TIME [epoch: 2.69 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2055191557671446		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.2055191557671446 | validation: 0.17699640835417682]
	TIME [epoch: 2.69 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21010709029515207		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.21010709029515207 | validation: 0.15243219871771252]
	TIME [epoch: 2.69 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21362229004130803		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.21362229004130803 | validation: 0.19228981026932143]
	TIME [epoch: 2.69 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23065721010564733		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.23065721010564733 | validation: 0.15210506219576247]
	TIME [epoch: 2.69 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2179703960668887		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.2179703960668887 | validation: 0.1767105056241145]
	TIME [epoch: 2.69 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20739966096752527		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.20739966096752527 | validation: 0.14803029754676253]
	TIME [epoch: 2.69 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20410677525345935		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.20410677525345935 | validation: 0.16278792065916228]
	TIME [epoch: 2.69 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20282261755845368		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.20282261755845368 | validation: 0.14510551160207083]
	TIME [epoch: 2.69 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20073997864940935		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.20073997864940935 | validation: 0.16587294893988502]
	TIME [epoch: 2.69 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2006394552188197		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.2006394552188197 | validation: 0.15249841622740076]
	TIME [epoch: 2.69 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2084791123964133		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.2084791123964133 | validation: 0.22810467613175636]
	TIME [epoch: 2.7 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2420029471293413		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.2420029471293413 | validation: 0.16537491896166398]
	TIME [epoch: 2.69 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2379614826650598		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.2379614826650598 | validation: 0.19582401033285335]
	TIME [epoch: 2.69 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21484044364099802		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.21484044364099802 | validation: 0.15144240770814552]
	TIME [epoch: 2.69 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19974456240230054		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.19974456240230054 | validation: 0.14958402297371]
	TIME [epoch: 2.69 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19532435567634296		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.19532435567634296 | validation: 0.14734156391030412]
	TIME [epoch: 2.69 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19457066103686313		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.19457066103686313 | validation: 0.14250814479407553]
	TIME [epoch: 2.69 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19170735276176373		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.19170735276176373 | validation: 0.1563181604645822]
	TIME [epoch: 2.67 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19790618415082045		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.19790618415082045 | validation: 0.16325911872596086]
	TIME [epoch: 2.67 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19182515788705692		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.19182515788705692 | validation: 0.1603948928990482]
	TIME [epoch: 2.67 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2134073146387724		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.2134073146387724 | validation: 0.22367063193763315]
	TIME [epoch: 2.68 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23468130526849226		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.23468130526849226 | validation: 0.16255134010666203]
	TIME [epoch: 2.68 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22601746196196928		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.22601746196196928 | validation: 0.1916159315085322]
	TIME [epoch: 2.67 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2074762978064647		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.2074762978064647 | validation: 0.15785643990999723]
	TIME [epoch: 2.67 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19520282443088793		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.19520282443088793 | validation: 0.15247857728247227]
	TIME [epoch: 2.67 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18720929426932104		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.18720929426932104 | validation: 0.14433526009054262]
	TIME [epoch: 2.68 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1905340705683219		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.1905340705683219 | validation: 0.17033323259534383]
	TIME [epoch: 2.69 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19074639325528606		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.19074639325528606 | validation: 0.14413738383246819]
	TIME [epoch: 2.69 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19506080759486055		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.19506080759486055 | validation: 0.188788239310987]
	TIME [epoch: 2.68 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21411372340628254		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.21411372340628254 | validation: 0.13697963335399574]
	TIME [epoch: 2.68 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20335268736392137		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.20335268736392137 | validation: 0.18029080159373198]
	TIME [epoch: 2.67 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2081946679527097		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.2081946679527097 | validation: 0.13581384525507018]
	TIME [epoch: 2.67 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19678259590598246		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.19678259590598246 | validation: 0.16202740604690163]
	TIME [epoch: 2.68 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18471903709491524		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.18471903709491524 | validation: 0.14542028077672908]
	TIME [epoch: 2.68 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17971976765057673		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.17971976765057673 | validation: 0.1367852434280229]
	TIME [epoch: 2.67 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18084436328631984		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.18084436328631984 | validation: 0.14065671065553037]
	TIME [epoch: 2.68 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18651597309352735		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.18651597309352735 | validation: 0.12992286041717735]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_720.pth
	Model improved!!!
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18779561052716293		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.18779561052716293 | validation: 0.15702505640821773]
	TIME [epoch: 2.69 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18318963749455666		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.18318963749455666 | validation: 0.13358700255066547]
	TIME [epoch: 2.68 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1864650412215026		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.1864650412215026 | validation: 0.169049332694318]
	TIME [epoch: 2.69 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1998150887685031		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.1998150887685031 | validation: 0.17046717598422703]
	TIME [epoch: 2.69 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2442033110770192		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.2442033110770192 | validation: 0.18168607692521474]
	TIME [epoch: 2.69 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20937737365443973		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.20937737365443973 | validation: 0.24427057110489023]
	TIME [epoch: 2.69 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23047932420056264		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.23047932420056264 | validation: 0.1442632848562434]
	TIME [epoch: 2.68 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20771001284154358		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.20771001284154358 | validation: 0.16597710770934512]
	TIME [epoch: 2.68 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18676051613806693		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.18676051613806693 | validation: 0.14725623313006636]
	TIME [epoch: 2.68 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17179822153323968		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.17179822153323968 | validation: 0.12614104601057383]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_730.pth
	Model improved!!!
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18048995274237484		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.18048995274237484 | validation: 0.1588101565974356]
	TIME [epoch: 2.69 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18383430845375995		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.18383430845375995 | validation: 0.13586928475957863]
	TIME [epoch: 2.69 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17769622643586108		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.17769622643586108 | validation: 0.13950314858019172]
	TIME [epoch: 2.68 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17499078013478708		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.17499078013478708 | validation: 0.13329992786005715]
	TIME [epoch: 2.68 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17625821864234517		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.17625821864234517 | validation: 0.15639409985027095]
	TIME [epoch: 2.68 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18203737642377393		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.18203737642377393 | validation: 0.14249735984371323]
	TIME [epoch: 2.69 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20279478777749013		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.20279478777749013 | validation: 0.17447718446286373]
	TIME [epoch: 2.69 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19752205287787145		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.19752205287787145 | validation: 0.12461631651863683]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_738.pth
	Model improved!!!
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.178809774303493		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.178809774303493 | validation: 0.13876971403395874]
	TIME [epoch: 2.68 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16927132076020976		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.16927132076020976 | validation: 0.13065885871497004]
	TIME [epoch: 2.68 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1699674432730918		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.1699674432730918 | validation: 0.12343706598263758]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_741.pth
	Model improved!!!
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17065595071048237		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.17065595071048237 | validation: 0.1346615005907692]
	TIME [epoch: 2.68 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.171626976656025		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.171626976656025 | validation: 0.1499331862858218]
	TIME [epoch: 2.68 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17424758403730917		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.17424758403730917 | validation: 0.1373276756832544]
	TIME [epoch: 2.68 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17346582294297122		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.17346582294297122 | validation: 0.14951225786390898]
	TIME [epoch: 2.68 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17000563703934604		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.17000563703934604 | validation: 0.12019274022209792]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_746.pth
	Model improved!!!
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16914295172529842		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.16914295172529842 | validation: 0.25933290502894424]
	TIME [epoch: 2.69 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4333935723050014		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.4333935723050014 | validation: 0.18394177299126183]
	TIME [epoch: 2.68 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2896222270584305		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.2896222270584305 | validation: 0.2690536756648247]
	TIME [epoch: 2.68 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2556902000815698		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.2556902000815698 | validation: 0.16633234095726307]
	TIME [epoch: 2.68 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2113402707648642		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.2113402707648642 | validation: 0.1339368675930372]
	TIME [epoch: 2.68 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1840627853327031		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.1840627853327031 | validation: 0.15854105772415106]
	TIME [epoch: 2.7 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17467946648151078		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.17467946648151078 | validation: 0.12340021508646155]
	TIME [epoch: 2.68 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16758265474441125		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.16758265474441125 | validation: 0.11732127786764837]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_754.pth
	Model improved!!!
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16920449504706558		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.16920449504706558 | validation: 0.12219496111513724]
	TIME [epoch: 2.68 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16793842440701784		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.16793842440701784 | validation: 0.11182088969780653]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_756.pth
	Model improved!!!
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16222683023528275		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.16222683023528275 | validation: 0.12724989797963737]
	TIME [epoch: 2.68 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16324457731784686		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.16324457731784686 | validation: 0.128004583447082]
	TIME [epoch: 2.69 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16103864311984317		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.16103864311984317 | validation: 0.11880623462314006]
	TIME [epoch: 2.68 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16036498244430758		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.16036498244430758 | validation: 0.11247013578318253]
	TIME [epoch: 2.68 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16169113570169927		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.16169113570169927 | validation: 0.11797983741119222]
	TIME [epoch: 2.68 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15860226188576837		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.15860226188576837 | validation: 0.12699873247334234]
	TIME [epoch: 2.68 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15855256337576937		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.15855256337576937 | validation: 0.1345261822490927]
	TIME [epoch: 2.68 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16331477860106822		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.16331477860106822 | validation: 0.12522704177465616]
	TIME [epoch: 2.68 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17388377905478059		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.17388377905478059 | validation: 0.17786881453129555]
	TIME [epoch: 2.68 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1916378843782403		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.1916378843782403 | validation: 0.1374404215346928]
	TIME [epoch: 2.68 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18454684269965704		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.18454684269965704 | validation: 0.13917281530380626]
	TIME [epoch: 2.68 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16711160374224246		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.16711160374224246 | validation: 0.1291781677946251]
	TIME [epoch: 2.68 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1605159976907132		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.1605159976907132 | validation: 0.11164589556038643]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_769.pth
	Model improved!!!
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15977872882308952		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.15977872882308952 | validation: 0.13971317047472748]
	TIME [epoch: 2.68 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16455522058332459		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.16455522058332459 | validation: 0.12031848466357209]
	TIME [epoch: 2.68 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16027231306795447		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.16027231306795447 | validation: 0.1245100615728561]
	TIME [epoch: 2.68 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.155174267722388		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.155174267722388 | validation: 0.13358758717508043]
	TIME [epoch: 2.68 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.162614446331159		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.162614446331159 | validation: 0.23041405832530926]
	TIME [epoch: 2.67 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3699760432132769		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.3699760432132769 | validation: 0.17035714259589496]
	TIME [epoch: 2.68 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2751759265573594		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.2751759265573594 | validation: 0.24230245285242022]
	TIME [epoch: 2.68 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22969991258052416		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.22969991258052416 | validation: 0.18648670229942332]
	TIME [epoch: 2.68 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20479888577148253		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.20479888577148253 | validation: 0.12956817746740687]
	TIME [epoch: 2.68 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17393488341846208		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.17393488341846208 | validation: 0.1306613973305644]
	TIME [epoch: 2.68 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16044948824600055		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.16044948824600055 | validation: 0.11959111181033195]
	TIME [epoch: 2.68 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15953074738144918		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.15953074738144918 | validation: 0.10525289761734916]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_781.pth
	Model improved!!!
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15960860921558193		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.15960860921558193 | validation: 0.10427495734061475]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_782.pth
	Model improved!!!
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15779204247093323		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.15779204247093323 | validation: 0.11832244864711364]
	TIME [epoch: 2.68 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15677348713287365		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.15677348713287365 | validation: 0.12097852292789271]
	TIME [epoch: 2.68 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15775936853569547		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.15775936853569547 | validation: 0.10452466406864397]
	TIME [epoch: 2.68 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1560739710802754		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.1560739710802754 | validation: 0.11779083647455982]
	TIME [epoch: 2.68 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15579337655070324		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.15579337655070324 | validation: 0.11720317491269744]
	TIME [epoch: 2.68 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15830051983675042		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.15830051983675042 | validation: 0.11006767990974187]
	TIME [epoch: 2.68 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15006780961124064		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.15006780961124064 | validation: 0.11166461809516691]
	TIME [epoch: 2.68 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1542686490479201		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.1542686490479201 | validation: 0.11762082945892349]
	TIME [epoch: 2.68 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15542341036160007		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.15542341036160007 | validation: 0.13088246158910474]
	TIME [epoch: 2.68 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.157578972266781		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.157578972266781 | validation: 0.10872047044843541]
	TIME [epoch: 2.68 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16356761692955288		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.16356761692955288 | validation: 0.1477865185862792]
	TIME [epoch: 2.67 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18063719190800007		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.18063719190800007 | validation: 0.11174857292814391]
	TIME [epoch: 2.68 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1614496277210186		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.1614496277210186 | validation: 0.11381516858892687]
	TIME [epoch: 2.67 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1557415461262616		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.1557415461262616 | validation: 0.12115912108237688]
	TIME [epoch: 2.68 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15919462287667255		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.15919462287667255 | validation: 0.1318705128416379]
	TIME [epoch: 2.67 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15662579224755535		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.15662579224755535 | validation: 0.09557386068470217]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_798.pth
	Model improved!!!
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15677220919823884		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.15677220919823884 | validation: 0.1148159813834495]
	TIME [epoch: 2.68 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14898737724834415		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.14898737724834415 | validation: 0.10821132453659753]
	TIME [epoch: 2.68 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15065024622139914		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.15065024622139914 | validation: 0.09679346230005773]
	TIME [epoch: 2.69 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1491634594331371		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.1491634594331371 | validation: 0.10568931274164757]
	TIME [epoch: 2.69 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14694280768700085		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.14694280768700085 | validation: 0.10628980804026225]
	TIME [epoch: 2.69 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14981730194548004		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.14981730194548004 | validation: 0.13027459291958599]
	TIME [epoch: 2.69 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1806893351300491		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.1806893351300491 | validation: 0.17255118421622972]
	TIME [epoch: 2.69 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17628545236834697		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.17628545236834697 | validation: 0.10444877688609895]
	TIME [epoch: 2.69 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15373199826072642		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.15373199826072642 | validation: 0.09300818613018985]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_807.pth
	Model improved!!!
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14642288188274732		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.14642288188274732 | validation: 0.24625464923111]
	TIME [epoch: 2.68 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3639319839703889		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.3639319839703889 | validation: 0.19353840662870223]
	TIME [epoch: 2.68 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29170515816155396		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.29170515816155396 | validation: 0.1966078240032405]
	TIME [epoch: 2.67 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20336861792588729		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.20336861792588729 | validation: 0.19916950839720346]
	TIME [epoch: 2.68 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1947650391464054		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.1947650391464054 | validation: 0.1334850830070346]
	TIME [epoch: 2.68 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17001636955474211		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.17001636955474211 | validation: 0.11309323247505135]
	TIME [epoch: 2.68 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15441010478273712		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.15441010478273712 | validation: 0.1284322407055766]
	TIME [epoch: 2.68 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14909032865331692		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.14909032865331692 | validation: 0.11263467969287629]
	TIME [epoch: 2.68 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1429216244645834		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.1429216244645834 | validation: 0.10534067182632818]
	TIME [epoch: 2.68 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14844156944140027		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.14844156944140027 | validation: 0.1062334383854686]
	TIME [epoch: 2.68 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14697829450863972		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.14697829450863972 | validation: 0.10031524793989872]
	TIME [epoch: 2.68 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14505509773448663		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.14505509773448663 | validation: 0.10969471662201345]
	TIME [epoch: 2.68 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14307191353710935		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.14307191353710935 | validation: 0.11847417988582731]
	TIME [epoch: 2.68 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1518689001191609		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.1518689001191609 | validation: 0.11375069601295534]
	TIME [epoch: 2.68 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14841667223937122		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.14841667223937122 | validation: 0.11479287599445827]
	TIME [epoch: 2.68 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14477121562577128		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.14477121562577128 | validation: 0.11239745232033845]
	TIME [epoch: 2.68 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1448906512582408		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.1448906512582408 | validation: 0.10440260922406996]
	TIME [epoch: 2.68 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14711449791686396		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.14711449791686396 | validation: 0.10275164572213678]
	TIME [epoch: 2.68 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14687694553386726		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.14687694553386726 | validation: 0.09783941183438993]
	TIME [epoch: 2.67 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15014964904899938		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.15014964904899938 | validation: 0.1184733644615697]
	TIME [epoch: 2.68 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14992856286116585		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.14992856286116585 | validation: 0.11094072228765249]
	TIME [epoch: 2.67 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1598221373155334		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.1598221373155334 | validation: 0.1255367096454747]
	TIME [epoch: 2.68 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14890972061979665		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.14890972061979665 | validation: 0.11220335095977299]
	TIME [epoch: 2.68 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14930295700047846		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.14930295700047846 | validation: 0.1224889507878093]
	TIME [epoch: 2.68 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16135022636858445		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.16135022636858445 | validation: 0.1327248409215603]
	TIME [epoch: 2.68 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15405455693486736		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.15405455693486736 | validation: 0.0963250859997409]
	TIME [epoch: 2.68 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1456977602321972		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.1456977602321972 | validation: 0.1049774656159221]
	TIME [epoch: 2.68 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14535890754473116		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.14535890754473116 | validation: 0.11650259173695157]
	TIME [epoch: 2.68 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14881537885848573		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.14881537885848573 | validation: 0.096652475623695]
	TIME [epoch: 2.68 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14172418147991156		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.14172418147991156 | validation: 0.09165971755481708]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_837.pth
	Model improved!!!
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14357535751384792		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.14357535751384792 | validation: 0.10237418512811361]
	TIME [epoch: 2.69 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14349197876796965		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.14349197876796965 | validation: 0.11338471625335747]
	TIME [epoch: 2.69 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14124904957267312		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.14124904957267312 | validation: 0.0887603048572903]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_840.pth
	Model improved!!!
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1435492973875821		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.1435492973875821 | validation: 0.10423154231438002]
	TIME [epoch: 2.67 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14112879688801672		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.14112879688801672 | validation: 0.0855735497867185]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_842.pth
	Model improved!!!
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13731557362180646		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.13731557362180646 | validation: 0.10271044203786409]
	TIME [epoch: 2.67 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14020584820786552		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.14020584820786552 | validation: 0.09314291281498598]
	TIME [epoch: 2.67 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1400221039609406		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.1400221039609406 | validation: 0.10037385303038487]
	TIME [epoch: 2.68 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13928328289556127		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.13928328289556127 | validation: 0.1020096375927859]
	TIME [epoch: 2.68 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14723892189448917		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.14723892189448917 | validation: 0.13993264324321844]
	TIME [epoch: 2.67 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1657004770140948		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.1657004770140948 | validation: 0.09847346169997699]
	TIME [epoch: 2.67 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15092956355746456		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.15092956355746456 | validation: 0.09897317882678403]
	TIME [epoch: 2.67 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13809264384194334		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.13809264384194334 | validation: 0.08961017364044202]
	TIME [epoch: 2.67 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13677498188071618		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.13677498188071618 | validation: 0.08708441331856899]
	TIME [epoch: 2.67 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13665150399250756		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.13665150399250756 | validation: 0.1044426100908371]
	TIME [epoch: 2.68 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14270358144525513		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.14270358144525513 | validation: 0.10517562928567759]
	TIME [epoch: 2.7 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1402297020870853		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.1402297020870853 | validation: 0.11011836307981074]
	TIME [epoch: 2.68 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13615634696573423		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.13615634696573423 | validation: 0.10913373937978527]
	TIME [epoch: 2.69 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1358453828135153		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.1358453828135153 | validation: 0.08316459954821366]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_856.pth
	Model improved!!!
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13728539541060678		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.13728539541060678 | validation: 0.08803409209542058]
	TIME [epoch: 2.68 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1387981466234818		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.1387981466234818 | validation: 0.08011355760839362]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_858.pth
	Model improved!!!
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13583031055114905		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.13583031055114905 | validation: 0.0987928703003127]
	TIME [epoch: 2.68 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13646579485403257		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.13646579485403257 | validation: 0.08489127884178689]
	TIME [epoch: 2.68 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1405799766672808		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.1405799766672808 | validation: 0.10911803242029033]
	TIME [epoch: 2.68 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14038926370870458		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.14038926370870458 | validation: 0.08629667512224004]
	TIME [epoch: 2.68 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13306840361567632		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.13306840361567632 | validation: 0.08260255043082076]
	TIME [epoch: 2.68 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13844165638609024		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.13844165638609024 | validation: 0.08340643370221246]
	TIME [epoch: 2.68 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13682098627005626		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.13682098627005626 | validation: 0.10292317518109716]
	TIME [epoch: 2.68 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13717756682364846		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.13717756682364846 | validation: 0.06286110513809419]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_866.pth
	Model improved!!!
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1345899507092089		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.1345899507092089 | validation: 0.07846931387462097]
	TIME [epoch: 2.69 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13015691126090387		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.13015691126090387 | validation: 0.08594776136856788]
	TIME [epoch: 2.69 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13012596200774193		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.13012596200774193 | validation: 0.09022978849925918]
	TIME [epoch: 2.69 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13205129005454508		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.13205129005454508 | validation: 0.09120270859089634]
	TIME [epoch: 2.69 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12846081287160244		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.12846081287160244 | validation: 0.08652110822383235]
	TIME [epoch: 2.7 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12957156170895812		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.12957156170895812 | validation: 0.09722062062423657]
	TIME [epoch: 2.69 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13334389950030054		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.13334389950030054 | validation: 0.08023000845492241]
	TIME [epoch: 2.69 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1386864887584014		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.1386864887584014 | validation: 0.1337485580464001]
	TIME [epoch: 2.68 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1510738569676756		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.1510738569676756 | validation: 0.1391594865217261]
	TIME [epoch: 2.68 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14802694098833674		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.14802694098833674 | validation: 0.11539980691049792]
	TIME [epoch: 2.68 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17414824831401343		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.17414824831401343 | validation: 0.1282318020027073]
	TIME [epoch: 2.68 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13945016638301336		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.13945016638301336 | validation: 0.14494020035316074]
	TIME [epoch: 2.68 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1597466772709711		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.1597466772709711 | validation: 0.08521746247767642]
	TIME [epoch: 2.68 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13135865754963247		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.13135865754963247 | validation: 0.078136254872541]
	TIME [epoch: 2.68 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12691585564734154		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.12691585564734154 | validation: 0.1018363177658722]
	TIME [epoch: 2.68 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13153783872934266		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.13153783872934266 | validation: 0.08119145193772186]
	TIME [epoch: 2.68 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1285422069498634		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.1285422069498634 | validation: 0.07529542905899594]
	TIME [epoch: 2.67 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1280269934346		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.1280269934346 | validation: 0.07302247944748141]
	TIME [epoch: 2.68 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12284725391996121		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.12284725391996121 | validation: 0.08356544418178789]
	TIME [epoch: 2.67 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12104699214125443		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.12104699214125443 | validation: 0.08954709134664131]
	TIME [epoch: 2.67 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1231082131651283		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.1231082131651283 | validation: 0.08368690066342958]
	TIME [epoch: 2.67 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11958056216793925		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.11958056216793925 | validation: 0.08041217878327213]
	TIME [epoch: 2.68 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12240561954174489		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.12240561954174489 | validation: 0.06486450069014994]
	TIME [epoch: 2.68 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12343422437230935		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.12343422437230935 | validation: 0.07211285172035233]
	TIME [epoch: 2.67 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12317908389392926		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.12317908389392926 | validation: 0.07309965078328641]
	TIME [epoch: 2.68 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12385986641108769		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.12385986641108769 | validation: 0.09739124828627718]
	TIME [epoch: 2.67 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12276779299191304		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.12276779299191304 | validation: 0.07606759047463281]
	TIME [epoch: 2.68 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1228229799384542		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.1228229799384542 | validation: 0.07764939121808706]
	TIME [epoch: 2.67 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12107772526420078		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.12107772526420078 | validation: 0.06804942205698512]
	TIME [epoch: 2.68 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12089546286126625		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.12089546286126625 | validation: 0.08078986768491937]
	TIME [epoch: 2.67 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1276314696871205		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.1276314696871205 | validation: 0.12552421207160763]
	TIME [epoch: 2.68 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1434373352800625		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.1434373352800625 | validation: 0.10121376794487197]
	TIME [epoch: 2.67 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14697989842273526		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.14697989842273526 | validation: 0.08898653348522081]
	TIME [epoch: 2.7 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12300202789151936		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.12300202789151936 | validation: 0.11347924346619112]
	TIME [epoch: 2.69 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13542879147253822		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.13542879147253822 | validation: 0.08385709178220913]
	TIME [epoch: 2.68 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12771187627416639		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.12771187627416639 | validation: 0.08676475024856709]
	TIME [epoch: 2.68 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12116238163541602		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.12116238163541602 | validation: 0.09600006944534231]
	TIME [epoch: 2.68 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1222460362596895		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.1222460362596895 | validation: 0.08191789857453793]
	TIME [epoch: 2.68 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12235959552745763		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.12235959552745763 | validation: 0.09544438568053268]
	TIME [epoch: 2.68 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11755784713039408		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.11755784713039408 | validation: 0.08274354023431768]
	TIME [epoch: 2.69 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11738049939127049		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.11738049939127049 | validation: 0.0688218305008491]
	TIME [epoch: 2.69 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11577755359476992		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.11577755359476992 | validation: 0.07935140386655441]
	TIME [epoch: 2.69 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11701683349887664		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.11701683349887664 | validation: 0.06374713925461829]
	TIME [epoch: 2.69 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11377837601378081		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.11377837601378081 | validation: 0.07897856964882558]
	TIME [epoch: 2.7 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12394633048605919		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.12394633048605919 | validation: 0.09554410777672467]
	TIME [epoch: 2.69 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12379668122239086		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.12379668122239086 | validation: 0.07005566774615059]
	TIME [epoch: 2.69 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11640924929020681		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.11640924929020681 | validation: 0.078757092362289]
	TIME [epoch: 2.69 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11455475372883647		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.11455475372883647 | validation: 0.07455614010817993]
	TIME [epoch: 2.68 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11345470997670082		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.11345470997670082 | validation: 0.09862075242688256]
	TIME [epoch: 2.69 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13631723589160025		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.13631723589160025 | validation: 0.09991902914630438]
	TIME [epoch: 2.69 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12220645837217539		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.12220645837217539 | validation: 0.07554374060191385]
	TIME [epoch: 2.69 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11631946507817688		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.11631946507817688 | validation: 0.08716225635674002]
	TIME [epoch: 2.69 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10936771619654206		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.10936771619654206 | validation: 0.07749148018673527]
	TIME [epoch: 2.69 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1145679137949675		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.1145679137949675 | validation: 0.07552951916148218]
	TIME [epoch: 2.69 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11479368914668442		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.11479368914668442 | validation: 0.0729980997223514]
	TIME [epoch: 2.7 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11335446614888732		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.11335446614888732 | validation: 0.06495440480888993]
	TIME [epoch: 2.69 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11141084892117357		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.11141084892117357 | validation: 0.08044057904909173]
	TIME [epoch: 2.69 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11339238636743355		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.11339238636743355 | validation: 0.07313944087979969]
	TIME [epoch: 2.69 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11134469932943183		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.11134469932943183 | validation: 0.0762173276071034]
	TIME [epoch: 2.69 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11185697716006039		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.11185697716006039 | validation: 0.061879004790056646]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_926.pth
	Model improved!!!
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11101416527757546		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.11101416527757546 | validation: 0.07140411977774755]
	TIME [epoch: 2.68 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1069390604396746		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.1069390604396746 | validation: 0.06420035976789461]
	TIME [epoch: 2.68 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10742618731402306		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.10742618731402306 | validation: 0.10475326574837646]
	TIME [epoch: 2.68 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11797364524411887		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.11797364524411887 | validation: 0.06978970099204218]
	TIME [epoch: 2.69 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13761692548980273		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.13761692548980273 | validation: 0.09942257128632938]
	TIME [epoch: 2.69 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11266037636028152		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.11266037636028152 | validation: 0.07598751545573679]
	TIME [epoch: 2.69 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10987271907554473		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.10987271907554473 | validation: 0.06379561755857298]
	TIME [epoch: 2.69 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1119284177347383		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.1119284177347383 | validation: 0.0779331123097921]
	TIME [epoch: 2.69 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1064065296143842		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.1064065296143842 | validation: 0.06461463159678373]
	TIME [epoch: 2.69 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10908725415117992		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.10908725415117992 | validation: 0.06782371678821447]
	TIME [epoch: 2.69 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1112547655600389		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.1112547655600389 | validation: 0.08057269028026329]
	TIME [epoch: 2.69 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10613374755496077		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.10613374755496077 | validation: 0.06468565347403267]
	TIME [epoch: 2.69 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10504775777940005		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.10504775777940005 | validation: 0.06954426320164779]
	TIME [epoch: 2.69 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10776359548776598		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.10776359548776598 | validation: 0.09449202347727047]
	TIME [epoch: 2.69 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11193631535296969		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.11193631535296969 | validation: 0.05659139310859171]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_941.pth
	Model improved!!!
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1132926292349101		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.1132926292349101 | validation: 0.07401983631725653]
	TIME [epoch: 2.68 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10857320072706628		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.10857320072706628 | validation: 0.07874909244401436]
	TIME [epoch: 2.68 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10933710602062899		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.10933710602062899 | validation: 0.08341232490637772]
	TIME [epoch: 2.68 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10641466347010795		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.10641466347010795 | validation: 0.11327967603201668]
	TIME [epoch: 2.68 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15191013902182435		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.15191013902182435 | validation: 0.11825185795231033]
	TIME [epoch: 2.68 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13566996843053833		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.13566996843053833 | validation: 0.1233930615293668]
	TIME [epoch: 2.68 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12254853132080248		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.12254853132080248 | validation: 0.09226688169271338]
	TIME [epoch: 2.68 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10927229150961086		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.10927229150961086 | validation: 0.08174184599444645]
	TIME [epoch: 2.68 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10542369365164024		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.10542369365164024 | validation: 0.09337075143661464]
	TIME [epoch: 2.69 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10351024982704185		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.10351024982704185 | validation: 0.08231570269158804]
	TIME [epoch: 2.68 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10469307211611824		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.10469307211611824 | validation: 0.07880244520312468]
	TIME [epoch: 2.69 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10639638161548284		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.10639638161548284 | validation: 0.08318329288203656]
	TIME [epoch: 2.69 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10558512733225517		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.10558512733225517 | validation: 0.07891190273503203]
	TIME [epoch: 2.7 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10440018976119159		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.10440018976119159 | validation: 0.07423416653475107]
	TIME [epoch: 2.69 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10365025402464464		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.10365025402464464 | validation: 0.07349558498113044]
	TIME [epoch: 2.69 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10212236751920817		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.10212236751920817 | validation: 0.08327333408194496]
	TIME [epoch: 2.69 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1039434893393633		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.1039434893393633 | validation: 0.0658028431863636]
	TIME [epoch: 2.69 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10099778981817452		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.10099778981817452 | validation: 0.08107014914352315]
	TIME [epoch: 2.69 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09811508924526491		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.09811508924526491 | validation: 0.06639867093640128]
	TIME [epoch: 2.69 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10387770390112554		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.10387770390112554 | validation: 0.07426218507684129]
	TIME [epoch: 2.69 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10549600088975321		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.10549600088975321 | validation: 0.06070447305531271]
	TIME [epoch: 2.69 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10157544548473872		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.10157544548473872 | validation: 0.08221986439337581]
	TIME [epoch: 2.69 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09906532791669356		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.09906532791669356 | validation: 0.055550133263297496]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_964.pth
	Model improved!!!
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10920589005091458		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.10920589005091458 | validation: 0.09357065430743387]
	TIME [epoch: 2.69 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10564670717960177		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.10564670717960177 | validation: 0.06768772902274757]
	TIME [epoch: 2.69 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10645191775770364		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.10645191775770364 | validation: 0.08082589035527049]
	TIME [epoch: 2.69 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09913521582691086		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.09913521582691086 | validation: 0.06905119615351994]
	TIME [epoch: 2.69 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09829149774596921		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.09829149774596921 | validation: 0.07502072314103053]
	TIME [epoch: 2.69 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09881218795602292		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.09881218795602292 | validation: 0.07862391301844782]
	TIME [epoch: 2.69 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09957266449311013		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.09957266449311013 | validation: 0.05166087543208532]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_971.pth
	Model improved!!!
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10148885990624897		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.10148885990624897 | validation: 0.07988525345259107]
	TIME [epoch: 2.69 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10180673320598654		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.10180673320598654 | validation: 0.06827211303827145]
	TIME [epoch: 2.69 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10217106880366501		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.10217106880366501 | validation: 0.08055874217696998]
	TIME [epoch: 2.69 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09951538400256432		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.09951538400256432 | validation: 0.061489059337041074]
	TIME [epoch: 2.7 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09835693224797395		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.09835693224797395 | validation: 0.07607632213385081]
	TIME [epoch: 2.69 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10776066807298991		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.10776066807298991 | validation: 0.10180862820323547]
	TIME [epoch: 2.69 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10878703732509737		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.10878703732509737 | validation: 0.06717436525810863]
	TIME [epoch: 2.69 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09825488833403259		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.09825488833403259 | validation: 0.06373691433362731]
	TIME [epoch: 2.69 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09753134660673425		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.09753134660673425 | validation: 0.10078708897830363]
	TIME [epoch: 2.69 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10158543145003371		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.10158543145003371 | validation: 0.06552442767647669]
	TIME [epoch: 2.69 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09775866218177616		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.09775866218177616 | validation: 0.06977795136939906]
	TIME [epoch: 2.69 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09742337646624219		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.09742337646624219 | validation: 0.06791075149101822]
	TIME [epoch: 2.69 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09272243255652882		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.09272243255652882 | validation: 0.07337856860775195]
	TIME [epoch: 2.69 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09408513346533695		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.09408513346533695 | validation: 0.07049708102659633]
	TIME [epoch: 2.69 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0929598724186096		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.0929598724186096 | validation: 0.07697663484467143]
	TIME [epoch: 2.7 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09448700121440086		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.09448700121440086 | validation: 0.059446554997747404]
	TIME [epoch: 2.69 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09351040144440251		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.09351040144440251 | validation: 0.07825875228973746]
	TIME [epoch: 2.69 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09986618510858267		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.09986618510858267 | validation: 0.09237996135049169]
	TIME [epoch: 2.69 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10348916365349214		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.10348916365349214 | validation: 0.0616131799863494]
	TIME [epoch: 2.69 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11116272078154193		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.11116272078154193 | validation: 0.09121237475262961]
	TIME [epoch: 2.69 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09860051957645408		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.09860051957645408 | validation: 0.06541585501233084]
	TIME [epoch: 2.69 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09175030811657557		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.09175030811657557 | validation: 0.06232929300192763]
	TIME [epoch: 2.69 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09546793666191078		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.09546793666191078 | validation: 0.08458288937841947]
	TIME [epoch: 2.69 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09282376961454654		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.09282376961454654 | validation: 0.06633216424567165]
	TIME [epoch: 2.69 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09338222028044632		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.09338222028044632 | validation: 0.072637642003016]
	TIME [epoch: 2.69 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09098986117382181		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.09098986117382181 | validation: 0.07549784479881345]
	TIME [epoch: 2.7 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09097302928288803		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.09097302928288803 | validation: 0.06308388393547154]
	TIME [epoch: 2.69 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09068313336590794		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.09068313336590794 | validation: 0.0687734387416835]
	TIME [epoch: 2.69 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0905177268834363		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.0905177268834363 | validation: 0.06034626456364261]
	TIME [epoch: 2.69 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0937916913983708		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.0937916913983708 | validation: 0.0822485241070773]
	TIME [epoch: 186 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09497727188665021		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.09497727188665021 | validation: 0.06530077249722387]
	TIME [epoch: 5.76 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09345490964085883		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.09345490964085883 | validation: 0.07567949806348521]
	TIME [epoch: 5.73 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0893553590907073		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.0893553590907073 | validation: 0.06789451303496807]
	TIME [epoch: 5.74 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09490632528876386		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.09490632528876386 | validation: 0.0671060250885524]
	TIME [epoch: 5.73 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09308024362626459		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.09308024362626459 | validation: 0.08645681180329501]
	TIME [epoch: 5.73 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09881090941878341		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.09881090941878341 | validation: 0.06639626541521826]
	TIME [epoch: 5.74 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09530271608249068		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.09530271608249068 | validation: 0.09085680356746945]
	TIME [epoch: 5.73 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10237066883217479		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.10237066883217479 | validation: 0.1116750875790412]
	TIME [epoch: 5.74 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09807710885377262		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.09807710885377262 | validation: 0.07906449138891809]
	TIME [epoch: 5.73 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0948641223346082		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.0948641223346082 | validation: 0.07556083398915772]
	TIME [epoch: 5.73 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08753682497262506		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.08753682497262506 | validation: 0.09127150762357165]
	TIME [epoch: 5.73 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09191302278458405		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.09191302278458405 | validation: 0.06832977588911415]
	TIME [epoch: 5.73 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09172223170023973		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.09172223170023973 | validation: 0.07869877288313021]
	TIME [epoch: 5.74 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09222155209606613		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.09222155209606613 | validation: 0.07088332904356347]
	TIME [epoch: 5.74 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08757229812472751		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.08757229812472751 | validation: 0.076238358333162]
	TIME [epoch: 5.73 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08865879189276996		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.08865879189276996 | validation: 0.08787347117697947]
	TIME [epoch: 5.73 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09387645249965042		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.09387645249965042 | validation: 0.06366062538159951]
	TIME [epoch: 5.73 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09083498024835575		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.09083498024835575 | validation: 0.08141092566515787]
	TIME [epoch: 5.73 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08943017965246948		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.08943017965246948 | validation: 0.08169751386774095]
	TIME [epoch: 5.73 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09180715414411239		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.09180715414411239 | validation: 0.06343798071822745]
	TIME [epoch: 5.73 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09057312301122178		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.09057312301122178 | validation: 0.08946207968727773]
	TIME [epoch: 5.73 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09006021926584949		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.09006021926584949 | validation: 0.06879321044811919]
	TIME [epoch: 5.73 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09011120057436553		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.09011120057436553 | validation: 0.07465475089639707]
	TIME [epoch: 5.73 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08774274078170123		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.08774274078170123 | validation: 0.06680717585045316]
	TIME [epoch: 5.74 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08820445516289403		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.08820445516289403 | validation: 0.06664192460338021]
	TIME [epoch: 5.73 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08646734684444163		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.08646734684444163 | validation: 0.07729674619841989]
	TIME [epoch: 5.73 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08489362045804168		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.08489362045804168 | validation: 0.0659677214944307]
	TIME [epoch: 5.73 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09084911159461037		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.09084911159461037 | validation: 0.07390899045519682]
	TIME [epoch: 5.73 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08646896013074347		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.08646896013074347 | validation: 0.07057278668033873]
	TIME [epoch: 5.73 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08715763558011222		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.08715763558011222 | validation: 0.06650032860893244]
	TIME [epoch: 5.73 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08468000213546191		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.08468000213546191 | validation: 0.07195717828362183]
	TIME [epoch: 5.73 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08798895625929436		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.08798895625929436 | validation: 0.07474738657265566]
	TIME [epoch: 5.74 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08468936344427634		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.08468936344427634 | validation: 0.06751727843893614]
	TIME [epoch: 5.73 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08394700169916312		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.08394700169916312 | validation: 0.0712230783385222]
	TIME [epoch: 5.74 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0813852127314955		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.0813852127314955 | validation: 0.06706765026921578]
	TIME [epoch: 5.73 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08562228716973033		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.08562228716973033 | validation: 0.07035760026159912]
	TIME [epoch: 5.73 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08720646578861622		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.08720646578861622 | validation: 0.07491124847586085]
	TIME [epoch: 5.73 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08561014943736633		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.08561014943736633 | validation: 0.06737898608860686]
	TIME [epoch: 5.73 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08278151096617985		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.08278151096617985 | validation: 0.06801518436658709]
	TIME [epoch: 5.73 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09632866645991006		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.09632866645991006 | validation: 0.10430998307329481]
	TIME [epoch: 5.73 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10161195349946336		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.10161195349946336 | validation: 0.0575486655450327]
	TIME [epoch: 5.73 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.088188275868042		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.088188275868042 | validation: 0.06533491961937557]
	TIME [epoch: 5.73 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08231126674193633		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.08231126674193633 | validation: 0.0756653127461559]
	TIME [epoch: 5.73 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08365756666240547		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.08365756666240547 | validation: 0.06050351764331252]
	TIME [epoch: 5.73 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08549953261560429		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.08549953261560429 | validation: 0.06323856266717985]
	TIME [epoch: 5.73 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07995736707977619		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.07995736707977619 | validation: 0.07038326668162638]
	TIME [epoch: 5.73 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08206350413282137		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.08206350413282137 | validation: 0.06657227102704098]
	TIME [epoch: 5.73 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08027294340742927		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.08027294340742927 | validation: 0.06771180006183106]
	TIME [epoch: 5.73 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08125692354378221		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.08125692354378221 | validation: 0.06762606740959606]
	TIME [epoch: 5.73 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08223580190933127		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.08223580190933127 | validation: 0.06458010490912347]
	TIME [epoch: 5.74 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08238039613575328		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.08238039613575328 | validation: 0.0625357125848031]
	TIME [epoch: 5.73 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08009582235603352		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.08009582235603352 | validation: 0.06520139215110485]
	TIME [epoch: 5.73 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08083559336612198		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.08083559336612198 | validation: 0.07634802874747645]
	TIME [epoch: 5.73 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08113232794254753		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.08113232794254753 | validation: 0.07751654721899345]
	TIME [epoch: 5.73 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08149457796336355		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.08149457796336355 | validation: 0.06795526951059809]
	TIME [epoch: 5.73 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08905107717511095		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.08905107717511095 | validation: 0.10057225288728627]
	TIME [epoch: 5.73 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09050103682737107		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.09050103682737107 | validation: 0.07047431750836723]
	TIME [epoch: 5.73 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07922385444838068		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.07922385444838068 | validation: 0.06794146004138993]
	TIME [epoch: 5.73 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07981251556656294		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.07981251556656294 | validation: 0.08184944230485895]
	TIME [epoch: 5.73 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08437598991093623		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.08437598991093623 | validation: 0.06809341657309728]
	TIME [epoch: 5.74 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08078349005589353		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.08078349005589353 | validation: 0.07292062217157395]
	TIME [epoch: 5.73 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08053935911275421		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.08053935911275421 | validation: 0.07686048749809152]
	TIME [epoch: 5.73 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08091738254351773		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.08091738254351773 | validation: 0.07316322957761633]
	TIME [epoch: 5.73 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08133824791474442		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.08133824791474442 | validation: 0.08067960078145744]
	TIME [epoch: 5.73 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.080470255774952		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.080470255774952 | validation: 0.08820331883511338]
	TIME [epoch: 5.73 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08201547099320038		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.08201547099320038 | validation: 0.06342977049116406]
	TIME [epoch: 5.73 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0827500521760508		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.0827500521760508 | validation: 0.08746361325746306]
	TIME [epoch: 5.73 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0819930010402227		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.0819930010402227 | validation: 0.060512425151615805]
	TIME [epoch: 5.73 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0830641422656506		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.0830641422656506 | validation: 0.07949895570899397]
	TIME [epoch: 5.72 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07869724360994937		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.07869724360994937 | validation: 0.06853111297699575]
	TIME [epoch: 5.73 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07702768324033088		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.07702768324033088 | validation: 0.05840546008758782]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_5_v_mmd4_1072.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 3100.275 seconds.
