Args:
Namespace(name='model_phi1_1a_distortion_v2r_1_v_mmd1', outdir='out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1', training_data='data/training_data/distortions/paraboloids/data_phi1_1a_distortion_v2r_1/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_1a_distortion_v2r_1/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.03961945, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 889910106

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.3172789863608525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.3172789863608525 | validation: 6.470708658822613]
	TIME [epoch: 356 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.761326800142112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.761326800142112 | validation: 6.097600167334571]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.359223688320095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.359223688320095 | validation: 5.867038550027468]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.24788748368587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.24788748368587 | validation: 5.968222030615177]
	TIME [epoch: 5.93 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.139353018100912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.139353018100912 | validation: 5.614229379350871]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.717536602223253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.717536602223253 | validation: 5.372889370735289]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.437266934426906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.437266934426906 | validation: 5.769049475065312]
	TIME [epoch: 5.94 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.5347103009118666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5347103009118666 | validation: 4.856260416946851]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.172758797139455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.172758797139455 | validation: 4.711017602242862]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9602687682345703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9602687682345703 | validation: 4.510855538393766]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8362979265548893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8362979265548893 | validation: 4.368947247509897]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7648649731783026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7648649731783026 | validation: 4.257795772354516]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5509559035362286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5509559035362286 | validation: 4.021286879281577]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.731251432246569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.731251432246569 | validation: 3.784841226257515]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2544250739191067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2544250739191067 | validation: 3.440283181305162]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0022393956850006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0022393956850006 | validation: 3.0930939589849182]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.920723773930048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.920723773930048 | validation: 2.9768341104441127]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7710788716303365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7710788716303365 | validation: 3.007735901595547]
	TIME [epoch: 5.93 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.741041953453547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.741041953453547 | validation: 2.785906686385702]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7438034750222084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7438034750222084 | validation: 2.7528464972943594]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6542110096679226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6542110096679226 | validation: 2.7777029955313584]
	TIME [epoch: 5.93 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6015498888560806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6015498888560806 | validation: 2.7487557684220594]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.661116422095678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.661116422095678 | validation: 2.7142590505382156]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.569511481944895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.569511481944895 | validation: 2.697500547259579]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7525437250418094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7525437250418094 | validation: 2.8859322990807383]
	TIME [epoch: 5.93 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5236112666117516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5236112666117516 | validation: 2.6039347019539365]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5983076482680847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5983076482680847 | validation: 2.593714457329802]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4745480127402715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4745480127402715 | validation: 2.5898327534220336]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.398567427863257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.398567427863257 | validation: 2.9328257394518564]
	TIME [epoch: 5.94 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.637882116329845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.637882116329845 | validation: 2.5188531422264644]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.369340534052471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.369340534052471 | validation: 2.489574519466817]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.456283837186358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.456283837186358 | validation: 2.4727262936186998]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5052960870653114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5052960870653114 | validation: 2.445817446381185]
	TIME [epoch: 6.04 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2860732752241177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2860732752241177 | validation: 2.706747663215146]
	TIME [epoch: 6.08 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.438233801952343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.438233801952343 | validation: 2.1775492240268015]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3752035756329395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3752035756329395 | validation: 2.510331354711755]
	TIME [epoch: 5.93 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.485268029757974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.485268029757974 | validation: 2.5294091018979596]
	TIME [epoch: 5.94 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4331243274219054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4331243274219054 | validation: 2.4523277945469784]
	TIME [epoch: 5.93 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3110130387649037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3110130387649037 | validation: 2.207511679108751]
	TIME [epoch: 5.93 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3673253634260525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3673253634260525 | validation: 2.4332142253773066]
	TIME [epoch: 5.93 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.527871778942725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.527871778942725 | validation: 2.3296043814563996]
	TIME [epoch: 5.93 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1272818252375845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1272818252375845 | validation: 2.131466261244077]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.09050483225186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.09050483225186 | validation: 2.1643970886092485]
	TIME [epoch: 5.94 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2937778909706377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2937778909706377 | validation: 2.0174220017602638]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.079829136710712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.079829136710712 | validation: 2.138206853905845]
	TIME [epoch: 5.93 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1002165191884146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1002165191884146 | validation: 1.8669058237851677]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0489569670372356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0489569670372356 | validation: 2.044093308072021]
	TIME [epoch: 6.18 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9263764725723387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9263764725723387 | validation: 1.8581678451640948]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.75255046334578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.75255046334578 | validation: 1.8094217410525752]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.035960208569927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.035960208569927 | validation: 2.063568621100445]
	TIME [epoch: 5.93 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7424764984309207		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 1.7424764984309207 | validation: 1.4942420877055793]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.467995483580611		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 1.467995483580611 | validation: 1.6726175889545307]
	TIME [epoch: 5.93 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.783681821492194		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 1.783681821492194 | validation: 1.5071431440350656]
	TIME [epoch: 5.92 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5108923600840614		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 1.5108923600840614 | validation: 1.2050645495849055]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4173400923465702		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 1.4173400923465702 | validation: 1.502264036546059]
	TIME [epoch: 5.92 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4162560483605366		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 1.4162560483605366 | validation: 1.0157606791865992]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1612286487974066		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 1.1612286487974066 | validation: 1.5608556966111329]
	TIME [epoch: 5.93 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1455863966494793		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 1.1455863966494793 | validation: 0.9504764644377771]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0357770088855205		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 1.0357770088855205 | validation: 1.4057917393892874]
	TIME [epoch: 5.94 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3631645546558875		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 1.3631645546558875 | validation: 0.9673604867581079]
	TIME [epoch: 5.92 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9117369434204781		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 0.9117369434204781 | validation: 1.1230739815125064]
	TIME [epoch: 5.92 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9446277457585099		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 0.9446277457585099 | validation: 0.9496532258256141]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8970182612826699		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 0.8970182612826699 | validation: 1.355517369317078]
	TIME [epoch: 5.95 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1413320536702694		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 1.1413320536702694 | validation: 1.0557450117911054]
	TIME [epoch: 5.93 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9615603212792744		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.9615603212792744 | validation: 0.7681774037211264]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.896931578762356		[learning rate: 0.0094573]
	Learning Rate: 0.00945735
	LOSS [training: 0.896931578762356 | validation: 0.6899329035725074]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7911337307776002		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.7911337307776002 | validation: 1.2902385022098808]
	TIME [epoch: 5.94 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3558651442110121		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 1.3558651442110121 | validation: 1.0821722439679182]
	TIME [epoch: 6.06 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8781300390233131		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 0.8781300390233131 | validation: 0.7906509122127603]
	TIME [epoch: 5.93 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7779842588584035		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.7779842588584035 | validation: 0.9425767169929629]
	TIME [epoch: 5.93 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8534033406954938		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.8534033406954938 | validation: 0.6566544676454291]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8576144321075391		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.8576144321075391 | validation: 0.8708963961783489]
	TIME [epoch: 5.94 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7637631986777882		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.7637631986777882 | validation: 0.9248746032675363]
	TIME [epoch: 5.93 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7931632739038723		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.7931632739038723 | validation: 0.7705283229696316]
	TIME [epoch: 5.93 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7188107703051392		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.7188107703051392 | validation: 0.5912629244111329]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8963712364918153		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.8963712364918153 | validation: 0.7752043870141272]
	TIME [epoch: 5.94 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7795553761179026		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.7795553761179026 | validation: 0.7052262679531048]
	TIME [epoch: 5.93 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6711073318848502		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.6711073318848502 | validation: 0.790564866225229]
	TIME [epoch: 5.93 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8218483352691981		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.8218483352691981 | validation: 0.7472674520963254]
	TIME [epoch: 5.92 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8011272429727786		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.8011272429727786 | validation: 0.9580086584077563]
	TIME [epoch: 5.94 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7624081645268455		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.7624081645268455 | validation: 0.6020876400054812]
	TIME [epoch: 5.95 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.67776394305622		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.67776394305622 | validation: 0.559596352623652]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6274337905940885		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.6274337905940885 | validation: 0.6037733478238569]
	TIME [epoch: 5.96 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6929006516313443		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.6929006516313443 | validation: 0.6429886319181366]
	TIME [epoch: 5.94 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9431200660987213		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.9431200660987213 | validation: 1.0372862927000353]
	TIME [epoch: 5.93 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9918454056119856		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.9918454056119856 | validation: 0.5337036358122108]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6540256909996798		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.6540256909996798 | validation: 0.6305118781712646]
	TIME [epoch: 5.94 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5688706212205052		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.5688706212205052 | validation: 0.6694094217324209]
	TIME [epoch: 5.93 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.648561966071012		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.648561966071012 | validation: 0.49781961640742195]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.677344945132371		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.677344945132371 | validation: 0.443588446761719]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8162222105200867		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.8162222105200867 | validation: 0.9324463730277819]
	TIME [epoch: 5.94 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7854197704668026		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.7854197704668026 | validation: 0.7170948669405744]
	TIME [epoch: 5.93 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7424185563758261		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.7424185563758261 | validation: 0.5824078533154072]
	TIME [epoch: 5.92 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6144150456743263		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.6144150456743263 | validation: 0.5919754378760926]
	TIME [epoch: 5.92 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6495864543718681		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.6495864543718681 | validation: 0.45563699906737976]
	TIME [epoch: 5.92 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.553279891562474		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.553279891562474 | validation: 0.4159417859147969]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5040434192315436		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.5040434192315436 | validation: 0.6381800244518304]
	TIME [epoch: 5.93 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5761609205051628		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.5761609205051628 | validation: 0.755823541129915]
	TIME [epoch: 5.92 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6045255445406934		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.6045255445406934 | validation: 0.4894166123563875]
	TIME [epoch: 5.93 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5851621716059763		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.5851621716059763 | validation: 0.47532467051554683]
	TIME [epoch: 5.92 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5441344041037146		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.5441344041037146 | validation: 0.580928824022692]
	TIME [epoch: 5.94 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5732993459715124		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.5732993459715124 | validation: 0.5904055112517854]
	TIME [epoch: 5.92 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6533169937567622		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 1.6533169937567622 | validation: 2.5324211609176452]
	TIME [epoch: 5.92 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2271812973022294		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 2.2271812973022294 | validation: 3.367700359450084]
	TIME [epoch: 5.92 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2054749413989274		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 3.2054749413989274 | validation: 2.932863308234582]
	TIME [epoch: 5.93 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.44532409310127		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 3.44532409310127 | validation: 3.7207246514334686]
	TIME [epoch: 5.93 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5036774415898395		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 3.5036774415898395 | validation: 3.7467600958636016]
	TIME [epoch: 5.93 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4120364311511917		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 3.4120364311511917 | validation: 2.876846670478742]
	TIME [epoch: 5.92 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1986539144881814		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 3.1986539144881814 | validation: 2.768752725622222]
	TIME [epoch: 5.92 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0909527656928426		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 3.0909527656928426 | validation: 3.101963653242047]
	TIME [epoch: 5.92 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0214029534777707		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 3.0214029534777707 | validation: 3.0112954790114643]
	TIME [epoch: 5.93 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.816351178484079		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 2.816351178484079 | validation: 2.636238180772877]
	TIME [epoch: 5.92 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.687972056159944		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 2.687972056159944 | validation: 2.607072722999798]
	TIME [epoch: 5.92 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.519075787378536		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 2.519075787378536 | validation: 2.357512162878396]
	TIME [epoch: 5.93 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.339471152809398		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 2.339471152809398 | validation: 2.05627147166193]
	TIME [epoch: 5.92 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5668707590261235		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 1.5668707590261235 | validation: 1.0259895307976628]
	TIME [epoch: 5.93 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9933164513589017		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.9933164513589017 | validation: 0.9330710496076406]
	TIME [epoch: 5.92 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8236938749947177		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.8236938749947177 | validation: 1.032793062528963]
	TIME [epoch: 5.92 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0107830161879183		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 1.0107830161879183 | validation: 0.8093535517934167]
	TIME [epoch: 5.93 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7424139041483815		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.7424139041483815 | validation: 1.0730789881918055]
	TIME [epoch: 5.92 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7994925509108131		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.7994925509108131 | validation: 0.8948758565684519]
	TIME [epoch: 5.93 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.788733724100368		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.788733724100368 | validation: 0.6329420695548772]
	TIME [epoch: 6.26 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8772769835812767		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.8772769835812767 | validation: 0.8256927163050162]
	TIME [epoch: 5.93 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7664688116868664		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.7664688116868664 | validation: 1.016547293730614]
	TIME [epoch: 5.94 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.779739638279843		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.779739638279843 | validation: 0.6130302409561124]
	TIME [epoch: 5.94 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7061051065922812		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.7061051065922812 | validation: 1.1697719723969393]
	TIME [epoch: 5.95 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7662257331281604		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.7662257331281604 | validation: 0.6559745090467656]
	TIME [epoch: 5.93 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6907645996781753		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.6907645996781753 | validation: 0.6405607690473373]
	TIME [epoch: 5.95 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5711411263700829		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.5711411263700829 | validation: 0.7434837115126576]
	TIME [epoch: 5.94 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6713837905403285		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.6713837905403285 | validation: 0.7476739668643074]
	TIME [epoch: 5.95 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8494041305261091		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.8494041305261091 | validation: 0.7469049866716035]
	TIME [epoch: 5.94 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6031636406758969		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.6031636406758969 | validation: 0.8177817138043753]
	TIME [epoch: 5.94 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7843450348162828		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.7843450348162828 | validation: 0.6721303905379242]
	TIME [epoch: 5.94 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5760277324887871		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.5760277324887871 | validation: 0.9067080485287042]
	TIME [epoch: 5.94 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6018061471595861		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.6018061471595861 | validation: 0.5157040876869964]
	TIME [epoch: 5.95 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6264342719844896		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.6264342719844896 | validation: 0.5481537428963612]
	TIME [epoch: 5.95 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5838064141055094		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.5838064141055094 | validation: 0.546468891762887]
	TIME [epoch: 5.94 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6281653251182333		[learning rate: 0.0073282]
	Learning Rate: 0.00732824
	LOSS [training: 0.6281653251182333 | validation: 0.5418400677923015]
	TIME [epoch: 5.95 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5971052155545037		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.5971052155545037 | validation: 0.5589708650049243]
	TIME [epoch: 5.95 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47393250357762284		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.47393250357762284 | validation: 0.513937509894249]
	TIME [epoch: 5.95 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49971429516009824		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.49971429516009824 | validation: 0.7044387263232741]
	TIME [epoch: 5.95 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6766864671951176		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.6766864671951176 | validation: 0.6406932230617705]
	TIME [epoch: 5.94 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5587012012703172		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.5587012012703172 | validation: 3.9406910696315443]
	TIME [epoch: 5.94 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.542893579799486		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 2.542893579799486 | validation: 0.9676557382632671]
	TIME [epoch: 5.95 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.674303294179626		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.674303294179626 | validation: 0.5860187254923623]
	TIME [epoch: 5.95 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4679851925166323		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.4679851925166323 | validation: 0.3760254004527587]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.533719658553012		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.533719658553012 | validation: 0.783646900519813]
	TIME [epoch: 5.94 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5571234983320457		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.5571234983320457 | validation: 0.492721349421701]
	TIME [epoch: 5.94 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.549705904961148		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.549705904961148 | validation: 0.4134309742486635]
	TIME [epoch: 5.95 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4525728626943708		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.4525728626943708 | validation: 0.5459660953286785]
	TIME [epoch: 5.94 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5551169317475768		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.5551169317475768 | validation: 0.5574578798070473]
	TIME [epoch: 5.94 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6458495509309776		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.6458495509309776 | validation: 0.6508892380914959]
	TIME [epoch: 5.92 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6327437585670442		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.6327437585670442 | validation: 0.6603896178057964]
	TIME [epoch: 5.95 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5417536131042177		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.5417536131042177 | validation: 0.5208295904932474]
	TIME [epoch: 5.94 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4874150411475544		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.4874150411475544 | validation: 0.36647980902137567]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4361392769981246		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.4361392769981246 | validation: 0.5244005880090709]
	TIME [epoch: 5.93 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6075050406987398		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.6075050406987398 | validation: 0.7592928992197645]
	TIME [epoch: 5.94 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5715062313223457		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.5715062313223457 | validation: 0.40025951228000645]
	TIME [epoch: 5.94 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40873391403246107		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.40873391403246107 | validation: 0.7644872978494013]
	TIME [epoch: 5.94 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48881321479339157		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.48881321479339157 | validation: 0.32121868768323647]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48084505130319294		[learning rate: 0.0067548]
	Learning Rate: 0.00675484
	LOSS [training: 0.48084505130319294 | validation: 0.9475095695373512]
	TIME [epoch: 5.93 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6248171081097051		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.6248171081097051 | validation: 0.894934888619233]
	TIME [epoch: 5.94 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5074944657381939		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.5074944657381939 | validation: 0.33918031135173865]
	TIME [epoch: 5.93 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40929347077585665		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.40929347077585665 | validation: 0.2841067346737628]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.384198228667473		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.384198228667473 | validation: 0.470987384642754]
	TIME [epoch: 5.94 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41663833900289093		[learning rate: 0.0066363]
	Learning Rate: 0.00663626
	LOSS [training: 0.41663833900289093 | validation: 0.34568002510430973]
	TIME [epoch: 5.94 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38179703397081755		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.38179703397081755 | validation: 0.4852141251992443]
	TIME [epoch: 5.94 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4161144043442143		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.4161144043442143 | validation: 0.8491268709380226]
	TIME [epoch: 5.94 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.691192487372552		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.691192487372552 | validation: 0.4333000619397347]
	TIME [epoch: 5.93 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3201401990945842		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.3201401990945842 | validation: 0.34169665868136984]
	TIME [epoch: 5.94 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39422080554232297		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.39422080554232297 | validation: 0.24971397488754016]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_171.pth
	Model improved!!!
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36228141306578837		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.36228141306578837 | validation: 0.846646685690639]
	TIME [epoch: 5.94 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6781295603862108		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.6781295603862108 | validation: 0.8549243325385125]
	TIME [epoch: 5.94 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5533388203380469		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.5533388203380469 | validation: 0.3741210034377632]
	TIME [epoch: 5.93 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.365490874790299		[learning rate: 0.006428]
	Learning Rate: 0.00642802
	LOSS [training: 0.365490874790299 | validation: 0.8565266031544385]
	TIME [epoch: 5.95 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45949757092716725		[learning rate: 0.0064053]
	Learning Rate: 0.00640528
	LOSS [training: 0.45949757092716725 | validation: 0.38894011125481476]
	TIME [epoch: 5.94 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33011798238980145		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.33011798238980145 | validation: 0.293695630780573]
	TIME [epoch: 5.92 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3140667691243734		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.3140667691243734 | validation: 0.38710583318880404]
	TIME [epoch: 6.25 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37621524821056934		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.37621524821056934 | validation: 0.5896560070318282]
	TIME [epoch: 5.94 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43950016176818946		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.43950016176818946 | validation: 0.29544077312966605]
	TIME [epoch: 5.93 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2822630166922597		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.2822630166922597 | validation: 0.38790107514805]
	TIME [epoch: 6.1 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36461152810313047		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.36461152810313047 | validation: 0.24832641529676674]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3422748663576139		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.3422748663576139 | validation: 0.24187204706318077]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_183.pth
	Model improved!!!
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44292563977803484		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.44292563977803484 | validation: 0.4606610627371819]
	TIME [epoch: 5.94 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39936812390554977		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.39936812390554977 | validation: 0.5572474609984323]
	TIME [epoch: 5.95 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4107286088756843		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.4107286088756843 | validation: 0.28020479020145883]
	TIME [epoch: 6.01 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2854431011798429		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.2854431011798429 | validation: 0.46180685503026375]
	TIME [epoch: 5.94 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4741425192199067		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.4741425192199067 | validation: 0.41560038240329483]
	TIME [epoch: 5.94 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2910887013262364		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.2910887013262364 | validation: 0.2234135536518782]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.290196247916738		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.290196247916738 | validation: 0.5409620980521912]
	TIME [epoch: 6.08 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33886834679751665		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.33886834679751665 | validation: 0.42148222228703003]
	TIME [epoch: 5.94 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35599638120298066		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.35599638120298066 | validation: 0.7596871251726913]
	TIME [epoch: 5.95 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5297035782258493		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.5297035782258493 | validation: 0.2692509257675394]
	TIME [epoch: 5.94 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2903514564527171		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.2903514564527171 | validation: 0.21358448768311922]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2558609826013409		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.2558609826013409 | validation: 0.22861560405287173]
	TIME [epoch: 5.94 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4263745801791567		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.4263745801791567 | validation: 0.36101281639294625]
	TIME [epoch: 5.94 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2564496774192775		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.2564496774192775 | validation: 0.21732398068678282]
	TIME [epoch: 5.94 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2778060524287941		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.2778060524287941 | validation: 0.2360795622574528]
	TIME [epoch: 5.94 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28467563999967554		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.28467563999967554 | validation: 0.21512849575368043]
	TIME [epoch: 5.95 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31201042647989063		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.31201042647989063 | validation: 0.42703273836989364]
	TIME [epoch: 5.95 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35499277799022805		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.35499277799022805 | validation: 0.40751806526841816]
	TIME [epoch: 381 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40679846907470474		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.40679846907470474 | validation: 0.296723940506431]
	TIME [epoch: 11.7 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26521690954635485		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.26521690954635485 | validation: 0.34147551492152284]
	TIME [epoch: 11.7 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3035022460398896		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.3035022460398896 | validation: 0.3120893614829885]
	TIME [epoch: 11.7 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36135115385292527		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.36135115385292527 | validation: 0.25368225321870375]
	TIME [epoch: 11.7 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3570227311806072		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.3570227311806072 | validation: 0.25336111738983214]
	TIME [epoch: 11.7 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20134171030231854		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.20134171030231854 | validation: 0.1999898073811482]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_207.pth
	Model improved!!!
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2467852684242529		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.2467852684242529 | validation: 0.17487352875592646]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45994111245216823		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.45994111245216823 | validation: 0.46035345548766926]
	TIME [epoch: 11.7 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31489155191899443		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.31489155191899443 | validation: 0.25226203365240896]
	TIME [epoch: 11.7 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3076116459532898		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.3076116459532898 | validation: 0.2671553630597985]
	TIME [epoch: 11.7 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2459691771700571		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.2459691771700571 | validation: 0.18187216648973556]
	TIME [epoch: 11.7 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2082327826933329		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.2082327826933329 | validation: 0.24536350595989848]
	TIME [epoch: 11.7 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22533657237385654		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.22533657237385654 | validation: 0.24608329781412352]
	TIME [epoch: 11.7 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31411337096615943		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.31411337096615943 | validation: 0.2978882281537482]
	TIME [epoch: 11.7 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27715507174585413		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.27715507174585413 | validation: 0.18242830179968256]
	TIME [epoch: 11.7 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20872403695889044		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.20872403695889044 | validation: 0.24223215555540462]
	TIME [epoch: 11.7 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25096466618959334		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.25096466618959334 | validation: 0.21422096865756243]
	TIME [epoch: 11.7 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30906917948874274		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.30906917948874274 | validation: 0.3557510597599485]
	TIME [epoch: 11.7 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21132535755202		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.21132535755202 | validation: 0.37480765135104516]
	TIME [epoch: 11.7 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2737342967085352		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.2737342967085352 | validation: 0.28302007724035694]
	TIME [epoch: 11.7 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24065330865994974		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.24065330865994974 | validation: 0.23783937871282312]
	TIME [epoch: 11.7 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.228597250856664		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.228597250856664 | validation: 0.22775714497351257]
	TIME [epoch: 11.7 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2132415322538348		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.2132415322538348 | validation: 0.15852517503088964]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_224.pth
	Model improved!!!
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.325850262493002		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.325850262493002 | validation: 0.38211159646385373]
	TIME [epoch: 11.7 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2673190249399686		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.2673190249399686 | validation: 0.13284024485002938]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2101204114148957		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.2101204114148957 | validation: 0.21276287649578512]
	TIME [epoch: 11.7 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18594645920746986		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.18594645920746986 | validation: 0.13163208471865623]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_228.pth
	Model improved!!!
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20611689770659697		[learning rate: 0.0053088]
	Learning Rate: 0.00530884
	LOSS [training: 0.20611689770659697 | validation: 0.24002301542622528]
	TIME [epoch: 11.7 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23387669954819967		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.23387669954819967 | validation: 0.23503253232147098]
	TIME [epoch: 11.7 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3117838910920558		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.3117838910920558 | validation: 0.1590417669246854]
	TIME [epoch: 11.7 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19646321784258522		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.19646321784258522 | validation: 0.18664494102226784]
	TIME [epoch: 11.8 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18975670711827106		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.18975670711827106 | validation: 0.1526736083479336]
	TIME [epoch: 11.7 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27873628666903427		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.27873628666903427 | validation: 0.32578892188885256]
	TIME [epoch: 11.7 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2221814922271441		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.2221814922271441 | validation: 0.20412954177528075]
	TIME [epoch: 11.7 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1698720334289484		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.1698720334289484 | validation: 0.3928773310490326]
	TIME [epoch: 11.7 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2163351271144444		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.2163351271144444 | validation: 0.4903313765715691]
	TIME [epoch: 11.7 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28995506310446745		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.28995506310446745 | validation: 0.2727530962062102]
	TIME [epoch: 11.7 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18449275255086445		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.18449275255086445 | validation: 0.21244136241197747]
	TIME [epoch: 11.7 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20131770467625626		[learning rate: 0.005106]
	Learning Rate: 0.00510595
	LOSS [training: 0.20131770467625626 | validation: 0.36487831905607127]
	TIME [epoch: 11.7 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.268981076172926		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.268981076172926 | validation: 0.12287975856705102]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_241.pth
	Model improved!!!
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18373279536833362		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.18373279536833362 | validation: 0.3027352404513043]
	TIME [epoch: 11.7 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2153449232359022		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.2153449232359022 | validation: 0.26380870489560426]
	TIME [epoch: 11.7 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21266587367438738		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.21266587367438738 | validation: 0.2043067404069948]
	TIME [epoch: 11.7 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23719645602836162		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.23719645602836162 | validation: 0.2711571824684874]
	TIME [epoch: 11.7 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1710741959006749		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.1710741959006749 | validation: 0.10026564619264562]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17516689504773503		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.17516689504773503 | validation: 0.1777862815570109]
	TIME [epoch: 11.7 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18623742422798267		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.18623742422798267 | validation: 0.2507682232043493]
	TIME [epoch: 11.7 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19602793673479332		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.19602793673479332 | validation: 0.11566780453589182]
	TIME [epoch: 11.7 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1921944935423939		[learning rate: 0.0049282]
	Learning Rate: 0.00492825
	LOSS [training: 0.1921944935423939 | validation: 0.2961447232518518]
	TIME [epoch: 11.7 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26916906953362174		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.26916906953362174 | validation: 0.1349413242223199]
	TIME [epoch: 11.7 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18499641246157367		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.18499641246157367 | validation: 0.15203336284436084]
	TIME [epoch: 11.7 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1679803404860435		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.1679803404860435 | validation: 0.20238996930705097]
	TIME [epoch: 11.7 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1596312546183204		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.1596312546183204 | validation: 0.21625950560560983]
	TIME [epoch: 11.7 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2001081500813731		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.2001081500813731 | validation: 0.21597450779537417]
	TIME [epoch: 11.7 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1497696121794772		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.1497696121794772 | validation: 0.18244797561940646]
	TIME [epoch: 11.7 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23198078915085885		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.23198078915085885 | validation: 0.1382524616419453]
	TIME [epoch: 11.7 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15425312744915773		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.15425312744915773 | validation: 0.5455774230561642]
	TIME [epoch: 11.7 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22077776526164994		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.22077776526164994 | validation: 0.16832537597930203]
	TIME [epoch: 11.7 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1478679159648003		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.1478679159648003 | validation: 0.18070196325678278]
	TIME [epoch: 12.1 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23273483658956312		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.23273483658956312 | validation: 0.31884802028290715]
	TIME [epoch: 11.7 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22583599434847812		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.22583599434847812 | validation: 0.14986659741117356]
	TIME [epoch: 11.7 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2242037563667098		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.2242037563667098 | validation: 0.13534024580260684]
	TIME [epoch: 11.7 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1533875795515816		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.1533875795515816 | validation: 0.13470326906565439]
	TIME [epoch: 11.7 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16150061852519404		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.16150061852519404 | validation: 0.1432191494941717]
	TIME [epoch: 12.2 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12658957831431056		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.12658957831431056 | validation: 0.21173849564371786]
	TIME [epoch: 11.7 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1879642207133833		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.1879642207133833 | validation: 0.1275156886978922]
	TIME [epoch: 11.7 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11986666413316961		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.11986666413316961 | validation: 0.07854997431088029]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10980761859375493		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.10980761859375493 | validation: 0.36039736373383874]
	TIME [epoch: 11.7 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2350024898993787		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.2350024898993787 | validation: 0.12843711727083626]
	TIME [epoch: 11.7 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18443029919574416		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.18443029919574416 | validation: 0.17908231905074823]
	TIME [epoch: 11.7 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14390131549021878		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.14390131549021878 | validation: 0.20167973060328911]
	TIME [epoch: 11.7 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14626396703769168		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.14626396703769168 | validation: 0.22272913626223528]
	TIME [epoch: 11.7 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23747389083340015		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.23747389083340015 | validation: 0.17670254788506423]
	TIME [epoch: 11.7 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16056314266223656		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.16056314266223656 | validation: 0.12302733101113474]
	TIME [epoch: 11.7 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15739219265089183		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.15739219265089183 | validation: 0.30486617852163406]
	TIME [epoch: 11.7 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18795936538627503		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.18795936538627503 | validation: 0.11950430950055141]
	TIME [epoch: 11.7 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16263112497489457		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.16263112497489457 | validation: 0.10820268829472152]
	TIME [epoch: 11.7 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1710010894412305		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.1710010894412305 | validation: 0.10955575558511248]
	TIME [epoch: 11.7 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.099018333890874		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.099018333890874 | validation: 0.19271689144239473]
	TIME [epoch: 11.7 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22108703237159025		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.22108703237159025 | validation: 0.18600687523059944]
	TIME [epoch: 11.7 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17086748402031487		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.17086748402031487 | validation: 0.17792631430343964]
	TIME [epoch: 11.7 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21432591077870264		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.21432591077870264 | validation: 0.19640222525329787]
	TIME [epoch: 11.7 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14735072059946724		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.14735072059946724 | validation: 0.12321736084303214]
	TIME [epoch: 11.7 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12699006496231177		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.12699006496231177 | validation: 0.09794949128778763]
	TIME [epoch: 11.7 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12312910261913174		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.12312910261913174 | validation: 0.24270761302746824]
	TIME [epoch: 11.7 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16730628699333389		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.16730628699333389 | validation: 0.18633270030068075]
	TIME [epoch: 11.7 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11151605182379298		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.11151605182379298 | validation: 0.1831469539792025]
	TIME [epoch: 11.7 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1575663719680819		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.1575663719680819 | validation: 0.39326782909440705]
	TIME [epoch: 11.7 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2420539313513106		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.2420539313513106 | validation: 0.11926894586825676]
	TIME [epoch: 11.7 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11609084460237531		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.11609084460237531 | validation: 0.1023749885755646]
	TIME [epoch: 11.7 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07623714795307701		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.07623714795307701 | validation: 0.05984676643747405]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_292.pth
	Model improved!!!
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.136213857428199		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.136213857428199 | validation: 0.1152595723707728]
	TIME [epoch: 11.7 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14715280408757495		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.14715280408757495 | validation: 0.0860832045511207]
	TIME [epoch: 11.7 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18751798716811488		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.18751798716811488 | validation: 0.10461677519935772]
	TIME [epoch: 11.7 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1068941586879476		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.1068941586879476 | validation: 0.2236249973753022]
	TIME [epoch: 11.7 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23448800173732798		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.23448800173732798 | validation: 0.2788691904543893]
	TIME [epoch: 11.7 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21715701210183286		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.21715701210183286 | validation: 0.1741972041000202]
	TIME [epoch: 11.8 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15409701685562338		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.15409701685562338 | validation: 0.31047035034482184]
	TIME [epoch: 11.7 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17405042871299775		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.17405042871299775 | validation: 0.11661133386219484]
	TIME [epoch: 11.7 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10387276726626185		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.10387276726626185 | validation: 0.09034129999300307]
	TIME [epoch: 11.7 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24247726007828474		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.24247726007828474 | validation: 0.3141369502203886]
	TIME [epoch: 11.7 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1875643224725307		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.1875643224725307 | validation: 0.22294172080439684]
	TIME [epoch: 11.7 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1521600816952378		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.1521600816952378 | validation: 0.11574865866056104]
	TIME [epoch: 11.7 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0910405941547257		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.0910405941547257 | validation: 0.16340029374939324]
	TIME [epoch: 11.7 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12081764332337022		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.12081764332337022 | validation: 0.1170684248256065]
	TIME [epoch: 11.7 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11792224237567611		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.11792224237567611 | validation: 0.14165556600978915]
	TIME [epoch: 11.7 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13641762018651693		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.13641762018651693 | validation: 0.08273871596735902]
	TIME [epoch: 11.7 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1264516630537854		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.1264516630537854 | validation: 0.11362597927574969]
	TIME [epoch: 11.7 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12057829615377744		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.12057829615377744 | validation: 0.08638080610692343]
	TIME [epoch: 11.7 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10044022625463585		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.10044022625463585 | validation: 0.20963518368541714]
	TIME [epoch: 11.7 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1488326317627862		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.1488326317627862 | validation: 0.07724383726654888]
	TIME [epoch: 11.7 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12380526006630281		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.12380526006630281 | validation: 0.18444906059737992]
	TIME [epoch: 11.7 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10762009091762373		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.10762009091762373 | validation: 0.10020858469624354]
	TIME [epoch: 11.7 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11093039132101107		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.11093039132101107 | validation: 0.14651570150679344]
	TIME [epoch: 11.7 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14768259840478498		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.14768259840478498 | validation: 0.1298578540856522]
	TIME [epoch: 12.1 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09103369720687048		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.09103369720687048 | validation: 0.11424184254533856]
	TIME [epoch: 11.7 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12892268682192626		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.12892268682192626 | validation: 0.10680370361232701]
	TIME [epoch: 11.7 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08294461719010957		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.08294461719010957 | validation: 0.052379952887094405]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_319.pth
	Model improved!!!
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12302020565733984		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.12302020565733984 | validation: 0.09217838005666752]
	TIME [epoch: 11.7 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14099212577733597		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.14099212577733597 | validation: 0.12374345822510405]
	TIME [epoch: 11.7 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15042221816918672		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.15042221816918672 | validation: 0.176016142325025]
	TIME [epoch: 11.7 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12301943747832622		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.12301943747832622 | validation: 0.08416186246311812]
	TIME [epoch: 11.7 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07994143593518244		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.07994143593518244 | validation: 0.08199067029299609]
	TIME [epoch: 11.7 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13628710140781358		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.13628710140781358 | validation: 0.058385085979830914]
	TIME [epoch: 11.7 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0975719329966131		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.0975719329966131 | validation: 0.0702298273508751]
	TIME [epoch: 11.7 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32118031491458177		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.32118031491458177 | validation: 0.36310882246722387]
	TIME [epoch: 11.7 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24804476231012723		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.24804476231012723 | validation: 0.1411068260512075]
	TIME [epoch: 11.7 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1574039759141843		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.1574039759141843 | validation: 0.10229519249774237]
	TIME [epoch: 11.7 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09951718194918824		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.09951718194918824 | validation: 0.09509566316976337]
	TIME [epoch: 11.7 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14411877460896436		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.14411877460896436 | validation: 0.07760633641353537]
	TIME [epoch: 11.7 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09315002692642302		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.09315002692642302 | validation: 0.06033652494951458]
	TIME [epoch: 12 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.102506728502658		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.102506728502658 | validation: 0.13429608146475625]
	TIME [epoch: 11.7 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10599980096008252		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.10599980096008252 | validation: 0.10008760560710606]
	TIME [epoch: 11.7 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12434620691891594		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.12434620691891594 | validation: 0.2173264683516537]
	TIME [epoch: 11.7 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11469486362827705		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.11469486362827705 | validation: 0.06819428581597128]
	TIME [epoch: 11.7 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0980864896646504		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.0980864896646504 | validation: 0.11021030138482987]
	TIME [epoch: 11.7 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0979509799404605		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.0979509799404605 | validation: 0.08907297764070024]
	TIME [epoch: 11.7 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11659021756319285		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.11659021756319285 | validation: 0.1270738463783123]
	TIME [epoch: 11.7 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09176717286776946		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.09176717286776946 | validation: 0.09239402093140808]
	TIME [epoch: 11.7 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13566870593445593		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.13566870593445593 | validation: 0.09605166701480053]
	TIME [epoch: 11.7 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07964845591903917		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.07964845591903917 | validation: 0.09172400331752698]
	TIME [epoch: 11.7 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10487338395136148		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.10487338395136148 | validation: 0.13074807300312358]
	TIME [epoch: 11.7 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10575506313482075		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.10575506313482075 | validation: 0.09183457814837873]
	TIME [epoch: 11.7 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0860902592713522		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.0860902592713522 | validation: 0.17723956996740275]
	TIME [epoch: 11.7 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11165716738082322		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.11165716738082322 | validation: 0.0740356000608124]
	TIME [epoch: 11.7 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09457486898316993		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.09457486898316993 | validation: 0.10596382387533262]
	TIME [epoch: 11.7 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09135032521390765		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.09135032521390765 | validation: 0.0630920249741587]
	TIME [epoch: 11.7 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09456617330813336		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.09456617330813336 | validation: 0.22857587235209856]
	TIME [epoch: 11.7 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1650937373281605		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.1650937373281605 | validation: 0.20060447430875444]
	TIME [epoch: 11.7 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11176689863377029		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.11176689863377029 | validation: 0.06104881787357999]
	TIME [epoch: 11.7 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08151080048674157		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.08151080048674157 | validation: 0.08100116720298381]
	TIME [epoch: 11.7 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1040992695808161		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.1040992695808161 | validation: 0.05677331068218201]
	TIME [epoch: 11.7 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07094265427546363		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.07094265427546363 | validation: 0.10006067827408968]
	TIME [epoch: 11.7 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09629964059816896		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.09629964059816896 | validation: 0.07782375077453933]
	TIME [epoch: 11.7 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07343524926352846		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.07343524926352846 | validation: 0.08433906835311944]
	TIME [epoch: 11.7 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09320655785899248		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.09320655785899248 | validation: 0.056267961546945064]
	TIME [epoch: 11.7 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08944823918689782		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.08944823918689782 | validation: 0.07864565544736324]
	TIME [epoch: 11.7 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08006262242461454		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.08006262242461454 | validation: 0.11939771232706191]
	TIME [epoch: 11.7 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10846502935817545		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.10846502935817545 | validation: 0.11932515463728297]
	TIME [epoch: 11.7 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09668207265467116		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.09668207265467116 | validation: 0.07441002877979272]
	TIME [epoch: 11.7 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07386231641699813		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.07386231641699813 | validation: 0.07370773047850668]
	TIME [epoch: 11.7 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09457494754478127		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.09457494754478127 | validation: 0.1989291831112149]
	TIME [epoch: 11.7 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10064997778649068		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.10064997778649068 | validation: 0.05097437486759235]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_364.pth
	Model improved!!!
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08331739372733263		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.08331739372733263 | validation: 0.1497426142884169]
	TIME [epoch: 11.7 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0888845548466373		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.0888845548466373 | validation: 0.0433949068893332]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_366.pth
	Model improved!!!
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07873818179720207		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.07873818179720207 | validation: 0.1823962998770831]
	TIME [epoch: 11.7 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09144065142121656		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.09144065142121656 | validation: 0.06134048198771733]
	TIME [epoch: 11.7 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09514808891554949		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.09514808891554949 | validation: 0.12663052280376857]
	TIME [epoch: 11.7 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11520112257898227		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.11520112257898227 | validation: 0.0775161801119425]
	TIME [epoch: 11.7 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09320619782497494		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.09320619782497494 | validation: 0.10277674749934836]
	TIME [epoch: 11.7 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08227472500407239		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.08227472500407239 | validation: 0.06410566922514283]
	TIME [epoch: 11.7 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06152490492074334		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.06152490492074334 | validation: 0.08967520246050155]
	TIME [epoch: 11.7 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09946797747157227		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.09946797747157227 | validation: 0.11117562923489514]
	TIME [epoch: 11.7 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08455366055469815		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.08455366055469815 | validation: 0.06175342810329132]
	TIME [epoch: 11.7 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06304302526242211		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.06304302526242211 | validation: 0.055086527110139547]
	TIME [epoch: 11.7 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08965324095100757		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.08965324095100757 | validation: 0.05596068799589504]
	TIME [epoch: 11.7 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07075049133942043		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.07075049133942043 | validation: 0.0660267826754583]
	TIME [epoch: 11.7 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0882183142008394		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.0882183142008394 | validation: 0.057465659607062736]
	TIME [epoch: 11.7 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06759284884698553		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.06759284884698553 | validation: 0.05999725848672241]
	TIME [epoch: 11.7 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07692566613686698		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.07692566613686698 | validation: 0.06525326130886336]
	TIME [epoch: 11.7 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08419217779069413		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.08419217779069413 | validation: 0.0750934258612363]
	TIME [epoch: 11.7 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07033811240850088		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.07033811240850088 | validation: 0.12299825854174112]
	TIME [epoch: 11.7 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0691262755870771		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.0691262755870771 | validation: 0.06430381014416067]
	TIME [epoch: 11.7 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09068723434707106		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.09068723434707106 | validation: 0.07360104169921508]
	TIME [epoch: 11.7 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08181146367573092		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.08181146367573092 | validation: 0.04869139596249072]
	TIME [epoch: 11.7 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047729389105664516		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.047729389105664516 | validation: 0.0646864615518355]
	TIME [epoch: 11.7 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11056056222505574		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.11056056222505574 | validation: 0.12191013784976001]
	TIME [epoch: 11.7 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06388937386933935		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.06388937386933935 | validation: 0.033164340967733935]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_389.pth
	Model improved!!!
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06822084973081552		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.06822084973081552 | validation: 0.07820932134456685]
	TIME [epoch: 11.7 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06445969932250444		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.06445969932250444 | validation: 0.08078515512524914]
	TIME [epoch: 11.7 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0805346094170513		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.0805346094170513 | validation: 0.07476599588925749]
	TIME [epoch: 11.7 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24381689630788977		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.24381689630788977 | validation: 0.12074077876686123]
	TIME [epoch: 11.7 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09894372040103229		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.09894372040103229 | validation: 0.0566871698291855]
	TIME [epoch: 11.7 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058301138220828774		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.058301138220828774 | validation: 0.039189162147112266]
	TIME [epoch: 11.7 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050932874669193454		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.050932874669193454 | validation: 0.0652936111184344]
	TIME [epoch: 11.7 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08237042743348387		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.08237042743348387 | validation: 0.03601555756157135]
	TIME [epoch: 11.7 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048660876191286964		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.048660876191286964 | validation: 0.06744277855213027]
	TIME [epoch: 11.7 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04619887640695618		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.04619887640695618 | validation: 0.03804665539635173]
	TIME [epoch: 11.7 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07636471331956594		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.07636471331956594 | validation: 0.09089326506126201]
	TIME [epoch: 11.8 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11745549739844852		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.11745549739844852 | validation: 0.09388040568186692]
	TIME [epoch: 11.7 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06382559074618838		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.06382559074618838 | validation: 0.06901008675777498]
	TIME [epoch: 11.7 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06384826162613014		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.06384826162613014 | validation: 0.08441585946616988]
	TIME [epoch: 11.7 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06504945528456293		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.06504945528456293 | validation: 0.06409790690706665]
	TIME [epoch: 11.7 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07930680474799272		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.07930680474799272 | validation: 0.045177334862699696]
	TIME [epoch: 11.7 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060483634547390516		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.060483634547390516 | validation: 0.07730956109354423]
	TIME [epoch: 11.7 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08080385407351832		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.08080385407351832 | validation: 0.054570609195760206]
	TIME [epoch: 11.9 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05406242871213825		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.05406242871213825 | validation: 0.06667643327544973]
	TIME [epoch: 11.7 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08252436728465394		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.08252436728465394 | validation: 0.05815016736096649]
	TIME [epoch: 11.7 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05085156115402423		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.05085156115402423 | validation: 0.047455665837517605]
	TIME [epoch: 11.7 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07280783152898021		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.07280783152898021 | validation: 0.05713276621985326]
	TIME [epoch: 11.7 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06662427141204937		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.06662427141204937 | validation: 0.049751758835777954]
	TIME [epoch: 11.7 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07355756698626376		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.07355756698626376 | validation: 0.05278403570628262]
	TIME [epoch: 11.8 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06369576677114325		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.06369576677114325 | validation: 0.05209241649447499]
	TIME [epoch: 11.7 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06577586982643742		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.06577586982643742 | validation: 0.12573658409993613]
	TIME [epoch: 11.7 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09297127938612565		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.09297127938612565 | validation: 0.04688386323017765]
	TIME [epoch: 11.7 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05818798219850307		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.05818798219850307 | validation: 0.04047265181575183]
	TIME [epoch: 11.9 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04208205177539392		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.04208205177539392 | validation: 0.1078542627534867]
	TIME [epoch: 11.7 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09467631199726108		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.09467631199726108 | validation: 0.0400484799606598]
	TIME [epoch: 11.7 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04399577448004064		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.04399577448004064 | validation: 0.06360862418587901]
	TIME [epoch: 11.7 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06339941602462447		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.06339941602462447 | validation: 0.07306120241226893]
	TIME [epoch: 11.7 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05529443486127988		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.05529443486127988 | validation: 0.0811129949928055]
	TIME [epoch: 11.7 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07119159681958317		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.07119159681958317 | validation: 0.06610771846027966]
	TIME [epoch: 11.7 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04919960512505253		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.04919960512505253 | validation: 0.07898943789642564]
	TIME [epoch: 11.7 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06607567586890133		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.06607567586890133 | validation: 0.03903612961372872]
	TIME [epoch: 11.7 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06511053075975026		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.06511053075975026 | validation: 0.05379353303383885]
	TIME [epoch: 11.7 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0438555737764238		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.0438555737764238 | validation: 0.05067527580795275]
	TIME [epoch: 11.7 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08325052986446362		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.08325052986446362 | validation: 0.049552769051576705]
	TIME [epoch: 11.7 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044580708008412136		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.044580708008412136 | validation: 0.04295301284644949]
	TIME [epoch: 11.7 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06100574636776773		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.06100574636776773 | validation: 0.07121083098060708]
	TIME [epoch: 11.7 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06408522451742095		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.06408522451742095 | validation: 0.03459133166810388]
	TIME [epoch: 11.7 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035548760691288726		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.035548760691288726 | validation: 0.038664094700323115]
	TIME [epoch: 11.7 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08533978064411515		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.08533978064411515 | validation: 0.10603431630930565]
	TIME [epoch: 11.7 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07842687739180051		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.07842687739180051 | validation: 0.035591108151175756]
	TIME [epoch: 11.8 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03601034192046079		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.03601034192046079 | validation: 0.028659601340537605]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_435.pth
	Model improved!!!
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07450014844039163		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.07450014844039163 | validation: 0.05589925564244844]
	TIME [epoch: 11.7 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059924945477964175		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.059924945477964175 | validation: 0.0818811533269686]
	TIME [epoch: 11.7 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05743757138662568		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.05743757138662568 | validation: 0.09656821021486267]
	TIME [epoch: 11.7 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06289945907417817		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.06289945907417817 | validation: 0.061492737910858374]
	TIME [epoch: 11.7 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05202210443694295		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.05202210443694295 | validation: 0.04925410519536237]
	TIME [epoch: 11.7 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05407113203915128		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.05407113203915128 | validation: 0.039676300607936324]
	TIME [epoch: 11.7 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05809152587183168		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.05809152587183168 | validation: 0.038638116284632776]
	TIME [epoch: 11.7 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03651268736561536		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.03651268736561536 | validation: 0.04338674521475879]
	TIME [epoch: 11.7 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07141795137806711		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.07141795137806711 | validation: 0.04007309744554403]
	TIME [epoch: 11.7 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05123909119633808		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.05123909119633808 | validation: 0.034625585380114864]
	TIME [epoch: 11.7 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05564637710131837		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.05564637710131837 | validation: 0.06004288080966773]
	TIME [epoch: 11.7 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04768191549969108		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.04768191549969108 | validation: 0.05395805676109952]
	TIME [epoch: 11.7 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0655876587810379		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.0655876587810379 | validation: 0.03368916661160593]
	TIME [epoch: 11.7 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03993474365328258		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.03993474365328258 | validation: 0.08646371978702849]
	TIME [epoch: 11.7 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06419447651422025		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.06419447651422025 | validation: 0.03391055109644056]
	TIME [epoch: 11.7 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05159758181358804		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.05159758181358804 | validation: 0.06403045871641179]
	TIME [epoch: 11.7 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06355862484231616		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.06355862484231616 | validation: 0.03888502168768378]
	TIME [epoch: 11.7 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04939700761832519		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.04939700761832519 | validation: 0.06144954238036865]
	TIME [epoch: 11.7 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0526701129467729		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.0526701129467729 | validation: 0.03726866323065621]
	TIME [epoch: 11.7 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043084274538474174		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.043084274538474174 | validation: 0.057438309519576174]
	TIME [epoch: 11.7 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04033677159658718		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.04033677159658718 | validation: 0.03905023838747829]
	TIME [epoch: 11.7 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06803326700262943		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.06803326700262943 | validation: 0.03861814797797464]
	TIME [epoch: 11.7 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06571508807531569		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.06571508807531569 | validation: 0.050000568825497166]
	TIME [epoch: 11.7 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03288002063463534		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.03288002063463534 | validation: 0.029255490363970668]
	TIME [epoch: 11.7 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06356886589805553		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.06356886589805553 | validation: 0.06450175310600674]
	TIME [epoch: 11.7 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044859774814138866		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.044859774814138866 | validation: 0.028926822353506396]
	TIME [epoch: 11.7 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044537375471888116		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.044537375471888116 | validation: 0.05128147217884952]
	TIME [epoch: 11.7 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04669344761556289		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.04669344761556289 | validation: 0.025974811486694057]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_463.pth
	Model improved!!!
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04547578992922168		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.04547578992922168 | validation: 0.04087852284832694]
	TIME [epoch: 11.7 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0738976773870351		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.0738976773870351 | validation: 0.04991794198408221]
	TIME [epoch: 11.7 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04131109837405376		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.04131109837405376 | validation: 0.0345841413668282]
	TIME [epoch: 11.7 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04317420715113673		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.04317420715113673 | validation: 0.050770675615690486]
	TIME [epoch: 11.7 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05252484269055745		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.05252484269055745 | validation: 0.04523082513957273]
	TIME [epoch: 11.7 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05363540265480936		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.05363540265480936 | validation: 0.057429292004302204]
	TIME [epoch: 11.7 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060857053993468635		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.060857053993468635 | validation: 0.051902280493093894]
	TIME [epoch: 11.7 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06552027915722373		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.06552027915722373 | validation: 0.0493375633167992]
	TIME [epoch: 11.7 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036423346563434926		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.036423346563434926 | validation: 0.025280249930312942]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_472.pth
	Model improved!!!
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03621775680371666		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.03621775680371666 | validation: 0.055805247026049866]
	TIME [epoch: 11.7 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056374941034484524		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.056374941034484524 | validation: 0.02856376974182145]
	TIME [epoch: 11.7 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04473387242634008		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.04473387242634008 | validation: 0.03813984087304974]
	TIME [epoch: 11.7 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04681337434528628		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.04681337434528628 | validation: 0.074027422365191]
	TIME [epoch: 11.7 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0544733915940866		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.0544733915940866 | validation: 0.06786225649467147]
	TIME [epoch: 11.7 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046039732843439915		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.046039732843439915 | validation: 0.05886951088236593]
	TIME [epoch: 11.7 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05787158513672061		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.05787158513672061 | validation: 0.07688993390130693]
	TIME [epoch: 11.7 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053696964821153555		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.053696964821153555 | validation: 0.06472522397821187]
	TIME [epoch: 11.7 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03693775232488748		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.03693775232488748 | validation: 0.032626805235631495]
	TIME [epoch: 11.7 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0449150430351556		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.0449150430351556 | validation: 0.05631389125521276]
	TIME [epoch: 11.7 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05431752750201938		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.05431752750201938 | validation: 0.03445404496630045]
	TIME [epoch: 11.7 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045303593620282454		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.045303593620282454 | validation: 0.04644514432832543]
	TIME [epoch: 11.7 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03645743554859902		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.03645743554859902 | validation: 0.057225571602930216]
	TIME [epoch: 11.7 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04248342952719582		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.04248342952719582 | validation: 0.05283663789780002]
	TIME [epoch: 11.7 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05960377580909097		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.05960377580909097 | validation: 0.04535638796800694]
	TIME [epoch: 11.7 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04995898910372826		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.04995898910372826 | validation: 0.043387235647332587]
	TIME [epoch: 11.7 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03893746173444434		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.03893746173444434 | validation: 0.08112072924993356]
	TIME [epoch: 11.7 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049256221575158785		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.049256221575158785 | validation: 0.02945123868078965]
	TIME [epoch: 11.7 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03509254253954086		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.03509254253954086 | validation: 0.04256549194535021]
	TIME [epoch: 11.7 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044065019978484964		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.044065019978484964 | validation: 0.024409454088295743]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_492.pth
	Model improved!!!
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037610614562480583		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.037610614562480583 | validation: 0.11365023707086305]
	TIME [epoch: 11.7 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06760862220417532		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.06760862220417532 | validation: 0.06707798950131258]
	TIME [epoch: 11.7 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04213269459700186		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.04213269459700186 | validation: 0.04294249187753789]
	TIME [epoch: 11.7 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04503087977338792		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.04503087977338792 | validation: 0.08923451065061452]
	TIME [epoch: 11.7 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05067807838715455		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.05067807838715455 | validation: 0.03262690916707764]
	TIME [epoch: 11.7 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035443363091237516		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.035443363091237516 | validation: 0.03130024316002515]
	TIME [epoch: 11.7 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04856065019614472		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.04856065019614472 | validation: 0.04399259359117447]
	TIME [epoch: 11.7 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03773421403323942		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.03773421403323942 | validation: 0.038931762751691495]
	TIME [epoch: 11.7 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04883017574058189		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.04883017574058189 | validation: 0.04974860925170928]
	TIME [epoch: 397 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03973983795192431		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.03973983795192431 | validation: 0.03252610657405694]
	TIME [epoch: 25.3 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0371795571990877		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.0371795571990877 | validation: 0.027634178256556553]
	TIME [epoch: 25.2 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04314163833417566		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.04314163833417566 | validation: 0.05696357485212378]
	TIME [epoch: 25.2 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0378518702870324		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.0378518702870324 | validation: 0.04124458570308469]
	TIME [epoch: 25.2 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049452018637605434		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.049452018637605434 | validation: 0.05273903910422318]
	TIME [epoch: 25.2 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0427882989206099		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.0427882989206099 | validation: 0.02272715361038706]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_507.pth
	Model improved!!!
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1102708069042896		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.1102708069042896 | validation: 0.11066581729519703]
	TIME [epoch: 25.2 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05898077344000064		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.05898077344000064 | validation: 0.03408666232652954]
	TIME [epoch: 25.2 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029094679018008898		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.029094679018008898 | validation: 0.03580830168134809]
	TIME [epoch: 25.2 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03456999602562462		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.03456999602562462 | validation: 0.05479905890670895]
	TIME [epoch: 25.2 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042742855771137934		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.042742855771137934 | validation: 0.024743787040678006]
	TIME [epoch: 25.2 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02741467731897267		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.02741467731897267 | validation: 0.039658369547384706]
	TIME [epoch: 25.2 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046985385740882166		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.046985385740882166 | validation: 0.0635503242492908]
	TIME [epoch: 25.2 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052526512954248886		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.052526512954248886 | validation: 0.028033623144145047]
	TIME [epoch: 25.2 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06118798070737955		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.06118798070737955 | validation: 0.06997557180830372]
	TIME [epoch: 25.2 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04056646183643335		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.04056646183643335 | validation: 0.03480538615901663]
	TIME [epoch: 25.2 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04030883131303398		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.04030883131303398 | validation: 0.027532471241354294]
	TIME [epoch: 25.2 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031157203029640396		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.031157203029640396 | validation: 0.04563808431513236]
	TIME [epoch: 25.2 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05111018153590257		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.05111018153590257 | validation: 0.03241780479418965]
	TIME [epoch: 25.2 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03776632559743351		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.03776632559743351 | validation: 0.0337280555374611]
	TIME [epoch: 25.2 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029520532940698296		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.029520532940698296 | validation: 0.03081453938388049]
	TIME [epoch: 25.2 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04014004510930675		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.04014004510930675 | validation: 0.030847973061202454]
	TIME [epoch: 25.2 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04968338379317426		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.04968338379317426 | validation: 0.03027681981383607]
	TIME [epoch: 25.2 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03538241743660936		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.03538241743660936 | validation: 0.03049141114135444]
	TIME [epoch: 25.2 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02911063564089614		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.02911063564089614 | validation: 0.026703331023601916]
	TIME [epoch: 25.2 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045446978910060824		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.045446978910060824 | validation: 0.04745464790341573]
	TIME [epoch: 25.2 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033766192417709014		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.033766192417709014 | validation: 0.026079792240849598]
	TIME [epoch: 25.2 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026491955788108697		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.026491955788108697 | validation: 0.0407335691358168]
	TIME [epoch: 25.2 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042593960456947896		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.042593960456947896 | validation: 0.05123340255885979]
	TIME [epoch: 25.2 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06060804452025013		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.06060804452025013 | validation: 0.065157465639548]
	TIME [epoch: 25.2 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042940723896062155		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.042940723896062155 | validation: 0.025867267297873336]
	TIME [epoch: 25.2 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02818739278226249		[learning rate: 0.0018085]
	Learning Rate: 0.00180846
	LOSS [training: 0.02818739278226249 | validation: 0.0344463135407127]
	TIME [epoch: 25.2 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04550779090140592		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.04550779090140592 | validation: 0.03424033431527524]
	TIME [epoch: 25.2 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027818341521751548		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.027818341521751548 | validation: 0.02520202650099373]
	TIME [epoch: 25.2 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04217450814884751		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.04217450814884751 | validation: 0.030773987784840383]
	TIME [epoch: 25.2 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033127609381534015		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.033127609381534015 | validation: 0.026517321876320738]
	TIME [epoch: 25.2 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029355002437369923		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.029355002437369923 | validation: 0.060174304847148155]
	TIME [epoch: 25.2 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046207365824965936		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.046207365824965936 | validation: 0.037297283292764924]
	TIME [epoch: 25.2 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035881413266707594		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.035881413266707594 | validation: 0.044315664956982456]
	TIME [epoch: 25.2 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035302617010648824		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.035302617010648824 | validation: 0.044262904714076165]
	TIME [epoch: 25.2 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03109660460031143		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.03109660460031143 | validation: 0.04891379386354361]
	TIME [epoch: 25.2 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04496682873128703		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.04496682873128703 | validation: 0.02967228095927637]
	TIME [epoch: 25.2 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031390868590331716		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.031390868590331716 | validation: 0.062045767479591576]
	TIME [epoch: 25.2 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05335710143181406		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.05335710143181406 | validation: 0.03466351743693049]
	TIME [epoch: 25.2 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030502037709098772		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.030502037709098772 | validation: 0.025847623022264052]
	TIME [epoch: 25.2 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030685909991690966		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.030685909991690966 | validation: 0.035200527055270464]
	TIME [epoch: 25.2 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03458751495724368		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.03458751495724368 | validation: 0.03016047351158945]
	TIME [epoch: 25.2 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047143645135405296		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.047143645135405296 | validation: 0.03225654559068893]
	TIME [epoch: 25.2 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02743269484394481		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.02743269484394481 | validation: 0.03103793079915835]
	TIME [epoch: 25.2 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03567430572219137		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.03567430572219137 | validation: 0.046084610634266485]
	TIME [epoch: 25.3 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03971493198501965		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.03971493198501965 | validation: 0.03312015673155906]
	TIME [epoch: 25.2 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03427027286784203		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.03427027286784203 | validation: 0.051665280625627114]
	TIME [epoch: 25.2 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03105706271652209		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.03105706271652209 | validation: 0.03618302433674177]
	TIME [epoch: 25.2 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03506499235567052		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.03506499235567052 | validation: 0.04543610806058837]
	TIME [epoch: 25.2 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036784721063558255		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.036784721063558255 | validation: 0.03227298414918642]
	TIME [epoch: 25.2 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030777529605932632		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.030777529605932632 | validation: 0.0678046872349]
	TIME [epoch: 25.2 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047655184313234754		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.047655184313234754 | validation: 0.0314891955883773]
	TIME [epoch: 25.2 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029947541841091115		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.029947541841091115 | validation: 0.03516218838246121]
	TIME [epoch: 25.2 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03828989455030382		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.03828989455030382 | validation: 0.03907397254246213]
	TIME [epoch: 25.2 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02750423593231925		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.02750423593231925 | validation: 0.028640499463827032]
	TIME [epoch: 25.2 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02873205831156842		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.02873205831156842 | validation: 0.044429785326737664]
	TIME [epoch: 25.2 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044805123811988895		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.044805123811988895 | validation: 0.04163572401317654]
	TIME [epoch: 25.2 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028218075718925248		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.028218075718925248 | validation: 0.030270789107679115]
	TIME [epoch: 25.2 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033038422130746745		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.033038422130746745 | validation: 0.035396396436431415]
	TIME [epoch: 25.2 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032854148968627525		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.032854148968627525 | validation: 0.02877568730685465]
	TIME [epoch: 25.2 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02525072595660593		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.02525072595660593 | validation: 0.02884940918562886]
	TIME [epoch: 25.2 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02503238435510733		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.02503238435510733 | validation: 0.02818237328754669]
	TIME [epoch: 25.3 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11521523525975938		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.11521523525975938 | validation: 0.14669180214597993]
	TIME [epoch: 25.2 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057705475386383875		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.057705475386383875 | validation: 0.04076313097056671]
	TIME [epoch: 25.2 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02953253525751983		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.02953253525751983 | validation: 0.02887737832404509]
	TIME [epoch: 25.2 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023928204337488594		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.023928204337488594 | validation: 0.021653893702526083]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_572.pth
	Model improved!!!
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03363489114218816		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.03363489114218816 | validation: 0.032930684838946005]
	TIME [epoch: 25.2 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029822434492706253		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.029822434492706253 | validation: 0.04394535515736363]
	TIME [epoch: 25.2 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029727921501593823		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.029727921501593823 | validation: 0.02708909187094989]
	TIME [epoch: 25.2 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0224232954472701		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.0224232954472701 | validation: 0.030550162724024766]
	TIME [epoch: 25.2 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039192309653490125		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.039192309653490125 | validation: 0.038097912139067575]
	TIME [epoch: 25.2 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02644010274366737		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.02644010274366737 | validation: 0.04624761813048251]
	TIME [epoch: 25.2 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043632729734000626		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.043632729734000626 | validation: 0.024593700429821126]
	TIME [epoch: 25.2 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02683149243756521		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.02683149243756521 | validation: 0.022595064485666283]
	TIME [epoch: 25.2 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03164968540539139		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.03164968540539139 | validation: 0.035290093249263674]
	TIME [epoch: 25.2 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031831897434125955		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.031831897434125955 | validation: 0.04600792862873038]
	TIME [epoch: 25.2 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029004316136815155		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.029004316136815155 | validation: 0.021906600775001932]
	TIME [epoch: 25.2 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02882270440285773		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.02882270440285773 | validation: 0.03424466938251858]
	TIME [epoch: 25.2 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036171389073895714		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.036171389073895714 | validation: 0.02481187132309261]
	TIME [epoch: 25.2 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024318079406162874		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.024318079406162874 | validation: 0.04929728816692132]
	TIME [epoch: 25.2 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03677088470353569		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.03677088470353569 | validation: 0.03810524898394911]
	TIME [epoch: 25.2 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026330654875840506		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.026330654875840506 | validation: 0.0360513044335376]
	TIME [epoch: 25.2 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03128177818818529		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.03128177818818529 | validation: 0.03175620259302058]
	TIME [epoch: 25.2 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03022878823577016		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.03022878823577016 | validation: 0.0298884653390509]
	TIME [epoch: 25.2 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02477888252245576		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.02477888252245576 | validation: 0.020228360768459033]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_591.pth
	Model improved!!!
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04085304232417736		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.04085304232417736 | validation: 0.032963915628232775]
	TIME [epoch: 25.2 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026140816819889653		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.026140816819889653 | validation: 0.030190255267339358]
	TIME [epoch: 25.2 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0516128970359328		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.0516128970359328 | validation: 0.03476220789149959]
	TIME [epoch: 25.2 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027121663550195706		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.027121663550195706 | validation: 0.022959006334984138]
	TIME [epoch: 25.2 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02849382269686207		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.02849382269686207 | validation: 0.026660202991153674]
	TIME [epoch: 25.2 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023701579229172452		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.023701579229172452 | validation: 0.022869639115373118]
	TIME [epoch: 25.2 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037307598856925694		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.037307598856925694 | validation: 0.026921192019847863]
	TIME [epoch: 25.2 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03240108906739504		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.03240108906739504 | validation: 0.030980927041264025]
	TIME [epoch: 25.2 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02440847265262342		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.02440847265262342 | validation: 0.03656790280593328]
	TIME [epoch: 25.2 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027657560045766874		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.027657560045766874 | validation: 0.02887298263652971]
	TIME [epoch: 25.2 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027814721462995365		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.027814721462995365 | validation: 0.03424331240312747]
	TIME [epoch: 25.2 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039536719576966146		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.039536719576966146 | validation: 0.03287728785044765]
	TIME [epoch: 25.2 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022504344986588133		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.022504344986588133 | validation: 0.02096568992348228]
	TIME [epoch: 25.2 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06270278593934585		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.06270278593934585 | validation: 0.03850588728750734]
	TIME [epoch: 25.2 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030222863506226327		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.030222863506226327 | validation: 0.02197781553525237]
	TIME [epoch: 25.2 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0218601965096393		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.0218601965096393 | validation: 0.025145409624335154]
	TIME [epoch: 25.2 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023460491373721		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.023460491373721 | validation: 0.028488627476369432]
	TIME [epoch: 25.2 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03599346411673871		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.03599346411673871 | validation: 0.03300086834260067]
	TIME [epoch: 25.2 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029646026228863503		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.029646026228863503 | validation: 0.022346060211991363]
	TIME [epoch: 25.2 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02720287964462074		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.02720287964462074 | validation: 0.05180670831331484]
	TIME [epoch: 25.2 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03105718074823023		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.03105718074823023 | validation: 0.020615170020638725]
	TIME [epoch: 25.2 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025557758884925147		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.025557758884925147 | validation: 0.03632769301361331]
	TIME [epoch: 25.2 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038063371588553475		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.038063371588553475 | validation: 0.03458618744031319]
	TIME [epoch: 25.2 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02542465743745599		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.02542465743745599 | validation: 0.021399239345305472]
	TIME [epoch: 25.4 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025465590518209614		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.025465590518209614 | validation: 0.026499074721744786]
	TIME [epoch: 25.3 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03135312006987646		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.03135312006987646 | validation: 0.04538554654199105]
	TIME [epoch: 25.3 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03188616424790319		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.03188616424790319 | validation: 0.020818961985200293]
	TIME [epoch: 25.2 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021861756315533026		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.021861756315533026 | validation: 0.019900529557493413]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_619.pth
	Model improved!!!
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022653204250351798		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.022653204250351798 | validation: 0.031469698162980446]
	TIME [epoch: 25.2 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03437284930488632		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.03437284930488632 | validation: 0.018526851246694533]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_621.pth
	Model improved!!!
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030586117868334362		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.030586117868334362 | validation: 0.03215226671531931]
	TIME [epoch: 25.2 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029195060508386404		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.029195060508386404 | validation: 0.021318656776404113]
	TIME [epoch: 25.2 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02339441733957168		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.02339441733957168 | validation: 0.03225708973988133]
	TIME [epoch: 25.2 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028875893256526952		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.028875893256526952 | validation: 0.03597832839036291]
	TIME [epoch: 25.2 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029713560000269024		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.029713560000269024 | validation: 0.02277179372128966]
	TIME [epoch: 25.2 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02777093314697461		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.02777093314697461 | validation: 0.032068786873337446]
	TIME [epoch: 25.2 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02693464993102205		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.02693464993102205 | validation: 0.02762637605492288]
	TIME [epoch: 25.2 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02920640194582027		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.02920640194582027 | validation: 0.02397654998768004]
	TIME [epoch: 25.2 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020992496973383096		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.020992496973383096 | validation: 0.022127056242320718]
	TIME [epoch: 25.2 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025429420102798594		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.025429420102798594 | validation: 0.037102576131313576]
	TIME [epoch: 25.2 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025563365034527115		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.025563365034527115 | validation: 0.026892886052537427]
	TIME [epoch: 25.2 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0232098968331206		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.0232098968331206 | validation: 0.02178792622249181]
	TIME [epoch: 25.2 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031139051754188136		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.031139051754188136 | validation: 0.07338617950233679]
	TIME [epoch: 25.2 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03504290198773837		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.03504290198773837 | validation: 0.02423334892550351]
	TIME [epoch: 25.3 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020955254745813195		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.020955254745813195 | validation: 0.02395827737511528]
	TIME [epoch: 25.2 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024393961932166516		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.024393961932166516 | validation: 0.03370944247486482]
	TIME [epoch: 25.2 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025931175281989106		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.025931175281989106 | validation: 0.02507926028997428]
	TIME [epoch: 25.3 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024221449221181076		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.024221449221181076 | validation: 0.027497694644782387]
	TIME [epoch: 25.2 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030664709812516393		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.030664709812516393 | validation: 0.02448753932085084]
	TIME [epoch: 25.2 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02377258352321615		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.02377258352321615 | validation: 0.02409985451806132]
	TIME [epoch: 25.2 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02685736127857074		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.02685736127857074 | validation: 0.04393269081031467]
	TIME [epoch: 25.3 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030807887070805267		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.030807887070805267 | validation: 0.028019485905924305]
	TIME [epoch: 25.2 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024913638620951497		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.024913638620951497 | validation: 0.028651783034295455]
	TIME [epoch: 25.2 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025937435575410797		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.025937435575410797 | validation: 0.023153289648083736]
	TIME [epoch: 25.2 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023090751675920528		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.023090751675920528 | validation: 0.02586188101708825]
	TIME [epoch: 25.2 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023070907478620558		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.023070907478620558 | validation: 0.04755730145318117]
	TIME [epoch: 25.2 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02750453496261629		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.02750453496261629 | validation: 0.026151922535315168]
	TIME [epoch: 25.3 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02895700381195965		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.02895700381195965 | validation: 0.028133191489169228]
	TIME [epoch: 25.2 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034020959414478004		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.034020959414478004 | validation: 0.024972572819059735]
	TIME [epoch: 25.3 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02443435928813554		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.02443435928813554 | validation: 0.027588300766062952]
	TIME [epoch: 25.5 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02171729692744618		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.02171729692744618 | validation: 0.022350751381726867]
	TIME [epoch: 25.3 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02987516485446133		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.02987516485446133 | validation: 0.029525359607118905]
	TIME [epoch: 25.2 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023190900424656446		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.023190900424656446 | validation: 0.03362601242619237]
	TIME [epoch: 25.2 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024039724495502474		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.024039724495502474 | validation: 0.021683827747680906]
	TIME [epoch: 25.2 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02185637388357571		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.02185637388357571 | validation: 0.030423732833402765]
	TIME [epoch: 25.3 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02408634680741956		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.02408634680741956 | validation: 0.021993579600141765]
	TIME [epoch: 25.2 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02476380357091254		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.02476380357091254 | validation: 0.02321484540672015]
	TIME [epoch: 25.3 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02479183372678424		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.02479183372678424 | validation: 0.02385463141919178]
	TIME [epoch: 25.3 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02038824534112512		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.02038824534112512 | validation: 0.026131851059690913]
	TIME [epoch: 25.2 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0263365584239003		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.0263365584239003 | validation: 0.02484053630048217]
	TIME [epoch: 25.2 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020734777509581144		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.020734777509581144 | validation: 0.027156301900822646]
	TIME [epoch: 25.2 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027283248322646923		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.027283248322646923 | validation: 0.02812684886326864]
	TIME [epoch: 25.2 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027716451131077444		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.027716451131077444 | validation: 0.031493719024853914]
	TIME [epoch: 25.2 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01989812475358694		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.01989812475358694 | validation: 0.01767337716228153]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_665.pth
	Model improved!!!
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020548457641981436		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.020548457641981436 | validation: 0.026820532885089995]
	TIME [epoch: 25.2 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02799911403039272		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.02799911403039272 | validation: 0.019278846948879616]
	TIME [epoch: 25.2 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025332331902018723		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.025332331902018723 | validation: 0.021242555834928258]
	TIME [epoch: 25.2 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023990294371837945		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.023990294371837945 | validation: 0.025574771232740483]
	TIME [epoch: 25.2 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02505524095458205		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.02505524095458205 | validation: 0.03733893245358509]
	TIME [epoch: 25.2 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027134828089831683		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.027134828089831683 | validation: 0.01909253548854025]
	TIME [epoch: 25.2 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018829470338278535		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.018829470338278535 | validation: 0.022399061031851568]
	TIME [epoch: 25.2 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020000787441062155		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.020000787441062155 | validation: 0.02064497056340124]
	TIME [epoch: 25.2 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027841217273755846		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.027841217273755846 | validation: 0.03150774506399265]
	TIME [epoch: 25.3 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021455704747108662		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.021455704747108662 | validation: 0.02425545248857354]
	TIME [epoch: 25.2 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022921820263130777		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.022921820263130777 | validation: 0.02996619436341677]
	TIME [epoch: 25.2 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022373514134418508		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.022373514134418508 | validation: 0.022718379481751795]
	TIME [epoch: 25.2 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020816900782642266		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.020816900782642266 | validation: 0.022061353539529295]
	TIME [epoch: 25.3 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026634754603314652		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.026634754603314652 | validation: 0.023252090387633106]
	TIME [epoch: 25.2 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0221350512561155		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.0221350512561155 | validation: 0.03272569572732031]
	TIME [epoch: 25.2 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022334830146886857		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.022334830146886857 | validation: 0.02374293689843885]
	TIME [epoch: 25.2 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02320511964578019		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.02320511964578019 | validation: 0.018171405288633855]
	TIME [epoch: 25.2 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02540066063000275		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.02540066063000275 | validation: 0.028630910251790022]
	TIME [epoch: 25.2 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022239224145236908		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.022239224145236908 | validation: 0.02085873935486833]
	TIME [epoch: 25.2 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01826982830286048		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.01826982830286048 | validation: 0.019235031773496346]
	TIME [epoch: 25.2 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023115743547152838		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.023115743547152838 | validation: 0.02045297949732288]
	TIME [epoch: 25.2 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028023109022536324		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.028023109022536324 | validation: 0.01855960439921516]
	TIME [epoch: 25.2 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020230014918430288		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.020230014918430288 | validation: 0.01829833324186536]
	TIME [epoch: 25.2 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01828293886358181		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.01828293886358181 | validation: 0.018954903001741318]
	TIME [epoch: 25.2 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023511822850476403		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.023511822850476403 | validation: 0.02578363403481898]
	TIME [epoch: 25.2 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02359248478448416		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.02359248478448416 | validation: 0.022291436850342863]
	TIME [epoch: 25.2 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019145565552019732		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.019145565552019732 | validation: 0.027203523712123926]
	TIME [epoch: 25.2 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026381934445543518		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.026381934445543518 | validation: 0.02115459829634638]
	TIME [epoch: 25.2 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023321781667830464		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.023321781667830464 | validation: 0.02043048133421772]
	TIME [epoch: 25.2 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01859079767881642		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.01859079767881642 | validation: 0.03200897202270402]
	TIME [epoch: 25.2 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02220297908985696		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.02220297908985696 | validation: 0.020728230488737495]
	TIME [epoch: 25.2 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02266981646065063		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.02266981646065063 | validation: 0.027336879645101363]
	TIME [epoch: 25.2 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0250065038665709		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.0250065038665709 | validation: 0.02464552683190209]
	TIME [epoch: 25.2 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022305834165836393		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.022305834165836393 | validation: 0.0221585491600578]
	TIME [epoch: 25.2 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01981323935474754		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.01981323935474754 | validation: 0.02585383604566352]
	TIME [epoch: 25.2 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020745652865634914		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.020745652865634914 | validation: 0.018384537862778376]
	TIME [epoch: 25.2 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020031057756478614		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.020031057756478614 | validation: 0.020009264403044916]
	TIME [epoch: 25.2 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021198407601925734		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.021198407601925734 | validation: 0.025511923502340897]
	TIME [epoch: 25.3 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023225711340148395		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.023225711340148395 | validation: 0.025609217048189917]
	TIME [epoch: 25.2 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02048379925730335		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.02048379925730335 | validation: 0.02613755776679711]
	TIME [epoch: 25.3 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026519530562561493		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.026519530562561493 | validation: 0.03297035069247818]
	TIME [epoch: 25.2 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022695041930334225		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.022695041930334225 | validation: 0.020492173111359608]
	TIME [epoch: 25.4 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017553038277992022		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.017553038277992022 | validation: 0.02144597819531075]
	TIME [epoch: 25.2 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02234465248139745		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.02234465248139745 | validation: 0.02129251741772527]
	TIME [epoch: 25.2 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02510340798905564		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.02510340798905564 | validation: 0.02220500419326957]
	TIME [epoch: 25.2 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01761824717781441		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.01761824717781441 | validation: 0.019771160238832353]
	TIME [epoch: 25.2 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02002690930697794		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.02002690930697794 | validation: 0.020016788535272816]
	TIME [epoch: 25.2 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019994666601755817		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.019994666601755817 | validation: 0.019899894817298015]
	TIME [epoch: 25.2 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018947570692870973		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.018947570692870973 | validation: 0.022340567880837055]
	TIME [epoch: 25.2 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021689449894415073		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.021689449894415073 | validation: 0.0204311849376142]
	TIME [epoch: 25.3 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0192926832602274		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.0192926832602274 | validation: 0.021229783637965294]
	TIME [epoch: 25.2 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024824748094487717		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.024824748094487717 | validation: 0.026089655364446195]
	TIME [epoch: 25.2 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01932281509065938		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.01932281509065938 | validation: 0.018415940365368325]
	TIME [epoch: 25.2 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020870277844531705		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.020870277844531705 | validation: 0.022463041167658272]
	TIME [epoch: 25.2 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020168746582767568		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.020168746582767568 | validation: 0.023924994397050415]
	TIME [epoch: 25.4 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021605391955514586		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.021605391955514586 | validation: 0.019276581021822642]
	TIME [epoch: 25.2 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02040121295632913		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.02040121295632913 | validation: 0.024734242514002885]
	TIME [epoch: 25.2 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020879815867325652		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.020879815867325652 | validation: 0.01916347026208115]
	TIME [epoch: 25.2 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017331426133512816		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.017331426133512816 | validation: 0.018638166800853374]
	TIME [epoch: 25.2 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022300106787200503		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.022300106787200503 | validation: 0.031738351882210025]
	TIME [epoch: 25.2 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019042160984994765		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.019042160984994765 | validation: 0.01956448259260672]
	TIME [epoch: 25.2 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02084222123425268		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.02084222123425268 | validation: 0.019575678386691678]
	TIME [epoch: 25.2 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01888481979109186		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.01888481979109186 | validation: 0.025003815302090993]
	TIME [epoch: 25.2 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020062869094480587		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.020062869094480587 | validation: 0.019928771920572137]
	TIME [epoch: 25.2 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01691241718769056		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.01691241718769056 | validation: 0.021085011365582537]
	TIME [epoch: 25.2 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02183414554493381		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.02183414554493381 | validation: 0.02260269933000184]
	TIME [epoch: 25.2 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018055074365954635		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.018055074365954635 | validation: 0.017997229346248082]
	TIME [epoch: 25.2 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019520445213249735		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.019520445213249735 | validation: 0.020292510293328462]
	TIME [epoch: 25.2 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022401469463193276		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.022401469463193276 | validation: 0.017941885723059083]
	TIME [epoch: 25.2 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01719912872421965		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.01719912872421965 | validation: 0.016738944265344766]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_735.pth
	Model improved!!!
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018713215829644376		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.018713215829644376 | validation: 0.026295669131787935]
	TIME [epoch: 25.2 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02229125488043332		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.02229125488043332 | validation: 0.01874355149423223]
	TIME [epoch: 25.2 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018556555575386473		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.018556555575386473 | validation: 0.025682289251563045]
	TIME [epoch: 25.2 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0391870308012988		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.0391870308012988 | validation: 0.02148783229199843]
	TIME [epoch: 25.2 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018129745754722433		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.018129745754722433 | validation: 0.02063916387424415]
	TIME [epoch: 25.2 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01737142293784192		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.01737142293784192 | validation: 0.021321897959115946]
	TIME [epoch: 25.2 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017908277528477217		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.017908277528477217 | validation: 0.02078260812456778]
	TIME [epoch: 25.2 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02008805636981689		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.02008805636981689 | validation: 0.016909042653786843]
	TIME [epoch: 25.2 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019116050355186036		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.019116050355186036 | validation: 0.021727528453291317]
	TIME [epoch: 25.2 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01725882088924593		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.01725882088924593 | validation: 0.018328600303972986]
	TIME [epoch: 25.2 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0229011219792836		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.0229011219792836 | validation: 0.021176831151588384]
	TIME [epoch: 25.2 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019122767652011347		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.019122767652011347 | validation: 0.015782150279644424]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_747.pth
	Model improved!!!
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017222297167692094		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.017222297167692094 | validation: 0.02102479050457739]
	TIME [epoch: 25.2 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019498326746084448		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.019498326746084448 | validation: 0.017042949538243892]
	TIME [epoch: 25.2 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017830388998125574		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.017830388998125574 | validation: 0.020785931476902372]
	TIME [epoch: 25.2 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019929695592934598		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.019929695592934598 | validation: 0.0272360866341375]
	TIME [epoch: 25.2 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018803256551903402		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.018803256551903402 | validation: 0.017637786630817793]
	TIME [epoch: 25.2 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01673757707700055		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.01673757707700055 | validation: 0.017606252958575083]
	TIME [epoch: 25.2 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01815281729019954		[learning rate: 0.00082662]
	Learning Rate: 0.000826624
	LOSS [training: 0.01815281729019954 | validation: 0.021470334069677532]
	TIME [epoch: 25.2 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020268123511405015		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.020268123511405015 | validation: 0.021990565718330354]
	TIME [epoch: 25.2 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02229009562355623		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.02229009562355623 | validation: 0.021104542927471034]
	TIME [epoch: 25.2 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01839829770478852		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.01839829770478852 | validation: 0.020388661037310973]
	TIME [epoch: 25.2 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017994136678000525		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.017994136678000525 | validation: 0.02182113528718868]
	TIME [epoch: 25.2 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018943208356058193		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.018943208356058193 | validation: 0.024020911600316518]
	TIME [epoch: 25.2 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02172442223669173		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.02172442223669173 | validation: 0.017570223342110267]
	TIME [epoch: 25.2 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0180948569370052		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.0180948569370052 | validation: 0.021885514556125182]
	TIME [epoch: 25.2 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01877928593352183		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.01877928593352183 | validation: 0.019765325694197842]
	TIME [epoch: 25.2 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01736593901245891		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.01736593901245891 | validation: 0.018904748602000504]
	TIME [epoch: 25.2 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017407953378725822		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.017407953378725822 | validation: 0.017797636025736023]
	TIME [epoch: 25.2 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015974239272317326		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.015974239272317326 | validation: 0.01905176618894057]
	TIME [epoch: 25.2 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018221019748560923		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.018221019748560923 | validation: 0.023235427531444287]
	TIME [epoch: 25.2 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020483654513909516		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.020483654513909516 | validation: 0.020691066850275046]
	TIME [epoch: 25.2 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01755054621308051		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.01755054621308051 | validation: 0.018075391313444197]
	TIME [epoch: 25.2 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020120191182110017		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.020120191182110017 | validation: 0.02461810991094996]
	TIME [epoch: 25.2 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018570074655334504		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.018570074655334504 | validation: 0.021398708074840917]
	TIME [epoch: 25.2 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01783872865802811		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.01783872865802811 | validation: 0.018350873900493357]
	TIME [epoch: 25.2 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015588704193255991		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.015588704193255991 | validation: 0.01564038045492446]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_772.pth
	Model improved!!!
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020487247539495865		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.020487247539495865 | validation: 0.024289957903074873]
	TIME [epoch: 25.2 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01648696178290275		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.01648696178290275 | validation: 0.01785508472929742]
	TIME [epoch: 25.2 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016756289149400312		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.016756289149400312 | validation: 0.018461971496304436]
	TIME [epoch: 25.2 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01808856429474908		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.01808856429474908 | validation: 0.0227066466946189]
	TIME [epoch: 25.2 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019620325082726764		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.019620325082726764 | validation: 0.02047422754566586]
	TIME [epoch: 25.2 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015482308963653574		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.015482308963653574 | validation: 0.029640836765736728]
	TIME [epoch: 25.2 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02281684870722941		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.02281684870722941 | validation: 0.01601927585130939]
	TIME [epoch: 25.2 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016481134688156554		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.016481134688156554 | validation: 0.023449252059648464]
	TIME [epoch: 25.2 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017616022639510724		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.017616022639510724 | validation: 0.019827091750066847]
	TIME [epoch: 25.2 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015766454857890397		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.015766454857890397 | validation: 0.017300326868246917]
	TIME [epoch: 25.2 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019208554099650804		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.019208554099650804 | validation: 0.02288082396257117]
	TIME [epoch: 25.2 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017116276171126953		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.017116276171126953 | validation: 0.016159747637231384]
	TIME [epoch: 25.2 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016375046684932757		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.016375046684932757 | validation: 0.019803010489344637]
	TIME [epoch: 25.2 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017216131347829244		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.017216131347829244 | validation: 0.019250805375470144]
	TIME [epoch: 25.2 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017816494466247078		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.017816494466247078 | validation: 0.02334353688309572]
	TIME [epoch: 25.2 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0159198362283132		[learning rate: 0.00073282]
	Learning Rate: 0.000732825
	LOSS [training: 0.0159198362283132 | validation: 0.01703981896682528]
	TIME [epoch: 25.2 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015748871859542646		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.015748871859542646 | validation: 0.025773193825240688]
	TIME [epoch: 25.2 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019000513579210912		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.019000513579210912 | validation: 0.025969474412130964]
	TIME [epoch: 25.2 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017348443257840226		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.017348443257840226 | validation: 0.01801990791945184]
	TIME [epoch: 25.2 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015894449175047894		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.015894449175047894 | validation: 0.02077156898074644]
	TIME [epoch: 25.2 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01817329580559052		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.01817329580559052 | validation: 0.018687540593753306]
	TIME [epoch: 25.2 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015666219892712645		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.015666219892712645 | validation: 0.01529986832145675]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_794.pth
	Model improved!!!
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015556712851060453		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.015556712851060453 | validation: 0.02046032590682326]
	TIME [epoch: 25.2 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019353152145217115		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.019353152145217115 | validation: 0.01673636769510173]
	TIME [epoch: 25.2 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015123870605543141		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.015123870605543141 | validation: 0.015535870613279074]
	TIME [epoch: 25.2 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016625298174556914		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.016625298174556914 | validation: 0.020260128898783796]
	TIME [epoch: 25.2 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015682484500575373		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.015682484500575373 | validation: 0.01881578674432267]
	TIME [epoch: 25.2 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016644271764553492		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.016644271764553492 | validation: 0.020859252633343657]
	TIME [epoch: 25.2 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016310945091812495		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.016310945091812495 | validation: 0.01900539086086988]
	TIME [epoch: 25.2 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016637023561477858		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.016637023561477858 | validation: 0.024775850993329823]
	TIME [epoch: 25.2 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01746808671730166		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.01746808671730166 | validation: 0.02290689058057238]
	TIME [epoch: 25.2 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016974578375978162		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.016974578375978162 | validation: 0.01707465701884335]
	TIME [epoch: 25.2 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01600201671471864		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.01600201671471864 | validation: 0.016179013486593608]
	TIME [epoch: 25.2 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015379903809414586		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.015379903809414586 | validation: 0.01795726863282495]
	TIME [epoch: 25.2 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018792462998959376		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.018792462998959376 | validation: 0.015297318306187883]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_807.pth
	Model improved!!!
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017999122684619263		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.017999122684619263 | validation: 0.019708155985145887]
	TIME [epoch: 25.2 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01877176406150815		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.01877176406150815 | validation: 0.021195242822692577]
	TIME [epoch: 25.2 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016822501187776007		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.016822501187776007 | validation: 0.015641636374123885]
	TIME [epoch: 25.2 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014943513225302154		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.014943513225302154 | validation: 0.01737198483121509]
	TIME [epoch: 25.2 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014299805969630262		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.014299805969630262 | validation: 0.017159554428361264]
	TIME [epoch: 25.2 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013942818501557602		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.013942818501557602 | validation: 0.019433235725675284]
	TIME [epoch: 25.2 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0182546109710274		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.0182546109710274 | validation: 0.01810606463255946]
	TIME [epoch: 25.2 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014951003452305186		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.014951003452305186 | validation: 0.017455424211015834]
	TIME [epoch: 25.2 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016099181505450224		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.016099181505450224 | validation: 0.0175021022566871]
	TIME [epoch: 25.2 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015631504839641527		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.015631504839641527 | validation: 0.016286529096750334]
	TIME [epoch: 25.2 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01756057584760393		[learning rate: 0.00065894]
	Learning Rate: 0.00065894
	LOSS [training: 0.01756057584760393 | validation: 0.01575169319786529]
	TIME [epoch: 25.2 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014955925546189703		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.014955925546189703 | validation: 0.019650005375547244]
	TIME [epoch: 25.2 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014805786756708614		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.014805786756708614 | validation: 0.017044497755532788]
	TIME [epoch: 25.2 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01656685397374475		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.01656685397374475 | validation: 0.01824867196778003]
	TIME [epoch: 25.2 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015210186315645758		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.015210186315645758 | validation: 0.02026507149632201]
	TIME [epoch: 25.2 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021540527563538817		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.021540527563538817 | validation: 0.020472855009660628]
	TIME [epoch: 25.2 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0150836262072989		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.0150836262072989 | validation: 0.01710472840482483]
	TIME [epoch: 25.2 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01571999902764206		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.01571999902764206 | validation: 0.019257072605681887]
	TIME [epoch: 25.2 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015742401653315138		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.015742401653315138 | validation: 0.020054899747979325]
	TIME [epoch: 25.2 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01707733866345171		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.01707733866345171 | validation: 0.018126969305237505]
	TIME [epoch: 25.2 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015170842228109377		[learning rate: 0.00063601]
	Learning Rate: 0.000636007
	LOSS [training: 0.015170842228109377 | validation: 0.01934585854295292]
	TIME [epoch: 25.2 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015793219762903377		[learning rate: 0.00063376]
	Learning Rate: 0.000633758
	LOSS [training: 0.015793219762903377 | validation: 0.016792155332592026]
	TIME [epoch: 25.3 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01415485581243137		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.01415485581243137 | validation: 0.015856322340000444]
	TIME [epoch: 25.2 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014810611474918831		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.014810611474918831 | validation: 0.0201032153221804]
	TIME [epoch: 25.2 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01901883299874656		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.01901883299874656 | validation: 0.05031665174142417]
	TIME [epoch: 25.2 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02894595512125693		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.02894595512125693 | validation: 0.01896268269279378]
	TIME [epoch: 25.2 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014759705841275129		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.014759705841275129 | validation: 0.01678205162134312]
	TIME [epoch: 25.2 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014106853347099633		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.014106853347099633 | validation: 0.015454305679764527]
	TIME [epoch: 25.2 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014438495924375919		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.014438495924375919 | validation: 0.020503009128290044]
	TIME [epoch: 25.2 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015920310077083444		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.015920310077083444 | validation: 0.017365074042445784]
	TIME [epoch: 25.2 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014269914001236634		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.014269914001236634 | validation: 0.018307963885961096]
	TIME [epoch: 25.2 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01762341882101974		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.01762341882101974 | validation: 0.018077558572229604]
	TIME [epoch: 25.2 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01574473539005226		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.01574473539005226 | validation: 0.01744155561687178]
	TIME [epoch: 25.2 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01633354117412024		[learning rate: 0.00060738]
	Learning Rate: 0.000607382
	LOSS [training: 0.01633354117412024 | validation: 0.020882132532107543]
	TIME [epoch: 25.2 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015651718157530258		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.015651718157530258 | validation: 0.017707663397582382]
	TIME [epoch: 25.2 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014676862335763652		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.014676862335763652 | validation: 0.016746348084395478]
	TIME [epoch: 25.2 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014782098918812879		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.014782098918812879 | validation: 0.02255099605029435]
	TIME [epoch: 25.2 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015287144468468402		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.015287144468468402 | validation: 0.01827303651424827]
	TIME [epoch: 25.2 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014676519434866406		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.014676519434866406 | validation: 0.0193536756969844]
	TIME [epoch: 25.2 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01581478163192319		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.01581478163192319 | validation: 0.015096412821516766]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_847.pth
	Model improved!!!
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013568058148643305		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.013568058148643305 | validation: 0.016081215363691687]
	TIME [epoch: 25.2 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016344475677313328		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.016344475677313328 | validation: 0.016566361768296806]
	TIME [epoch: 25.2 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014446228052495333		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.014446228052495333 | validation: 0.014799438967571141]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_850.pth
	Model improved!!!
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013583957986150601		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.013583957986150601 | validation: 0.017423726408135682]
	TIME [epoch: 25.2 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015210318474301578		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.015210318474301578 | validation: 0.01756751533196712]
	TIME [epoch: 25.2 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014420041956658036		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.014420041956658036 | validation: 0.01883367398038659]
	TIME [epoch: 25.2 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013879906566769693		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.013879906566769693 | validation: 0.017009346320905106]
	TIME [epoch: 25.2 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015072599731946051		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.015072599731946051 | validation: 0.018747517291158367]
	TIME [epoch: 25.2 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016017511564737484		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.016017511564737484 | validation: 0.021018796982546085]
	TIME [epoch: 25.2 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017263039099000857		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.017263039099000857 | validation: 0.016218138835937125]
	TIME [epoch: 25.2 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01380741337851087		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.01380741337851087 | validation: 0.015960961767988648]
	TIME [epoch: 25.2 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01585425597612135		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.01585425597612135 | validation: 0.016378851178838406]
	TIME [epoch: 25.2 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015184761795526986		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.015184761795526986 | validation: 0.015454592379471863]
	TIME [epoch: 25.2 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013780306495086329		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.013780306495086329 | validation: 0.016094403872634465]
	TIME [epoch: 25.2 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013253938955491712		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.013253938955491712 | validation: 0.016378499216698768]
	TIME [epoch: 25.2 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014737010230205141		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.014737010230205141 | validation: 0.015361983194675738]
	TIME [epoch: 25.2 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01525922057921631		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.01525922057921631 | validation: 0.01940199559389175]
	TIME [epoch: 25.2 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014340117818102288		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.014340117818102288 | validation: 0.017080320512458547]
	TIME [epoch: 25.2 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013688203990620365		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.013688203990620365 | validation: 0.015701636525010894]
	TIME [epoch: 25.2 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015414748890151639		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.015414748890151639 | validation: 0.01672765567109458]
	TIME [epoch: 25.2 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014542847143145882		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.014542847143145882 | validation: 0.017209056144502233]
	TIME [epoch: 25.2 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014607773798651809		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.014607773798651809 | validation: 0.018538003613826345]
	TIME [epoch: 25.2 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015189342468779485		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.015189342468779485 | validation: 0.016289066965442862]
	TIME [epoch: 25.2 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014491054570855151		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.014491054570855151 | validation: 0.017674793803448517]
	TIME [epoch: 25.2 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015097867254638804		[learning rate: 0.00054421]
	Learning Rate: 0.000544214
	LOSS [training: 0.015097867254638804 | validation: 0.019224500382967018]
	TIME [epoch: 25.2 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015465043971082853		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.015465043971082853 | validation: 0.014627944297636765]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_873.pth
	Model improved!!!
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013026132699278404		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.013026132699278404 | validation: 0.01661899451916883]
	TIME [epoch: 25.2 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013537338773647229		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.013537338773647229 | validation: 0.018185683650318214]
	TIME [epoch: 25.3 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015056412586227747		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.015056412586227747 | validation: 0.020476833316542202]
	TIME [epoch: 25.2 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014988738075394636		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.014988738075394636 | validation: 0.016560343430879865]
	TIME [epoch: 25.2 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0132585140304455		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.0132585140304455 | validation: 0.015451186885296828]
	TIME [epoch: 25.2 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013985422926594158		[learning rate: 0.00053088]
	Learning Rate: 0.000530885
	LOSS [training: 0.013985422926594158 | validation: 0.016457153321939]
	TIME [epoch: 25.2 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014089728522558338		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.014089728522558338 | validation: 0.01909749375359014]
	TIME [epoch: 25.2 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013739710814783559		[learning rate: 0.00052714]
	Learning Rate: 0.000527137
	LOSS [training: 0.013739710814783559 | validation: 0.017231204301081174]
	TIME [epoch: 25.2 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013802662037008757		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.013802662037008757 | validation: 0.017334868022147447]
	TIME [epoch: 25.2 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01330881948224821		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.01330881948224821 | validation: 0.0167746475644349]
	TIME [epoch: 25.2 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01486931298776822		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.01486931298776822 | validation: 0.018421730594227796]
	TIME [epoch: 25.2 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014920969459135517		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.014920969459135517 | validation: 0.017375000395372046]
	TIME [epoch: 25.2 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014780287134567957		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.014780287134567957 | validation: 0.017435417040026963]
	TIME [epoch: 25.2 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014083079787370016		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.014083079787370016 | validation: 0.01422583443644865]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_887.pth
	Model improved!!!
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012578238130739237		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.012578238130739237 | validation: 0.016583773108081255]
	TIME [epoch: 25.2 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014365247367687043		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.014365247367687043 | validation: 0.015314770310666904]
	TIME [epoch: 25.2 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013554519522443067		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.013554519522443067 | validation: 0.019473093996589766]
	TIME [epoch: 25.2 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01596285270385595		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.01596285270385595 | validation: 0.016450793671786434]
	TIME [epoch: 25.2 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0137299170261517		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.0137299170261517 | validation: 0.014587833708873125]
	TIME [epoch: 25.2 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013471122073769791		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.013471122073769791 | validation: 0.01672535263729651]
	TIME [epoch: 25.2 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014907675565531513		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.014907675565531513 | validation: 0.01470113873399101]
	TIME [epoch: 25.2 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013145609113752113		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.013145609113752113 | validation: 0.019237884938472394]
	TIME [epoch: 25.2 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014073343319983252		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.014073343319983252 | validation: 0.015148706441657016]
	TIME [epoch: 25.2 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012804711639162247		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.012804711639162247 | validation: 0.017688704069637247]
	TIME [epoch: 25.2 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01657597849606624		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.01657597849606624 | validation: 0.016833991465134875]
	TIME [epoch: 25.2 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013861547087490106		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.013861547087490106 | validation: 0.017323118526890745]
	TIME [epoch: 25.2 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013642651659668893		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.013642651659668893 | validation: 0.01481063829254958]
	TIME [epoch: 25.2 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013665621334351133		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.013665621334351133 | validation: 0.015938126451562697]
	TIME [epoch: 25.2 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014165716382523355		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.014165716382523355 | validation: 0.01874362921704797]
	TIME [epoch: 25.2 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01343073595775709		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.01343073595775709 | validation: 0.015524200191067195]
	TIME [epoch: 25.2 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013727306471692399		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.013727306471692399 | validation: 0.017601002985375005]
	TIME [epoch: 25.2 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013205835049451414		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.013205835049451414 | validation: 0.015250144516994048]
	TIME [epoch: 25.2 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013843824644883491		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.013843824644883491 | validation: 0.015252760399382014]
	TIME [epoch: 25.2 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014125597848781011		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.014125597848781011 | validation: 0.017488010771789735]
	TIME [epoch: 25.3 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01300170170541786		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.01300170170541786 | validation: 0.016966807895246922]
	TIME [epoch: 25.2 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013094610356771208		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.013094610356771208 | validation: 0.02633457620681221]
	TIME [epoch: 25.2 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017509407538705394		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.017509407538705394 | validation: 0.01671196714543787]
	TIME [epoch: 25.2 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013874243621025066		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.013874243621025066 | validation: 0.014956736766620697]
	TIME [epoch: 25.2 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012439617554054795		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.012439617554054795 | validation: 0.01478574286251025]
	TIME [epoch: 25.2 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0133675644699201		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.0133675644699201 | validation: 0.017887686871031837]
	TIME [epoch: 25.2 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013638893865293867		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.013638893865293867 | validation: 0.017660755010167]
	TIME [epoch: 25.2 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014489204443639589		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.014489204443639589 | validation: 0.014469821720267697]
	TIME [epoch: 25.2 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0123146380501691		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.0123146380501691 | validation: 0.015962983628935577]
	TIME [epoch: 25.2 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013466590180979739		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.013466590180979739 | validation: 0.015546539518884432]
	TIME [epoch: 25.2 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013079073318991871		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.013079073318991871 | validation: 0.01715277643906729]
	TIME [epoch: 25.2 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01646667600337462		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.01646667600337462 | validation: 0.04051962812928196]
	TIME [epoch: 25.2 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02105124580392347		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.02105124580392347 | validation: 0.016664806882918708]
	TIME [epoch: 25.2 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013468880509838192		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.013468880509838192 | validation: 0.013785981357568453]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_921.pth
	Model improved!!!
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015422587550868842		[learning rate: 0.00045588]
	Learning Rate: 0.000455876
	LOSS [training: 0.015422587550868842 | validation: 0.014360264362788444]
	TIME [epoch: 25.2 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012649843883423686		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.012649843883423686 | validation: 0.014577817555136565]
	TIME [epoch: 25.2 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013047771563820448		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.013047771563820448 | validation: 0.014510613363032863]
	TIME [epoch: 25.2 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012181990078597578		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.012181990078597578 | validation: 0.01679292220732799]
	TIME [epoch: 25.2 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012808143057645025		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.012808143057645025 | validation: 0.016054302414579513]
	TIME [epoch: 25.2 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012713175214045549		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.012713175214045549 | validation: 0.01451784454477996]
	TIME [epoch: 25.2 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013160636770220186		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.013160636770220186 | validation: 0.0147953872010914]
	TIME [epoch: 25.2 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01330481924674252		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.01330481924674252 | validation: 0.018541303139501662]
	TIME [epoch: 25.2 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013476949083681095		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.013476949083681095 | validation: 0.01579297965482487]
	TIME [epoch: 25.2 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012286211677833518		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.012286211677833518 | validation: 0.014344543512942788]
	TIME [epoch: 25.2 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01265706996658567		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.01265706996658567 | validation: 0.014895790375953512]
	TIME [epoch: 25.2 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01343981591813288		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.01343981591813288 | validation: 0.01601579049989132]
	TIME [epoch: 25.2 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012381934850122883		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.012381934850122883 | validation: 0.016611267058683113]
	TIME [epoch: 25.2 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013523539368885375		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.013523539368885375 | validation: 0.017236744526962007]
	TIME [epoch: 25.2 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014286980838936703		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.014286980838936703 | validation: 0.017238349190695303]
	TIME [epoch: 25.2 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014450732947680341		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.014450732947680341 | validation: 0.015322945614475954]
	TIME [epoch: 25.2 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01355786934642031		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.01355786934642031 | validation: 0.015259684673413891]
	TIME [epoch: 25.2 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01157570846952007		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.01157570846952007 | validation: 0.014100827507267651]
	TIME [epoch: 25.2 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012711785908032814		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.012711785908032814 | validation: 0.015069374259559346]
	TIME [epoch: 25.2 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013026025353360792		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.013026025353360792 | validation: 0.013178877603417153]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_941.pth
	Model improved!!!
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013085550722314937		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.013085550722314937 | validation: 0.015363205419739137]
	TIME [epoch: 25.2 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013760125770530731		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.013760125770530731 | validation: 0.016678987175971403]
	TIME [epoch: 25.2 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013245093019210522		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.013245093019210522 | validation: 0.01868724930548809]
	TIME [epoch: 25.2 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012995266595982817		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.012995266595982817 | validation: 0.01998695366076478]
	TIME [epoch: 25.2 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013152188589332054		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.013152188589332054 | validation: 0.014914271748321599]
	TIME [epoch: 25.2 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012904432259732819		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.012904432259732819 | validation: 0.017183642750444015]
	TIME [epoch: 25.2 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017348356257917728		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.017348356257917728 | validation: 0.016035202724769076]
	TIME [epoch: 25.2 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012885782542955913		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.012885782542955913 | validation: 0.015008180947344352]
	TIME [epoch: 25.2 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012144645797403831		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.012144645797403831 | validation: 0.014957850913217085]
	TIME [epoch: 25.2 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01383724490323199		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.01383724490323199 | validation: 0.014069245444751223]
	TIME [epoch: 25.2 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01290348259732552		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.01290348259732552 | validation: 0.014965450212649486]
	TIME [epoch: 25.2 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012403451110305344		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.012403451110305344 | validation: 0.013081331008067382]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_953.pth
	Model improved!!!
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012066528233822878		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.012066528233822878 | validation: 0.014941699437409348]
	TIME [epoch: 25.2 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012194435369962113		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.012194435369962113 | validation: 0.01548159867380534]
	TIME [epoch: 25.2 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013287583948773572		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.013287583948773572 | validation: 0.01646898653652613]
	TIME [epoch: 25.2 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013032472967306197		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.013032472967306197 | validation: 0.013533837289213115]
	TIME [epoch: 25.2 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012497471536109968		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.012497471536109968 | validation: 0.014594678013709124]
	TIME [epoch: 25.2 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01375460455691753		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.01375460455691753 | validation: 0.015254179811864629]
	TIME [epoch: 25.2 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012786498378783678		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.012786498378783678 | validation: 0.015220631214691319]
	TIME [epoch: 25.2 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013305285155415472		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.013305285155415472 | validation: 0.016654684457023276]
	TIME [epoch: 25.2 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012600976084205429		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.012600976084205429 | validation: 0.015519274692663395]
	TIME [epoch: 25.2 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011596253284292545		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.011596253284292545 | validation: 0.015405342612800365]
	TIME [epoch: 25.2 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012621893019568191		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.012621893019568191 | validation: 0.016812046282151268]
	TIME [epoch: 25.2 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013087626917654901		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.013087626917654901 | validation: 0.01577695863628741]
	TIME [epoch: 25.2 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012080193386333711		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.012080193386333711 | validation: 0.015313929336914892]
	TIME [epoch: 25.2 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012112773189819011		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.012112773189819011 | validation: 0.013565296054362828]
	TIME [epoch: 25.2 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014263429825563237		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.014263429825563237 | validation: 0.019788583771635955]
	TIME [epoch: 25.2 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014578241245527413		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.014578241245527413 | validation: 0.01494196191724614]
	TIME [epoch: 25.2 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011587064680507372		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.011587064680507372 | validation: 0.013785103418101005]
	TIME [epoch: 25.2 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01635704494146492		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.01635704494146492 | validation: 0.023470279647443512]
	TIME [epoch: 25.2 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015419072904894495		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.015419072904894495 | validation: 0.014983617458909526]
	TIME [epoch: 25.2 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013491010063249365		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.013491010063249365 | validation: 0.016140747562494985]
	TIME [epoch: 25.2 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012219135300586008		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.012219135300586008 | validation: 0.013414229351465074]
	TIME [epoch: 25.2 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011505611786952875		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.011505611786952875 | validation: 0.015590548354886026]
	TIME [epoch: 25.2 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012136112043393977		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.012136112043393977 | validation: 0.0163508413533364]
	TIME [epoch: 25.2 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012066102144446582		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.012066102144446582 | validation: 0.01656793522902238]
	TIME [epoch: 25.2 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012002847723022665		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.012002847723022665 | validation: 0.014997732646840015]
	TIME [epoch: 25.2 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012002126498838979		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.012002126498838979 | validation: 0.01439330045082719]
	TIME [epoch: 25.2 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011984502827427773		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.011984502827427773 | validation: 0.01626501280433339]
	TIME [epoch: 25.2 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013485434887207048		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.013485434887207048 | validation: 0.014767349275887218]
	TIME [epoch: 25.2 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012742243650224302		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.012742243650224302 | validation: 0.015043763052087553]
	TIME [epoch: 25.2 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01207774502929239		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.01207774502929239 | validation: 0.021326826758114502]
	TIME [epoch: 25.2 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01266723692031766		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.01266723692031766 | validation: 0.01655124209491411]
	TIME [epoch: 25.2 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013167455691099204		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.013167455691099204 | validation: 0.01417801673286192]
	TIME [epoch: 25.2 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011760633877158052		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.011760633877158052 | validation: 0.013514583258833462]
	TIME [epoch: 25.2 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011991657036666572		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.011991657036666572 | validation: 0.01390671059208818]
	TIME [epoch: 25.2 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01240293486428852		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.01240293486428852 | validation: 0.014777256294502689]
	TIME [epoch: 25.2 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011910358685007408		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.011910358685007408 | validation: 0.014486252049560397]
	TIME [epoch: 25.2 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011375028141936104		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.011375028141936104 | validation: 0.015851989419685165]
	TIME [epoch: 25.2 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01253861343636017		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.01253861343636017 | validation: 0.013092585612653435]
	TIME [epoch: 25.2 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012407703624782527		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.012407703624782527 | validation: 0.013326907728326995]
	TIME [epoch: 25.2 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011625772527816266		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.011625772527816266 | validation: 0.013986130396399177]
	TIME [epoch: 25.2 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011117281849401982		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.011117281849401982 | validation: 0.013709140533245508]
	TIME [epoch: 25.2 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012091250736140277		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.012091250736140277 | validation: 0.01436124004248765]
	TIME [epoch: 25.2 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012277818178810422		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.012277818178810422 | validation: 0.015465306799642786]
	TIME [epoch: 25.2 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012765985663466343		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.012765985663466343 | validation: 0.012941744319249413]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_997.pth
	Model improved!!!
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0122556679636874		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.0122556679636874 | validation: 0.014826067145761]
	TIME [epoch: 25.2 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011606596600057868		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.011606596600057868 | validation: 0.013172180329520982]
	TIME [epoch: 25.2 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011636656770262148		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.011636656770262148 | validation: 0.013953516004156282]
	TIME [epoch: 25.2 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01138883540978948		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.01138883540978948 | validation: 0.015748747315075414]
	TIME [epoch: 461 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01200876723097274		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.01200876723097274 | validation: 0.01365912206357689]
	TIME [epoch: 53.8 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012337618770672656		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.012337618770672656 | validation: 0.014863729519435357]
	TIME [epoch: 53.8 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012643092811643506		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.012643092811643506 | validation: 0.014342806284008892]
	TIME [epoch: 53.8 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01228505981462226		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.01228505981462226 | validation: 0.01335016549960644]
	TIME [epoch: 53.8 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011651107180809039		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.011651107180809039 | validation: 0.0131276051128017]
	TIME [epoch: 53.8 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01172552761756958		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.01172552761756958 | validation: 0.015062440217660844]
	TIME [epoch: 53.8 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011875595028639484		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.011875595028639484 | validation: 0.013038793837907996]
	TIME [epoch: 53.8 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01187036136706602		[learning rate: 0.00033497]
	Learning Rate: 0.000334966
	LOSS [training: 0.01187036136706602 | validation: 0.013701222580950882]
	TIME [epoch: 53.8 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011554084611917363		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.011554084611917363 | validation: 0.013702435363813105]
	TIME [epoch: 53.8 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011662470492013011		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.011662470492013011 | validation: 0.014398158197492513]
	TIME [epoch: 53.8 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013100863496288848		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.013100863496288848 | validation: 0.01351961300276595]
	TIME [epoch: 53.8 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012432102639499752		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.012432102639499752 | validation: 0.012852241501075567]
	TIME [epoch: 53.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_1013.pth
	Model improved!!!
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012009969225446952		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.012009969225446952 | validation: 0.01374494084072173]
	TIME [epoch: 53.8 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011537964864464791		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.011537964864464791 | validation: 0.013802899676607572]
	TIME [epoch: 53.8 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012423448426964549		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.012423448426964549 | validation: 0.013906383893299526]
	TIME [epoch: 53.8 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011152092335714688		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.011152092335714688 | validation: 0.013704930487136596]
	TIME [epoch: 53.8 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010878144438773269		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.010878144438773269 | validation: 0.017189275407437997]
	TIME [epoch: 53.8 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012191169218177124		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.012191169218177124 | validation: 0.014816976020540412]
	TIME [epoch: 53.8 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012284270602074751		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.012284270602074751 | validation: 0.01485765552410985]
	TIME [epoch: 53.8 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011675707140257754		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.011675707140257754 | validation: 0.014852752964873647]
	TIME [epoch: 53.8 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01158108197697185		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.01158108197697185 | validation: 0.014584332722130122]
	TIME [epoch: 53.8 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011855847968851375		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.011855847968851375 | validation: 0.014000474272251977]
	TIME [epoch: 53.8 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01148573832218758		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.01148573832218758 | validation: 0.015450510309921516]
	TIME [epoch: 53.8 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011756158987205325		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.011756158987205325 | validation: 0.014080637877237904]
	TIME [epoch: 53.8 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011625953183080458		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.011625953183080458 | validation: 0.015036412513004867]
	TIME [epoch: 53.8 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01197288938579789		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.01197288938579789 | validation: 0.012615400756364432]
	TIME [epoch: 53.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_1027.pth
	Model improved!!!
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011207728343022819		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.011207728343022819 | validation: 0.014202201867507255]
	TIME [epoch: 53.8 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011189029927667467		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.011189029927667467 | validation: 0.014018349265740599]
	TIME [epoch: 53.8 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01199787073450792		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.01199787073450792 | validation: 0.012906426884845156]
	TIME [epoch: 53.8 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011253312783001977		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.011253312783001977 | validation: 0.013632255102038145]
	TIME [epoch: 53.8 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011407053965018576		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.011407053965018576 | validation: 0.011419657832305755]
	TIME [epoch: 53.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_1032.pth
	Model improved!!!
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01144985137244315		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.01144985137244315 | validation: 0.01534402130678256]
	TIME [epoch: 53.8 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012674875166144212		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.012674875166144212 | validation: 0.0166233304917443]
	TIME [epoch: 53.8 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012578007444763149		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.012578007444763149 | validation: 0.01562306867754536]
	TIME [epoch: 53.8 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011285225119679217		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.011285225119679217 | validation: 0.01232892988609014]
	TIME [epoch: 53.8 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01216614369076975		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.01216614369076975 | validation: 0.01495984806255953]
	TIME [epoch: 53.8 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01166660820839683		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.01166660820839683 | validation: 0.015325625091614283]
	TIME [epoch: 53.8 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011776299109633959		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.011776299109633959 | validation: 0.014456147827876194]
	TIME [epoch: 53.8 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01098374647071857		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.01098374647071857 | validation: 0.014031253962612455]
	TIME [epoch: 53.8 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011206384851042228		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.011206384851042228 | validation: 0.011593047379134695]
	TIME [epoch: 53.8 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010777068410101115		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.010777068410101115 | validation: 0.012822284356948432]
	TIME [epoch: 53.8 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011388289709532796		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.011388289709532796 | validation: 0.014341825754705075]
	TIME [epoch: 53.8 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01151570394378186		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.01151570394378186 | validation: 0.015550368282871178]
	TIME [epoch: 53.8 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018381244273421443		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.018381244273421443 | validation: 0.02009390596590247]
	TIME [epoch: 53.8 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013399830506048468		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.013399830506048468 | validation: 0.014424425243958847]
	TIME [epoch: 53.8 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011546551370220435		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.011546551370220435 | validation: 0.012850117920215967]
	TIME [epoch: 53.8 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010857584902656571		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.010857584902656571 | validation: 0.012053445623800348]
	TIME [epoch: 53.8 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01098399500736644		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.01098399500736644 | validation: 0.01451644940802403]
	TIME [epoch: 53.8 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011188846152819568		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.011188846152819568 | validation: 0.011910768214163446]
	TIME [epoch: 53.8 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010745462732042287		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.010745462732042287 | validation: 0.012919409916154338]
	TIME [epoch: 53.8 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010779769017069998		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.010779769017069998 | validation: 0.01369259979163518]
	TIME [epoch: 53.8 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010910974363221889		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.010910974363221889 | validation: 0.013759652802460085]
	TIME [epoch: 53.8 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011285730835910383		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.011285730835910383 | validation: 0.015072783623382241]
	TIME [epoch: 53.8 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01067166549026717		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.01067166549026717 | validation: 0.012863630975354253]
	TIME [epoch: 53.8 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010898835550312692		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.010898835550312692 | validation: 0.014119002440284224]
	TIME [epoch: 53.8 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011075561065900994		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.011075561065900994 | validation: 0.014049667545390093]
	TIME [epoch: 53.8 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010704719663676036		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.010704719663676036 | validation: 0.01316676714118651]
	TIME [epoch: 53.8 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01095510413696556		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.01095510413696556 | validation: 0.013231926274805367]
	TIME [epoch: 53.8 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01074482358762023		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.01074482358762023 | validation: 0.012958492639693411]
	TIME [epoch: 53.8 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0114139432268291		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.0114139432268291 | validation: 0.01186500151529413]
	TIME [epoch: 53.8 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012147870011601022		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.012147870011601022 | validation: 0.014412594162676373]
	TIME [epoch: 53.8 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011014186702293507		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.011014186702293507 | validation: 0.013772563170780823]
	TIME [epoch: 53.8 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01066193061311925		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.01066193061311925 | validation: 0.012414579028503119]
	TIME [epoch: 53.8 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01111830898028948		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.01111830898028948 | validation: 0.01376160693538682]
	TIME [epoch: 53.8 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011367333105421738		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.011367333105421738 | validation: 0.013360710766497613]
	TIME [epoch: 53.8 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010986894864015039		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.010986894864015039 | validation: 0.013595352541632846]
	TIME [epoch: 53.8 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010435213396155382		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.010435213396155382 | validation: 0.015109652971905202]
	TIME [epoch: 53.8 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010754149768037517		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.010754149768037517 | validation: 0.013006424050647803]
	TIME [epoch: 53.8 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011008672264082014		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.011008672264082014 | validation: 0.01361218133225307]
	TIME [epoch: 53.8 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01081808427025123		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.01081808427025123 | validation: 0.014773703780883541]
	TIME [epoch: 53.8 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011568492220287035		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.011568492220287035 | validation: 0.012586016402061594]
	TIME [epoch: 53.8 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010793662949289552		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.010793662949289552 | validation: 0.011612761410021537]
	TIME [epoch: 53.8 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010664213773345971		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.010664213773345971 | validation: 0.013205505260182998]
	TIME [epoch: 53.8 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01100887358888971		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.01100887358888971 | validation: 0.013012861708666806]
	TIME [epoch: 53.8 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01097744889129184		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.01097744889129184 | validation: 0.014970696544292221]
	TIME [epoch: 53.8 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010648511232590594		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.010648511232590594 | validation: 0.013008537555949704]
	TIME [epoch: 53.8 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010761959247921752		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.010761959247921752 | validation: 0.013831450411974089]
	TIME [epoch: 53.8 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010701184681515345		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.010701184681515345 | validation: 0.014812081382977186]
	TIME [epoch: 53.8 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011304837553706319		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.011304837553706319 | validation: 0.01441112437707805]
	TIME [epoch: 53.8 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01142352866223872		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.01142352866223872 | validation: 0.015122837278057253]
	TIME [epoch: 53.8 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011747451454934207		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.011747451454934207 | validation: 0.013673357695990868]
	TIME [epoch: 53.8 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010882916724617197		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.010882916724617197 | validation: 0.014069865593350998]
	TIME [epoch: 53.8 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010769287045361127		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.010769287045361127 | validation: 0.014120217872538962]
	TIME [epoch: 53.8 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010547234168442282		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.010547234168442282 | validation: 0.013760184967293698]
	TIME [epoch: 53.8 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011339914383508643		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.011339914383508643 | validation: 0.015433692757528997]
	TIME [epoch: 53.8 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011094476265805191		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.011094476265805191 | validation: 0.013304117063360037]
	TIME [epoch: 53.8 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010496902872793213		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.010496902872793213 | validation: 0.013835441275812169]
	TIME [epoch: 53.8 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011447238999484365		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.011447238999484365 | validation: 0.014407578012442385]
	TIME [epoch: 53.8 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010217185550717583		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.010217185550717583 | validation: 0.0132223586479336]
	TIME [epoch: 53.8 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010615649701631306		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.010615649701631306 | validation: 0.012373921694358848]
	TIME [epoch: 53.8 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01043833264392664		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.01043833264392664 | validation: 0.012506943279782457]
	TIME [epoch: 53.8 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01044616803112343		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.01044616803112343 | validation: 0.01289930549512424]
	TIME [epoch: 53.8 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0106941551419322		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.0106941551419322 | validation: 0.012798740570388963]
	TIME [epoch: 53.8 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010770162042210232		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.010770162042210232 | validation: 0.012126185770476722]
	TIME [epoch: 53.8 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010537445237897614		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.010537445237897614 | validation: 0.014328482306129126]
	TIME [epoch: 53.8 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010846885221296069		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.010846885221296069 | validation: 0.013485492332466415]
	TIME [epoch: 53.8 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010704740809207688		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.010704740809207688 | validation: 0.014003680819409184]
	TIME [epoch: 53.8 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010176915136378578		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.010176915136378578 | validation: 0.01368094243376997]
	TIME [epoch: 53.8 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010810898838521203		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.010810898838521203 | validation: 0.012609605151917523]
	TIME [epoch: 53.8 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010786563743886824		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 0.010786563743886824 | validation: 0.01471431735948585]
	TIME [epoch: 53.8 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011195271956850463		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 0.011195271956850463 | validation: 0.013261883108032623]
	TIME [epoch: 53.8 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010258774372779205		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 0.010258774372779205 | validation: 0.013137417206181213]
	TIME [epoch: 53.8 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010733727970318553		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 0.010733727970318553 | validation: 0.012907388670256276]
	TIME [epoch: 53.8 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01021407411071033		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 0.01021407411071033 | validation: 0.013638736642147951]
	TIME [epoch: 53.8 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010634071522569722		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 0.010634071522569722 | validation: 0.014531757492546434]
	TIME [epoch: 53.8 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01037328521713251		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 0.01037328521713251 | validation: 0.013002982582598346]
	TIME [epoch: 53.8 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010734423442195653		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 0.010734423442195653 | validation: 0.01417701958995225]
	TIME [epoch: 53.8 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010550849781990786		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 0.010550849781990786 | validation: 0.013137899008169843]
	TIME [epoch: 53.8 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011766499175032703		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 0.011766499175032703 | validation: 0.013673421109803938]
	TIME [epoch: 53.8 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010722371701328916		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 0.010722371701328916 | validation: 0.012876436760622507]
	TIME [epoch: 53.8 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01097762797231654		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 0.01097762797231654 | validation: 0.013292817507629912]
	TIME [epoch: 54 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01031797158095061		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.01031797158095061 | validation: 0.019968570059420723]
	TIME [epoch: 53.8 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013400144057774283		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 0.013400144057774283 | validation: 0.012562171835898759]
	TIME [epoch: 53.8 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009876231237064761		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 0.009876231237064761 | validation: 0.012572437311662677]
	TIME [epoch: 53.8 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01035538213330855		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 0.01035538213330855 | validation: 0.012632268486333108]
	TIME [epoch: 53.8 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010176521699402844		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 0.010176521699402844 | validation: 0.011992996213909831]
	TIME [epoch: 53.8 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010394177895576083		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 0.010394177895576083 | validation: 0.012349399904111966]
	TIME [epoch: 53.8 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010640151503384555		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 0.010640151503384555 | validation: 0.012563271234780486]
	TIME [epoch: 53.8 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01082578983325941		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 0.01082578983325941 | validation: 0.013181553604686456]
	TIME [epoch: 53.8 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009893170988786195		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 0.009893170988786195 | validation: 0.01244014488672494]
	TIME [epoch: 53.8 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010411742262024518		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 0.010411742262024518 | validation: 0.013475565945673649]
	TIME [epoch: 53.8 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011499090216501294		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 0.011499090216501294 | validation: 0.01801098460602997]
	TIME [epoch: 53.8 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010693251339134065		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 0.010693251339134065 | validation: 0.012897450667221717]
	TIME [epoch: 53.8 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010297819734812034		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 0.010297819734812034 | validation: 0.012972241891900998]
	TIME [epoch: 53.8 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010886140598846896		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 0.010886140598846896 | validation: 0.013272442343107704]
	TIME [epoch: 53.8 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010443184912800257		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 0.010443184912800257 | validation: 0.013413194927713316]
	TIME [epoch: 53.8 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01052923458805149		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 0.01052923458805149 | validation: 0.011767825676518904]
	TIME [epoch: 53.8 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011887176732338026		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 0.011887176732338026 | validation: 0.014045142480669098]
	TIME [epoch: 53.8 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010607297747838867		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 0.010607297747838867 | validation: 0.01619471687016788]
	TIME [epoch: 53.8 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010091160134005085		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 0.010091160134005085 | validation: 0.012920360245558564]
	TIME [epoch: 53.8 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010183775790750612		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 0.010183775790750612 | validation: 0.01353818378295431]
	TIME [epoch: 53.8 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010596735015163797		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 0.010596735015163797 | validation: 0.013004531931082074]
	TIME [epoch: 53.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_1_v_mmd1_1133.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 26974.661 seconds.
