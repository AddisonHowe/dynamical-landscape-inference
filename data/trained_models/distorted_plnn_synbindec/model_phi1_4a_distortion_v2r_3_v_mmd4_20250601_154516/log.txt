Args:
Namespace(name='model_phi1_4a_distortion_v2r_3_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v2r_3_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2r_3/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2r_3/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.022194553, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2674442398

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2r_3_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_3_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.614334431404932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.614334431404932 | validation: 3.284984602607881]
	TIME [epoch: 167 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_3_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_3_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.841177492415321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.841177492415321 | validation: 3.799704758217837]
	TIME [epoch: 0.815 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.574180088128004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.574180088128004 | validation: 4.065685948459249]
	TIME [epoch: 0.695 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.414001594679226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.414001594679226 | validation: 5.571970228630042]
	TIME [epoch: 0.691 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.053900848972728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.053900848972728 | validation: 4.962214132574793]
	TIME [epoch: 0.692 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5010289359458198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5010289359458198 | validation: 4.160904564144481]
	TIME [epoch: 0.696 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.408460884101471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.408460884101471 | validation: 4.221634742913218]
	TIME [epoch: 0.757 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.087080308631058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.087080308631058 | validation: 3.9572041234559023]
	TIME [epoch: 0.699 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.928146777821261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.928146777821261 | validation: 3.5229548083528877]
	TIME [epoch: 0.695 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.76231076709988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.76231076709988 | validation: 3.5552697539159084]
	TIME [epoch: 0.697 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6438822120145065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6438822120145065 | validation: 2.6990085012858174]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_3_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_3_v_mmd4_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1125373977712627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1125373977712627 | validation: 3.1995326765892047]
	TIME [epoch: 0.694 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4134410277754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4134410277754 | validation: 2.877198022703246]
	TIME [epoch: 0.864 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.307424005775407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.307424005775407 | validation: 2.6855387682289114]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_3_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_3_v_mmd4_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.177795036122673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.177795036122673 | validation: 2.4787618108214957]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_3_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_3_v_mmd4_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0489171774771466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0489171774771466 | validation: 2.0820690871627536]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_3_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_3_v_mmd4_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9357583260414841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9357583260414841 | validation: 2.9695895546009723]
	TIME [epoch: 0.694 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5520356314395265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5520356314395265 | validation: 2.0983889335608015]
	TIME [epoch: 0.704 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8456414284789266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8456414284789266 | validation: 1.8611093690682698]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_3_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_3_v_mmd4_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.464926604485096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.464926604485096 | validation: 2.0953202977316177]
	TIME [epoch: 0.694 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.010029743606844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.010029743606844 | validation: 2.2503492870970256]
	TIME [epoch: 0.69 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8204832492107748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8204832492107748 | validation: 2.113218164946908]
	TIME [epoch: 0.691 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7818458078764174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7818458078764174 | validation: 1.8993731253697428]
	TIME [epoch: 0.692 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7492679838043501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7492679838043501 | validation: 1.5910224296073083]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_3_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_3_v_mmd4_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6180617411643943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6180617411643943 | validation: 1.8369241747665221]
	TIME [epoch: 0.702 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5861349351516925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5861349351516925 | validation: 1.4858614498880949]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_3_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_3_v_mmd4_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5862371889019369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5862371889019369 | validation: 1.9580668497902032]
	TIME [epoch: 0.694 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7825482485413633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7825482485413633 | validation: 1.3301283937103918]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_3_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_3_v_mmd4_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5529157425024107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5529157425024107 | validation: 1.7550081957644181]
	TIME [epoch: 0.699 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.503973042820324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.503973042820324 | validation: 1.1759066358962273]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_3_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_3_v_mmd4_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4397751400922678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4397751400922678 | validation: 1.612411064814047]
	TIME [epoch: 0.697 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.394139971714929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.394139971714929 | validation: 1.0614551426384766]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_3_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_3_v_mmd4_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4503577799206533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4503577799206533 | validation: 1.5603037323151314]
	TIME [epoch: 0.697 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3815768659922707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3815768659922707 | validation: 1.058831967367921]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_3_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_3_v_mmd4_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3878036401227667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3878036401227667 | validation: 1.294320944428386]
	TIME [epoch: 0.694 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3050316217817726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3050316217817726 | validation: 1.5105239781029773]
	TIME [epoch: 0.691 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5027140771673866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5027140771673866 | validation: 1.3370799451333426]
	TIME [epoch: 0.691 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5533995438678567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5533995438678567 | validation: 1.474483413075629]
	TIME [epoch: 0.692 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3587229090978241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3587229090978241 | validation: 1.0043236193444514]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_3_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_3_v_mmd4_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.251188416110486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.251188416110486 | validation: 1.4845723622199676]
	TIME [epoch: 0.692 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.334953008508597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.334953008508597 | validation: 0.9451061853679334]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_3_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_3_v_mmd4_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6038111597040992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6038111597040992 | validation: 1.2258132378174338]
	TIME [epoch: 0.696 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2118982915709464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2118982915709464 | validation: 1.3183387365601391]
	TIME [epoch: 0.695 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2436802239179416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2436802239179416 | validation: 1.0096656219493323]
	TIME [epoch: 0.695 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3490373588181876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3490373588181876 | validation: 1.2011392383904456]
	TIME [epoch: 0.694 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2234916776973328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2234916776973328 | validation: 1.2978763788576655]
	TIME [epoch: 0.697 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2674410565445104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2674410565445104 | validation: 1.073156896482438]
	TIME [epoch: 0.694 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3076100365707632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3076100365707632 | validation: 1.4308229660982743]
	TIME [epoch: 0.694 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3208074611471432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3208074611471432 | validation: 0.9712435271445113]
	TIME [epoch: 0.693 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.193753811524292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.193753811524292 | validation: 1.206349097205614]
	TIME [epoch: 0.692 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1578672940532966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1578672940532966 | validation: 0.8990972375817095]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_3_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_3_v_mmd4_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.192262722454214		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.192262722454214 | validation: 1.452103724466005]
	TIME [epoch: 0.693 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3662041830544738		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.3662041830544738 | validation: 1.0093877108106912]
	TIME [epoch: 0.694 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.470341384100828		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.470341384100828 | validation: 1.018273161634341]
	TIME [epoch: 0.697 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1605913623379323		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.1605913623379323 | validation: 1.302103174858265]
	TIME [epoch: 0.692 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2293418742803728		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.2293418742803728 | validation: 0.9684815357862733]
	TIME [epoch: 0.691 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2303751739344961		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.2303751739344961 | validation: 1.0657785077800659]
	TIME [epoch: 0.695 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1615880115566848		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.1615880115566848 | validation: 1.2159377850326143]
	TIME [epoch: 0.692 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1779237321505431		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.1779237321505431 | validation: 0.9908510845351061]
	TIME [epoch: 0.691 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1892856805516472		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.1892856805516472 | validation: 1.2221651768552868]
	TIME [epoch: 0.691 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1836696147072918		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.1836696147072918 | validation: 1.0457287370077926]
	TIME [epoch: 0.689 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1541210141926714		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.1541210141926714 | validation: 1.0561845321293366]
	TIME [epoch: 0.691 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1415062142420627		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.1415062142420627 | validation: 1.131802532166078]
	TIME [epoch: 0.693 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1659665944835402		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.1659665944835402 | validation: 0.977102321257564]
	TIME [epoch: 0.695 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3015946973399093		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.3015946973399093 | validation: 1.2991315052384864]
	TIME [epoch: 0.702 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2213819230918201		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.2213819230918201 | validation: 0.9384737456372867]
	TIME [epoch: 0.692 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1652331281886839		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.1652331281886839 | validation: 1.0361175409528622]
	TIME [epoch: 0.691 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.092561098399317		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.092561098399317 | validation: 1.0249138061271914]
	TIME [epoch: 0.69 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0794419457055713		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.0794419457055713 | validation: 0.9760071441042641]
	TIME [epoch: 0.689 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0842414569945722		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.0842414569945722 | validation: 1.0346857589363907]
	TIME [epoch: 0.724 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.084210395018667		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.084210395018667 | validation: 1.0359826324426697]
	TIME [epoch: 0.69 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1281718416810227		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.1281718416810227 | validation: 1.3136574565240542]
	TIME [epoch: 0.694 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2901387854246698		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.2901387854246698 | validation: 1.1797590624633123]
	TIME [epoch: 0.692 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2430065814220894		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.2430065814220894 | validation: 0.9631571107378503]
	TIME [epoch: 0.691 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1937860370687168		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.1937860370687168 | validation: 1.3615254745969776]
	TIME [epoch: 0.691 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2516463444738284		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.2516463444738284 | validation: 0.8673556302379108]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_3_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_3_v_mmd4_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.293221496814912		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.293221496814912 | validation: 1.0929645366842953]
	TIME [epoch: 0.692 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0981962978602753		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.0981962978602753 | validation: 1.1176523700263805]
	TIME [epoch: 0.691 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1291258255350327		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.1291258255350327 | validation: 0.9262761492001421]
	TIME [epoch: 0.689 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1625098907099112		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.1625098907099112 | validation: 1.1184426905608642]
	TIME [epoch: 0.689 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1069359894486592		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.1069359894486592 | validation: 0.9922990025614422]
	TIME [epoch: 0.692 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0964206216443932		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.0964206216443932 | validation: 0.9885937509240783]
	TIME [epoch: 0.691 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0837812575084742		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.0837812575084742 | validation: 1.108630414115854]
	TIME [epoch: 0.69 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1110306269453682		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.1110306269453682 | validation: 0.995899728161278]
	TIME [epoch: 0.689 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1337773981878903		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.1337773981878903 | validation: 1.164598623162772]
	TIME [epoch: 0.691 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1716406671659116		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.1716406671659116 | validation: 1.0385528855807298]
	TIME [epoch: 0.691 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1349194131703255		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.1349194131703255 | validation: 0.9866992368365382]
	TIME [epoch: 0.69 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1362059657623627		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.1362059657623627 | validation: 1.332785948011862]
	TIME [epoch: 0.689 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2470697831060558		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.2470697831060558 | validation: 0.9320459825123607]
	TIME [epoch: 0.689 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3693314393824974		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.3693314393824974 | validation: 1.0781800813892515]
	TIME [epoch: 0.791 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0792197971572877		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.0792197971572877 | validation: 1.1218179663629335]
	TIME [epoch: 0.693 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1088146133301389		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.1088146133301389 | validation: 0.9440367044305357]
	TIME [epoch: 0.701 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.151768700839708		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.151768700839708 | validation: 1.0480783356329597]
	TIME [epoch: 0.69 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0814268867182046		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.0814268867182046 | validation: 1.0142887507901601]
	TIME [epoch: 0.69 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0740345967503298		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.0740345967503298 | validation: 0.9550348472495735]
	TIME [epoch: 0.69 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0709397298152923		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.0709397298152923 | validation: 1.0537264480979174]
	TIME [epoch: 0.698 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0760499453033392		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.0760499453033392 | validation: 0.9474313356089802]
	TIME [epoch: 0.689 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0985675001820736		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.0985675001820736 | validation: 1.220848711393364]
	TIME [epoch: 0.69 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1638540625590443		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.1638540625590443 | validation: 0.9900980928313938]
	TIME [epoch: 0.689 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1420133870646967		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.1420133870646967 | validation: 1.1360017905307924]
	TIME [epoch: 0.701 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1127687647686548		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.1127687647686548 | validation: 0.9244062877581577]
	TIME [epoch: 0.696 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.053381681665179		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.053381681665179 | validation: 1.0054391323629908]
	TIME [epoch: 0.693 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0475924930720237		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.0475924930720237 | validation: 0.9089338024611273]
	TIME [epoch: 0.79 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.054166886065898		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.054166886065898 | validation: 1.3065373901621022]
	TIME [epoch: 0.692 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.207439884001387		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.207439884001387 | validation: 1.0729075205337775]
	TIME [epoch: 0.704 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5536532908234528		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.5536532908234528 | validation: 1.0214739281602108]
	TIME [epoch: 0.69 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0879089384713576		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.0879089384713576 | validation: 1.1549401075256842]
	TIME [epoch: 0.709 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.122653124078471		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.122653124078471 | validation: 0.9892716878984926]
	TIME [epoch: 0.694 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2006206291020864		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.2006206291020864 | validation: 1.0246976799427026]
	TIME [epoch: 0.691 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0737151975755896		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.0737151975755896 | validation: 1.0387694961771978]
	TIME [epoch: 0.69 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0580152754074883		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.0580152754074883 | validation: 0.9231305915424767]
	TIME [epoch: 0.689 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0471479432751707		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.0471479432751707 | validation: 1.0261125429490074]
	TIME [epoch: 0.694 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0506723960275266		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.0506723960275266 | validation: 0.9093173173463418]
	TIME [epoch: 0.693 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0599535152663844		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.0599535152663844 | validation: 1.078607673912457]
	TIME [epoch: 0.692 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0630849894660397		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.0630849894660397 | validation: 0.8784255100649414]
	TIME [epoch: 0.693 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1061480968640354		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.1061480968640354 | validation: 1.191970238213926]
	TIME [epoch: 0.694 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1557496812348729		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.1557496812348729 | validation: 0.9954526693577886]
	TIME [epoch: 0.695 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2343523609068263		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.2343523609068263 | validation: 1.0473016954884036]
	TIME [epoch: 0.699 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.075320178105498		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.075320178105498 | validation: 1.0280843929832875]
	TIME [epoch: 0.693 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0548763399323577		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.0548763399323577 | validation: 0.9128258533793607]
	TIME [epoch: 0.695 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0576680239019216		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.0576680239019216 | validation: 1.1269534340008966]
	TIME [epoch: 0.692 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0804386074825107		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.0804386074825107 | validation: 0.87883011736436]
	TIME [epoch: 0.696 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0949501063417504		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.0949501063417504 | validation: 1.179216257558211]
	TIME [epoch: 0.7 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1075842977960526		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.1075842977960526 | validation: 0.9060755482982036]
	TIME [epoch: 0.695 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0563770021214407		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.0563770021214407 | validation: 1.019385765846471]
	TIME [epoch: 0.695 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0324517522288303		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.0324517522288303 | validation: 0.9322559650656315]
	TIME [epoch: 0.694 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0213657731774786		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.0213657731774786 | validation: 1.0071627545562907]
	TIME [epoch: 0.694 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0228071815309547		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.0228071815309547 | validation: 0.9298324702417116]
	TIME [epoch: 0.695 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0623624992778604		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.0623624992778604 | validation: 1.3674697096069184]
	TIME [epoch: 0.694 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.300338028286655		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.300338028286655 | validation: 1.0512725191446382]
	TIME [epoch: 0.693 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3987557092136134		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.3987557092136134 | validation: 0.9856772957856716]
	TIME [epoch: 0.74 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0355407079536798		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.0355407079536798 | validation: 1.2070300509580747]
	TIME [epoch: 0.697 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1867859148937419		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.1867859148937419 | validation: 0.9330838836009996]
	TIME [epoch: 0.693 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1297634563520853		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.1297634563520853 | validation: 1.0246307479821508]
	TIME [epoch: 0.693 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0437638530069804		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.0437638530069804 | validation: 0.9777984164426172]
	TIME [epoch: 0.693 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0545986146810002		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.0545986146810002 | validation: 0.9435344649572923]
	TIME [epoch: 0.701 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.03712001784823		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.03712001784823 | validation: 0.9829896516558136]
	TIME [epoch: 0.693 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0246949151663136		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.0246949151663136 | validation: 0.9353513894084995]
	TIME [epoch: 0.694 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0246396458361011		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.0246396458361011 | validation: 1.0203009913255217]
	TIME [epoch: 0.694 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0311119079985118		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.0311119079985118 | validation: 0.8811693877129656]
	TIME [epoch: 0.694 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0554922989285895		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.0554922989285895 | validation: 1.2155124018121324]
	TIME [epoch: 0.694 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1135010885052814		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.1135010885052814 | validation: 0.8643802727779694]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_3_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_3_v_mmd4_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1567085067215748		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.1567085067215748 | validation: 1.1514767252195128]
	TIME [epoch: 0.694 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.080542468313852		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.080542468313852 | validation: 0.9273637954425041]
	TIME [epoch: 0.691 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0190666566497382		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.0190666566497382 | validation: 0.9624117204227602]
	TIME [epoch: 0.691 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.016941022559274		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.016941022559274 | validation: 0.9999868633730117]
	TIME [epoch: 0.692 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.038696227697679		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.038696227697679 | validation: 0.9783043040807109]
	TIME [epoch: 0.703 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0583063656317546		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.0583063656317546 | validation: 1.0607780831912408]
	TIME [epoch: 0.691 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.113906427508349		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.113906427508349 | validation: 1.1076907875688304]
	TIME [epoch: 0.691 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0877331003114137		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.0877331003114137 | validation: 0.8532176325981553]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_3_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_3_v_mmd4_150.pth
	Model improved!!!
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.156222846716732		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.156222846716732 | validation: 1.1785076195523936]
	TIME [epoch: 0.697 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1016337973832586		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.1016337973832586 | validation: 0.8814455505852976]
	TIME [epoch: 0.695 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0411472254107568		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.0411472254107568 | validation: 0.9778568229824217]
	TIME [epoch: 0.697 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0109171278877438		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.0109171278877438 | validation: 0.9680282858404091]
	TIME [epoch: 0.694 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0095416139699613		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.0095416139699613 | validation: 0.9025848024466886]
	TIME [epoch: 0.691 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.010216541687913		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.010216541687913 | validation: 1.0198998307031286]
	TIME [epoch: 0.69 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0219547345637445		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.0219547345637445 | validation: 0.8711209724191344]
	TIME [epoch: 0.691 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0630492442059096		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.0630492442059096 | validation: 1.2166848695548769]
	TIME [epoch: 0.701 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.151077258127751		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.151077258127751 | validation: 0.9435848931249105]
	TIME [epoch: 0.69 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1928066275254312		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.1928066275254312 | validation: 1.0046805455179395]
	TIME [epoch: 0.69 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0464179095772401		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.0464179095772401 | validation: 1.043517871425814]
	TIME [epoch: 0.691 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0355012989931165		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.0355012989931165 | validation: 0.8701923039936974]
	TIME [epoch: 0.691 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0411664722440523		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.0411664722440523 | validation: 1.0446070674547376]
	TIME [epoch: 0.691 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0378964898025238		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.0378964898025238 | validation: 0.8868274093283589]
	TIME [epoch: 0.696 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.030581524634829		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.030581524634829 | validation: 1.0203865492525015]
	TIME [epoch: 0.69 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0167743293369496		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.0167743293369496 | validation: 0.8819418491461392]
	TIME [epoch: 0.692 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0173333214158973		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.0173333214158973 | validation: 1.037473825534074]
	TIME [epoch: 0.69 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.022545791206294		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.022545791206294 | validation: 0.8501137792635529]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_3_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_3_v_mmd4_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0815697241595807		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.0815697241595807 | validation: 1.1819202285406303]
	TIME [epoch: 0.697 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.147022047987097		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.147022047987097 | validation: 0.9459860607530213]
	TIME [epoch: 0.695 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1640420234651299		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.1640420234651299 | validation: 0.9734010482330206]
	TIME [epoch: 0.695 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0230167534711054		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.0230167534711054 | validation: 1.0061179594100442]
	TIME [epoch: 0.694 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.009833965599101		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.009833965599101 | validation: 0.8785840143681803]
	TIME [epoch: 0.702 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0354394787452992		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.0354394787452992 | validation: 1.0414377927870728]
	TIME [epoch: 0.692 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0302113828838086		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.0302113828838086 | validation: 0.8540711314855074]
	TIME [epoch: 0.693 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.039506338853287		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.039506338853287 | validation: 1.065938345067411]
	TIME [epoch: 0.693 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0377360241928755		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.0377360241928755 | validation: 0.8638920512907728]
	TIME [epoch: 0.692 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0460778339875576		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.0460778339875576 | validation: 1.0841980476575714]
	TIME [epoch: 0.69 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0502812438319946		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.0502812438319946 | validation: 0.9355578670588489]
	TIME [epoch: 0.69 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0545114139958895		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.0545114139958895 | validation: 1.0926301613780678]
	TIME [epoch: 0.69 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.076820553136673		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.076820553136673 | validation: 0.9629697875826061]
	TIME [epoch: 0.69 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0366213553288446		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.0366213553288446 | validation: 0.9267822917449131]
	TIME [epoch: 0.757 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.021775511686128		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.021775511686128 | validation: 1.0790607966885912]
	TIME [epoch: 0.69 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0456445883910168		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.0456445883910168 | validation: 0.8282437948379755]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_3_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_3_v_mmd4_184.pth
	Model improved!!!
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0939562739117998		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.0939562739117998 | validation: 1.0886770760016453]
	TIME [epoch: 0.784 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.047786211528889		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.047786211528889 | validation: 0.9095828685177181]
	TIME [epoch: 0.691 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0266358340292423		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.0266358340292423 | validation: 0.9561655722134828]
	TIME [epoch: 0.699 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0121654063489098		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.0121654063489098 | validation: 0.9885294736324255]
	TIME [epoch: 0.69 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0231015381240414		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.0231015381240414 | validation: 0.9293871115520841]
	TIME [epoch: 0.689 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0257222539599726		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.0257222539599726 | validation: 0.9998102893334114]
	TIME [epoch: 0.69 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0274345157464386		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.0274345157464386 | validation: 0.9428351155513965]
	TIME [epoch: 0.692 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0217054914329957		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.0217054914329957 | validation: 0.9550333724528687]
	TIME [epoch: 0.69 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0111618256315813		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.0111618256315813 | validation: 1.015534568128167]
	TIME [epoch: 0.69 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0252919179467075		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.0252919179467075 | validation: 0.8650557104555872]
	TIME [epoch: 0.701 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0867159789272418		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.0867159789272418 | validation: 1.2361213078779656]
	TIME [epoch: 0.69 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1373528486817899		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.1373528486817899 | validation: 0.8418154092241217]
	TIME [epoch: 0.689 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0627456096016064		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.0627456096016064 | validation: 0.9678516769734872]
	TIME [epoch: 0.689 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0107006355836599		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.0107006355836599 | validation: 0.9541588522543967]
	TIME [epoch: 0.69 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9915731042995622		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.9915731042995622 | validation: 0.9167700557258812]
	TIME [epoch: 0.691 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0017521510188576		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.0017521510188576 | validation: 0.9628838506714246]
	TIME [epoch: 0.737 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9963275062672244		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.9963275062672244 | validation: 0.8707816545891337]
	TIME [epoch: 176 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0005524007338553		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.0005524007338553 | validation: 1.0520634179033248]
	TIME [epoch: 1.37 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.031389893145865		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.031389893145865 | validation: 0.8349095350571125]
	TIME [epoch: 1.35 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0805407885417382		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.0805407885417382 | validation: 1.0812398512467383]
	TIME [epoch: 1.35 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0488930155834568		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.0488930155834568 | validation: 0.9246340587623522]
	TIME [epoch: 1.35 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0700916088135577		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.0700916088135577 | validation: 1.0419008029001446]
	TIME [epoch: 1.35 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.063578621585005		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.063578621585005 | validation: 0.970498186807931]
	TIME [epoch: 1.35 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0224549264686238		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.0224549264686238 | validation: 0.9086640905970564]
	TIME [epoch: 1.35 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0031853179636991		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.0031853179636991 | validation: 0.9784099798157944]
	TIME [epoch: 1.37 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.001684133330019		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.001684133330019 | validation: 0.9014835221929464]
	TIME [epoch: 1.35 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.999483437128903		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.999483437128903 | validation: 1.0027455883949792]
	TIME [epoch: 1.44 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0072645223645034		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.0072645223645034 | validation: 0.844019655545608]
	TIME [epoch: 1.35 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0463807986163345		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 1.0463807986163345 | validation: 1.0889293173695975]
	TIME [epoch: 1.35 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.048234568130422		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.048234568130422 | validation: 0.841769110228622]
	TIME [epoch: 1.35 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0644886122134956		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 1.0644886122134956 | validation: 1.024504667554441]
	TIME [epoch: 1.35 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0273584501628106		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.0273584501628106 | validation: 0.8887757580225375]
	TIME [epoch: 1.35 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0051484065939387		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.0051484065939387 | validation: 0.969115395645415]
	TIME [epoch: 1.35 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0112482940593763		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.0112482940593763 | validation: 1.0194310508746671]
	TIME [epoch: 1.35 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0320598665761913		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.0320598665761913 | validation: 0.963903284282576]
	TIME [epoch: 1.35 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0465018211741102		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.0465018211741102 | validation: 1.007590928785192]
	TIME [epoch: 1.36 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0234533836903807		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 1.0234533836903807 | validation: 0.8853006354027116]
	TIME [epoch: 1.35 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9888965620028082		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.9888965620028082 | validation: 1.0048486891534292]
	TIME [epoch: 1.35 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9989009502406779		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.9989009502406779 | validation: 0.8528031861489449]
	TIME [epoch: 1.35 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0286856524281247		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.0286856524281247 | validation: 1.1379511690410486]
	TIME [epoch: 1.35 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0609773604106916		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 1.0609773604106916 | validation: 0.8534915781252697]
	TIME [epoch: 1.35 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0724841270268237		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 1.0724841270268237 | validation: 1.017490382397871]
	TIME [epoch: 1.35 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0160256441327065		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.0160256441327065 | validation: 0.94823874974342]
	TIME [epoch: 1.35 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0045286650945928		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.0045286650945928 | validation: 0.8899457756404066]
	TIME [epoch: 1.35 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0099453578139004		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 1.0099453578139004 | validation: 1.0217260723664061]
	TIME [epoch: 1.35 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0130206006637865		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.0130206006637865 | validation: 0.8754445228385486]
	TIME [epoch: 1.35 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0130767758774266		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 1.0130767758774266 | validation: 1.0039091620358847]
	TIME [epoch: 1.36 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0098579994823442		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.0098579994823442 | validation: 0.8617093612073653]
	TIME [epoch: 1.35 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0171324756140048		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 1.0171324756140048 | validation: 0.9945960799467275]
	TIME [epoch: 1.35 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.014536797448452		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.014536797448452 | validation: 0.8789127852386405]
	TIME [epoch: 1.35 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.038854283458068		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 1.038854283458068 | validation: 1.0710195373627085]
	TIME [epoch: 1.35 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0484451940898754		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.0484451940898754 | validation: 0.873452007138512]
	TIME [epoch: 1.35 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0376519844087724		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 1.0376519844087724 | validation: 1.0573860232880095]
	TIME [epoch: 1.35 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0108698691270765		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 1.0108698691270765 | validation: 0.8731044543364942]
	TIME [epoch: 1.35 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0023784106561273		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 1.0023784106561273 | validation: 1.0033164781133594]
	TIME [epoch: 1.35 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0015948763832925		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 1.0015948763832925 | validation: 0.9074300594763042]
	TIME [epoch: 1.35 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9932681576748426		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.9932681576748426 | validation: 0.9369519971428226]
	TIME [epoch: 1.35 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9897356406475741		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.9897356406475741 | validation: 0.9339657795423837]
	TIME [epoch: 1.36 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9886970552145843		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.9886970552145843 | validation: 0.9381398833052907]
	TIME [epoch: 1.35 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9977635170908421		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.9977635170908421 | validation: 0.9828290982221408]
	TIME [epoch: 1.35 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0224219111801451		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 1.0224219111801451 | validation: 0.9250014181586231]
	TIME [epoch: 1.35 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0037828655867873		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.0037828655867873 | validation: 1.081076268280245]
	TIME [epoch: 1.35 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0213350138351165		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 1.0213350138351165 | validation: 0.8004041485669842]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_3_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_3_v_mmd4_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0681931188832947		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 1.0681931188832947 | validation: 1.075252638026861]
	TIME [epoch: 1.35 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0426618321567267		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 1.0426618321567267 | validation: 0.885942239366806]
	TIME [epoch: 1.36 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0254799215321384		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 1.0254799215321384 | validation: 0.971040998288779]
	TIME [epoch: 1.35 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0153501220111174		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 1.0153501220111174 | validation: 0.9984416712919384]
	TIME [epoch: 1.35 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0177040153607175		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 1.0177040153607175 | validation: 0.9060480859236624]
	TIME [epoch: 1.35 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.998755590768547		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.998755590768547 | validation: 0.9581177414896122]
	TIME [epoch: 1.36 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9820201833804137		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.9820201833804137 | validation: 0.8950933985482067]
	TIME [epoch: 1.35 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9863557766302694		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.9863557766302694 | validation: 0.9524665890377308]
	TIME [epoch: 1.35 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9887920258152239		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.9887920258152239 | validation: 0.8889898560873664]
	TIME [epoch: 1.36 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0002492023521914		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 1.0002492023521914 | validation: 1.071658193247378]
	TIME [epoch: 1.35 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.043698973901493		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 1.043698973901493 | validation: 0.8649830392609842]
	TIME [epoch: 1.35 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0877428893402512		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 1.0877428893402512 | validation: 1.0230537549152492]
	TIME [epoch: 1.36 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0099847846116672		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 1.0099847846116672 | validation: 0.9045045088179777]
	TIME [epoch: 1.35 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9896981779822668		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.9896981779822668 | validation: 0.9340053649341962]
	TIME [epoch: 1.35 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9863843376461766		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.9863843376461766 | validation: 0.9813873372362976]
	TIME [epoch: 1.35 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9998404377027287		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.9998404377027287 | validation: 0.9231342374083487]
	TIME [epoch: 1.35 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9877548742441729		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.9877548742441729 | validation: 0.943153906334479]
	TIME [epoch: 1.35 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9872722346926872		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.9872722346926872 | validation: 0.9275393679260155]
	TIME [epoch: 1.35 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9873168038387291		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.9873168038387291 | validation: 0.9458897830788161]
	TIME [epoch: 1.35 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9977897894260126		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.9977897894260126 | validation: 0.9350093252196712]
	TIME [epoch: 1.36 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0154476247409565		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 1.0154476247409565 | validation: 0.9724997751057931]
	TIME [epoch: 1.35 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0010788346357198		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 1.0010788346357198 | validation: 0.8781526535012258]
	TIME [epoch: 1.35 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9992073217723347		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.9992073217723347 | validation: 1.097266115446759]
	TIME [epoch: 1.35 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0439505926601367		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 1.0439505926601367 | validation: 0.7714802454775541]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_3_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_3_v_mmd4_271.pth
	Model improved!!!
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0851439860403789		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 1.0851439860403789 | validation: 0.9871607931679744]
	TIME [epoch: 1.35 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9943164251567922		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.9943164251567922 | validation: 0.9526989775096082]
	TIME [epoch: 1.35 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9799871593077569		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.9799871593077569 | validation: 0.8688828226862239]
	TIME [epoch: 1.36 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0000074224243167		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 1.0000074224243167 | validation: 1.0174705507161708]
	TIME [epoch: 1.35 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0186612870946299		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 1.0186612870946299 | validation: 0.9201546317618347]
	TIME [epoch: 1.35 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0099716486399875		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 1.0099716486399875 | validation: 0.9494612017501081]
	TIME [epoch: 1.35 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0015839022628938		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 1.0015839022628938 | validation: 0.9231835184639636]
	TIME [epoch: 1.35 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9782611498085112		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.9782611498085112 | validation: 0.9081111871505823]
	TIME [epoch: 1.35 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9828574079727221		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.9828574079727221 | validation: 0.9386089825624165]
	TIME [epoch: 1.35 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9864388919518237		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.9864388919518237 | validation: 0.9128438185559671]
	TIME [epoch: 1.36 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9983047228230834		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.9983047228230834 | validation: 0.9740259982236256]
	TIME [epoch: 1.35 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0047492724193101		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 1.0047492724193101 | validation: 0.8584960209788257]
	TIME [epoch: 1.37 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0126829755106985		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 1.0126829755106985 | validation: 1.066094399059001]
	TIME [epoch: 1.35 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0086279986700162		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 1.0086279986700162 | validation: 0.8514577886707871]
	TIME [epoch: 1.35 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.021424688621473		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 1.021424688621473 | validation: 1.005239226391978]
	TIME [epoch: 1.35 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9971883116028829		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.9971883116028829 | validation: 0.9096325022213776]
	TIME [epoch: 1.35 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9746257620922261		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.9746257620922261 | validation: 0.8746442880435487]
	TIME [epoch: 1.35 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9811982337819657		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.9811982337819657 | validation: 0.9907708400049213]
	TIME [epoch: 1.35 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9960977854157593		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.9960977854157593 | validation: 0.8767661721529196]
	TIME [epoch: 1.35 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0023351661310493		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 1.0023351661310493 | validation: 1.00421080867709]
	TIME [epoch: 1.35 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0046402579859814		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 1.0046402579859814 | validation: 0.88649741560602]
	TIME [epoch: 1.36 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9864279279848872		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.9864279279848872 | validation: 0.9273071586443639]
	TIME [epoch: 1.35 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9849152684062489		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.9849152684062489 | validation: 0.9395126299664732]
	TIME [epoch: 1.35 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0009292120895126		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 1.0009292120895126 | validation: 0.9782481326038657]
	TIME [epoch: 1.36 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9939935222650078		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.9939935222650078 | validation: 0.8388438890396313]
	TIME [epoch: 1.36 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0033884997468214		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 1.0033884997468214 | validation: 1.0549488773046523]
	TIME [epoch: 1.35 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0192129074913168		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 1.0192129074913168 | validation: 0.8608659052064369]
	TIME [epoch: 1.36 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0061694907397343		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 1.0061694907397343 | validation: 0.980112026546318]
	TIME [epoch: 1.35 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9865521666755052		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.9865521666755052 | validation: 0.903810188910558]
	TIME [epoch: 1.36 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9735985195969792		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.9735985195969792 | validation: 0.8908636127873516]
	TIME [epoch: 1.36 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9754431295480595		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.9754431295480595 | validation: 0.949733368303058]
	TIME [epoch: 1.36 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9750483622319445		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.9750483622319445 | validation: 0.8459953232793626]
	TIME [epoch: 1.36 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9788851333246926		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.9788851333246926 | validation: 1.0056315463882635]
	TIME [epoch: 1.36 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9837598829272753		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.9837598829272753 | validation: 0.8442722043268702]
	TIME [epoch: 1.36 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9971057558608973		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.9971057558608973 | validation: 1.0003937454619]
	TIME [epoch: 1.37 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9885697076579633		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.9885697076579633 | validation: 0.861979017629437]
	TIME [epoch: 1.36 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9870663995984503		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.9870663995984503 | validation: 0.9825167960200963]
	TIME [epoch: 1.35 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0144775920818316		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 1.0144775920818316 | validation: 0.9459602879372645]
	TIME [epoch: 1.35 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.026482511219052		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 1.026482511219052 | validation: 0.9499991210887754]
	TIME [epoch: 1.36 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9914422778081874		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.9914422778081874 | validation: 0.9638815023179239]
	TIME [epoch: 1.36 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0748386046818654		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 1.0748386046818654 | validation: 1.006338620360563]
	TIME [epoch: 1.36 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0201355096924072		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 1.0201355096924072 | validation: 0.8820652989513725]
	TIME [epoch: 1.36 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9734188678056986		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.9734188678056986 | validation: 0.9176843754705454]
	TIME [epoch: 1.35 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9669252881454401		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.9669252881454401 | validation: 0.8930319490164487]
	TIME [epoch: 1.36 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9718580087422766		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.9718580087422766 | validation: 0.9046377290133525]
	TIME [epoch: 1.36 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9763149510695456		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.9763149510695456 | validation: 0.9207413937366824]
	TIME [epoch: 1.37 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9728736200262216		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.9728736200262216 | validation: 0.8994872171871571]
	TIME [epoch: 1.35 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9778228485628726		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.9778228485628726 | validation: 0.9127532378673614]
	TIME [epoch: 1.36 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9759507687383223		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.9759507687383223 | validation: 0.9302510372393148]
	TIME [epoch: 1.36 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9786972108300778		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.9786972108300778 | validation: 0.9213106928597011]
	TIME [epoch: 1.36 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9826680137703895		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.9826680137703895 | validation: 0.8787268443115848]
	TIME [epoch: 1.36 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9911974718570741		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.9911974718570741 | validation: 0.9841033531962039]
	TIME [epoch: 1.36 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0001860792353516		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 1.0001860792353516 | validation: 0.813838104590686]
	TIME [epoch: 1.36 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0139475493839378		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 1.0139475493839378 | validation: 1.0713783334248823]
	TIME [epoch: 1.36 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0094478840751075		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 1.0094478840751075 | validation: 0.8309233956459589]
	TIME [epoch: 1.35 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9816508123572311		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.9816508123572311 | validation: 0.9360195915251176]
	TIME [epoch: 1.36 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9836154652716366		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.9836154652716366 | validation: 0.9213563289323283]
	TIME [epoch: 1.37 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.980944050809752		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.980944050809752 | validation: 0.9188721884362206]
	TIME [epoch: 1.36 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9826669388511552		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.9826669388511552 | validation: 0.8989546045554031]
	TIME [epoch: 1.36 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9589165624449012		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.9589165624449012 | validation: 0.9170044282928583]
	TIME [epoch: 1.36 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9630441118802403		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.9630441118802403 | validation: 0.8593808366197223]
	TIME [epoch: 1.35 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9640441956248857		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.9640441956248857 | validation: 0.9340340130964357]
	TIME [epoch: 1.36 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.971228313384775		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.971228313384775 | validation: 0.8311463340874479]
	TIME [epoch: 1.36 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9757355226134528		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.9757355226134528 | validation: 1.01024861745567]
	TIME [epoch: 1.36 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9863793829809393		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.9863793829809393 | validation: 0.8214952976406587]
	TIME [epoch: 1.36 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9958441653764618		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.9958441653764618 | validation: 0.979336389543229]
	TIME [epoch: 1.36 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9969841893944369		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.9969841893944369 | validation: 0.9612315952925725]
	TIME [epoch: 1.36 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0438460488502082		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 1.0438460488502082 | validation: 1.0139869015766243]
	TIME [epoch: 1.36 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0558570708863184		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 1.0558570708863184 | validation: 0.9686576982145596]
	TIME [epoch: 1.36 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0228737283620208		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 1.0228737283620208 | validation: 0.906172720028091]
	TIME [epoch: 1.36 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9523676204860572		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.9523676204860572 | validation: 0.9042570912600102]
	TIME [epoch: 1.36 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.97715955386069		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.97715955386069 | validation: 0.9045848702970756]
	TIME [epoch: 1.37 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9678612790065921		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.9678612790065921 | validation: 0.9233918481347821]
	TIME [epoch: 1.36 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9674760549912462		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.9674760549912462 | validation: 0.8939491533573038]
	TIME [epoch: 1.36 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9568724426093715		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.9568724426093715 | validation: 0.9252238810815726]
	TIME [epoch: 1.36 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9596068605071559		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.9596068605071559 | validation: 0.8635647900111652]
	TIME [epoch: 1.36 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9581745108732704		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.9581745108732704 | validation: 0.9728228993061396]
	TIME [epoch: 1.36 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9624922232186748		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.9624922232186748 | validation: 0.82108222535123]
	TIME [epoch: 1.36 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9758274161612002		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.9758274161612002 | validation: 0.9911705836528747]
	TIME [epoch: 1.38 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9766429209450144		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.9766429209450144 | validation: 0.8264670358210462]
	TIME [epoch: 1.37 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.978865442194176		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.978865442194176 | validation: 0.951853017159301]
	TIME [epoch: 1.37 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9774751793808589		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.9774751793808589 | validation: 0.9232029955941174]
	TIME [epoch: 1.37 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0155032021713115		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 1.0155032021713115 | validation: 0.9921198355211782]
	TIME [epoch: 1.37 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0190917819779433		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 1.0190917819779433 | validation: 0.9887639411906548]
	TIME [epoch: 1.36 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0036114715408484		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 1.0036114715408484 | validation: 0.8654624523483481]
	TIME [epoch: 1.36 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9606081898375254		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.9606081898375254 | validation: 0.9172006960620562]
	TIME [epoch: 1.36 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9556989200717964		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.9556989200717964 | validation: 0.8960600964715083]
	TIME [epoch: 1.36 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9606480008347387		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.9606480008347387 | validation: 0.9048154598608333]
	TIME [epoch: 1.36 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9539521048958443		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.9539521048958443 | validation: 0.8913812908892057]
	TIME [epoch: 1.37 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9617884378476612		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.9617884378476612 | validation: 0.9376770587172264]
	TIME [epoch: 1.37 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9615940143695153		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.9615940143695153 | validation: 0.8707892592229739]
	TIME [epoch: 1.36 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9564535108481949		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.9564535108481949 | validation: 0.9446395069134601]
	TIME [epoch: 1.36 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9768303241261148		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.9768303241261148 | validation: 0.8803236360577453]
	TIME [epoch: 1.36 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9796291467174179		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.9796291467174179 | validation: 0.9492108243255993]
	TIME [epoch: 1.36 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9702582930710705		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.9702582930710705 | validation: 0.8592005586004883]
	TIME [epoch: 1.36 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9668717501527809		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.9668717501527809 | validation: 0.9152040902656946]
	TIME [epoch: 1.36 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9565153971210285		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.9565153971210285 | validation: 0.837154920063446]
	TIME [epoch: 1.36 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.969352003239648		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.969352003239648 | validation: 1.0097872930646299]
	TIME [epoch: 1.36 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9917791988875172		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.9917791988875172 | validation: 0.8111265709142071]
	TIME [epoch: 1.55 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0129154898987773		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 1.0129154898987773 | validation: 0.9318309379370927]
	TIME [epoch: 1.36 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9649816887372852		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.9649816887372852 | validation: 0.9209988064182497]
	TIME [epoch: 1.36 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9573669998495291		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.9573669998495291 | validation: 0.8807264366308287]
	TIME [epoch: 1.36 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9889703622592493		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.9889703622592493 | validation: 0.9352717470868553]
	TIME [epoch: 1.36 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9885223966656892		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.9885223966656892 | validation: 0.919854666942222]
	TIME [epoch: 1.36 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9784925211095026		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.9784925211095026 | validation: 0.8747891110948331]
	TIME [epoch: 1.36 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9580561102982994		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.9580561102982994 | validation: 0.916867498832676]
	TIME [epoch: 1.36 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9582472215046687		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.9582472215046687 | validation: 0.8676006544552424]
	TIME [epoch: 1.36 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9546625621520373		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.9546625621520373 | validation: 0.9025037605423952]
	TIME [epoch: 1.36 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9527888150274879		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.9527888150274879 | validation: 0.881846151978775]
	TIME [epoch: 1.36 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9575210660717925		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.9575210660717925 | validation: 0.905484030383115]
	TIME [epoch: 1.36 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9656111262480014		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.9656111262480014 | validation: 0.9157181028014005]
	TIME [epoch: 1.36 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.978303921901235		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.978303921901235 | validation: 0.9647324365884652]
	TIME [epoch: 1.35 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0083441328669072		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 1.0083441328669072 | validation: 0.8992177863689751]
	TIME [epoch: 1.36 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9715087224974466		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.9715087224974466 | validation: 0.8391093450016381]
	TIME [epoch: 1.36 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9505465383465651		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.9505465383465651 | validation: 0.9343016788368325]
	TIME [epoch: 1.36 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9519211560261002		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.9519211560261002 | validation: 0.8178550860155696]
	TIME [epoch: 1.36 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9631939146537346		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.9631939146537346 | validation: 0.9707811292614648]
	TIME [epoch: 1.36 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9596449640127052		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.9596449640127052 | validation: 0.8199475507929277]
	TIME [epoch: 1.36 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9566638814955036		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.9566638814955036 | validation: 0.9309774240843574]
	TIME [epoch: 1.36 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9501517830316166		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.9501517830316166 | validation: 0.8831170428948307]
	TIME [epoch: 1.36 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.949511153016617		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.949511153016617 | validation: 0.9027929680712101]
	TIME [epoch: 1.36 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9517241476254811		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.9517241476254811 | validation: 0.9341508404602946]
	TIME [epoch: 1.36 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9816766292544596		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.9816766292544596 | validation: 0.9607190729664975]
	TIME [epoch: 1.37 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0626612317422044		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 1.0626612317422044 | validation: 0.9596593111151741]
	TIME [epoch: 1.36 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9898899783381953		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.9898899783381953 | validation: 0.8190847897516925]
	TIME [epoch: 1.36 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9447311722084405		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.9447311722084405 | validation: 0.9071787225098614]
	TIME [epoch: 1.36 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9425755344129296		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.9425755344129296 | validation: 0.832436954155443]
	TIME [epoch: 1.36 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9476712358383343		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.9476712358383343 | validation: 0.9257623788775672]
	TIME [epoch: 1.36 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9591831653959401		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.9591831653959401 | validation: 0.8500135325119977]
	TIME [epoch: 1.36 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9569110924969384		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.9569110924969384 | validation: 0.9454535250050751]
	TIME [epoch: 1.36 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9662635281650276		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.9662635281650276 | validation: 0.881301509409234]
	TIME [epoch: 1.36 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9517941670798269		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.9517941670798269 | validation: 0.8704899053581168]
	TIME [epoch: 1.36 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9451863704846869		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.9451863704846869 | validation: 0.9096042259890045]
	TIME [epoch: 1.37 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9478784312362925		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.9478784312362925 | validation: 0.8567851602193346]
	TIME [epoch: 1.36 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9567704060984958		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.9567704060984958 | validation: 0.9553588846819419]
	TIME [epoch: 1.35 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9602551416936069		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.9602551416936069 | validation: 0.8752837665984455]
	TIME [epoch: 1.36 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9834586328407744		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.9834586328407744 | validation: 0.9573736954831138]
	TIME [epoch: 1.35 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9750969632730073		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.9750969632730073 | validation: 0.8548150717556565]
	TIME [epoch: 1.36 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9647215028182524		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.9647215028182524 | validation: 0.8818269163802861]
	TIME [epoch: 1.36 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9409852795262443		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.9409852795262443 | validation: 0.8748524472857293]
	TIME [epoch: 1.36 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9433303292204045		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.9433303292204045 | validation: 0.8676136147006028]
	TIME [epoch: 1.36 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9310475530857084		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.9310475530857084 | validation: 0.9148898155847347]
	TIME [epoch: 1.36 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9342363309615063		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.9342363309615063 | validation: 0.7900046791998405]
	TIME [epoch: 1.35 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9494905245099724		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.9494905245099724 | validation: 0.9906342083807896]
	TIME [epoch: 1.35 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9577894669746805		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.9577894669746805 | validation: 0.8359150530862526]
	TIME [epoch: 1.35 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.957058512078162		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.957058512078162 | validation: 0.904554343562414]
	TIME [epoch: 1.36 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9566837030851801		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.9566837030851801 | validation: 0.9836054917098365]
	TIME [epoch: 1.36 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0076685909783554		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 1.0076685909783554 | validation: 0.9217424304071206]
	TIME [epoch: 1.35 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0090171670544594		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 1.0090171670544594 | validation: 0.9095907212547183]
	TIME [epoch: 1.35 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9509504874139321		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.9509504874139321 | validation: 0.8820464949337847]
	TIME [epoch: 1.36 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9308408802768053		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.9308408802768053 | validation: 0.832936762035121]
	TIME [epoch: 1.35 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9302293733415695		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.9302293733415695 | validation: 0.877353715252472]
	TIME [epoch: 1.36 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9363690313759727		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.9363690313759727 | validation: 0.8983304047086018]
	TIME [epoch: 1.35 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9443231241354297		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.9443231241354297 | validation: 0.826595854637894]
	TIME [epoch: 1.36 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9494873359607787		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.9494873359607787 | validation: 0.9179445281052154]
	TIME [epoch: 1.36 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9600126913031912		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.9600126913031912 | validation: 0.8829505625152856]
	TIME [epoch: 1.36 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9648611037376391		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.9648611037376391 | validation: 0.8798141547195567]
	TIME [epoch: 1.35 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9544180345013328		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.9544180345013328 | validation: 0.8898198219623125]
	TIME [epoch: 1.37 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9355559119495978		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.9355559119495978 | validation: 0.8366385118763198]
	TIME [epoch: 1.36 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9295598055584998		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.9295598055584998 | validation: 0.9025744634835009]
	TIME [epoch: 1.35 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9390070077094091		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.9390070077094091 | validation: 0.8452315671989071]
	TIME [epoch: 1.36 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9287435995644805		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.9287435995644805 | validation: 0.8905751237581367]
	TIME [epoch: 1.36 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9327239944755595		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.9327239944755595 | validation: 0.8736823940975956]
	TIME [epoch: 1.35 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9365745174253354		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.9365745174253354 | validation: 0.8975481103316837]
	TIME [epoch: 1.36 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9653225986429778		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.9653225986429778 | validation: 0.9517845043163226]
	TIME [epoch: 1.36 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9944281844597971		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.9944281844597971 | validation: 0.9203756144798172]
	TIME [epoch: 1.35 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9875570542001865		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.9875570542001865 | validation: 0.892180407016242]
	TIME [epoch: 1.35 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9343125968537425		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.9343125968537425 | validation: 0.826021124380881]
	TIME [epoch: 1.35 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9211751168547976		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.9211751168547976 | validation: 0.9407587837211708]
	TIME [epoch: 1.36 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.935286835690011		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.935286835690011 | validation: 0.8263380630246747]
	TIME [epoch: 1.36 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9334361750127912		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.9334361750127912 | validation: 0.9039380438075764]
	TIME [epoch: 1.36 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9239498748211236		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.9239498748211236 | validation: 0.8589231744223561]
	TIME [epoch: 1.36 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9326619149892811		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.9326619149892811 | validation: 0.8767902920312043]
	TIME [epoch: 1.36 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9476947490317761		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.9476947490317761 | validation: 0.9575729221045383]
	TIME [epoch: 1.36 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.985814874348543		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.985814874348543 | validation: 0.8568771357492246]
	TIME [epoch: 1.36 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9704902022972285		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.9704902022972285 | validation: 0.8743374614284671]
	TIME [epoch: 1.36 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9302012541969458		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.9302012541969458 | validation: 0.84489667006003]
	TIME [epoch: 1.35 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9194364401605957		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.9194364401605957 | validation: 0.8377019176087469]
	TIME [epoch: 1.36 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9143484775484039		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.9143484775484039 | validation: 0.8721336473750306]
	TIME [epoch: 1.35 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9227970965302903		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.9227970965302903 | validation: 0.8347196098265073]
	TIME [epoch: 1.36 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9316721704326816		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.9316721704326816 | validation: 0.899021126055334]
	TIME [epoch: 1.35 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9396022134324639		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.9396022134324639 | validation: 0.882114559514805]
	TIME [epoch: 1.35 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9735736984578535		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.9735736984578535 | validation: 0.9134131621209369]
	TIME [epoch: 1.35 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9670882572467794		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.9670882572467794 | validation: 0.8850886254437692]
	TIME [epoch: 1.36 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9450936006370136		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.9450936006370136 | validation: 0.8314248937406016]
	TIME [epoch: 1.35 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9304792886176361		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.9304792886176361 | validation: 0.9051051088363917]
	TIME [epoch: 1.36 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9313113284868894		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.9313113284868894 | validation: 0.8114366354423794]
	TIME [epoch: 1.37 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9224367714966363		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.9224367714966363 | validation: 0.8756184629698116]
	TIME [epoch: 1.35 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9164561394635147		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.9164561394635147 | validation: 0.8543694276621304]
	TIME [epoch: 1.35 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9105644502641012		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.9105644502641012 | validation: 0.8507078945064264]
	TIME [epoch: 1.36 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9212391392947272		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.9212391392947272 | validation: 0.8849557940201701]
	TIME [epoch: 1.35 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.943462035731996		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.943462035731996 | validation: 0.9447289499374663]
	TIME [epoch: 1.36 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9882494763881221		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.9882494763881221 | validation: 0.912879881221663]
	TIME [epoch: 1.35 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.968122855728791		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.968122855728791 | validation: 0.8605586252696725]
	TIME [epoch: 1.36 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9315291797443516		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.9315291797443516 | validation: 0.8663167100401733]
	TIME [epoch: 1.36 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9181682721356995		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.9181682721356995 | validation: 0.8169121737464536]
	TIME [epoch: 1.35 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9091392203538239		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.9091392203538239 | validation: 0.838791321714462]
	TIME [epoch: 1.37 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9194394558068968		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.9194394558068968 | validation: 0.8309007902445427]
	TIME [epoch: 1.36 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9165420576875773		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.9165420576875773 | validation: 0.8983020761377661]
	TIME [epoch: 1.35 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9278269279842479		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.9278269279842479 | validation: 0.8205077044746114]
	TIME [epoch: 1.36 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9434161563233664		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.9434161563233664 | validation: 0.9415003751047148]
	TIME [epoch: 1.36 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9591210194027405		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.9591210194027405 | validation: 0.8946969101259357]
	TIME [epoch: 1.36 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9527543346392523		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.9527543346392523 | validation: 0.8098915506803532]
	TIME [epoch: 1.36 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9478124545052063		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.9478124545052063 | validation: 0.9036577300743405]
	TIME [epoch: 1.36 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9295240118880551		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.9295240118880551 | validation: 0.8158496930891783]
	TIME [epoch: 1.36 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9198957348782776		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.9198957348782776 | validation: 0.8501959918031764]
	TIME [epoch: 1.37 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9097310231658408		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.9097310231658408 | validation: 0.8339711496613795]
	TIME [epoch: 1.36 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9122608880828914		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.9122608880828914 | validation: 0.8291231425874926]
	TIME [epoch: 1.36 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9075457946584823		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.9075457946584823 | validation: 0.8678065628006412]
	TIME [epoch: 1.36 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9110664707390547		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.9110664707390547 | validation: 0.822167330750661]
	TIME [epoch: 1.36 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9105798817510538		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.9105798817510538 | validation: 0.8908081243708973]
	TIME [epoch: 1.36 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9224792347514001		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.9224792347514001 | validation: 0.8914672648239995]
	TIME [epoch: 1.36 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9597685254021802		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.9597685254021802 | validation: 0.9460873343587277]
	TIME [epoch: 1.36 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0205986701104057		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 1.0205986701104057 | validation: 0.9063388408057009]
	TIME [epoch: 1.36 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9465237648430693		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.9465237648430693 | validation: 0.817266943628664]
	TIME [epoch: 1.37 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.90340057349826		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.90340057349826 | validation: 0.830710109567519]
	TIME [epoch: 1.36 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.899708917490933		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.899708917490933 | validation: 0.8268181071452688]
	TIME [epoch: 1.36 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9047792084305485		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.9047792084305485 | validation: 0.8671162411569807]
	TIME [epoch: 1.36 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9030404248816569		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.9030404248816569 | validation: 0.81530231897636]
	TIME [epoch: 1.36 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9096106774006506		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.9096106774006506 | validation: 0.9095461595123195]
	TIME [epoch: 1.36 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9341915279301771		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.9341915279301771 | validation: 0.857220063430238]
	TIME [epoch: 1.36 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9478296664197744		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.9478296664197744 | validation: 0.9108261536314122]
	TIME [epoch: 1.36 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9409824304995954		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.9409824304995954 | validation: 0.8500416467116019]
	TIME [epoch: 1.36 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9241639648880206		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.9241639648880206 | validation: 0.8270843115499865]
	TIME [epoch: 1.36 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9093742249250305		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.9093742249250305 | validation: 0.8839373826219575]
	TIME [epoch: 1.36 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.92089632998458		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.92089632998458 | validation: 0.778679393863828]
	TIME [epoch: 1.36 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9143087888019839		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.9143087888019839 | validation: 0.8428171039893968]
	TIME [epoch: 1.36 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9153607747425815		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.9153607747425815 | validation: 0.8453606781724251]
	TIME [epoch: 1.37 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9117407571858803		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.9117407571858803 | validation: 0.8342426392989731]
	TIME [epoch: 1.36 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9086938129335163		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.9086938129335163 | validation: 0.8849748302755812]
	TIME [epoch: 180 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9212235274905537		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.9212235274905537 | validation: 0.8472706619082175]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_3_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_3_v_mmd4_502.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 1123.221 seconds.
