Args:
Namespace(name='model_phi1_4a_distortion_v2_8_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v2_8_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_8/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_8/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.012915855, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2656701184

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2_8_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_8_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.286093782688739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.286093782688739 | validation: 6.050575887801824]
	TIME [epoch: 176 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_8_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_8_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.042776883358788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.042776883358788 | validation: 6.476660127316606]
	TIME [epoch: 0.703 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.0671225652530545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.0671225652530545 | validation: 6.923125359631334]
	TIME [epoch: 0.657 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.279065552430514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.279065552430514 | validation: 6.314199481318304]
	TIME [epoch: 0.657 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.590758447363615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.590758447363615 | validation: 5.773470002370482]
	TIME [epoch: 0.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_8_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_8_v_mmd4_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.227359230146809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.227359230146809 | validation: 5.224731469853442]
	TIME [epoch: 0.659 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_8_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_8_v_mmd4_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.991983914281104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.991983914281104 | validation: 4.55520536717395]
	TIME [epoch: 0.659 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_8_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_8_v_mmd4_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.4484943446893785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4484943446893785 | validation: 4.255357449637368]
	TIME [epoch: 0.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_8_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_8_v_mmd4_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.803648002429923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.803648002429923 | validation: 4.508773975137365]
	TIME [epoch: 0.657 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.121506131283906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.121506131283906 | validation: 3.020426259978734]
	TIME [epoch: 0.661 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_8_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_8_v_mmd4_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.223259184763856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.223259184763856 | validation: 3.3988941757339406]
	TIME [epoch: 0.661 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9908934178962547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9908934178962547 | validation: 2.7772858421773865]
	TIME [epoch: 0.657 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_8_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_8_v_mmd4_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7441028013668745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7441028013668745 | validation: 2.9891450122454355]
	TIME [epoch: 0.659 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4455715226542565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4455715226542565 | validation: 2.9481437171173153]
	TIME [epoch: 0.659 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1422102364860507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1422102364860507 | validation: 2.5714311406067463]
	TIME [epoch: 0.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_8_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_8_v_mmd4_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.452596917263302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.452596917263302 | validation: 2.353903105468246]
	TIME [epoch: 0.656 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_8_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_8_v_mmd4_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7221620966103353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7221620966103353 | validation: 2.233945348003119]
	TIME [epoch: 0.658 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_8_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_8_v_mmd4_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.401130019205938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.401130019205938 | validation: 2.308850563043501]
	TIME [epoch: 0.657 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.147706588110416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.147706588110416 | validation: 1.730323981761709]
	TIME [epoch: 0.657 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_8_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_8_v_mmd4_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9791158042028663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9791158042028663 | validation: 1.8277814827894527]
	TIME [epoch: 0.658 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0657535136763205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0657535136763205 | validation: 1.651641480390283]
	TIME [epoch: 0.661 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_8_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_8_v_mmd4_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.067280875564555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.067280875564555 | validation: 1.7778453213972]
	TIME [epoch: 0.659 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9036040205658165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9036040205658165 | validation: 1.5738478503157094]
	TIME [epoch: 0.656 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_8_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_8_v_mmd4_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0401533697595893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0401533697595893 | validation: 2.4216769746663913]
	TIME [epoch: 0.659 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0336151752790186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0336151752790186 | validation: 1.5984907892253248]
	TIME [epoch: 0.657 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6682265217444587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6682265217444587 | validation: 1.4528625284282926]
	TIME [epoch: 0.656 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_8_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_8_v_mmd4_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8811754704695163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8811754704695163 | validation: 1.7028013943904015]
	TIME [epoch: 0.66 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6769735608119456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6769735608119456 | validation: 1.5496342641464989]
	TIME [epoch: 0.659 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5960363659795753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5960363659795753 | validation: 1.1768089626851947]
	TIME [epoch: 0.658 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_8_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_8_v_mmd4_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5333600591767453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5333600591767453 | validation: 1.3347879998429921]
	TIME [epoch: 0.661 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4998005981783635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4998005981783635 | validation: 1.3899381961254988]
	TIME [epoch: 0.663 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5224071707420381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5224071707420381 | validation: 1.3192619723548633]
	TIME [epoch: 0.658 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4985737760436586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4985737760436586 | validation: 1.3191911342928038]
	TIME [epoch: 0.661 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5034146422696615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5034146422696615 | validation: 1.4020437637098657]
	TIME [epoch: 0.655 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.446344216682625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.446344216682625 | validation: 1.2552266202766884]
	TIME [epoch: 0.656 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4097293867957859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4097293867957859 | validation: 1.2132853071710614]
	TIME [epoch: 0.655 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3991682389456082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3991682389456082 | validation: 1.339000334156509]
	TIME [epoch: 0.658 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.435150537748471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.435150537748471 | validation: 1.182302743261138]
	TIME [epoch: 0.657 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3925138198344018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3925138198344018 | validation: 1.2928139493704913]
	TIME [epoch: 0.656 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3946316179376712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3946316179376712 | validation: 1.2117060137155153]
	TIME [epoch: 0.656 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3320381480551502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3320381480551502 | validation: 1.2515160725166083]
	TIME [epoch: 0.658 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3787695499700698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3787695499700698 | validation: 1.262043943643227]
	TIME [epoch: 0.657 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.318537391349103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.318537391349103 | validation: 1.2417290745576297]
	TIME [epoch: 0.657 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3784507482000923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3784507482000923 | validation: 1.445256873188282]
	TIME [epoch: 0.658 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3571521967539206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3571521967539206 | validation: 1.1425670258951104]
	TIME [epoch: 0.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_8_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_8_v_mmd4_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.37077027387493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.37077027387493 | validation: 1.3205496788497646]
	TIME [epoch: 0.658 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2906613840335948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2906613840335948 | validation: 1.2190681377734913]
	TIME [epoch: 0.657 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2945102871016194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2945102871016194 | validation: 1.102601127897542]
	TIME [epoch: 0.655 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_8_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_8_v_mmd4_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3340661954957582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3340661954957582 | validation: 1.488322973595137]
	TIME [epoch: 0.658 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.463039501616178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.463039501616178 | validation: 1.0808301795311812]
	TIME [epoch: 0.657 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_8_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_8_v_mmd4_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2091818039384457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2091818039384457 | validation: 1.0918456363832185]
	TIME [epoch: 0.658 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1969232857803096		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.1969232857803096 | validation: 1.4239784131016266]
	TIME [epoch: 0.657 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3889450607836602		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.3889450607836602 | validation: 1.0561243857747413]
	TIME [epoch: 0.656 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_8_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_8_v_mmd4_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.206373238224311		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.206373238224311 | validation: 1.2128252480947321]
	TIME [epoch: 0.655 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2061932769939527		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.2061932769939527 | validation: 1.3331675308231092]
	TIME [epoch: 0.655 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6876051606822067		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.6876051606822067 | validation: 1.1440927285810638]
	TIME [epoch: 0.658 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.241452461726255		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.241452461726255 | validation: 1.2654357687958022]
	TIME [epoch: 0.655 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3225834859013974		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.3225834859013974 | validation: 1.2394207305674714]
	TIME [epoch: 0.655 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3247738595952023		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.3247738595952023 | validation: 1.3095596979723916]
	TIME [epoch: 0.656 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3651587429190528		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.3651587429190528 | validation: 1.1447289047139564]
	TIME [epoch: 0.658 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2504522356675634		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.2504522356675634 | validation: 1.1380553147542958]
	TIME [epoch: 0.655 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2321619455307764		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.2321619455307764 | validation: 1.1565471058244612]
	TIME [epoch: 0.656 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.231901791797627		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.231901791797627 | validation: 1.223333774283898]
	TIME [epoch: 0.656 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2706458799963545		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.2706458799963545 | validation: 1.1871873903704129]
	TIME [epoch: 0.656 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3066595050730996		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.3066595050730996 | validation: 1.4339147271997776]
	TIME [epoch: 0.654 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4159848419686936		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.4159848419686936 | validation: 1.1352474687154737]
	TIME [epoch: 0.656 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2207381378390512		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.2207381378390512 | validation: 1.1190496213402827]
	TIME [epoch: 0.656 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.195897414743648		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.195897414743648 | validation: 1.1356793921891415]
	TIME [epoch: 0.654 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1899533006534873		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.1899533006534873 | validation: 1.104379430383353]
	TIME [epoch: 0.657 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1879730159519182		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.1879730159519182 | validation: 1.1777421727678137]
	TIME [epoch: 0.659 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2036869053957504		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.2036869053957504 | validation: 1.0970053117038476]
	TIME [epoch: 0.659 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.196230227125455		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.196230227125455 | validation: 1.1654011004791525]
	TIME [epoch: 0.658 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2253129404146887		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.2253129404146887 | validation: 1.3442225322507118]
	TIME [epoch: 0.663 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.369142773855856		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.369142773855856 | validation: 1.2783157371619893]
	TIME [epoch: 0.657 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3845912348964737		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.3845912348964737 | validation: 1.5418248715952716]
	TIME [epoch: 0.655 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4634806937201341		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.4634806937201341 | validation: 1.064097305975899]
	TIME [epoch: 0.654 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.235190674306748		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.235190674306748 | validation: 1.2712240765557181]
	TIME [epoch: 0.656 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3051898248993246		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.3051898248993246 | validation: 1.4799603242280308]
	TIME [epoch: 0.655 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.420218908349563		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.420218908349563 | validation: 1.0922414393583963]
	TIME [epoch: 0.654 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.205957184555878		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.205957184555878 | validation: 1.1897193097270813]
	TIME [epoch: 0.658 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2556949318510018		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.2556949318510018 | validation: 1.386485016097096]
	TIME [epoch: 0.657 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3491944394321544		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.3491944394321544 | validation: 1.0782817875888304]
	TIME [epoch: 0.656 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2167218222060106		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.2167218222060106 | validation: 1.1700914960253652]
	TIME [epoch: 0.656 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1906896485968894		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.1906896485968894 | validation: 1.1482024409513614]
	TIME [epoch: 0.656 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1948233317826367		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.1948233317826367 | validation: 1.1172703266770894]
	TIME [epoch: 0.659 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1891448371216458		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.1891448371216458 | validation: 1.1329016007427526]
	TIME [epoch: 0.658 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1869410494804395		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.1869410494804395 | validation: 1.1456983846849944]
	TIME [epoch: 0.658 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1927904644306642		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.1927904644306642 | validation: 1.1273375174489828]
	TIME [epoch: 0.656 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2264650979024565		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.2264650979024565 | validation: 1.4135733019387764]
	TIME [epoch: 0.656 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3871836364575114		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.3871836364575114 | validation: 1.130128075530487]
	TIME [epoch: 0.654 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2683810466271228		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.2683810466271228 | validation: 1.3110130729664702]
	TIME [epoch: 0.655 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2957950664062146		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.2957950664062146 | validation: 1.1026829561266531]
	TIME [epoch: 0.655 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2250439360395458		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.2250439360395458 | validation: 1.2289739210486847]
	TIME [epoch: 0.656 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2519228007545093		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.2519228007545093 | validation: 1.127021790949989]
	TIME [epoch: 0.657 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2333563495748707		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.2333563495748707 | validation: 1.2733336102708812]
	TIME [epoch: 0.655 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2661401302004527		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.2661401302004527 | validation: 1.319270230851199]
	TIME [epoch: 0.659 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4824913053020499		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.4824913053020499 | validation: 1.1197687371252079]
	TIME [epoch: 0.656 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2282094149452605		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.2282094149452605 | validation: 1.2267187123671215]
	TIME [epoch: 0.656 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2560475960940096		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.2560475960940096 | validation: 1.138581188608787]
	TIME [epoch: 0.657 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2365794804446975		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.2365794804446975 | validation: 1.1477693197786174]
	TIME [epoch: 0.662 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2227421602261168		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.2227421602261168 | validation: 1.11884197621461]
	TIME [epoch: 0.658 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2211898447859102		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.2211898447859102 | validation: 1.1547424740281913]
	TIME [epoch: 0.657 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2117382912746992		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.2117382912746992 | validation: 1.1629530243834942]
	TIME [epoch: 0.656 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2116154375273973		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.2116154375273973 | validation: 1.2103567409624276]
	TIME [epoch: 0.653 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.240273742193419		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.240273742193419 | validation: 1.1549422003273049]
	TIME [epoch: 0.655 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2655978104998804		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.2655978104998804 | validation: 1.3435188506003395]
	TIME [epoch: 0.655 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3239276487145832		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.3239276487145832 | validation: 1.1202226759780367]
	TIME [epoch: 0.655 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2157836946545988		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.2157836946545988 | validation: 1.1438736925870152]
	TIME [epoch: 0.653 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1857078858332317		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.1857078858332317 | validation: 1.0980622723493312]
	TIME [epoch: 0.656 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1890512550884424		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.1890512550884424 | validation: 1.133250296027797]
	TIME [epoch: 0.654 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1815002525287523		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.1815002525287523 | validation: 1.0988845118999329]
	TIME [epoch: 0.655 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.180776787509941		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.180776787509941 | validation: 1.1543066483081676]
	TIME [epoch: 0.653 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1957730483508713		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.1957730483508713 | validation: 1.1678303104535002]
	TIME [epoch: 0.658 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2444891646376681		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.2444891646376681 | validation: 1.4367469841374398]
	TIME [epoch: 0.658 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4005226902493235		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.4005226902493235 | validation: 1.124717362652626]
	TIME [epoch: 0.659 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2041960509369103		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.2041960509369103 | validation: 1.1600330074945997]
	TIME [epoch: 0.656 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.189874469208955		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.189874469208955 | validation: 1.1366908571548942]
	TIME [epoch: 0.663 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1894707714374833		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.1894707714374833 | validation: 1.163271157766096]
	TIME [epoch: 0.654 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.211041675057956		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.211041675057956 | validation: 1.2830435838388674]
	TIME [epoch: 0.656 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2870339212758302		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.2870339212758302 | validation: 1.1459538989213116]
	TIME [epoch: 0.655 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.271013817923696		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.271013817923696 | validation: 1.3390316982255552]
	TIME [epoch: 0.658 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2906170330905302		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.2906170330905302 | validation: 1.064202432503897]
	TIME [epoch: 0.652 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2083299739153497		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.2083299739153497 | validation: 1.193644185723227]
	TIME [epoch: 0.655 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1954368379498543		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.1954368379498543 | validation: 1.1005006376264759]
	TIME [epoch: 0.654 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1861127654096575		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.1861127654096575 | validation: 1.1545629969816524]
	TIME [epoch: 0.656 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1976693164857597		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.1976693164857597 | validation: 1.1463195194967177]
	TIME [epoch: 0.654 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2139497785770883		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.2139497785770883 | validation: 1.2802087692675332]
	TIME [epoch: 0.655 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2818554314277133		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.2818554314277133 | validation: 1.1124482465792436]
	TIME [epoch: 0.657 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2354955612935026		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.2354955612935026 | validation: 1.2529052129409528]
	TIME [epoch: 0.659 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2445812370275935		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.2445812370275935 | validation: 1.0907893896240641]
	TIME [epoch: 0.656 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.21070263599982		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.21070263599982 | validation: 1.1838535491500208]
	TIME [epoch: 0.656 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2067142886779527		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.2067142886779527 | validation: 1.1227263197416586]
	TIME [epoch: 0.654 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2009577855737406		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.2009577855737406 | validation: 1.2063291052448535]
	TIME [epoch: 0.655 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2275553259542002		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.2275553259542002 | validation: 1.149350498869755]
	TIME [epoch: 0.654 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2202987785547075		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.2202987785547075 | validation: 1.2425362010221752]
	TIME [epoch: 0.655 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2487353069818345		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.2487353069818345 | validation: 1.1208715461009926]
	TIME [epoch: 0.655 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.206075138716667		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.206075138716667 | validation: 1.1645296858783556]
	TIME [epoch: 0.654 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1982473459514515		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.1982473459514515 | validation: 1.0960427351386646]
	TIME [epoch: 0.653 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1878916087697224		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.1878916087697224 | validation: 1.20337746678969]
	TIME [epoch: 0.659 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2029057415270157		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.2029057415270157 | validation: 1.0480385884703847]
	TIME [epoch: 0.654 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_8_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_8_v_mmd4_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2105152401631722		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.2105152401631722 | validation: 1.2153776983827373]
	TIME [epoch: 0.659 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2178547977870087		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.2178547977870087 | validation: 1.1336847777389367]
	TIME [epoch: 0.659 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2140333678176969		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.2140333678176969 | validation: 1.219293601007308]
	TIME [epoch: 0.658 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2453304412812394		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.2453304412812394 | validation: 1.143673708043252]
	TIME [epoch: 0.658 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.192702883150197		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.192702883150197 | validation: 1.1435973150738523]
	TIME [epoch: 0.658 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.190813105834217		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.190813105834217 | validation: 1.0936750361031524]
	TIME [epoch: 0.656 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1880581440607154		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.1880581440607154 | validation: 1.1915874835213311]
	TIME [epoch: 0.656 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.20524262564906		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.20524262564906 | validation: 1.1128701284978428]
	TIME [epoch: 0.656 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2105497106912868		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.2105497106912868 | validation: 1.2476608984700939]
	TIME [epoch: 0.654 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.246086289957086		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.246086289957086 | validation: 1.1201641276261112]
	TIME [epoch: 0.654 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.207168207288625		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.207168207288625 | validation: 1.1739622618789898]
	TIME [epoch: 0.653 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2019898714822115		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.2019898714822115 | validation: 1.1222340094875443]
	TIME [epoch: 0.656 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.193215596707404		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.193215596707404 | validation: 1.1638779300849993]
	TIME [epoch: 0.654 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.192416074823272		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.192416074823272 | validation: 1.0959904778375955]
	TIME [epoch: 0.655 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2058688766503707		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.2058688766503707 | validation: 1.2142153608629305]
	TIME [epoch: 0.654 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2180958120751666		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.2180958120751666 | validation: 1.0885960025302737]
	TIME [epoch: 0.656 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.208594934267785		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.208594934267785 | validation: 1.2181690500134053]
	TIME [epoch: 0.658 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2075709644719377		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.2075709644719377 | validation: 1.096870986203231]
	TIME [epoch: 0.657 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1969341734409016		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.1969341734409016 | validation: 1.1711795066601323]
	TIME [epoch: 0.659 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1981243305734794		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.1981243305734794 | validation: 1.105663391670252]
	TIME [epoch: 0.656 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.203526869938276		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.203526869938276 | validation: 1.185969900899798]
	TIME [epoch: 0.661 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.210498311050447		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.210498311050447 | validation: 1.0981555974171788]
	TIME [epoch: 0.655 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1847050768114922		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.1847050768114922 | validation: 1.1242071024511628]
	TIME [epoch: 0.655 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.191542429147589		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.191542429147589 | validation: 1.100643568712268]
	TIME [epoch: 0.655 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1754969846221361		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.1754969846221361 | validation: 1.1613631776658553]
	TIME [epoch: 0.654 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1899732993663563		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.1899732993663563 | validation: 1.0871046087919827]
	TIME [epoch: 0.654 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.205105432740793		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.205105432740793 | validation: 1.2376629093550289]
	TIME [epoch: 0.66 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2404172849655195		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.2404172849655195 | validation: 1.085448560184689]
	TIME [epoch: 0.654 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1957662201001		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.1957662201001 | validation: 1.1406227153751882]
	TIME [epoch: 0.656 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1842016265739035		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.1842016265739035 | validation: 1.0963165249988378]
	TIME [epoch: 0.656 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1773056843669583		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.1773056843669583 | validation: 1.157174753634584]
	TIME [epoch: 0.659 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1804607984117694		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.1804607984117694 | validation: 1.1199532146695947]
	TIME [epoch: 0.657 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.182434601790515		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.182434601790515 | validation: 1.1857284869567146]
	TIME [epoch: 0.658 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2135295829763697		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.2135295829763697 | validation: 1.0911679494821545]
	TIME [epoch: 0.657 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2032451564471531		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.2032451564471531 | validation: 1.228814017843157]
	TIME [epoch: 0.657 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.222638502752915		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.222638502752915 | validation: 1.0632878105492078]
	TIME [epoch: 0.659 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.19195313354055		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.19195313354055 | validation: 1.1502474351288097]
	TIME [epoch: 0.658 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1800227013040634		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.1800227013040634 | validation: 1.0932316763894905]
	TIME [epoch: 0.658 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1684777174849479		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.1684777174849479 | validation: 1.1062553826000336]
	TIME [epoch: 0.659 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1766464845722373		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.1766464845722373 | validation: 1.1257280717082392]
	TIME [epoch: 0.656 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1757292859576178		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.1757292859576178 | validation: 1.1375002539886836]
	TIME [epoch: 0.656 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1761251989447647		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.1761251989447647 | validation: 1.1111594621692178]
	TIME [epoch: 0.656 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2078752868459801		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.2078752868459801 | validation: 1.2915399504091962]
	TIME [epoch: 0.659 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2628248157030504		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.2628248157030504 | validation: 1.0833933083254634]
	TIME [epoch: 0.655 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1784526854669823		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.1784526854669823 | validation: 1.1185283340592063]
	TIME [epoch: 0.659 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.176196346374753		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.176196346374753 | validation: 1.1067937708695477]
	TIME [epoch: 0.659 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1659511698122582		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.1659511698122582 | validation: 1.1003110620185381]
	TIME [epoch: 0.659 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1652715299094667		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.1652715299094667 | validation: 1.1381411626058149]
	TIME [epoch: 0.658 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1770078495233582		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.1770078495233582 | validation: 1.1471748404390647]
	TIME [epoch: 0.657 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1888000374600987		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.1888000374600987 | validation: 1.1088010090354912]
	TIME [epoch: 0.657 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.216881375564409		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.216881375564409 | validation: 1.2919170929131831]
	TIME [epoch: 0.655 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2664444715016674		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.2664444715016674 | validation: 1.1128574545951608]
	TIME [epoch: 0.655 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1662328969068154		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.1662328969068154 | validation: 1.0588656973616648]
	TIME [epoch: 0.655 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.172617587598437		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.172617587598437 | validation: 1.156362044157306]
	TIME [epoch: 0.656 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.199297412976586		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.199297412976586 | validation: 1.1339408999535074]
	TIME [epoch: 0.654 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2048639347512478		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.2048639347512478 | validation: 1.1676311034347973]
	TIME [epoch: 0.655 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.207045495987905		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.207045495987905 | validation: 1.087919879765867]
	TIME [epoch: 0.655 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.172298460657895		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.172298460657895 | validation: 1.1145928519468806]
	TIME [epoch: 0.654 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1666340295955895		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.1666340295955895 | validation: 1.084172065677416]
	TIME [epoch: 0.656 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1682414674279469		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.1682414674279469 | validation: 1.114497968757421]
	TIME [epoch: 0.66 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1696479063365741		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.1696479063365741 | validation: 1.091749331801308]
	TIME [epoch: 185 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.167946851889702		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.167946851889702 | validation: 1.1199655615109205]
	TIME [epoch: 1.3 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.169312247629276		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.169312247629276 | validation: 1.107897657555507]
	TIME [epoch: 1.28 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1825664297476712		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.1825664297476712 | validation: 1.2320053109432907]
	TIME [epoch: 1.28 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2085210305678382		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.2085210305678382 | validation: 1.065785601586612]
	TIME [epoch: 1.28 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2050230093556737		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.2050230093556737 | validation: 1.19639217234875]
	TIME [epoch: 1.29 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2011237915173432		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.2011237915173432 | validation: 1.0945104651936508]
	TIME [epoch: 1.28 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1575896446567056		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.1575896446567056 | validation: 1.0777902987521006]
	TIME [epoch: 1.28 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1704365370134227		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.1704365370134227 | validation: 1.1164118907216238]
	TIME [epoch: 1.28 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1562723909355517		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.1562723909355517 | validation: 1.087797522423474]
	TIME [epoch: 1.28 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1663182221593837		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.1663182221593837 | validation: 1.1109024709378126]
	TIME [epoch: 1.28 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.16041601194841		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.16041601194841 | validation: 1.1119595288321402]
	TIME [epoch: 1.29 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1865843066051032		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 1.1865843066051032 | validation: 1.154775667974366]
	TIME [epoch: 1.28 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1994785105449355		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.1994785105449355 | validation: 1.2074760977118508]
	TIME [epoch: 1.29 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2223736326732018		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 1.2223736326732018 | validation: 1.0499284732013159]
	TIME [epoch: 1.28 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.169822710759691		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.169822710759691 | validation: 1.1492679493280649]
	TIME [epoch: 1.28 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1610346785963748		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.1610346785963748 | validation: 1.0872982522877617]
	TIME [epoch: 1.28 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.166151631289809		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.166151631289809 | validation: 1.1016694308661907]
	TIME [epoch: 1.28 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.155500425825725		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.155500425825725 | validation: 1.1134608071396614]
	TIME [epoch: 1.28 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1698846069824287		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.1698846069824287 | validation: 1.0872208348526318]
	TIME [epoch: 1.28 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.165283232600532		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 1.165283232600532 | validation: 1.1811039404126622]
	TIME [epoch: 1.28 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1825198834052861		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.1825198834052861 | validation: 1.0872535930446474]
	TIME [epoch: 1.28 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2078975310850932		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 1.2078975310850932 | validation: 1.2159956124332236]
	TIME [epoch: 1.29 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2187690118103434		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.2187690118103434 | validation: 1.1154370013375672]
	TIME [epoch: 1.28 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1601806200826092		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 1.1601806200826092 | validation: 1.0902435039073242]
	TIME [epoch: 1.28 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1639269279533244		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 1.1639269279533244 | validation: 1.1699496471430066]
	TIME [epoch: 1.28 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1893894598409593		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.1893894598409593 | validation: 1.1031093887788548]
	TIME [epoch: 1.28 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.17786987197705		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.17786987197705 | validation: 1.165819424440609]
	TIME [epoch: 1.28 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.172508872215119		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 1.172508872215119 | validation: 1.0468397281155362]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_8_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_8_v_mmd4_229.pth
	Model improved!!!
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1610599117934166		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.1610599117934166 | validation: 1.1084089868797005]
	TIME [epoch: 1.28 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1579838181649462		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 1.1579838181649462 | validation: 1.0964865595935105]
	TIME [epoch: 1.29 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1518363908391702		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.1518363908391702 | validation: 1.1166152756689278]
	TIME [epoch: 1.28 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1700779067184648		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 1.1700779067184648 | validation: 1.127597420814565]
	TIME [epoch: 1.29 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1768569505681945		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.1768569505681945 | validation: 1.141990816834289]
	TIME [epoch: 1.28 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1830826438652013		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 1.1830826438652013 | validation: 1.0937977215350683]
	TIME [epoch: 1.28 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.159820360744905		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.159820360744905 | validation: 1.1902398581544724]
	TIME [epoch: 1.28 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1740624876556403		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 1.1740624876556403 | validation: 1.0536279669303328]
	TIME [epoch: 1.28 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1654802727012104		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 1.1654802727012104 | validation: 1.13455820077183]
	TIME [epoch: 1.28 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1504579031135802		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 1.1504579031135802 | validation: 1.124838023424641]
	TIME [epoch: 1.28 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1631112000582329		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 1.1631112000582329 | validation: 1.095710904718534]
	TIME [epoch: 1.28 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1632323157874767		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 1.1632323157874767 | validation: 1.1317458145436263]
	TIME [epoch: 1.28 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.173016328728389		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 1.173016328728389 | validation: 1.1444401483002782]
	TIME [epoch: 1.28 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.172959491801139		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 1.172959491801139 | validation: 1.0948407942898004]
	TIME [epoch: 1.28 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.17709308055879		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 1.17709308055879 | validation: 1.1843491380604103]
	TIME [epoch: 1.28 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1843402035227057		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 1.1843402035227057 | validation: 1.0751408240938911]
	TIME [epoch: 1.28 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1655328778237737		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.1655328778237737 | validation: 1.1321134909276511]
	TIME [epoch: 1.29 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.155499088870234		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 1.155499088870234 | validation: 1.0908820752221478]
	TIME [epoch: 1.28 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1567140140766168		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 1.1567140140766168 | validation: 1.1193752802038899]
	TIME [epoch: 1.28 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1576494194056735		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 1.1576494194056735 | validation: 1.0714205543541928]
	TIME [epoch: 1.28 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.165146824393805		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 1.165146824393805 | validation: 1.1652833952771073]
	TIME [epoch: 1.28 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1745791431022214		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 1.1745791431022214 | validation: 1.0466203534931684]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_8_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_8_v_mmd4_251.pth
	Model improved!!!
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.175239339860378		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 1.175239339860378 | validation: 1.162430662188947]
	TIME [epoch: 1.28 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1773215095616507		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 1.1773215095616507 | validation: 1.0683229984993758]
	TIME [epoch: 1.28 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.156102156888629		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 1.156102156888629 | validation: 1.0855555397553485]
	TIME [epoch: 1.28 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1508921691092153		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 1.1508921691092153 | validation: 1.1382420001719975]
	TIME [epoch: 1.28 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.155885491694895		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 1.155885491694895 | validation: 1.0769209047608395]
	TIME [epoch: 1.28 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1493835435253599		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 1.1493835435253599 | validation: 1.153955936131705]
	TIME [epoch: 1.28 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.150429060968852		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 1.150429060968852 | validation: 1.0874496829781193]
	TIME [epoch: 1.28 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1623236136851274		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 1.1623236136851274 | validation: 1.1484099236466399]
	TIME [epoch: 1.28 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1760034833784851		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 1.1760034833784851 | validation: 1.1426581914163731]
	TIME [epoch: 1.28 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.188100727184508		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 1.188100727184508 | validation: 1.1822691535512835]
	TIME [epoch: 1.29 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.178589394474698		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 1.178589394474698 | validation: 1.0769721952005096]
	TIME [epoch: 1.28 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.15650606996033		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 1.15650606996033 | validation: 1.1351158250739888]
	TIME [epoch: 1.28 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1506507473893228		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 1.1506507473893228 | validation: 1.0845994276278532]
	TIME [epoch: 1.28 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1470163501737805		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 1.1470163501737805 | validation: 1.1157677071755139]
	TIME [epoch: 1.28 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.154919249790219		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 1.154919249790219 | validation: 1.1019825072123277]
	TIME [epoch: 1.28 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1512164139466083		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 1.1512164139466083 | validation: 1.1125526947627717]
	TIME [epoch: 1.28 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1504316584025531		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 1.1504316584025531 | validation: 1.1572218109030086]
	TIME [epoch: 1.28 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1658148552510819		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 1.1658148552510819 | validation: 1.0623632418108704]
	TIME [epoch: 1.29 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.171828239214273		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 1.171828239214273 | validation: 1.1698709113975034]
	TIME [epoch: 1.28 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.183695887834792		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 1.183695887834792 | validation: 1.0978557863117606]
	TIME [epoch: 1.28 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1567829799736842		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 1.1567829799736842 | validation: 1.1125436945056044]
	TIME [epoch: 1.28 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1569212781009413		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 1.1569212781009413 | validation: 1.090984243771526]
	TIME [epoch: 1.28 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1525732499968897		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 1.1525732499968897 | validation: 1.1030220233779404]
	TIME [epoch: 1.28 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1525950149958377		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 1.1525950149958377 | validation: 1.1159866950391149]
	TIME [epoch: 1.28 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1499462583503113		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 1.1499462583503113 | validation: 1.1048508873392966]
	TIME [epoch: 1.28 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1498717931774098		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 1.1498717931774098 | validation: 1.140292784264737]
	TIME [epoch: 1.29 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.167449564439108		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 1.167449564439108 | validation: 1.1494056870340363]
	TIME [epoch: 1.28 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1759670410037797		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 1.1759670410037797 | validation: 1.0869380289043347]
	TIME [epoch: 1.28 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1632722953147674		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 1.1632722953147674 | validation: 1.169682738934047]
	TIME [epoch: 1.28 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1566247323675967		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 1.1566247323675967 | validation: 1.056963330540923]
	TIME [epoch: 1.28 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1582122387584124		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 1.1582122387584124 | validation: 1.1603683505699547]
	TIME [epoch: 1.28 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1614927482340796		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 1.1614927482340796 | validation: 1.0831887785971819]
	TIME [epoch: 1.28 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.16122213440531		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 1.16122213440531 | validation: 1.1475161901676925]
	TIME [epoch: 1.28 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1658936203844463		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 1.1658936203844463 | validation: 1.0811192055128096]
	TIME [epoch: 1.28 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1555933894131456		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 1.1555933894131456 | validation: 1.1330183200383583]
	TIME [epoch: 1.28 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1537720017650004		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 1.1537720017650004 | validation: 1.1017413583727265]
	TIME [epoch: 1.28 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1509896940519913		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 1.1509896940519913 | validation: 1.1299505890847088]
	TIME [epoch: 1.28 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1428533134519834		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 1.1428533134519834 | validation: 1.0942101334111634]
	TIME [epoch: 1.28 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1532028644977645		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 1.1532028644977645 | validation: 1.1636205403481397]
	TIME [epoch: 1.29 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.160694803971178		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 1.160694803971178 | validation: 1.0724672540993443]
	TIME [epoch: 1.28 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1528350954577813		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 1.1528350954577813 | validation: 1.136649471593202]
	TIME [epoch: 1.29 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1534681866868313		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 1.1534681866868313 | validation: 1.079655168619404]
	TIME [epoch: 1.28 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1502563178373841		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 1.1502563178373841 | validation: 1.1445607394342612]
	TIME [epoch: 1.28 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1574555932288133		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 1.1574555932288133 | validation: 1.0841903489760985]
	TIME [epoch: 1.28 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.144800395025989		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 1.144800395025989 | validation: 1.1382457275388416]
	TIME [epoch: 1.28 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1508281667755038		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 1.1508281667755038 | validation: 1.1221324935932113]
	TIME [epoch: 1.28 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.149858771769324		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 1.149858771769324 | validation: 1.0829220918796947]
	TIME [epoch: 1.28 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1449079476117274		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 1.1449079476117274 | validation: 1.1590596283809085]
	TIME [epoch: 1.28 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1413943824933768		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 1.1413943824933768 | validation: 1.1314640142810386]
	TIME [epoch: 1.28 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1463221540080193		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 1.1463221540080193 | validation: 1.0932456765847383]
	TIME [epoch: 1.28 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1444436186508202		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 1.1444436186508202 | validation: 1.1721130130858357]
	TIME [epoch: 1.28 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1588374060633462		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 1.1588374060633462 | validation: 1.0769869263451437]
	TIME [epoch: 1.28 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1559044878891518		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 1.1559044878891518 | validation: 1.1357836762353215]
	TIME [epoch: 1.28 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1441920703195811		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 1.1441920703195811 | validation: 1.1078493321478795]
	TIME [epoch: 1.28 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1417053739473564		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 1.1417053739473564 | validation: 1.0889923443467513]
	TIME [epoch: 1.28 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1415026017221719		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 1.1415026017221719 | validation: 1.1334159883631807]
	TIME [epoch: 1.28 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1464787615059517		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 1.1464787615059517 | validation: 1.0941996466600317]
	TIME [epoch: 1.28 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1404577039224497		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 1.1404577039224497 | validation: 1.097158979673096]
	TIME [epoch: 1.28 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1378707804579875		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 1.1378707804579875 | validation: 1.1092495727959468]
	TIME [epoch: 1.28 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.146187790513929		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 1.146187790513929 | validation: 1.0491688431839135]
	TIME [epoch: 1.28 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1535922126931295		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 1.1535922126931295 | validation: 1.1623918266824422]
	TIME [epoch: 1.29 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.16743216761547		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 1.16743216761547 | validation: 1.092495981000267]
	TIME [epoch: 1.28 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.178067037293093		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 1.178067037293093 | validation: 1.168472989898282]
	TIME [epoch: 1.28 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1666776849827132		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 1.1666776849827132 | validation: 1.113581797428703]
	TIME [epoch: 1.29 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1444027213646724		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 1.1444027213646724 | validation: 1.0764694132380963]
	TIME [epoch: 1.28 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1454072090481437		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 1.1454072090481437 | validation: 1.1461126010938798]
	TIME [epoch: 1.28 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1537542704749382		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 1.1537542704749382 | validation: 1.1020323008523447]
	TIME [epoch: 1.28 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.135565405763027		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 1.135565405763027 | validation: 1.09407669161197]
	TIME [epoch: 1.29 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1367660181852046		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 1.1367660181852046 | validation: 1.1062200760199834]
	TIME [epoch: 1.28 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1428489254946974		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 1.1428489254946974 | validation: 1.099693639220962]
	TIME [epoch: 1.28 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1416448357627293		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 1.1416448357627293 | validation: 1.1020429610423284]
	TIME [epoch: 1.28 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1381110948835012		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 1.1381110948835012 | validation: 1.0870048434308799]
	TIME [epoch: 1.28 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1402544891103341		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 1.1402544891103341 | validation: 1.1135027021360246]
	TIME [epoch: 1.28 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1432465751353607		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 1.1432465751353607 | validation: 1.0789059806310644]
	TIME [epoch: 1.28 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1319182536988863		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 1.1319182536988863 | validation: 1.1085700158555898]
	TIME [epoch: 1.28 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1418045786168969		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 1.1418045786168969 | validation: 1.0604358129592026]
	TIME [epoch: 1.28 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1466589921295183		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 1.1466589921295183 | validation: 1.1831605320877734]
	TIME [epoch: 1.28 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1717096487423562		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 1.1717096487423562 | validation: 1.102354332860176]
	TIME [epoch: 1.28 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1571916889846363		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 1.1571916889846363 | validation: 1.1176726486047261]
	TIME [epoch: 1.28 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.149446751092125		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 1.149446751092125 | validation: 1.1108538139071562]
	TIME [epoch: 1.28 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1315444267328079		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 1.1315444267328079 | validation: 1.0927031094493846]
	TIME [epoch: 1.28 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.140926601065572		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 1.140926601065572 | validation: 1.1266962843451076]
	TIME [epoch: 1.29 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.137226692107542		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 1.137226692107542 | validation: 1.0887989039597088]
	TIME [epoch: 1.28 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1425740961988897		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 1.1425740961988897 | validation: 1.1526482662020354]
	TIME [epoch: 1.28 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1530988264008313		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 1.1530988264008313 | validation: 1.0708532746175468]
	TIME [epoch: 1.28 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1464080922886297		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 1.1464080922886297 | validation: 1.116979711964693]
	TIME [epoch: 1.28 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1312643503616087		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 1.1312643503616087 | validation: 1.1267386880082424]
	TIME [epoch: 1.29 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1374349101639991		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 1.1374349101639991 | validation: 1.0538998856085589]
	TIME [epoch: 1.28 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1500671334459822		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 1.1500671334459822 | validation: 1.1147651548318027]
	TIME [epoch: 1.28 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1378092924731449		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 1.1378092924731449 | validation: 1.0984455781153422]
	TIME [epoch: 1.28 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1380076870441804		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 1.1380076870441804 | validation: 1.1140800583054418]
	TIME [epoch: 1.28 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1427148069359556		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 1.1427148069359556 | validation: 1.0801796457064232]
	TIME [epoch: 1.28 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1389677614644333		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 1.1389677614644333 | validation: 1.1296317352747482]
	TIME [epoch: 1.29 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1523427031946347		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 1.1523427031946347 | validation: 1.0760508565853584]
	TIME [epoch: 1.28 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1546214203330436		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 1.1546214203330436 | validation: 1.162012494500161]
	TIME [epoch: 1.28 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.15904684361982		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 1.15904684361982 | validation: 1.0664619010662797]
	TIME [epoch: 1.28 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.142811916929038		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 1.142811916929038 | validation: 1.1060867544743918]
	TIME [epoch: 1.28 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1302244886195052		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 1.1302244886195052 | validation: 1.0816619626421915]
	TIME [epoch: 1.28 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1333653626714664		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 1.1333653626714664 | validation: 1.096393846342776]
	TIME [epoch: 1.28 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1399402435756034		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 1.1399402435756034 | validation: 1.121857312048022]
	TIME [epoch: 1.28 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.129221394440479		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 1.129221394440479 | validation: 1.0975830832301579]
	TIME [epoch: 1.28 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1364347840193028		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 1.1364347840193028 | validation: 1.1406108613852972]
	TIME [epoch: 1.28 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1435682420206936		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 1.1435682420206936 | validation: 1.0286208055327826]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_8_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_8_v_mmd4_354.pth
	Model improved!!!
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1634374482227763		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 1.1634374482227763 | validation: 1.1659954600170461]
	TIME [epoch: 1.28 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1589297183789402		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 1.1589297183789402 | validation: 1.0940575281862996]
	TIME [epoch: 1.28 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1417189539052917		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 1.1417189539052917 | validation: 1.0822984495901025]
	TIME [epoch: 1.28 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1319368706427444		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 1.1319368706427444 | validation: 1.0999032653672856]
	TIME [epoch: 1.28 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1349301121351865		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 1.1349301121351865 | validation: 1.118821808086494]
	TIME [epoch: 1.28 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1426955988623095		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 1.1426955988623095 | validation: 1.0992918922055066]
	TIME [epoch: 1.29 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.140167768503028		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 1.140167768503028 | validation: 1.1371754877953693]
	TIME [epoch: 1.28 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1466888469185164		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 1.1466888469185164 | validation: 1.0827272097358687]
	TIME [epoch: 1.28 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1384172934683288		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 1.1384172934683288 | validation: 1.1212675274435646]
	TIME [epoch: 1.28 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1369402807542344		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 1.1369402807542344 | validation: 1.0610884250054748]
	TIME [epoch: 1.28 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1366462121795649		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 1.1366462121795649 | validation: 1.115901036148568]
	TIME [epoch: 1.28 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1303800044254746		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 1.1303800044254746 | validation: 1.0932654536973758]
	TIME [epoch: 1.28 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1274143548866093		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 1.1274143548866093 | validation: 1.0725815020570788]
	TIME [epoch: 1.28 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.134533782657347		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 1.134533782657347 | validation: 1.087488137363703]
	TIME [epoch: 1.28 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1260620909927068		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 1.1260620909927068 | validation: 1.091211717798415]
	TIME [epoch: 1.28 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.131250157292225		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 1.131250157292225 | validation: 1.1156891333614514]
	TIME [epoch: 1.28 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1471533193094983		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 1.1471533193094983 | validation: 1.093755514950967]
	TIME [epoch: 1.28 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1526873473804196		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 1.1526873473804196 | validation: 1.1292823009047814]
	TIME [epoch: 1.28 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1465794433170888		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 1.1465794433170888 | validation: 1.0728607783517115]
	TIME [epoch: 1.28 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1325581055493554		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 1.1325581055493554 | validation: 1.1135833235171475]
	TIME [epoch: 1.28 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1340847419699684		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 1.1340847419699684 | validation: 1.0728477596575725]
	TIME [epoch: 1.29 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1416191045643094		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 1.1416191045643094 | validation: 1.1437613087173741]
	TIME [epoch: 1.28 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1429294815791078		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 1.1429294815791078 | validation: 1.0699086362387948]
	TIME [epoch: 1.29 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1352743437322732		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 1.1352743437322732 | validation: 1.0975128356784205]
	TIME [epoch: 1.28 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1321887856914767		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 1.1321887856914767 | validation: 1.082953684138029]
	TIME [epoch: 1.28 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1305790691120579		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 1.1305790691120579 | validation: 1.0723200823408485]
	TIME [epoch: 1.28 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1276786889413035		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 1.1276786889413035 | validation: 1.0919810933319078]
	TIME [epoch: 1.28 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1352362949894765		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 1.1352362949894765 | validation: 1.084323074722408]
	TIME [epoch: 1.28 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1310451572968259		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 1.1310451572968259 | validation: 1.0615072340754348]
	TIME [epoch: 1.28 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1268824019850354		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 1.1268824019850354 | validation: 1.1291998804777823]
	TIME [epoch: 1.29 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1377998838389438		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 1.1377998838389438 | validation: 1.097150334924525]
	TIME [epoch: 1.28 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1560128110466057		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 1.1560128110466057 | validation: 1.1637832248181599]
	TIME [epoch: 1.28 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.163148134280405		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 1.163148134280405 | validation: 1.0605868012545063]
	TIME [epoch: 1.28 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1333561994449337		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 1.1333561994449337 | validation: 1.101666761106238]
	TIME [epoch: 1.28 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.134744533902255		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 1.134744533902255 | validation: 1.0814955293952786]
	TIME [epoch: 1.28 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1356317145723882		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 1.1356317145723882 | validation: 1.0980966136707442]
	TIME [epoch: 1.28 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1355446641633267		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 1.1355446641633267 | validation: 1.1072381040280441]
	TIME [epoch: 1.28 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1330460380006255		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 1.1330460380006255 | validation: 1.085453071500839]
	TIME [epoch: 1.28 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1351024417902726		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 1.1351024417902726 | validation: 1.1060078698731963]
	TIME [epoch: 1.28 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1313396877925608		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 1.1313396877925608 | validation: 1.0816744264973586]
	TIME [epoch: 1.28 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1378410011973366		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 1.1378410011973366 | validation: 1.0708693062062946]
	TIME [epoch: 1.28 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.137736948435174		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 1.137736948435174 | validation: 1.0697820479323725]
	TIME [epoch: 1.28 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.125472718238594		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 1.125472718238594 | validation: 1.1058048911791099]
	TIME [epoch: 1.28 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1407629440533416		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 1.1407629440533416 | validation: 1.0688599634880223]
	TIME [epoch: 1.28 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1466339778618724		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 1.1466339778618724 | validation: 1.1664955540482154]
	TIME [epoch: 1.28 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1615276513814663		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 1.1615276513814663 | validation: 1.0658146337164724]
	TIME [epoch: 1.29 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1366098404009468		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 1.1366098404009468 | validation: 1.0655141746099097]
	TIME [epoch: 1.29 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1319869759667345		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 1.1319869759667345 | validation: 1.1067713757804636]
	TIME [epoch: 1.28 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.131617331220726		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 1.131617331220726 | validation: 1.0636104725891913]
	TIME [epoch: 1.28 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.136475664906273		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 1.136475664906273 | validation: 1.0831428609020513]
	TIME [epoch: 1.28 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.128082148584294		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 1.128082148584294 | validation: 1.084452129096114]
	TIME [epoch: 1.28 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1306179775002851		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 1.1306179775002851 | validation: 1.0958929072193342]
	TIME [epoch: 1.29 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1398322546363482		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 1.1398322546363482 | validation: 1.0797372575549005]
	TIME [epoch: 1.28 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1344176163427293		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 1.1344176163427293 | validation: 1.1195941903579252]
	TIME [epoch: 1.28 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1377600798557135		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 1.1377600798557135 | validation: 1.0569133134608897]
	TIME [epoch: 1.28 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.13674168019999		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 1.13674168019999 | validation: 1.093933677530595]
	TIME [epoch: 1.28 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1322106263879612		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 1.1322106263879612 | validation: 1.0715418682311844]
	TIME [epoch: 1.28 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1310692774526299		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 1.1310692774526299 | validation: 1.1281013651665608]
	TIME [epoch: 1.28 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1368155772474733		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 1.1368155772474733 | validation: 1.0846030643205795]
	TIME [epoch: 1.28 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1314489873905056		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 1.1314489873905056 | validation: 1.103909542731022]
	TIME [epoch: 1.29 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1286042373464051		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 1.1286042373464051 | validation: 1.0650426998601747]
	TIME [epoch: 1.28 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1373991759095077		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 1.1373991759095077 | validation: 1.1135306018218132]
	TIME [epoch: 1.28 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1332270663274029		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 1.1332270663274029 | validation: 1.0844202158044525]
	TIME [epoch: 1.28 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.13073062140406		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 1.13073062140406 | validation: 1.0782768467632304]
	TIME [epoch: 1.28 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.143645415784566		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 1.143645415784566 | validation: 1.1258418938565105]
	TIME [epoch: 1.28 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1512503845630402		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 1.1512503845630402 | validation: 1.0793979058040473]
	TIME [epoch: 1.29 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1440885702444235		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 1.1440885702444235 | validation: 1.1066411941522003]
	TIME [epoch: 1.28 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1388105964684767		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 1.1388105964684767 | validation: 1.0681191772161789]
	TIME [epoch: 1.28 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1344641514722353		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 1.1344641514722353 | validation: 1.0951376413815799]
	TIME [epoch: 1.28 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1279084466470355		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 1.1279084466470355 | validation: 1.0986800406973867]
	TIME [epoch: 1.28 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1339875372918133		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 1.1339875372918133 | validation: 1.0592233806483027]
	TIME [epoch: 1.28 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1324037909891025		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 1.1324037909891025 | validation: 1.1184527040158885]
	TIME [epoch: 1.28 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.147173996692738		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 1.147173996692738 | validation: 1.0616705565402362]
	TIME [epoch: 1.28 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1294383517424396		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 1.1294383517424396 | validation: 1.095951024143486]
	TIME [epoch: 1.28 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1315450222744554		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 1.1315450222744554 | validation: 1.0912992932714718]
	TIME [epoch: 1.28 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1311205005728366		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 1.1311205005728366 | validation: 1.104186250959296]
	TIME [epoch: 1.28 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1285314975481984		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 1.1285314975481984 | validation: 1.0546024199167552]
	TIME [epoch: 1.28 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1335867520491574		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 1.1335867520491574 | validation: 1.1226956071306302]
	TIME [epoch: 1.28 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1334919435794308		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 1.1334919435794308 | validation: 1.0651963950417465]
	TIME [epoch: 1.28 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1309361256228991		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 1.1309361256228991 | validation: 1.1114451963556127]
	TIME [epoch: 1.28 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1393046852702793		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 1.1393046852702793 | validation: 1.0845921333354323]
	TIME [epoch: 1.28 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1351252524735955		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 1.1351252524735955 | validation: 1.0888945470892688]
	TIME [epoch: 1.28 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1367447702162201		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 1.1367447702162201 | validation: 1.05977578685406]
	TIME [epoch: 1.28 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1394283998763854		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 1.1394283998763854 | validation: 1.1203972735684136]
	TIME [epoch: 1.28 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1324231100327597		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 1.1324231100327597 | validation: 1.0634576295104055]
	TIME [epoch: 1.28 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1321460133750432		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 1.1321460133750432 | validation: 1.1038952203104968]
	TIME [epoch: 1.28 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.131131472418957		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 1.131131472418957 | validation: 1.092862076683057]
	TIME [epoch: 1.28 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1310721519507025		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 1.1310721519507025 | validation: 1.0662329111059285]
	TIME [epoch: 1.28 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1274805529415124		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 1.1274805529415124 | validation: 1.1031196281118232]
	TIME [epoch: 1.28 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1309750764442492		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 1.1309750764442492 | validation: 1.0680690472052985]
	TIME [epoch: 1.28 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1215108519290804		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 1.1215108519290804 | validation: 1.0738771660481914]
	TIME [epoch: 1.28 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.134771614855531		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 1.134771614855531 | validation: 1.09713106517489]
	TIME [epoch: 1.28 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1387239493628059		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 1.1387239493628059 | validation: 1.0890231245094177]
	TIME [epoch: 1.28 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.136051129369729		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 1.136051129369729 | validation: 1.0527584027916153]
	TIME [epoch: 1.28 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1352330781204438		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 1.1352330781204438 | validation: 1.12094610545689]
	TIME [epoch: 1.28 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1381887901673162		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 1.1381887901673162 | validation: 1.0552451264172702]
	TIME [epoch: 1.28 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1302507691049914		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 1.1302507691049914 | validation: 1.0849258538196391]
	TIME [epoch: 1.29 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1286229619534383		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 1.1286229619534383 | validation: 1.079250941048622]
	TIME [epoch: 1.28 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.130020486822999		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 1.130020486822999 | validation: 1.0678130162313895]
	TIME [epoch: 1.29 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1247046953130007		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 1.1247046953130007 | validation: 1.0849105302857927]
	TIME [epoch: 1.28 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1247787127376376		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 1.1247787127376376 | validation: 1.069317143605669]
	TIME [epoch: 1.28 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.131792415519525		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 1.131792415519525 | validation: 1.1072188300452876]
	TIME [epoch: 1.28 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.133732732093389		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 1.133732732093389 | validation: 1.0571715250967113]
	TIME [epoch: 1.28 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1329522159317766		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 1.1329522159317766 | validation: 1.123508744249981]
	TIME [epoch: 1.29 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1311525287476605		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 1.1311525287476605 | validation: 1.0699549172984413]
	TIME [epoch: 1.28 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1288087805655562		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 1.1288087805655562 | validation: 1.1011864839711685]
	TIME [epoch: 1.28 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1361613670258819		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 1.1361613670258819 | validation: 1.0842719664344664]
	TIME [epoch: 1.28 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1318433615531074		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 1.1318433615531074 | validation: 1.1028152782403384]
	TIME [epoch: 1.28 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1180402484325436		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 1.1180402484325436 | validation: 1.0626708218690695]
	TIME [epoch: 1.28 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1284509264654807		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 1.1284509264654807 | validation: 1.1149314817479965]
	TIME [epoch: 1.28 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.130273346609077		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 1.130273346609077 | validation: 1.0680814558423268]
	TIME [epoch: 1.28 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.134661790483047		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 1.134661790483047 | validation: 1.0875105697346557]
	TIME [epoch: 1.28 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1212539354399298		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 1.1212539354399298 | validation: 1.0869620190405072]
	TIME [epoch: 1.28 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1321782587332638		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 1.1321782587332638 | validation: 1.0871764142833136]
	TIME [epoch: 1.28 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1349729494830962		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 1.1349729494830962 | validation: 1.0976823797897002]
	TIME [epoch: 1.28 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1299086819181494		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 1.1299086819181494 | validation: 1.0686902047168723]
	TIME [epoch: 1.28 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.128589666203738		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 1.128589666203738 | validation: 1.080656486481154]
	TIME [epoch: 1.28 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1337486921446343		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 1.1337486921446343 | validation: 1.098178649510256]
	TIME [epoch: 1.28 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1302328774506496		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 1.1302328774506496 | validation: 1.068019594274098]
	TIME [epoch: 1.28 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1316132760141222		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 1.1316132760141222 | validation: 1.0858461014437075]
	TIME [epoch: 1.28 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.126766882208545		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 1.126766882208545 | validation: 1.0668167383622298]
	TIME [epoch: 1.28 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1307311868555607		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 1.1307311868555607 | validation: 1.0530424230189197]
	TIME [epoch: 1.28 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.133002013435035		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 1.133002013435035 | validation: 1.0974558619687755]
	TIME [epoch: 1.28 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1261325653112078		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 1.1261325653112078 | validation: 1.0526097884863757]
	TIME [epoch: 1.28 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1338927112731816		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 1.1338927112731816 | validation: 1.1118877832862957]
	TIME [epoch: 1.28 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1336224295562485		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 1.1336224295562485 | validation: 1.0623853146301732]
	TIME [epoch: 1.28 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.132760871801093		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 1.132760871801093 | validation: 1.0944578043382915]
	TIME [epoch: 1.28 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.134674036211654		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 1.134674036211654 | validation: 1.0540919201364591]
	TIME [epoch: 1.28 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1328471092871908		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 1.1328471092871908 | validation: 1.1034483571478604]
	TIME [epoch: 1.28 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1249954364715922		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 1.1249954364715922 | validation: 1.0498232822441025]
	TIME [epoch: 1.28 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1402150708256928		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 1.1402150708256928 | validation: 1.0835206929773593]
	TIME [epoch: 1.28 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1282939473582945		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 1.1282939473582945 | validation: 1.1014339012290646]
	TIME [epoch: 1.28 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1297098793108746		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 1.1297098793108746 | validation: 1.0765031748906841]
	TIME [epoch: 1.29 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1247444157280995		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 1.1247444157280995 | validation: 1.0971250894616347]
	TIME [epoch: 1.28 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1264201555206987		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 1.1264201555206987 | validation: 1.0659502498601425]
	TIME [epoch: 1.28 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1255137397452724		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 1.1255137397452724 | validation: 1.076845837112823]
	TIME [epoch: 1.28 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1261319921777166		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 1.1261319921777166 | validation: 1.0983383970814897]
	TIME [epoch: 1.28 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.13039283517927		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 1.13039283517927 | validation: 1.082365418267687]
	TIME [epoch: 1.28 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1387952478561116		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 1.1387952478561116 | validation: 1.0949724623866133]
	TIME [epoch: 1.28 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1342234951549113		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 1.1342234951549113 | validation: 1.0840033906135074]
	TIME [epoch: 1.29 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1206026250086685		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 1.1206026250086685 | validation: 1.080099446961819]
	TIME [epoch: 1.28 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1271140269267526		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 1.1271140269267526 | validation: 1.0842118639594926]
	TIME [epoch: 1.28 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1248846627292277		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 1.1248846627292277 | validation: 1.0590115516899237]
	TIME [epoch: 1.28 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.13297050847231		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 1.13297050847231 | validation: 1.103335327654391]
	TIME [epoch: 1.29 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1272941116807924		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 1.1272941116807924 | validation: 1.052109455833156]
	TIME [epoch: 1.28 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.126962902572181		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 1.126962902572181 | validation: 1.0734097572886603]
	TIME [epoch: 1.28 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1264260753669202		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 1.1264260753669202 | validation: 1.098386371464191]
	TIME [epoch: 188 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1271779464183427		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 1.1271779464183427 | validation: 1.0992172841614003]
	TIME [epoch: 2.55 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_8_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_8_v_mmd4_502.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 1090.821 seconds.
