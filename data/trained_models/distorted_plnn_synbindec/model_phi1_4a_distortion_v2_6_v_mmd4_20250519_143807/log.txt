Args:
Namespace(name='model_phi1_4a_distortion_v2_6_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_6/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_6/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.025136888027191162, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3319430868

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.825096372273007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.825096372273007 | validation: 4.966125383382971]
	TIME [epoch: 172 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.589421320656853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.589421320656853 | validation: 4.037341351202437]
	TIME [epoch: 1.09 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.456381490863348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.456381490863348 | validation: 3.8476909329410454]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.22217541710116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.22217541710116 | validation: 4.266312371121061]
	TIME [epoch: 0.706 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.532819441269619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.532819441269619 | validation: 3.7674082900286816]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.027543338489795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.027543338489795 | validation: 3.5052717343024455]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.956748586265159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.956748586265159 | validation: 3.9322448596289235]
	TIME [epoch: 0.703 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8892303804715174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8892303804715174 | validation: 3.3177400552323273]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7879024542545707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7879024542545707 | validation: 3.342547508413344]
	TIME [epoch: 0.7 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6804874973861685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6804874973861685 | validation: 3.4564810211913723]
	TIME [epoch: 0.699 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5971144493474503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5971144493474503 | validation: 3.0838286690417807]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5785832776072515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5785832776072515 | validation: 3.410685430068766]
	TIME [epoch: 0.704 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6230288856537176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6230288856537176 | validation: 3.3583891575224274]
	TIME [epoch: 0.703 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5885967886576373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5885967886576373 | validation: 2.93488339103862]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.352958114499368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.352958114499368 | validation: 3.1070916561839823]
	TIME [epoch: 0.707 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.320554199261077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.320554199261077 | validation: 2.9225778014633375]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2669568975186296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2669568975186296 | validation: 2.5271964413104158]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0502851040541974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0502851040541974 | validation: 3.456370381647579]
	TIME [epoch: 0.701 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1957407936759905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1957407936759905 | validation: 3.1039058695173183]
	TIME [epoch: 0.7 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.362002667066259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.362002667066259 | validation: 2.5707326765210334]
	TIME [epoch: 0.701 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9196486368112984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9196486368112984 | validation: 3.0871157496423516]
	TIME [epoch: 0.7 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9889688394438996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9889688394438996 | validation: 2.3616585943937256]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7945017912908234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7945017912908234 | validation: 2.324452178119708]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.72095962247531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.72095962247531 | validation: 2.4708412563967372]
	TIME [epoch: 0.699 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6003053263879745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6003053263879745 | validation: 2.1217061993649113]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5200787502217414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5200787502217414 | validation: 3.159756784020501]
	TIME [epoch: 0.697 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6837171725354607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6837171725354607 | validation: 3.040699061270079]
	TIME [epoch: 0.699 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.167294423374924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.167294423374924 | validation: 2.1804203114158054]
	TIME [epoch: 0.698 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2633460110955697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2633460110955697 | validation: 2.391745757161116]
	TIME [epoch: 0.697 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5996983971354077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5996983971354077 | validation: 2.7297544491117924]
	TIME [epoch: 0.699 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7052886040053665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7052886040053665 | validation: 2.3539134583958092]
	TIME [epoch: 0.696 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5472367463859147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5472367463859147 | validation: 2.038231461398659]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.515706005391095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.515706005391095 | validation: 2.053616812530355]
	TIME [epoch: 0.7 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4413106083870804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4413106083870804 | validation: 2.2335075136831466]
	TIME [epoch: 0.698 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.378873641090312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.378873641090312 | validation: 2.1387411264640117]
	TIME [epoch: 0.698 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3178128954841486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3178128954841486 | validation: 1.9213899604770668]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2750488179006485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2750488179006485 | validation: 2.3861311315972125]
	TIME [epoch: 0.698 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2594270251804494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2594270251804494 | validation: 1.8330282510328388]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5287965378171355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5287965378171355 | validation: 2.859973397355775]
	TIME [epoch: 0.697 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4748096352424933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4748096352424933 | validation: 1.6827360081689922]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.517132483275781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.517132483275781 | validation: 2.0282248594453587]
	TIME [epoch: 0.696 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2752244460201165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2752244460201165 | validation: 2.3363545744831757]
	TIME [epoch: 0.695 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2572388986058485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2572388986058485 | validation: 1.834509634976302]
	TIME [epoch: 0.693 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.234372748200463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.234372748200463 | validation: 1.9715015218020047]
	TIME [epoch: 0.694 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.180975705427157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.180975705427157 | validation: 2.0977210521002747]
	TIME [epoch: 0.696 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1072268126172857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1072268126172857 | validation: 1.5049711556046266]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1482717315638458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1482717315638458 | validation: 2.4660712287094757]
	TIME [epoch: 0.696 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.21704021897366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.21704021897366 | validation: 1.5002173834908115]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.351172933916901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.351172933916901 | validation: 1.8420117479841924]
	TIME [epoch: 0.696 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.021827858506602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.021827858506602 | validation: 2.054067618737224]
	TIME [epoch: 0.695 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.047590434911577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.047590434911577 | validation: 1.4429568313380279]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1124241919729387		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 2.1124241919729387 | validation: 2.1588239309121913]
	TIME [epoch: 0.695 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.040584192009655		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 2.040584192009655 | validation: 1.5199963958928397]
	TIME [epoch: 0.693 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0714790357034323		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 2.0714790357034323 | validation: 2.2902192870754576]
	TIME [epoch: 0.693 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.074626189324237		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 2.074626189324237 | validation: 1.8487580532283578]
	TIME [epoch: 0.694 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1151642025770956		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 2.1151642025770956 | validation: 1.99061057320171]
	TIME [epoch: 0.695 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.951801477906315		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.951801477906315 | validation: 1.3675890861183948]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.032801139709865		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 2.032801139709865 | validation: 2.189659087235114]
	TIME [epoch: 0.699 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0903150915866697		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 2.0903150915866697 | validation: 1.3662535457733995]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.070133997735811		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 2.070133997735811 | validation: 1.9480310297682573]
	TIME [epoch: 0.698 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9223562009612636		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.9223562009612636 | validation: 1.7152558079843634]
	TIME [epoch: 0.698 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8862464061751647		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.8862464061751647 | validation: 1.6739998998205665]
	TIME [epoch: 0.699 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.794323822882812		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.794323822882812 | validation: 1.3699276787442345]
	TIME [epoch: 0.698 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.81830045670579		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.81830045670579 | validation: 2.6542471033844564]
	TIME [epoch: 0.697 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3439358919819098		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 2.3439358919819098 | validation: 1.3778565561902223]
	TIME [epoch: 0.7 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.174543945872126		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 2.174543945872126 | validation: 1.4938845975550654]
	TIME [epoch: 0.695 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8275948538366407		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.8275948538366407 | validation: 2.078992380162809]
	TIME [epoch: 0.693 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.976535134972965		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.976535134972965 | validation: 1.5249065733241194]
	TIME [epoch: 0.693 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8216028827008233		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.8216028827008233 | validation: 1.5844308176424193]
	TIME [epoch: 0.694 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7903867162807172		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.7903867162807172 | validation: 1.7200251846417203]
	TIME [epoch: 0.693 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7634619928706183		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.7634619928706183 | validation: 1.355473296695301]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.777150046230376		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.777150046230376 | validation: 2.0454427815406455]
	TIME [epoch: 0.699 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8748166828169397		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.8748166828169397 | validation: 1.3991480282632784]
	TIME [epoch: 0.698 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.912647124043299		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.912647124043299 | validation: 1.842782257536468]
	TIME [epoch: 0.696 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7987867520255		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.7987867520255 | validation: 1.476405107310537]
	TIME [epoch: 0.698 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7182327814311997		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.7182327814311997 | validation: 1.7090572113673468]
	TIME [epoch: 0.697 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7059322311568315		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.7059322311568315 | validation: 1.2963697708973028]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.830508749497049		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.830508749497049 | validation: 2.003651676967617]
	TIME [epoch: 0.698 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.894126253333152		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.894126253333152 | validation: 1.2901905276946601]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8242363587855976		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.8242363587855976 | validation: 1.6299739472328025]
	TIME [epoch: 0.698 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6714271842797148		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.6714271842797148 | validation: 1.4837381828013863]
	TIME [epoch: 0.692 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6489810112529797		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.6489810112529797 | validation: 1.449989383411543]
	TIME [epoch: 0.695 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6273402512462714		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.6273402512462714 | validation: 1.4933814349094783]
	TIME [epoch: 0.691 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6379947876487244		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.6379947876487244 | validation: 2.1537523431968637]
	TIME [epoch: 0.691 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9197836788308482		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.9197836788308482 | validation: 1.6493257521662423]
	TIME [epoch: 0.691 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3086629961727763		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 2.3086629961727763 | validation: 1.6352690009751485]
	TIME [epoch: 0.694 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6625443230380443		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.6625443230380443 | validation: 1.5361346458837377]
	TIME [epoch: 0.693 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6697092791783166		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.6697092791783166 | validation: 1.4369872908814962]
	TIME [epoch: 0.691 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.660020549493224		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.660020549493224 | validation: 1.8215669566000032]
	TIME [epoch: 0.691 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7533884533684607		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.7533884533684607 | validation: 1.3071962504220396]
	TIME [epoch: 0.692 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8702169183341344		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.8702169183341344 | validation: 1.6273040038624342]
	TIME [epoch: 0.691 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6512889344371673		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.6512889344371673 | validation: 1.389346881427553]
	TIME [epoch: 0.691 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6491300065624137		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.6491300065624137 | validation: 1.7956016804373425]
	TIME [epoch: 0.692 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7141188631196735		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.7141188631196735 | validation: 1.287454286029474]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7017694179706635		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.7017694179706635 | validation: 1.6774356161565784]
	TIME [epoch: 0.696 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.657898606325919		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.657898606325919 | validation: 1.2806008174418355]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6806589193083705		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.6806589193083705 | validation: 1.7577012463440576]
	TIME [epoch: 0.698 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.685088828131582		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.685088828131582 | validation: 1.2819988907374993]
	TIME [epoch: 0.694 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7135001990687693		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.7135001990687693 | validation: 1.6641112014066193]
	TIME [epoch: 0.695 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6359114283021818		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.6359114283021818 | validation: 1.3070079944569364]
	TIME [epoch: 0.691 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6732560879105973		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.6732560879105973 | validation: 1.7972271506686912]
	TIME [epoch: 0.693 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7076229153461913		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.7076229153461913 | validation: 1.3736027474321855]
	TIME [epoch: 0.691 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6823736842512103		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.6823736842512103 | validation: 1.7091827841017553]
	TIME [epoch: 0.69 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6604387431628391		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.6604387431628391 | validation: 1.3562708880754528]
	TIME [epoch: 0.69 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.648283676639494		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.648283676639494 | validation: 1.7318041496337386]
	TIME [epoch: 0.692 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6735512071064351		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.6735512071064351 | validation: 1.2630855934196437]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7035209811312175		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.7035209811312175 | validation: 1.7144611492686739]
	TIME [epoch: 0.696 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6935390379790323		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.6935390379790323 | validation: 1.29158437182268]
	TIME [epoch: 0.693 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.703635904571421		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.703635904571421 | validation: 1.592261611692221]
	TIME [epoch: 0.693 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6102860977807705		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.6102860977807705 | validation: 1.2986976232792848]
	TIME [epoch: 0.692 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.626750173310752		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.626750173310752 | validation: 1.9159726987428372]
	TIME [epoch: 0.693 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7564908618010486		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.7564908618010486 | validation: 1.4048928355051569]
	TIME [epoch: 0.692 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.695511246831926		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.695511246831926 | validation: 1.6649814936687641]
	TIME [epoch: 0.691 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6323613330901838		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.6323613330901838 | validation: 1.3176503182424748]
	TIME [epoch: 0.692 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.605397128511101		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.605397128511101 | validation: 1.7115839248590505]
	TIME [epoch: 0.692 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6895939661085342		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.6895939661085342 | validation: 1.2527967752966642]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8146971633429838		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.8146971633429838 | validation: 1.516889356988591]
	TIME [epoch: 0.698 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5851379756733428		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.5851379756733428 | validation: 1.3693000475748984]
	TIME [epoch: 0.694 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.586676776218844		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.586676776218844 | validation: 1.798643922095683]
	TIME [epoch: 0.696 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6942398505667533		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.6942398505667533 | validation: 1.2963849464559265]
	TIME [epoch: 0.694 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.661316983393932		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.661316983393932 | validation: 1.6192729557739758]
	TIME [epoch: 0.694 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6097830515867568		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.6097830515867568 | validation: 1.2813538119009902]
	TIME [epoch: 0.697 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6088029803939332		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.6088029803939332 | validation: 1.6620574659113032]
	TIME [epoch: 0.693 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6579810471319059		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.6579810471319059 | validation: 1.238095270513078]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7017784162936347		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.7017784162936347 | validation: 1.5626252423457636]
	TIME [epoch: 0.699 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5969544134192442		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.5969544134192442 | validation: 1.248412309638873]
	TIME [epoch: 0.696 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6097833563247346		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.6097833563247346 | validation: 1.6968243667013547]
	TIME [epoch: 0.696 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6497082930791431		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.6497082930791431 | validation: 1.3271513918694424]
	TIME [epoch: 0.699 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6742364400122314		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.6742364400122314 | validation: 1.6931573846894423]
	TIME [epoch: 0.698 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6500884499233148		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.6500884499233148 | validation: 1.4363962369337782]
	TIME [epoch: 0.695 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5945069530698124		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.5945069530698124 | validation: 1.6418915611565041]
	TIME [epoch: 0.694 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6058721506459626		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.6058721506459626 | validation: 1.2033731494502964]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.697627818868221		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.697627818868221 | validation: 1.6145582604149327]
	TIME [epoch: 0.7 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.637729142099979		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.637729142099979 | validation: 1.2288443963334457]
	TIME [epoch: 0.698 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6339438417153695		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.6339438417153695 | validation: 1.5768520591801698]
	TIME [epoch: 0.694 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.576834381849103		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.576834381849103 | validation: 1.2736232311123492]
	TIME [epoch: 0.693 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6037443464027785		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.6037443464027785 | validation: 1.7738025030670077]
	TIME [epoch: 0.691 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6596547947852498		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.6596547947852498 | validation: 1.362834072668184]
	TIME [epoch: 0.693 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6065083999276464		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.6065083999276464 | validation: 1.612933695266753]
	TIME [epoch: 0.693 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5928795003809115		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.5928795003809115 | validation: 1.233127827245739]
	TIME [epoch: 0.694 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5768990655485773		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.5768990655485773 | validation: 1.6415668022562775]
	TIME [epoch: 0.695 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6432701983540838		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.6432701983540838 | validation: 1.2047369234867487]
	TIME [epoch: 0.692 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7087500804434579		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.7087500804434579 | validation: 1.4794660714013397]
	TIME [epoch: 0.695 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.534666597516228		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.534666597516228 | validation: 1.2551412087132727]
	TIME [epoch: 0.694 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5592838495147052		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.5592838495147052 | validation: 1.7714772683053839]
	TIME [epoch: 0.696 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6619628164748705		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.6619628164748705 | validation: 1.270574045538868]
	TIME [epoch: 1.09 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5847445861174032		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.5847445861174032 | validation: 1.5472426487297708]
	TIME [epoch: 0.696 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.554861939150212		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.554861939150212 | validation: 1.2073299863606448]
	TIME [epoch: 0.694 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5761309351195036		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.5761309351195036 | validation: 1.6554257759917306]
	TIME [epoch: 0.692 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.616928514562252		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.616928514562252 | validation: 1.2020943585904922]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_150.pth
	Model improved!!!
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6103043964915649		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.6103043964915649 | validation: 1.5526899180619758]
	TIME [epoch: 0.695 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5857840461853019		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.5857840461853019 | validation: 1.1856263809143872]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_152.pth
	Model improved!!!
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.624007031716035		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.624007031716035 | validation: 1.5393678410471363]
	TIME [epoch: 0.694 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5572053362978409		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.5572053362978409 | validation: 1.1921259933797284]
	TIME [epoch: 0.694 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5966359699488535		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.5966359699488535 | validation: 1.689882592256516]
	TIME [epoch: 0.694 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6075152817244862		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.6075152817244862 | validation: 1.3690045274036307]
	TIME [epoch: 0.695 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5970779410167406		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.5970779410167406 | validation: 1.6313192864781343]
	TIME [epoch: 0.695 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5711837922054637		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.5711837922054637 | validation: 1.24954357473714]
	TIME [epoch: 0.694 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.53591422623518		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.53591422623518 | validation: 1.6192204656256182]
	TIME [epoch: 0.693 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5901382948916842		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.5901382948916842 | validation: 1.157970544258637]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6607736154622357		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.6607736154622357 | validation: 1.4975626268848576]
	TIME [epoch: 0.694 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5334093146988006		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.5334093146988006 | validation: 1.2020980472032736]
	TIME [epoch: 0.692 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5401424668557224		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.5401424668557224 | validation: 1.6748379870868881]
	TIME [epoch: 0.691 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6099963783959077		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.6099963783959077 | validation: 1.2334110088036694]
	TIME [epoch: 0.694 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5495670292439077		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.5495670292439077 | validation: 1.538774475992002]
	TIME [epoch: 0.695 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5314362726569104		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.5314362726569104 | validation: 1.2208662741156013]
	TIME [epoch: 0.693 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5366701431403431		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.5366701431403431 | validation: 1.6109544526308868]
	TIME [epoch: 0.694 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5669820185875876		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.5669820185875876 | validation: 1.1654828325425495]
	TIME [epoch: 0.698 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.562527852270061		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.562527852270061 | validation: 1.5616599042951895]
	TIME [epoch: 0.692 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5508264907725289		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.5508264907725289 | validation: 1.2016240378417924]
	TIME [epoch: 0.693 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6219252985629988		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.6219252985629988 | validation: 1.514965521182386]
	TIME [epoch: 0.693 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5493228875232052		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.5493228875232052 | validation: 1.1708007516645473]
	TIME [epoch: 0.695 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5311340829389548		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.5311340829389548 | validation: 1.6243768519061934]
	TIME [epoch: 0.697 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5588365994584927		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.5588365994584927 | validation: 1.325056546630088]
	TIME [epoch: 0.693 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5911080803456594		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.5911080803456594 | validation: 1.5951918363869122]
	TIME [epoch: 0.692 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5391786257563036		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.5391786257563036 | validation: 1.2091215872994974]
	TIME [epoch: 0.696 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5048503768329864		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.5048503768329864 | validation: 1.5795372829920638]
	TIME [epoch: 0.694 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5561864201589066		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.5561864201589066 | validation: 1.1505609582787932]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5755455664474634		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.5755455664474634 | validation: 1.4795640848583997]
	TIME [epoch: 0.694 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5191402376075167		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.5191402376075167 | validation: 1.129448720383359]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_180.pth
	Model improved!!!
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.534722139352088		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.534722139352088 | validation: 1.4982185586201857]
	TIME [epoch: 0.693 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5238374898611295		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.5238374898611295 | validation: 1.1934509222780685]
	TIME [epoch: 0.694 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5401832173484342		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.5401832173484342 | validation: 1.5523622389625258]
	TIME [epoch: 0.694 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.531856493762433		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.531856493762433 | validation: 1.2574086019588107]
	TIME [epoch: 0.695 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5315758007916171		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.5315758007916171 | validation: 1.6340777848152634]
	TIME [epoch: 0.694 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.552975408826108		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.552975408826108 | validation: 1.1969020736158533]
	TIME [epoch: 0.694 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.49008695350232		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.49008695350232 | validation: 1.5231219948704386]
	TIME [epoch: 0.693 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5239484925647628		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.5239484925647628 | validation: 1.1563512004859655]
	TIME [epoch: 0.694 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6695746846513129		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.6695746846513129 | validation: 1.530840006841013]
	TIME [epoch: 0.693 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5717770072995256		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.5717770072995256 | validation: 1.1509398031127105]
	TIME [epoch: 0.694 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4808871056145474		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.4808871056145474 | validation: 1.6229538847363856]
	TIME [epoch: 0.694 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.583050099424587		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.583050099424587 | validation: 1.2133744861902203]
	TIME [epoch: 0.694 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5156696109877967		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.5156696109877967 | validation: 1.4693966615994478]
	TIME [epoch: 0.694 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4831927144635422		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.4831927144635422 | validation: 1.1621567203883392]
	TIME [epoch: 0.694 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5081860677703447		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.5081860677703447 | validation: 1.5246491125418686]
	TIME [epoch: 0.695 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5309139512348129		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.5309139512348129 | validation: 1.1292650152645312]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5274228923282973		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.5274228923282973 | validation: 1.4815949469629552]
	TIME [epoch: 0.695 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4973048140529381		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.4973048140529381 | validation: 1.1430335664578848]
	TIME [epoch: 0.693 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.522859966363414		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.522859966363414 | validation: 1.5054059638459851]
	TIME [epoch: 0.692 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4981034308132735		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.4981034308132735 | validation: 1.1312809692891008]
	TIME [epoch: 0.692 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5167121172073144		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.5167121172073144 | validation: 1.4429454908889552]
	TIME [epoch: 173 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.486470938473472		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.486470938473472 | validation: 1.1544382351302847]
	TIME [epoch: 1.38 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5224662358431527		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.5224662358431527 | validation: 1.4760893453287007]
	TIME [epoch: 1.35 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4849038215264283		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.4849038215264283 | validation: 1.147237586518208]
	TIME [epoch: 1.36 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4837419067905242		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.4837419067905242 | validation: 1.491120182463116]
	TIME [epoch: 1.36 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5035426124724705		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.5035426124724705 | validation: 1.1350389494013269]
	TIME [epoch: 1.36 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5219257025578141		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.5219257025578141 | validation: 1.5033442985626086]
	TIME [epoch: 1.36 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5182883484815262		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.5182883484815262 | validation: 1.1563746574473968]
	TIME [epoch: 1.36 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5658597048978846		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.5658597048978846 | validation: 1.4374080713619397]
	TIME [epoch: 1.36 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.491904772648272		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.491904772648272 | validation: 1.1278429743948357]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_210.pth
	Model improved!!!
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4802413706615534		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.4802413706615534 | validation: 1.6291925576626967]
	TIME [epoch: 1.36 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5426171926912369		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.5426171926912369 | validation: 1.28808945022265]
	TIME [epoch: 1.36 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5306103726537683		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 1.5306103726537683 | validation: 1.486125599877432]
	TIME [epoch: 1.36 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4973276019077764		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.4973276019077764 | validation: 1.1752515123176845]
	TIME [epoch: 1.36 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4664693501848316		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 1.4664693501848316 | validation: 1.4726844612064438]
	TIME [epoch: 1.36 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5003724224500048		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.5003724224500048 | validation: 1.1286622723983162]
	TIME [epoch: 1.36 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5179116278726854		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.5179116278726854 | validation: 1.4316712245396]
	TIME [epoch: 1.36 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.465752490601387		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.465752490601387 | validation: 1.0997852420429564]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4871618574695142		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.4871618574695142 | validation: 1.4940387074774018]
	TIME [epoch: 1.36 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4743369564779585		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.4743369564779585 | validation: 1.1546967579958336]
	TIME [epoch: 1.36 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.473777663292697		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 1.473777663292697 | validation: 1.4657287530958485]
	TIME [epoch: 1.36 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4750215738243342		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.4750215738243342 | validation: 1.2123640343024928]
	TIME [epoch: 1.35 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4643278910891906		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 1.4643278910891906 | validation: 1.4890624103350243]
	TIME [epoch: 1.36 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4773165751406692		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.4773165751406692 | validation: 1.15397580881351]
	TIME [epoch: 1.35 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4644274410732228		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 1.4644274410732228 | validation: 1.440329116677862]
	TIME [epoch: 1.36 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4694863887584655		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 1.4694863887584655 | validation: 1.120571969653113]
	TIME [epoch: 1.36 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5187875797488724		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.5187875797488724 | validation: 1.5071374587598305]
	TIME [epoch: 1.36 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5456324769802299		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.5456324769802299 | validation: 1.122782399552885]
	TIME [epoch: 1.35 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4968655004106584		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 1.4968655004106584 | validation: 1.4597529626759689]
	TIME [epoch: 1.35 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4551800676720592		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.4551800676720592 | validation: 1.2011940685169096]
	TIME [epoch: 1.35 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4755000237932807		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 1.4755000237932807 | validation: 1.5274701985557162]
	TIME [epoch: 1.35 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.489890558598957		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.489890558598957 | validation: 1.1547405458475344]
	TIME [epoch: 1.36 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.440373373948039		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 1.440373373948039 | validation: 1.4345415399177532]
	TIME [epoch: 1.35 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4501269149915925		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.4501269149915925 | validation: 1.09327783755103]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_234.pth
	Model improved!!!
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5041426989989741		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 1.5041426989989741 | validation: 1.4278879077250042]
	TIME [epoch: 1.36 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4675093736946208		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.4675093736946208 | validation: 1.1241541064913607]
	TIME [epoch: 1.36 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4597489614507877		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 1.4597489614507877 | validation: 1.442111899667368]
	TIME [epoch: 1.36 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4527039348766888		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 1.4527039348766888 | validation: 1.12797060626173]
	TIME [epoch: 1.36 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4492865638678885		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 1.4492865638678885 | validation: 1.444476930659329]
	TIME [epoch: 1.36 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4479820378983588		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 1.4479820378983588 | validation: 1.1802463318076009]
	TIME [epoch: 1.36 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4522169207061348		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 1.4522169207061348 | validation: 1.4237394558832244]
	TIME [epoch: 1.36 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4443687643378436		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 1.4443687643378436 | validation: 1.129005708654617]
	TIME [epoch: 1.36 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.442575371783969		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 1.442575371783969 | validation: 1.4427475796855005]
	TIME [epoch: 1.36 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.44553535350924		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 1.44553535350924 | validation: 1.1142355310209882]
	TIME [epoch: 1.36 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4414765716873865		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 1.4414765716873865 | validation: 1.4079733615647578]
	TIME [epoch: 1.35 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.450645950913609		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.450645950913609 | validation: 1.1470739638434337]
	TIME [epoch: 1.36 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5652262114648359		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 1.5652262114648359 | validation: 1.4356214997613144]
	TIME [epoch: 1.35 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5232135107167517		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 1.5232135107167517 | validation: 1.0967833557918905]
	TIME [epoch: 1.35 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4208746215183357		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 1.4208746215183357 | validation: 1.6453459544895055]
	TIME [epoch: 1.35 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5562152707824417		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 1.5562152707824417 | validation: 1.1485719357909052]
	TIME [epoch: 1.35 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4408417518139505		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 1.4408417518139505 | validation: 1.373178165018734]
	TIME [epoch: 1.35 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4173104901146127		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 1.4173104901146127 | validation: 1.0937118094691358]
	TIME [epoch: 1.36 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4627839061821382		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 1.4627839061821382 | validation: 1.4124268483990206]
	TIME [epoch: 1.35 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.444609264521136		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 1.444609264521136 | validation: 1.117859884743105]
	TIME [epoch: 1.35 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.447157853923528		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 1.447157853923528 | validation: 1.3867240023005853]
	TIME [epoch: 1.36 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.416041405778836		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 1.416041405778836 | validation: 1.1061849929190557]
	TIME [epoch: 1.35 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4439800973782126		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 1.4439800973782126 | validation: 1.4107974666590715]
	TIME [epoch: 1.35 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4304503594417335		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 1.4304503594417335 | validation: 1.1293693422120308]
	TIME [epoch: 1.35 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4337946399691432		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 1.4337946399691432 | validation: 1.3817530127369742]
	TIME [epoch: 1.35 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.433411398384087		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 1.433411398384087 | validation: 1.0772842942593202]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_260.pth
	Model improved!!!
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4660689305944985		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 1.4660689305944985 | validation: 1.3799429750101202]
	TIME [epoch: 1.36 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4543018568132917		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 1.4543018568132917 | validation: 1.1183437164029966]
	TIME [epoch: 1.36 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4486649294440306		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 1.4486649294440306 | validation: 1.3747672772144486]
	TIME [epoch: 1.36 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4309742943156818		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 1.4309742943156818 | validation: 1.0936142008551268]
	TIME [epoch: 1.36 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4448338998031391		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 1.4448338998031391 | validation: 1.4185530899687822]
	TIME [epoch: 1.35 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4326558307060868		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 1.4326558307060868 | validation: 1.1967969369346863]
	TIME [epoch: 1.35 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4440483218479896		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 1.4440483218479896 | validation: 1.478592236192656]
	TIME [epoch: 1.36 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4485054438569398		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 1.4485054438569398 | validation: 1.134268687696258]
	TIME [epoch: 1.36 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4119603780013232		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 1.4119603780013232 | validation: 1.3926043402088504]
	TIME [epoch: 1.35 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4218570928882281		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 1.4218570928882281 | validation: 1.0733686553464563]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4506121882913312		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 1.4506121882913312 | validation: 1.421270103204735]
	TIME [epoch: 1.36 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4601307251021372		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 1.4601307251021372 | validation: 1.0948602863942907]
	TIME [epoch: 1.36 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4772678701490651		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 1.4772678701490651 | validation: 1.3727846471232432]
	TIME [epoch: 1.36 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4233727486096555		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 1.4233727486096555 | validation: 1.1033306348623186]
	TIME [epoch: 1.36 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.411294047360949		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 1.411294047360949 | validation: 1.4576367291515617]
	TIME [epoch: 1.36 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4430585117266017		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 1.4430585117266017 | validation: 1.1749087699507514]
	TIME [epoch: 1.37 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4144406304746577		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 1.4144406304746577 | validation: 1.3912680578307963]
	TIME [epoch: 1.36 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.404712055788127		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 1.404712055788127 | validation: 1.1036073303165919]
	TIME [epoch: 1.36 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4230426216163883		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 1.4230426216163883 | validation: 1.3757157299952034]
	TIME [epoch: 1.36 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4217452023586732		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 1.4217452023586732 | validation: 1.0948612481673106]
	TIME [epoch: 1.36 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4520871594883573		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 1.4520871594883573 | validation: 1.3446727236414113]
	TIME [epoch: 1.36 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4260756308902642		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 1.4260756308902642 | validation: 1.094245190203536]
	TIME [epoch: 1.36 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4267091602291948		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 1.4267091602291948 | validation: 1.4038720628688077]
	TIME [epoch: 1.36 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4043221426235866		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 1.4043221426235866 | validation: 1.1268232801806939]
	TIME [epoch: 1.36 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3987110978692012		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 1.3987110978692012 | validation: 1.3907317089818878]
	TIME [epoch: 1.36 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4275523726948407		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 1.4275523726948407 | validation: 1.152499789584003]
	TIME [epoch: 1.36 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4153288080529791		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 1.4153288080529791 | validation: 1.3682695604056656]
	TIME [epoch: 1.36 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.397089908106628		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 1.397089908106628 | validation: 1.0998315768284719]
	TIME [epoch: 1.36 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3956981197874163		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 1.3956981197874163 | validation: 1.3714950306546563]
	TIME [epoch: 1.36 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4146120575003953		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 1.4146120575003953 | validation: 1.0997604833916117]
	TIME [epoch: 1.36 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4711809026888767		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 1.4711809026888767 | validation: 1.388001358775907]
	TIME [epoch: 1.36 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.467486291826098		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 1.467486291826098 | validation: 1.0909925250159893]
	TIME [epoch: 1.36 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.409177006026536		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 1.409177006026536 | validation: 1.4551122020864806]
	TIME [epoch: 1.36 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4433473733158717		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 1.4433473733158717 | validation: 1.1551817666026263]
	TIME [epoch: 1.36 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.408085640864924		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 1.408085640864924 | validation: 1.3569160368721607]
	TIME [epoch: 1.36 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3843544848521507		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 1.3843544848521507 | validation: 1.075596500177213]
	TIME [epoch: 1.36 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.395984269555956		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 1.395984269555956 | validation: 1.38372733577759]
	TIME [epoch: 1.36 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4206005612434287		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 1.4206005612434287 | validation: 1.0761049873803388]
	TIME [epoch: 1.36 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.41007880159016		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 1.41007880159016 | validation: 1.3381630632976373]
	TIME [epoch: 1.36 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3988702249822		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 1.3988702249822 | validation: 1.0640107238535208]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_300.pth
	Model improved!!!
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4149500019124606		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 1.4149500019124606 | validation: 1.359869072945453]
	TIME [epoch: 1.35 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.402180632013226		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 1.402180632013226 | validation: 1.0995098237155887]
	TIME [epoch: 1.36 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3953761145281613		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 1.3953761145281613 | validation: 1.3201008355431754]
	TIME [epoch: 1.35 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3905706846627193		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 1.3905706846627193 | validation: 1.0543280364288024]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_304.pth
	Model improved!!!
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4192192031864344		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 1.4192192031864344 | validation: 1.325961795834436]
	TIME [epoch: 1.36 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3796722660476608		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 1.3796722660476608 | validation: 1.1104185033614533]
	TIME [epoch: 1.36 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3923226367542105		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 1.3923226367542105 | validation: 1.3494773997715406]
	TIME [epoch: 1.36 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4034992194795797		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 1.4034992194795797 | validation: 1.0909430747523765]
	TIME [epoch: 1.36 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3986255890754706		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 1.3986255890754706 | validation: 1.3422230395779877]
	TIME [epoch: 1.36 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.38998311453611		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 1.38998311453611 | validation: 1.0517670974926785]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_310.pth
	Model improved!!!
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4518709245847385		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 1.4518709245847385 | validation: 1.3897502513183673]
	TIME [epoch: 1.36 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.472149055266579		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 1.472149055266579 | validation: 1.1021159119051263]
	TIME [epoch: 1.36 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4037714754154758		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 1.4037714754154758 | validation: 1.3681389611771901]
	TIME [epoch: 1.36 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4042450230562864		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 1.4042450230562864 | validation: 1.1128843482107755]
	TIME [epoch: 1.36 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4173262229709527		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 1.4173262229709527 | validation: 1.3547379673188986]
	TIME [epoch: 1.36 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4038389981917265		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 1.4038389981917265 | validation: 1.1251355958758897]
	TIME [epoch: 1.36 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3725893672956022		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 1.3725893672956022 | validation: 1.3610669281834595]
	TIME [epoch: 1.35 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4108302828433983		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 1.4108302828433983 | validation: 1.0576188825650554]
	TIME [epoch: 1.36 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4003636061586522		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 1.4003636061586522 | validation: 1.3131752048661343]
	TIME [epoch: 1.36 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.385001545955526		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 1.385001545955526 | validation: 1.058050358379742]
	TIME [epoch: 1.36 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.396717034758401		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 1.396717034758401 | validation: 1.3293203340646]
	TIME [epoch: 1.36 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3847136632887669		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 1.3847136632887669 | validation: 1.0711598803889364]
	TIME [epoch: 1.36 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3774968386165662		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 1.3774968386165662 | validation: 1.3263639431928038]
	TIME [epoch: 1.36 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.383449847134418		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 1.383449847134418 | validation: 1.1000792672598305]
	TIME [epoch: 1.36 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3866510418263789		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 1.3866510418263789 | validation: 1.3475524117194597]
	TIME [epoch: 1.36 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3774469959740372		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 1.3774469959740372 | validation: 1.100925062791158]
	TIME [epoch: 1.36 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.379426182759308		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 1.379426182759308 | validation: 1.3357200681944645]
	TIME [epoch: 1.36 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3847003291853202		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 1.3847003291853202 | validation: 1.0861090491994962]
	TIME [epoch: 1.36 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.364185024998797		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 1.364185024998797 | validation: 1.3191378714347284]
	TIME [epoch: 1.36 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4270236056654448		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 1.4270236056654448 | validation: 1.092079018596805]
	TIME [epoch: 1.36 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4846799559166663		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 1.4846799559166663 | validation: 1.3123929008940343]
	TIME [epoch: 1.36 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.388356918279111		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 1.388356918279111 | validation: 1.0894524652636814]
	TIME [epoch: 1.36 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3703124527873056		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 1.3703124527873056 | validation: 1.4476586466680186]
	TIME [epoch: 1.35 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4148054672650578		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 1.4148054672650578 | validation: 1.125457975743145]
	TIME [epoch: 1.36 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3598201535395933		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 1.3598201535395933 | validation: 1.29668789953068]
	TIME [epoch: 1.36 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3626994038584355		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 1.3626994038584355 | validation: 1.0419691374808606]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_336.pth
	Model improved!!!
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3915793555839024		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 1.3915793555839024 | validation: 1.2698679345400135]
	TIME [epoch: 1.35 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.368549864423946		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 1.368549864423946 | validation: 1.060603721701136]
	TIME [epoch: 1.36 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3699738849083423		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 1.3699738849083423 | validation: 1.3073508163431775]
	TIME [epoch: 1.36 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3847151313037682		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 1.3847151313037682 | validation: 1.05588995382461]
	TIME [epoch: 1.36 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3823720844943774		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 1.3823720844943774 | validation: 1.2807439628309991]
	TIME [epoch: 1.35 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3679878526607145		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 1.3679878526607145 | validation: 1.066044540137353]
	TIME [epoch: 1.35 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3689598603632889		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 1.3689598603632889 | validation: 1.30782606267457]
	TIME [epoch: 1.35 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3842424550067296		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 1.3842424550067296 | validation: 1.0795683601004051]
	TIME [epoch: 1.35 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4045630926217687		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 1.4045630926217687 | validation: 1.2764650959358648]
	TIME [epoch: 1.36 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3826010314747004		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 1.3826010314747004 | validation: 1.06156420908858]
	TIME [epoch: 1.36 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3751553584856064		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 1.3751553584856064 | validation: 1.396477316261716]
	TIME [epoch: 1.35 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3761892602640255		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 1.3761892602640255 | validation: 1.131516148188722]
	TIME [epoch: 1.36 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.372724858375386		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 1.372724858375386 | validation: 1.3245889057851445]
	TIME [epoch: 1.35 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3830761502784197		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 1.3830761502784197 | validation: 1.081577632911843]
	TIME [epoch: 1.35 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3574543018828427		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 1.3574543018828427 | validation: 1.2962850918680129]
	TIME [epoch: 1.35 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3659079028951104		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 1.3659079028951104 | validation: 1.0506501519531217]
	TIME [epoch: 1.36 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3861918089297185		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 1.3861918089297185 | validation: 1.3027708158828588]
	TIME [epoch: 1.36 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3797620776517983		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 1.3797620776517983 | validation: 1.0506988589356785]
	TIME [epoch: 1.35 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3744872084278665		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 1.3744872084278665 | validation: 1.2470651738602336]
	TIME [epoch: 1.36 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3506844655718164		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 1.3506844655718164 | validation: 1.0583307447030497]
	TIME [epoch: 1.35 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3588232639252353		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 1.3588232639252353 | validation: 1.3438457903398104]
	TIME [epoch: 1.36 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3777673874156608		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 1.3777673874156608 | validation: 1.0993683313275555]
	TIME [epoch: 1.36 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3594510224744454		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 1.3594510224744454 | validation: 1.2960517739863462]
	TIME [epoch: 1.36 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3753652831598908		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 1.3753652831598908 | validation: 1.1070569521245686]
	TIME [epoch: 1.36 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3600750294386126		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 1.3600750294386126 | validation: 1.277173576229335]
	TIME [epoch: 1.36 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3420662473725806		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 1.3420662473725806 | validation: 1.0738316907233043]
	TIME [epoch: 1.36 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3697128650062036		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 1.3697128650062036 | validation: 1.3496837260339964]
	TIME [epoch: 1.35 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.438620766237468		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 1.438620766237468 | validation: 1.058099974096043]
	TIME [epoch: 1.35 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.398090930724909		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 1.398090930724909 | validation: 1.2450673512064474]
	TIME [epoch: 1.36 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3508434209396671		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 1.3508434209396671 | validation: 1.0821398065209438]
	TIME [epoch: 1.36 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.365792665323662		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 1.365792665323662 | validation: 1.3446092730251415]
	TIME [epoch: 1.35 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3820954958719387		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 1.3820954958719387 | validation: 1.096243494406499]
	TIME [epoch: 1.36 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3446432298377662		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 1.3446432298377662 | validation: 1.25167425881821]
	TIME [epoch: 1.36 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3434330018855998		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 1.3434330018855998 | validation: 1.0598661244146748]
	TIME [epoch: 1.36 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.364823618053897		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 1.364823618053897 | validation: 1.2821218584594973]
	TIME [epoch: 1.36 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3583397505375225		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 1.3583397505375225 | validation: 1.050588542315621]
	TIME [epoch: 1.35 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.362520932691746		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 1.362520932691746 | validation: 1.2882371045311904]
	TIME [epoch: 1.36 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3588057057565124		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 1.3588057057565124 | validation: 1.0717124887182135]
	TIME [epoch: 1.36 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.340451991083176		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 1.340451991083176 | validation: 1.2609636372871937]
	TIME [epoch: 1.36 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3515644657614303		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 1.3515644657614303 | validation: 1.0485627074793633]
	TIME [epoch: 1.36 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3560944028797872		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 1.3560944028797872 | validation: 1.2669575533505177]
	TIME [epoch: 1.36 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3429239373463029		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 1.3429239373463029 | validation: 1.0721937282240896]
	TIME [epoch: 1.36 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3552962011266687		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 1.3552962011266687 | validation: 1.2817396169153703]
	TIME [epoch: 1.36 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.348035834904947		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 1.348035834904947 | validation: 1.0520422700172471]
	TIME [epoch: 1.36 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3547326398456598		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 1.3547326398456598 | validation: 1.2664624933303779]
	TIME [epoch: 1.36 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3740453414286953		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 1.3740453414286953 | validation: 1.070498953099891]
	TIME [epoch: 1.36 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.410272283077868		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 1.410272283077868 | validation: 1.2783610305993975]
	TIME [epoch: 1.36 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3747893683986927		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 1.3747893683986927 | validation: 1.0804014114398621]
	TIME [epoch: 1.36 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.327067900450887		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 1.327067900450887 | validation: 1.3650897238610824]
	TIME [epoch: 1.36 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3821992116390112		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 1.3821992116390112 | validation: 1.0897045390050726]
	TIME [epoch: 1.36 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3570409546631372		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 1.3570409546631372 | validation: 1.2357532620432234]
	TIME [epoch: 1.36 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3555147477949743		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 1.3555147477949743 | validation: 1.0545536059800147]
	TIME [epoch: 1.36 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3486387059093465		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 1.3486387059093465 | validation: 1.2699589939069322]
	TIME [epoch: 1.35 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3437914305366252		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 1.3437914305366252 | validation: 1.0523989769039612]
	TIME [epoch: 1.36 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3387107852496456		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 1.3387107852496456 | validation: 1.2663152830042366]
	TIME [epoch: 1.36 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3491500287134524		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 1.3491500287134524 | validation: 1.034873915844321]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_392.pth
	Model improved!!!
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3574974154500625		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 1.3574974154500625 | validation: 1.2424567048955666]
	TIME [epoch: 1.36 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.339245332537539		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 1.339245332537539 | validation: 1.0609789936230187]
	TIME [epoch: 1.36 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3522412381479623		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 1.3522412381479623 | validation: 1.2664348279762476]
	TIME [epoch: 1.36 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3492236235393569		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 1.3492236235393569 | validation: 1.0481508445749368]
	TIME [epoch: 1.36 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3292495288738044		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 1.3292495288738044 | validation: 1.2596214931711647]
	TIME [epoch: 1.36 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3456698977197357		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 1.3456698977197357 | validation: 1.082594851247469]
	TIME [epoch: 1.36 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3326884792359368		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 1.3326884792359368 | validation: 1.2408895140954463]
	TIME [epoch: 1.36 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3286042927945318		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 1.3286042927945318 | validation: 1.0392055559219784]
	TIME [epoch: 1.35 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3462557969081053		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 1.3462557969081053 | validation: 1.2908373576140368]
	TIME [epoch: 1.36 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3746140495672559		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 1.3746140495672559 | validation: 1.0852344333322008]
	TIME [epoch: 1.35 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3846641832352413		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 1.3846641832352413 | validation: 1.2529578634848932]
	TIME [epoch: 1.35 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3601134352035318		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 1.3601134352035318 | validation: 1.0339897153406135]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_404.pth
	Model improved!!!
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3393068080117292		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 1.3393068080117292 | validation: 1.339028539336765]
	TIME [epoch: 1.37 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3802715065017699		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 1.3802715065017699 | validation: 1.0997240573727678]
	TIME [epoch: 1.36 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3330383013122533		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 1.3330383013122533 | validation: 1.2227147418426985]
	TIME [epoch: 1.36 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.341431527626359		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 1.341431527626359 | validation: 1.038783476229957]
	TIME [epoch: 1.36 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3311418557131693		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 1.3311418557131693 | validation: 1.2367319309571412]
	TIME [epoch: 1.36 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.333174951271592		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 1.333174951271592 | validation: 1.057232503131617]
	TIME [epoch: 1.36 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3303893345913949		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 1.3303893345913949 | validation: 1.233735243759945]
	TIME [epoch: 1.36 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3298243077434218		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 1.3298243077434218 | validation: 1.0348776551468866]
	TIME [epoch: 1.36 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3358062558675363		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 1.3358062558675363 | validation: 1.2195169641057904]
	TIME [epoch: 1.36 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3458011455621193		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 1.3458011455621193 | validation: 1.0544287014655753]
	TIME [epoch: 1.36 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.353365585371644		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 1.353365585371644 | validation: 1.2478496838964497]
	TIME [epoch: 1.36 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3418276184923845		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 1.3418276184923845 | validation: 1.0548425953193676]
	TIME [epoch: 1.36 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.345526661392135		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 1.345526661392135 | validation: 1.216979345473629]
	TIME [epoch: 1.36 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3370697458815926		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 1.3370697458815926 | validation: 1.0777577084576675]
	TIME [epoch: 1.36 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3223108522487932		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 1.3223108522487932 | validation: 1.2542523000558192]
	TIME [epoch: 1.36 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3353821320470152		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 1.3353821320470152 | validation: 1.1201923519111625]
	TIME [epoch: 1.36 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3368701656110988		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 1.3368701656110988 | validation: 1.2467288247092019]
	TIME [epoch: 1.36 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3380427287357373		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 1.3380427287357373 | validation: 1.0558146450568082]
	TIME [epoch: 1.35 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3260525325777661		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 1.3260525325777661 | validation: 1.2480797241662662]
	TIME [epoch: 1.35 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3352787175787584		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 1.3352787175787584 | validation: 1.0615003709251025]
	TIME [epoch: 1.36 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3620798791394133		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 1.3620798791394133 | validation: 1.2399533572744184]
	TIME [epoch: 1.36 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3434964849404736		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 1.3434964849404736 | validation: 1.0558126869855426]
	TIME [epoch: 1.36 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.326276682621076		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 1.326276682621076 | validation: 1.255822493140669]
	TIME [epoch: 1.36 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3415763502122433		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 1.3415763502122433 | validation: 1.0517137437278856]
	TIME [epoch: 1.36 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.320300481780328		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 1.320300481780328 | validation: 1.2317798438451175]
	TIME [epoch: 1.36 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.316562588292411		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 1.316562588292411 | validation: 1.0650943853052557]
	TIME [epoch: 1.36 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3266984011344733		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 1.3266984011344733 | validation: 1.2578742494189055]
	TIME [epoch: 1.36 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.336602054766254		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 1.336602054766254 | validation: 1.0417098477841928]
	TIME [epoch: 1.36 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3337004789721263		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 1.3337004789721263 | validation: 1.212103294032917]
	TIME [epoch: 1.36 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3469545543927068		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 1.3469545543927068 | validation: 1.0505232934144517]
	TIME [epoch: 1.36 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3578076729363908		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 1.3578076729363908 | validation: 1.2217266898863104]
	TIME [epoch: 1.36 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3334743190728522		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 1.3334743190728522 | validation: 1.06044812663786]
	TIME [epoch: 1.36 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.335489220226633		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 1.335489220226633 | validation: 1.2888095643911068]
	TIME [epoch: 1.36 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3358719285345595		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 1.3358719285345595 | validation: 1.0870895083472079]
	TIME [epoch: 1.36 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3202565260330874		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 1.3202565260330874 | validation: 1.216603602887949]
	TIME [epoch: 1.36 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3180287838707647		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 1.3180287838707647 | validation: 1.0400716686806764]
	TIME [epoch: 1.36 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3195582659908633		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 1.3195582659908633 | validation: 1.2405015833208535]
	TIME [epoch: 1.36 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3310803747151707		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 1.3310803747151707 | validation: 1.0404070045593965]
	TIME [epoch: 1.36 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.334477578441585		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 1.334477578441585 | validation: 1.227316971766425]
	TIME [epoch: 1.36 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3457070288640176		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 1.3457070288640176 | validation: 1.0642393312726788]
	TIME [epoch: 1.36 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3371601337279106		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 1.3371601337279106 | validation: 1.1950176265474306]
	TIME [epoch: 1.36 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.332088048769146		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 1.332088048769146 | validation: 1.0480315114756462]
	TIME [epoch: 1.36 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.314513609066024		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 1.314513609066024 | validation: 1.2271383210337148]
	TIME [epoch: 1.36 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.327360785951622		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 1.327360785951622 | validation: 1.073029489136491]
	TIME [epoch: 1.36 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3194344393866337		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 1.3194344393866337 | validation: 1.235022021346435]
	TIME [epoch: 1.36 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.326289371868694		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 1.326289371868694 | validation: 1.0786174038810294]
	TIME [epoch: 1.36 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3259508299088179		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 1.3259508299088179 | validation: 1.228083475979001]
	TIME [epoch: 1.36 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3321735924101836		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 1.3321735924101836 | validation: 1.0924942548801613]
	TIME [epoch: 1.36 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3219151503508928		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 1.3219151503508928 | validation: 1.2574984296080807]
	TIME [epoch: 1.36 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3195497425068243		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 1.3195497425068243 | validation: 1.084154037147541]
	TIME [epoch: 1.36 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3150864028812197		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 1.3150864028812197 | validation: 1.2405589259141372]
	TIME [epoch: 1.36 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.328984450580671		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 1.328984450580671 | validation: 1.0576728651745502]
	TIME [epoch: 1.36 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3508452731495106		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 1.3508452731495106 | validation: 1.258200130469375]
	TIME [epoch: 1.36 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3726390167138742		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 1.3726390167138742 | validation: 1.057075613165845]
	TIME [epoch: 1.36 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.319110348793454		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 1.319110348793454 | validation: 1.2657099997658428]
	TIME [epoch: 1.36 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.333307134804457		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 1.333307134804457 | validation: 1.0749381176762367]
	TIME [epoch: 1.36 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3133920041853717		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 1.3133920041853717 | validation: 1.219569664127376]
	TIME [epoch: 1.36 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3095236709034808		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 1.3095236709034808 | validation: 1.0298107619586883]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_462.pth
	Model improved!!!
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.31593956628343		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 1.31593956628343 | validation: 1.2024321117199386]
	TIME [epoch: 1.36 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3202704292606038		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 1.3202704292606038 | validation: 1.0381721604177288]
	TIME [epoch: 1.36 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3128682652550976		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 1.3128682652550976 | validation: 1.2302672178931544]
	TIME [epoch: 1.36 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3110642144712228		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 1.3110642144712228 | validation: 1.0522902193086368]
	TIME [epoch: 1.36 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.31629734011609		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 1.31629734011609 | validation: 1.2091828610271764]
	TIME [epoch: 1.36 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.307147332476452		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 1.307147332476452 | validation: 1.0476037224931176]
	TIME [epoch: 1.36 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3192952828292244		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 1.3192952828292244 | validation: 1.221412078586405]
	TIME [epoch: 1.36 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3137853139309181		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 1.3137853139309181 | validation: 1.053546722041876]
	TIME [epoch: 1.37 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3129135977791118		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 1.3129135977791118 | validation: 1.233906438696595]
	TIME [epoch: 1.36 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.314917948516715		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 1.314917948516715 | validation: 1.0426684822439911]
	TIME [epoch: 1.36 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.318648408004556		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 1.318648408004556 | validation: 1.238523422352399]
	TIME [epoch: 1.36 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3133330513723396		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 1.3133330513723396 | validation: 1.0395858161209535]
	TIME [epoch: 1.36 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3331959878507278		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 1.3331959878507278 | validation: 1.229698416357853]
	TIME [epoch: 1.36 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.344460046602328		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 1.344460046602328 | validation: 1.0561556164462578]
	TIME [epoch: 1.36 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.322094307383881		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 1.322094307383881 | validation: 1.2309411508596173]
	TIME [epoch: 1.36 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3172043053480038		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 1.3172043053480038 | validation: 1.0857254273554957]
	TIME [epoch: 1.36 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3399154032225797		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 1.3399154032225797 | validation: 1.242157832177388]
	TIME [epoch: 1.36 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3223137839169383		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 1.3223137839169383 | validation: 1.0783872734588138]
	TIME [epoch: 1.36 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.304481442551734		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 1.304481442551734 | validation: 1.2326957967861494]
	TIME [epoch: 1.36 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.312825476650786		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 1.312825476650786 | validation: 1.0335020203419163]
	TIME [epoch: 1.36 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3218194820884797		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 1.3218194820884797 | validation: 1.2113590187587773]
	TIME [epoch: 1.36 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.308453042013072		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 1.308453042013072 | validation: 1.067236211046623]
	TIME [epoch: 1.36 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3089123783814542		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 1.3089123783814542 | validation: 1.1965290194413623]
	TIME [epoch: 1.36 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3094059638897293		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 1.3094059638897293 | validation: 1.0419747080575623]
	TIME [epoch: 1.36 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3299099894953719		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 1.3299099894953719 | validation: 1.2188118178389846]
	TIME [epoch: 1.36 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3272090524073459		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 1.3272090524073459 | validation: 1.0663424935503265]
	TIME [epoch: 1.36 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3068189055190311		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 1.3068189055190311 | validation: 1.1945234871021713]
	TIME [epoch: 1.36 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3069074740358122		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 1.3069074740358122 | validation: 1.0483470957498888]
	TIME [epoch: 1.36 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3166824713700223		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 1.3166824713700223 | validation: 1.227345890656671]
	TIME [epoch: 1.36 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.312984964284914		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 1.312984964284914 | validation: 1.0551344006964587]
	TIME [epoch: 1.36 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2999386107926916		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 1.2999386107926916 | validation: 1.2227138298218516]
	TIME [epoch: 1.36 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.307315724371311		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 1.307315724371311 | validation: 1.0649288537114754]
	TIME [epoch: 1.36 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3071910460556189		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 1.3071910460556189 | validation: 1.1982222702548433]
	TIME [epoch: 1.36 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3098331640699348		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 1.3098331640699348 | validation: 1.0562820442111456]
	TIME [epoch: 1.36 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2948742021565982		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 1.2948742021565982 | validation: 1.2188539703446406]
	TIME [epoch: 1.36 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3098850927369392		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 1.3098850927369392 | validation: 1.0389386536802812]
	TIME [epoch: 1.36 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3295478754520116		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 1.3295478754520116 | validation: 1.2803263893610657]
	TIME [epoch: 1.36 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3642100029419817		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 1.3642100029419817 | validation: 1.0400293535056466]
	TIME [epoch: 1.36 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.308451384935915		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 1.308451384935915 | validation: 1.2082860814704153]
	TIME [epoch: 179 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3090298409043752		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 1.3090298409043752 | validation: 1.0824347654615611]
	TIME [epoch: 2.69 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.310756171089835		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 1.310756171089835 | validation: 1.2144621050840803]
	TIME [epoch: 2.67 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3003521153394784		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 1.3003521153394784 | validation: 1.0597390746536517]
	TIME [epoch: 2.69 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3127105387989522		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 1.3127105387989522 | validation: 1.207912215503268]
	TIME [epoch: 2.68 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3097508342498216		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 1.3097508342498216 | validation: 1.0250208855644307]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_506.pth
	Model improved!!!
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3170163026602373		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 1.3170163026602373 | validation: 1.1809022070117756]
	TIME [epoch: 2.68 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.291486400445247		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 1.291486400445247 | validation: 1.0460232825214204]
	TIME [epoch: 2.68 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2910795596180382		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 1.2910795596180382 | validation: 1.208868121554579]
	TIME [epoch: 2.67 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.293525256698078		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 1.293525256698078 | validation: 1.0257432727966687]
	TIME [epoch: 2.67 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3087194651456826		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 1.3087194651456826 | validation: 1.1850391648408825]
	TIME [epoch: 2.67 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3021521528016415		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 1.3021521528016415 | validation: 1.0350448674639812]
	TIME [epoch: 2.67 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.306382958249331		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 1.306382958249331 | validation: 1.202786661788775]
	TIME [epoch: 2.67 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2942131589126373		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 1.2942131589126373 | validation: 1.033217989052279]
	TIME [epoch: 2.67 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.29551268368364		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 1.29551268368364 | validation: 1.182710381620441]
	TIME [epoch: 2.68 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3023829342394868		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 1.3023829342394868 | validation: 1.0347789460264145]
	TIME [epoch: 2.67 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3040240918615136		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 1.3040240918615136 | validation: 1.2109001017744976]
	TIME [epoch: 2.67 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3123452800287536		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 1.3123452800287536 | validation: 1.0777032554672386]
	TIME [epoch: 2.67 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3389330311289576		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 1.3389330311289576 | validation: 1.1880961144823616]
	TIME [epoch: 2.68 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3248382736405575		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 1.3248382736405575 | validation: 1.0344476212403515]
	TIME [epoch: 2.67 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.29644521827097		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 1.29644521827097 | validation: 1.2922846682579567]
	TIME [epoch: 2.69 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3292036432171708		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 1.3292036432171708 | validation: 1.0957110180313978]
	TIME [epoch: 2.67 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3075278396603833		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 1.3075278396603833 | validation: 1.1702213776648376]
	TIME [epoch: 2.67 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2861829544011232		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 1.2861829544011232 | validation: 1.0395497676307603]
	TIME [epoch: 2.67 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.302352247188445		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 1.302352247188445 | validation: 1.2111567776176688]
	TIME [epoch: 2.67 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.305945232757682		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 1.305945232757682 | validation: 1.0568034979512448]
	TIME [epoch: 2.67 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2854701380473892		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 1.2854701380473892 | validation: 1.2062181883336827]
	TIME [epoch: 2.67 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2888541460145688		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 1.2888541460145688 | validation: 1.0326590104335471]
	TIME [epoch: 2.67 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2988566070990915		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 1.2988566070990915 | validation: 1.2015230983433405]
	TIME [epoch: 2.67 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2907116725403023		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 1.2907116725403023 | validation: 1.0405557100990606]
	TIME [epoch: 2.68 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2916382670983597		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 1.2916382670983597 | validation: 1.1992190150869984]
	TIME [epoch: 2.67 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2967147817629086		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 1.2967147817629086 | validation: 1.0408244894199628]
	TIME [epoch: 2.67 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3088036031288939		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 1.3088036031288939 | validation: 1.1928716706985865]
	TIME [epoch: 2.68 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2995781735683838		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 1.2995781735683838 | validation: 1.045902666767021]
	TIME [epoch: 2.67 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2934073049687538		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 1.2934073049687538 | validation: 1.197469894863869]
	TIME [epoch: 2.68 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.29119415856449		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 1.29119415856449 | validation: 1.0581139837832305]
	TIME [epoch: 2.67 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.292036167192326		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 1.292036167192326 | validation: 1.1934542980442393]
	TIME [epoch: 2.68 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2902633524951859		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 1.2902633524951859 | validation: 1.0506599274752417]
	TIME [epoch: 2.67 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2877960144657596		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 1.2877960144657596 | validation: 1.218165693461717]
	TIME [epoch: 2.68 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3010300380440492		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 1.3010300380440492 | validation: 1.046637606582099]
	TIME [epoch: 2.67 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.284453749091955		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 1.284453749091955 | validation: 1.1850737106686215]
	TIME [epoch: 2.68 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2926020422447138		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 1.2926020422447138 | validation: 1.0787206058849355]
	TIME [epoch: 2.67 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2837052618959206		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 1.2837052618959206 | validation: 1.2001287018084703]
	TIME [epoch: 2.68 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.293347813660106		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 1.293347813660106 | validation: 1.0384321233172442]
	TIME [epoch: 2.67 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2873052867695765		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 1.2873052867695765 | validation: 1.180623272830308]
	TIME [epoch: 2.68 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2851620047742194		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 1.2851620047742194 | validation: 1.042337576233347]
	TIME [epoch: 2.67 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2821049251591472		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 1.2821049251591472 | validation: 1.1825923627765005]
	TIME [epoch: 2.68 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2888601041367784		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 1.2888601041367784 | validation: 1.0920225719870698]
	TIME [epoch: 2.67 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2833680486054533		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 1.2833680486054533 | validation: 1.1812905383777843]
	TIME [epoch: 2.68 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2866210142842416		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 1.2866210142842416 | validation: 1.1103213770982314]
	TIME [epoch: 2.67 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2817547040959314		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 1.2817547040959314 | validation: 1.1252973083887718]
	TIME [epoch: 2.68 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2792618862934058		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 1.2792618862934058 | validation: 1.2033861884513262]
	TIME [epoch: 2.67 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3023183733842236		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 1.3023183733842236 | validation: 0.9996823840963727]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_553.pth
	Model improved!!!
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.356583987752346		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 1.356583987752346 | validation: 1.1797304050351523]
	TIME [epoch: 2.67 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2790495449476595		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 1.2790495449476595 | validation: 1.060520396752813]
	TIME [epoch: 2.67 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2587398763229787		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 1.2587398763229787 | validation: 1.1651626975425022]
	TIME [epoch: 2.67 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.274711136869046		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 1.274711136869046 | validation: 1.02035343628088]
	TIME [epoch: 2.68 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.293849744875592		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 1.293849744875592 | validation: 1.2087307937486373]
	TIME [epoch: 2.67 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.294738050100935		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 1.294738050100935 | validation: 1.0400242108099695]
	TIME [epoch: 2.68 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2782499621379757		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 1.2782499621379757 | validation: 1.1674427753560916]
	TIME [epoch: 2.68 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2681453033823606		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 1.2681453033823606 | validation: 1.026424082998721]
	TIME [epoch: 2.67 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.281004941302876		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 1.281004941302876 | validation: 1.1838011679808034]
	TIME [epoch: 2.67 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2747856401858335		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 1.2747856401858335 | validation: 1.0317635727908419]
	TIME [epoch: 2.68 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2875847113461807		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 1.2875847113461807 | validation: 1.1818793108828693]
	TIME [epoch: 2.67 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2835047863842135		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 1.2835047863842135 | validation: 1.0346421637923622]
	TIME [epoch: 2.67 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2757920148875306		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 1.2757920148875306 | validation: 1.1525321095200323]
	TIME [epoch: 2.67 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2604937194940593		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 1.2604937194940593 | validation: 1.0246346975891814]
	TIME [epoch: 2.67 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2805666906540156		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 1.2805666906540156 | validation: 1.1785572686645454]
	TIME [epoch: 2.67 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2725812212556984		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 1.2725812212556984 | validation: 1.027008128058978]
	TIME [epoch: 2.67 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2772011278093711		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 1.2772011278093711 | validation: 1.1561013945797398]
	TIME [epoch: 2.68 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2833882978302285		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 1.2833882978302285 | validation: 1.036389299144625]
	TIME [epoch: 2.67 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.276989533159907		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 1.276989533159907 | validation: 1.156763861552349]
	TIME [epoch: 2.67 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2733933200950054		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 1.2733933200950054 | validation: 1.0529000726262951]
	TIME [epoch: 2.67 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.272725105468098		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 1.272725105468098 | validation: 1.187001126774431]
	TIME [epoch: 2.67 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2823975541245634		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 1.2823975541245634 | validation: 1.0073737292856706]
	TIME [epoch: 2.68 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2753530497945504		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 1.2753530497945504 | validation: 1.168132278812256]
	TIME [epoch: 2.67 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2725179292029825		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 1.2725179292029825 | validation: 1.0464289992520315]
	TIME [epoch: 2.67 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2667941082337206		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 1.2667941082337206 | validation: 1.2230801530618294]
	TIME [epoch: 2.67 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2929182324240693		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 1.2929182324240693 | validation: 1.113279624879856]
	TIME [epoch: 2.67 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2869014151054368		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 1.2869014151054368 | validation: 1.1681798829866896]
	TIME [epoch: 2.67 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.270561199499986		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 1.270561199499986 | validation: 1.0727736264072]
	TIME [epoch: 2.68 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.248068653150194		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 1.248068653150194 | validation: 1.142695321979707]
	TIME [epoch: 2.67 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.257875429644441		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 1.257875429644441 | validation: 1.0265419908288194]
	TIME [epoch: 2.68 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2808994500782893		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 1.2808994500782893 | validation: 1.1717356328856479]
	TIME [epoch: 2.68 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2777517136757928		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 1.2777517136757928 | validation: 1.0288864052893694]
	TIME [epoch: 2.67 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2701847146457266		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 1.2701847146457266 | validation: 1.15442536864089]
	TIME [epoch: 2.68 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2741504403223087		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 1.2741504403223087 | validation: 1.0404039736609838]
	TIME [epoch: 2.67 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2567112773092988		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 1.2567112773092988 | validation: 1.177122267511499]
	TIME [epoch: 2.67 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2608712047063972		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 1.2608712047063972 | validation: 1.0229238975695312]
	TIME [epoch: 2.67 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.28258396655004		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 1.28258396655004 | validation: 1.1602068848225129]
	TIME [epoch: 2.67 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2659803634457854		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 1.2659803634457854 | validation: 1.0425703800115573]
	TIME [epoch: 2.67 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2562973552921557		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 1.2562973552921557 | validation: 1.1783349107201653]
	TIME [epoch: 2.68 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2656551059515266		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 1.2656551059515266 | validation: 1.0270900742927016]
	TIME [epoch: 2.67 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2611432280248318		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 1.2611432280248318 | validation: 1.1629496619940634]
	TIME [epoch: 2.67 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2532913693141376		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 1.2532913693141376 | validation: 1.0830903148436553]
	TIME [epoch: 2.68 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2726883414724273		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 1.2726883414724273 | validation: 1.1911623940637828]
	TIME [epoch: 2.68 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2934099339162717		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 1.2934099339162717 | validation: 1.135187228322288]
	TIME [epoch: 2.68 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2659065575975132		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 1.2659065575975132 | validation: 1.0676797059417955]
	TIME [epoch: 2.68 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2497765210030247		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 1.2497765210030247 | validation: 1.1286815505216172]
	TIME [epoch: 2.68 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2562090456638142		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 1.2562090456638142 | validation: 1.0073621855678148]
	TIME [epoch: 2.68 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2641857276904396		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 1.2641857276904396 | validation: 1.1665558217003926]
	TIME [epoch: 2.67 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2705842856089737		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 1.2705842856089737 | validation: 1.0379552966702095]
	TIME [epoch: 2.67 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2635560305837252		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 1.2635560305837252 | validation: 1.1375995318280532]
	TIME [epoch: 2.67 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.257484614037893		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 1.257484614037893 | validation: 1.0267934782626336]
	TIME [epoch: 2.68 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2515491593203578		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 1.2515491593203578 | validation: 1.196214486742996]
	TIME [epoch: 2.67 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2602359394912774		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 1.2602359394912774 | validation: 1.0327811157316995]
	TIME [epoch: 2.67 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2693481762667544		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 1.2693481762667544 | validation: 1.142035569333006]
	TIME [epoch: 2.67 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2484881145913391		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 1.2484881145913391 | validation: 1.0472013743536088]
	TIME [epoch: 2.68 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2360401159329362		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 1.2360401159329362 | validation: 1.1409117529961892]
	TIME [epoch: 2.67 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2464654205064003		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 1.2464654205064003 | validation: 1.0401565336578302]
	TIME [epoch: 2.67 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2702853562870429		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 1.2702853562870429 | validation: 1.154102379293925]
	TIME [epoch: 2.67 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.262054517957931		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 1.262054517957931 | validation: 1.0429542723535323]
	TIME [epoch: 2.67 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2430442478196508		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 1.2430442478196508 | validation: 1.1542477479129467]
	TIME [epoch: 2.67 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2513062733207085		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 1.2513062733207085 | validation: 1.046888618222363]
	TIME [epoch: 2.67 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2944070560546173		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 1.2944070560546173 | validation: 1.1613832350433724]
	TIME [epoch: 2.67 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2788271890432839		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 1.2788271890432839 | validation: 1.0610648355535373]
	TIME [epoch: 2.67 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.240505278782931		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 1.240505278782931 | validation: 1.1030246606488565]
	TIME [epoch: 2.67 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.23945071931767		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 1.23945071931767 | validation: 1.0777014494833113]
	TIME [epoch: 2.67 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.240720493840234		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 1.240720493840234 | validation: 1.1316678301490535]
	TIME [epoch: 2.68 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2422523145594708		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 1.2422523145594708 | validation: 1.0382297690396556]
	TIME [epoch: 2.67 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2578797140398588		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 1.2578797140398588 | validation: 1.1878557818738182]
	TIME [epoch: 2.67 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.260873574633996		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 1.260873574633996 | validation: 1.015459594197928]
	TIME [epoch: 2.67 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2423819731519834		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 1.2423819731519834 | validation: 1.145479791686953]
	TIME [epoch: 2.67 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2388306893363463		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 1.2388306893363463 | validation: 1.0475176175922702]
	TIME [epoch: 2.67 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2525612975729454		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 1.2525612975729454 | validation: 1.1222208148867985]
	TIME [epoch: 2.67 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2523041916679503		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 1.2523041916679503 | validation: 1.0346627248944529]
	TIME [epoch: 2.67 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2572689750057509		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 1.2572689750057509 | validation: 1.141543397420422]
	TIME [epoch: 2.67 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2486766338445106		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 1.2486766338445106 | validation: 1.0281358848007138]
	TIME [epoch: 2.67 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2429215695023665		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 1.2429215695023665 | validation: 1.1642886291269687]
	TIME [epoch: 2.67 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.239512009494925		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 1.239512009494925 | validation: 1.016483789850517]
	TIME [epoch: 2.68 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2462550397770555		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 1.2462550397770555 | validation: 1.134875288816668]
	TIME [epoch: 2.67 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2405972143344213		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 1.2405972143344213 | validation: 1.0207989460867428]
	TIME [epoch: 2.67 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2357595413376548		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 1.2357595413376548 | validation: 1.146557796157978]
	TIME [epoch: 2.67 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2453960213308615		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 1.2453960213308615 | validation: 1.0366053307513674]
	TIME [epoch: 2.67 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2380169170482165		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 1.2380169170482165 | validation: 1.1492690878164933]
	TIME [epoch: 2.67 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2465741412739555		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 1.2465741412739555 | validation: 1.1061316309867963]
	TIME [epoch: 2.67 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2409280250351948		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 1.2409280250351948 | validation: 1.1383229724370645]
	TIME [epoch: 2.67 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2317218083214252		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 1.2317218083214252 | validation: 1.0149213201994776]
	TIME [epoch: 2.67 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2350941636201136		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 1.2350941636201136 | validation: 1.1401448701881396]
	TIME [epoch: 2.67 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2443431065193362		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 1.2443431065193362 | validation: 1.012651470117346]
	TIME [epoch: 2.67 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2578509050517754		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 1.2578509050517754 | validation: 1.149178421887301]
	TIME [epoch: 2.67 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2502253455450887		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 1.2502253455450887 | validation: 1.029998531450698]
	TIME [epoch: 2.68 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.234519228058354		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 1.234519228058354 | validation: 1.127295754488261]
	TIME [epoch: 2.67 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2278260177677485		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 1.2278260177677485 | validation: 1.0389119526235928]
	TIME [epoch: 2.67 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2220310620011003		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 1.2220310620011003 | validation: 1.1140992262792953]
	TIME [epoch: 2.67 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2236889336024936		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 1.2236889336024936 | validation: 1.003542352831055]
	TIME [epoch: 2.67 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2341042988766886		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 1.2341042988766886 | validation: 1.153265691588256]
	TIME [epoch: 2.67 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2380034054004168		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 1.2380034054004168 | validation: 0.9998685574905537]
	TIME [epoch: 2.67 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2299868121885567		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 1.2299868121885567 | validation: 1.1056912729053157]
	TIME [epoch: 2.67 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2194692068824466		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 1.2194692068824466 | validation: 1.0083613380959862]
	TIME [epoch: 2.67 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2214879697043186		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 1.2214879697043186 | validation: 1.1370796318080798]
	TIME [epoch: 2.67 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2299658814624126		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 1.2299658814624126 | validation: 1.0422224290286497]
	TIME [epoch: 2.68 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.224514715261553		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 1.224514715261553 | validation: 1.1383911392451682]
	TIME [epoch: 2.67 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2141186226593013		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 1.2141186226593013 | validation: 1.0367023549276717]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_6_v_mmd4_654.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 1522.975 seconds.
