Args:
Namespace(name='model_phi1_4a_distortion_v2_3_v_mmd1', outdir='out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_3/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_3/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2254039164

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.043781151085617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.043781151085617 | validation: 4.730956079373228]
	TIME [epoch: 162 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.833955103211314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.833955103211314 | validation: 4.554637756551523]
	TIME [epoch: 0.781 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.707798427172547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.707798427172547 | validation: 4.537416086754265]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.739539217390677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.739539217390677 | validation: 4.608449393520554]
	TIME [epoch: 0.697 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.494829151613492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.494829151613492 | validation: 4.446817141342698]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.3025114602381676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3025114602381676 | validation: 4.208875712412353]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.269962918873356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.269962918873356 | validation: 3.7173923413219585]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.700497360690148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.700497360690148 | validation: 3.1372774541208375]
	TIME [epoch: 0.685 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.362454775625579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.362454775625579 | validation: 2.5014417460909204]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.755964793181416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.755964793181416 | validation: 4.387776107868652]
	TIME [epoch: 0.685 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.453070844055623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.453070844055623 | validation: 3.4768176092443164]
	TIME [epoch: 0.684 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2610090912837792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2610090912837792 | validation: 2.7616626344667727]
	TIME [epoch: 0.685 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8999777865143597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8999777865143597 | validation: 3.056181039794554]
	TIME [epoch: 0.686 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4896228110377105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4896228110377105 | validation: 2.7525827935124028]
	TIME [epoch: 0.689 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.89690783510211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.89690783510211 | validation: 2.0942323368641973]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.508083624090561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.508083624090561 | validation: 2.208991913178951]
	TIME [epoch: 0.687 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4571527687893306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4571527687893306 | validation: 2.2136895537247385]
	TIME [epoch: 0.682 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.496659668608996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.496659668608996 | validation: 2.5240223781475084]
	TIME [epoch: 0.682 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.627255919945424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.627255919945424 | validation: 2.381936067138795]
	TIME [epoch: 0.681 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.688705706915318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.688705706915318 | validation: 2.2300276234121035]
	TIME [epoch: 0.683 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3877586789087393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3877586789087393 | validation: 2.0470210785756198]
	TIME [epoch: 0.683 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.319553981430247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.319553981430247 | validation: 2.1580214586699142]
	TIME [epoch: 0.691 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.287115554872122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.287115554872122 | validation: 2.1382575665073498]
	TIME [epoch: 0.684 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.353708066046435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.353708066046435 | validation: 2.2611317920655742]
	TIME [epoch: 0.687 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.36349905941676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.36349905941676 | validation: 2.2447937484286444]
	TIME [epoch: 0.683 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4015040251683777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4015040251683777 | validation: 2.012066341831163]
	TIME [epoch: 0.683 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1680428443089967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1680428443089967 | validation: 1.8698699534830485]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0894885706143116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0894885706143116 | validation: 1.862366666234938]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0525615976587024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0525615976587024 | validation: 1.8969879496318378]
	TIME [epoch: 0.686 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0607742468438057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0607742468438057 | validation: 1.9803419658533106]
	TIME [epoch: 0.683 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1399765688983083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1399765688983083 | validation: 2.313156157367248]
	TIME [epoch: 0.682 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3820801661443247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3820801661443247 | validation: 1.8711072270796008]
	TIME [epoch: 0.683 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.051952220610642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.051952220610642 | validation: 1.7526413516574928]
	TIME [epoch: 0.683 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.956743587154539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.956743587154539 | validation: 1.7340987094877098]
	TIME [epoch: 0.684 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9113128091647826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9113128091647826 | validation: 1.7639693363591669]
	TIME [epoch: 0.687 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.904785295367424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.904785295367424 | validation: 1.7998355423548276]
	TIME [epoch: 0.685 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9553272486159705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9553272486159705 | validation: 2.0605664720838752]
	TIME [epoch: 0.686 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.115567831640846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.115567831640846 | validation: 1.7387925381210296]
	TIME [epoch: 0.685 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9569281456612992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9569281456612992 | validation: 1.705957601389049]
	TIME [epoch: 0.685 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.868498275284747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.868498275284747 | validation: 1.6425971341587013]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7801893940797486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7801893940797486 | validation: 1.617606210853386]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7541252121531112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7541252121531112 | validation: 1.627590479434983]
	TIME [epoch: 0.686 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.759635450261532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.759635450261532 | validation: 1.7827425734610263]
	TIME [epoch: 0.684 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8269240488323788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8269240488323788 | validation: 1.553635769582218]
	TIME [epoch: 0.684 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.826402670626326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.826402670626326 | validation: 1.835733462744754]
	TIME [epoch: 0.692 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.80906955973356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.80906955973356 | validation: 1.3685642800511282]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6585799175654032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6585799175654032 | validation: 1.4960868588365042]
	TIME [epoch: 0.687 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.590183916094464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.590183916094464 | validation: 1.4858164095045034]
	TIME [epoch: 0.686 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5708458251597168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5708458251597168 | validation: 1.3829688777319824]
	TIME [epoch: 0.688 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5435773055246964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5435773055246964 | validation: 1.5932805791279896]
	TIME [epoch: 0.691 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.534666741822275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.534666741822275 | validation: 1.0918255994363846]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6209049877817798		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.6209049877817798 | validation: 2.1124914761705846]
	TIME [epoch: 0.687 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7913069173602525		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.7913069173602525 | validation: 1.0595108398711481]
	TIME [epoch: 0.685 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.557866158285366		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.557866158285366 | validation: 1.2739558958785253]
	TIME [epoch: 0.688 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4608339962899641		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.4608339962899641 | validation: 1.4899100087831163]
	TIME [epoch: 0.686 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4818700609159106		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.4818700609159106 | validation: 1.2716873418043761]
	TIME [epoch: 0.689 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5224681020521162		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.5224681020521162 | validation: 1.6345932615558227]
	TIME [epoch: 0.692 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6506387797172286		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.6506387797172286 | validation: 1.4348326338369877]
	TIME [epoch: 0.689 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5161920646924438		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.5161920646924438 | validation: 1.2405988995184278]
	TIME [epoch: 0.688 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4062528994986232		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.4062528994986232 | validation: 1.3311674300130019]
	TIME [epoch: 0.688 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3540098060317627		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.3540098060317627 | validation: 1.2013104903181266]
	TIME [epoch: 0.688 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3360728954654508		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.3360728954654508 | validation: 1.389834741471875]
	TIME [epoch: 0.689 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3307723374711629		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.3307723374711629 | validation: 1.0281270414456003]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.40064478290378		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.40064478290378 | validation: 1.769377854657111]
	TIME [epoch: 0.691 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6478414390703762		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.6478414390703762 | validation: 1.007871097862447]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3851086808439943		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.3851086808439943 | validation: 1.2326305424117434]
	TIME [epoch: 0.69 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2675481152424248		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.2675481152424248 | validation: 1.2109904891315268]
	TIME [epoch: 0.689 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2612777763207055		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.2612777763207055 | validation: 1.0520707197900483]
	TIME [epoch: 0.689 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.260162534203594		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.260162534203594 | validation: 1.3115756665644807]
	TIME [epoch: 0.688 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2784394510022707		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.2784394510022707 | validation: 1.0064931307146947]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.308402048683331		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.308402048683331 | validation: 1.4698557960628564]
	TIME [epoch: 0.691 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3571776366612844		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.3571776366612844 | validation: 0.9540042111484714]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3399408963111663		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.3399408963111663 | validation: 1.299613177477764]
	TIME [epoch: 0.691 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2528982326748377		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.2528982326748377 | validation: 1.060923147076101]
	TIME [epoch: 0.691 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2072812510116817		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.2072812510116817 | validation: 1.0972594237284778]
	TIME [epoch: 0.69 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1685037764332484		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.1685037764332484 | validation: 1.1767629709398173]
	TIME [epoch: 0.696 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1731797194454496		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.1731797194454496 | validation: 1.0961730294094172]
	TIME [epoch: 0.688 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2085799046901076		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.2085799046901076 | validation: 1.3308660080469465]
	TIME [epoch: 0.688 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2298527552224665		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.2298527552224665 | validation: 0.9142538548935729]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2292787732535146		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.2292787732535146 | validation: 1.5874662608191685]
	TIME [epoch: 0.69 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5372404695786615		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.5372404695786615 | validation: 1.2390410475678708]
	TIME [epoch: 0.692 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6825358286501342		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.6825358286501342 | validation: 1.0127509745164642]
	TIME [epoch: 0.688 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1413605757225735		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.1413605757225735 | validation: 1.2594795977291817]
	TIME [epoch: 0.689 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2202362593304865		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.2202362593304865 | validation: 0.9010543428260303]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1619623867869233		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.1619623867869233 | validation: 1.0078264797477037]
	TIME [epoch: 0.692 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.082474665131042		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.082474665131042 | validation: 1.0395867734233464]
	TIME [epoch: 0.689 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0800647217293868		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.0800647217293868 | validation: 0.9133638146611542]
	TIME [epoch: 0.691 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0763972874202554		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.0763972874202554 | validation: 1.0916389363029817]
	TIME [epoch: 0.689 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0723125962716749		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.0723125962716749 | validation: 0.8701121771817274]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.100805136880374		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.100805136880374 | validation: 1.3018112405264834]
	TIME [epoch: 0.69 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2411234285224213		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.2411234285224213 | validation: 0.9242827621628079]
	TIME [epoch: 0.69 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3848391859923934		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.3848391859923934 | validation: 1.0533429773520213]
	TIME [epoch: 0.692 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0606797265548522		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.0606797265548522 | validation: 0.9451651444843778]
	TIME [epoch: 0.688 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.993521885157968		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 0.993521885157968 | validation: 0.8677110164945305]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0084250089026556		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.0084250089026556 | validation: 1.0752246173559283]
	TIME [epoch: 0.692 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0446223348819634		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.0446223348819634 | validation: 0.8381180237968557]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1801228114198614		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.1801228114198614 | validation: 1.1945509794467786]
	TIME [epoch: 0.692 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.168347595272411		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.168347595272411 | validation: 0.8076306123738842]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1115435791074575		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.1115435791074575 | validation: 0.9596281935989032]
	TIME [epoch: 0.691 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9760769610234121		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.9760769610234121 | validation: 0.801006870272345]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9203541378314297		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.9203541378314297 | validation: 0.8499008150083296]
	TIME [epoch: 0.692 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8963647404135594		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.8963647404135594 | validation: 0.7921802767477247]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8565321906741845		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.8565321906741845 | validation: 0.7538496977842611]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8258199543792987		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.8258199543792987 | validation: 0.7406592767158913]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7893575748846571		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.7893575748846571 | validation: 0.64823325382862]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7540222088638455		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 0.7540222088638455 | validation: 0.814702618469763]
	TIME [epoch: 0.693 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8031963757414569		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.8031963757414569 | validation: 0.8832548942538017]
	TIME [epoch: 0.69 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9323168601974365		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.9323168601974365 | validation: 1.2337747219019164]
	TIME [epoch: 0.695 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8003129852896005		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.8003129852896005 | validation: 1.42466125960497]
	TIME [epoch: 0.689 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6805432893069294		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.6805432893069294 | validation: 0.5744665428705774]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7166265664529118		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.7166265664529118 | validation: 0.587266134826237]
	TIME [epoch: 0.691 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6773960696723347		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.6773960696723347 | validation: 0.6138134230220303]
	TIME [epoch: 0.69 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.648300190744151		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.648300190744151 | validation: 0.554296523884042]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6651336710830674		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.6651336710830674 | validation: 0.9983483267716358]
	TIME [epoch: 0.69 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.024292609845452		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.024292609845452 | validation: 0.8661817636050491]
	TIME [epoch: 0.69 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4692264328984055		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.4692264328984055 | validation: 0.6262838590505065]
	TIME [epoch: 0.688 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6528484995730347		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.6528484995730347 | validation: 0.5975855126677606]
	TIME [epoch: 0.688 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6291134445240705		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.6291134445240705 | validation: 0.5333357894798814]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7884006994467605		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.7884006994467605 | validation: 0.9909088931071537]
	TIME [epoch: 0.691 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0316218728365567		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.0316218728365567 | validation: 0.5140574453177582]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7396147773362162		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.7396147773362162 | validation: 0.6185879312957465]
	TIME [epoch: 0.691 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6268553506557679		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.6268553506557679 | validation: 0.4308349107423712]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6093336813440813		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.6093336813440813 | validation: 0.6587975030312325]
	TIME [epoch: 0.688 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6348030533204433		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.6348030533204433 | validation: 0.49707863385986795]
	TIME [epoch: 0.687 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.734524790213858		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.734524790213858 | validation: 0.7690073948717474]
	TIME [epoch: 0.688 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7865111239038616		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.7865111239038616 | validation: 0.46662961109935036]
	TIME [epoch: 0.691 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6839000868561148		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.6839000868561148 | validation: 0.6268167483778312]
	TIME [epoch: 0.689 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6179482366786296		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.6179482366786296 | validation: 0.4440712740490261]
	TIME [epoch: 0.688 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6063722977281713		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.6063722977281713 | validation: 0.621619851400749]
	TIME [epoch: 0.688 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.622521955951827		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.622521955951827 | validation: 0.4066946485049108]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6240042260525419		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.6240042260525419 | validation: 0.6546928910684134]
	TIME [epoch: 0.693 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6487653757833483		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.6487653757833483 | validation: 0.44934250196306424]
	TIME [epoch: 0.689 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6337881128612392		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.6337881128612392 | validation: 0.6611546018531601]
	TIME [epoch: 0.689 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6218420126053885		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.6218420126053885 | validation: 0.42169227418838306]
	TIME [epoch: 0.689 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6022230037603306		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.6022230037603306 | validation: 0.5797255054200549]
	TIME [epoch: 0.689 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5800398033763169		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.5800398033763169 | validation: 0.42503515962915484]
	TIME [epoch: 0.689 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5534164088210886		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.5534164088210886 | validation: 0.5983427235870832]
	TIME [epoch: 0.688 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5706240792718232		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.5706240792718232 | validation: 0.4156067158582505]
	TIME [epoch: 0.689 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5824084663840212		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.5824084663840212 | validation: 0.5912884289790367]
	TIME [epoch: 0.689 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5850120193337451		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.5850120193337451 | validation: 0.3875530796747807]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5727747381051649		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.5727747381051649 | validation: 0.6003807458433028]
	TIME [epoch: 0.69 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5957836961589839		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.5957836961589839 | validation: 0.3508234179457306]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6080027368111799		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.6080027368111799 | validation: 0.6914831214901059]
	TIME [epoch: 0.691 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6997244555141097		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.6997244555141097 | validation: 0.39772944212266714]
	TIME [epoch: 0.69 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5163693059975047		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.5163693059975047 | validation: 0.43037212754429577]
	TIME [epoch: 0.692 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45841139400659986		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.45841139400659986 | validation: 0.43327167857521565]
	TIME [epoch: 0.691 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4549599557930911		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.4549599557930911 | validation: 0.37528570766154945]
	TIME [epoch: 0.696 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46742508832337537		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.46742508832337537 | validation: 0.5458737629415706]
	TIME [epoch: 0.69 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5373299385534395		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.5373299385534395 | validation: 0.4908626669477716]
	TIME [epoch: 0.69 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7311287731949324		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.7311287731949324 | validation: 0.8276695118002299]
	TIME [epoch: 0.69 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7981753141444469		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.7981753141444469 | validation: 0.37852889106593773]
	TIME [epoch: 0.69 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5177261247039993		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.5177261247039993 | validation: 0.4075958303272613]
	TIME [epoch: 0.688 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4392746662970793		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.4392746662970793 | validation: 0.42307409808682517]
	TIME [epoch: 0.69 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4470168855377105		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.4470168855377105 | validation: 0.3315273411744971]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5129651857478499		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.5129651857478499 | validation: 0.6625407724754844]
	TIME [epoch: 0.691 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7010165553684473		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.7010165553684473 | validation: 0.317505234700733]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5919180940708714		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.5919180940708714 | validation: 0.5241272307904201]
	TIME [epoch: 0.691 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5209042216499501		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.5209042216499501 | validation: 0.40626514244834855]
	TIME [epoch: 0.69 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5336818836133859		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.5336818836133859 | validation: 0.5519646512819224]
	TIME [epoch: 0.69 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5553227874781982		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.5553227874781982 | validation: 0.3517306622416912]
	TIME [epoch: 0.689 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5163649964359583		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.5163649964359583 | validation: 0.480971859752825]
	TIME [epoch: 0.688 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4876845778248469		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.4876845778248469 | validation: 0.34123793452474854]
	TIME [epoch: 0.688 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47352910577927765		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.47352910577927765 | validation: 0.4926178974798281]
	TIME [epoch: 0.689 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4895314616693392		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.4895314616693392 | validation: 0.3402827393955978]
	TIME [epoch: 0.688 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5384811264014384		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.5384811264014384 | validation: 0.5410616188460481]
	TIME [epoch: 0.688 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5605796782020752		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.5605796782020752 | validation: 0.3342958824209916]
	TIME [epoch: 0.688 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5021387629168279		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.5021387629168279 | validation: 0.463150659301804]
	TIME [epoch: 0.693 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45980003488016763		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.45980003488016763 | validation: 0.33393716146481156]
	TIME [epoch: 0.689 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45492771877022575		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.45492771877022575 | validation: 0.48904769886431476]
	TIME [epoch: 0.688 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4869400906581193		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.4869400906581193 | validation: 0.31591353341686185]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5253584717016142		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.5253584717016142 | validation: 0.47572834759606963]
	TIME [epoch: 0.692 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4920472469819576		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.4920472469819576 | validation: 0.29834612276822164]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46198607380068285		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.46198607380068285 | validation: 0.40908892077291714]
	TIME [epoch: 0.69 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41315216016011347		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.41315216016011347 | validation: 0.2882529919648917]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4131798323695379		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.4131798323695379 | validation: 0.49254737294801276]
	TIME [epoch: 0.691 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49099272770278846		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.49099272770278846 | validation: 0.35264456418052825]
	TIME [epoch: 0.689 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.685594110468869		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.685594110468869 | validation: 0.6038230333918414]
	TIME [epoch: 0.689 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6616152828210321		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.6616152828210321 | validation: 0.32995774342238005]
	TIME [epoch: 0.689 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40690298397740715		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.40690298397740715 | validation: 0.37067925028069837]
	TIME [epoch: 0.689 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39158105482407296		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.39158105482407296 | validation: 0.3686190162559181]
	TIME [epoch: 0.688 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4116660738440998		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.4116660738440998 | validation: 0.365813688307781]
	TIME [epoch: 0.689 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39335384006498336		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.39335384006498336 | validation: 0.29211478075297503]
	TIME [epoch: 0.689 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40783754296801067		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.40783754296801067 | validation: 0.4675295072920058]
	TIME [epoch: 0.689 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43322653472840983		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.43322653472840983 | validation: 0.2672534199422369]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_184.pth
	Model improved!!!
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.403916110372783		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.403916110372783 | validation: 0.4292341848128297]
	TIME [epoch: 0.695 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39463671786208443		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.39463671786208443 | validation: 0.2791084533441052]
	TIME [epoch: 0.686 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46104390927173994		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.46104390927173994 | validation: 0.616763852435541]
	TIME [epoch: 0.684 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6447469002778388		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.6447469002778388 | validation: 0.35757207329949087]
	TIME [epoch: 0.685 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.520990677875455		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.520990677875455 | validation: 0.39409072173637344]
	TIME [epoch: 0.686 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3780860317609924		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.3780860317609924 | validation: 0.2782114685978244]
	TIME [epoch: 0.686 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3174063559885445		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.3174063559885445 | validation: 0.33293273772768783]
	TIME [epoch: 0.687 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2982035753805585		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.2982035753805585 | validation: 0.24835297099354278]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_192.pth
	Model improved!!!
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3205607134694198		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.3205607134694198 | validation: 0.42894361587044316]
	TIME [epoch: 0.694 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3998210131570833		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.3998210131570833 | validation: 0.2284396135399728]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47606173778907634		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.47606173778907634 | validation: 0.37905822598191524]
	TIME [epoch: 0.693 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33935258044395383		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.33935258044395383 | validation: 0.2455421929256518]
	TIME [epoch: 0.693 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30757534895829663		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.30757534895829663 | validation: 0.38051508378300647]
	TIME [epoch: 0.693 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35861167402012856		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.35861167402012856 | validation: 0.33618027448870347]
	TIME [epoch: 0.692 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5564738963405657		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.5564738963405657 | validation: 0.5685189619482885]
	TIME [epoch: 0.692 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5832524457681852		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.5832524457681852 | validation: 0.21424403828026667]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32796176812636674		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.32796176812636674 | validation: 0.4070470084890372]
	TIME [epoch: 171 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3637296405695936		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.3637296405695936 | validation: 0.25914800645556496]
	TIME [epoch: 1.36 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35197741725182496		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.35197741725182496 | validation: 0.3174811697152956]
	TIME [epoch: 1.35 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2791156802600969		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.2791156802600969 | validation: 0.25690298953652324]
	TIME [epoch: 1.35 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24516528008079555		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.24516528008079555 | validation: 0.27791447350192006]
	TIME [epoch: 1.35 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23711585975903632		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.23711585975903632 | validation: 0.23458539342694446]
	TIME [epoch: 1.35 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2390999621195278		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.2390999621195278 | validation: 0.3560979269515009]
	TIME [epoch: 1.35 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2924173212872169		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.2924173212872169 | validation: 0.17726901601361314]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3801243790365512		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.3801243790365512 | validation: 0.47134564878419893]
	TIME [epoch: 1.35 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4692770339041961		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.4692770339041961 | validation: 0.271632458853126]
	TIME [epoch: 1.35 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39053250969738457		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.39053250969738457 | validation: 0.40088925295421274]
	TIME [epoch: 1.35 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4426761991651611		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.4426761991651611 | validation: 0.2933987906004324]
	TIME [epoch: 1.35 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3955029255108389		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.3955029255108389 | validation: 0.35103924727987923]
	TIME [epoch: 1.35 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30053177168784595		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.30053177168784595 | validation: 0.1782745780352257]
	TIME [epoch: 1.35 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3043796726875754		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.3043796726875754 | validation: 0.44504763814832415]
	TIME [epoch: 1.35 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40724752938477876		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.40724752938477876 | validation: 0.27746582561900096]
	TIME [epoch: 1.35 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2546267299158639		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.2546267299158639 | validation: 0.20416004740578347]
	TIME [epoch: 1.35 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2649354622362583		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.2649354622362583 | validation: 0.43666076014379956]
	TIME [epoch: 1.35 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4204792020185864		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.4204792020185864 | validation: 0.3415592093728546]
	TIME [epoch: 1.35 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3120003455040847		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.3120003455040847 | validation: 0.23223267605226494]
	TIME [epoch: 1.35 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2783782487171289		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.2783782487171289 | validation: 0.26515288808265985]
	TIME [epoch: 1.36 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22726101591833833		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.22726101591833833 | validation: 0.2408364566366703]
	TIME [epoch: 1.35 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22484027707909987		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.22484027707909987 | validation: 0.2772701413264764]
	TIME [epoch: 1.35 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24247167054934188		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.24247167054934188 | validation: 0.24287824389608992]
	TIME [epoch: 1.35 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4678078952341103		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.4678078952341103 | validation: 0.5714391115993166]
	TIME [epoch: 1.35 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5842537991065566		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.5842537991065566 | validation: 0.2319412601992084]
	TIME [epoch: 1.35 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26561171832532865		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.26561171832532865 | validation: 0.21313334062025416]
	TIME [epoch: 1.35 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19077886107428252		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.19077886107428252 | validation: 0.22515588450459864]
	TIME [epoch: 1.35 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2191156290483793		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.2191156290483793 | validation: 0.23987086296394766]
	TIME [epoch: 1.35 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2732631029510619		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.2732631029510619 | validation: 0.3467246486195821]
	TIME [epoch: 1.35 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30377036777725663		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.30377036777725663 | validation: 0.18356491266647462]
	TIME [epoch: 1.35 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3023700903723702		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.3023700903723702 | validation: 0.3158627600619451]
	TIME [epoch: 1.35 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2522223807966137		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.2522223807966137 | validation: 0.16112783626143512]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_233.pth
	Model improved!!!
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21636288701042466		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.21636288701042466 | validation: 0.2740636840777215]
	TIME [epoch: 1.35 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21943706734819166		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.21943706734819166 | validation: 0.17444022917914595]
	TIME [epoch: 1.35 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2372653406684695		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.2372653406684695 | validation: 0.30159903297717205]
	TIME [epoch: 1.35 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2592700506117053		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.2592700506117053 | validation: 0.16492774543477046]
	TIME [epoch: 1.35 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23342483110574908		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.23342483110574908 | validation: 0.2534294432311071]
	TIME [epoch: 1.35 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22165143943057794		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.22165143943057794 | validation: 0.2108239259999973]
	TIME [epoch: 1.35 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1966351429277745		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.1966351429277745 | validation: 0.22328267067509186]
	TIME [epoch: 1.35 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2499957539162228		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.2499957539162228 | validation: 0.35904914757615325]
	TIME [epoch: 1.35 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33980787703666954		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.33980787703666954 | validation: 0.2502487231128371]
	TIME [epoch: 1.36 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4505163437052261		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.4505163437052261 | validation: 0.3138372124380605]
	TIME [epoch: 1.35 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2768026852473573		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.2768026852473573 | validation: 0.1443518497001206]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18995583258292903		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.18995583258292903 | validation: 0.20787679853369934]
	TIME [epoch: 1.35 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16648392946420928		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.16648392946420928 | validation: 0.29423119796268987]
	TIME [epoch: 1.35 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2477248783122651		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.2477248783122651 | validation: 0.16728791341413551]
	TIME [epoch: 1.35 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25044626983934554		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.25044626983934554 | validation: 0.3245142747961669]
	TIME [epoch: 1.35 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2844154904387801		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.2844154904387801 | validation: 0.17519402398230305]
	TIME [epoch: 1.35 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1680314994337244		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.1680314994337244 | validation: 0.1873690127732283]
	TIME [epoch: 1.35 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1446333535598166		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.1446333535598166 | validation: 0.2011180342157971]
	TIME [epoch: 1.35 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1770428583030118		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.1770428583030118 | validation: 0.2354202205980978]
	TIME [epoch: 1.35 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2386032625029559		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.2386032625029559 | validation: 0.25795390473869667]
	TIME [epoch: 1.35 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29260949502771133		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.29260949502771133 | validation: 0.3792749601153689]
	TIME [epoch: 1.35 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3442442696496274		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.3442442696496274 | validation: 0.17809924104933458]
	TIME [epoch: 1.35 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.306942117524443		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.306942117524443 | validation: 0.2602339342026107]
	TIME [epoch: 1.35 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20265530398590934		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.20265530398590934 | validation: 0.13862318120847103]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_257.pth
	Model improved!!!
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15155328661608178		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.15155328661608178 | validation: 0.16333961959064525]
	TIME [epoch: 1.35 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13385189685319648		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.13385189685319648 | validation: 0.16929122366096974]
	TIME [epoch: 1.35 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12351596391664035		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.12351596391664035 | validation: 0.1466123007807257]
	TIME [epoch: 1.35 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14162173512513332		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.14162173512513332 | validation: 0.30012873696975284]
	TIME [epoch: 1.35 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27539556015816696		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.27539556015816696 | validation: 0.17776281803293803]
	TIME [epoch: 1.35 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21524060459923952		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.21524060459923952 | validation: 0.2782655959910071]
	TIME [epoch: 1.36 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23568741604248164		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.23568741604248164 | validation: 0.1419216485036369]
	TIME [epoch: 1.35 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22171519949107876		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.22171519949107876 | validation: 0.36698787814527273]
	TIME [epoch: 1.35 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3410434392828611		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.3410434392828611 | validation: 0.28459906883324565]
	TIME [epoch: 1.35 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3579039008335342		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.3579039008335342 | validation: 0.23799954636229234]
	TIME [epoch: 1.35 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1892589489345862		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.1892589489345862 | validation: 0.1426596600381159]
	TIME [epoch: 1.35 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13428705071308575		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.13428705071308575 | validation: 0.20287313139513474]
	TIME [epoch: 1.35 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1490225993844013		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.1490225993844013 | validation: 0.11327432609026365]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21052810353168253		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.21052810353168253 | validation: 0.28070022567647146]
	TIME [epoch: 1.35 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2264076497886501		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.2264076497886501 | validation: 0.1398056250970948]
	TIME [epoch: 1.35 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11946458353229712		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.11946458353229712 | validation: 0.12876692544282295]
	TIME [epoch: 1.35 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14232136119806624		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.14232136119806624 | validation: 0.1957551684792858]
	TIME [epoch: 1.35 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15143460413200732		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.15143460413200732 | validation: 0.15100762899665315]
	TIME [epoch: 1.35 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14074125770492615		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.14074125770492615 | validation: 0.1954720287741367]
	TIME [epoch: 1.35 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13969277298413005		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.13969277298413005 | validation: 0.11736506248171122]
	TIME [epoch: 1.35 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15834510367315968		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.15834510367315968 | validation: 0.3285857505771014]
	TIME [epoch: 1.35 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.260440812845171		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.260440812845171 | validation: 0.2149227191202582]
	TIME [epoch: 1.35 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26999550238278186		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.26999550238278186 | validation: 0.42845384396242503]
	TIME [epoch: 1.35 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4412150432545815		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.4412150432545815 | validation: 0.17087866755229367]
	TIME [epoch: 1.35 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22306030047717687		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.22306030047717687 | validation: 0.21288387085078525]
	TIME [epoch: 1.35 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14997939608423258		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.14997939608423258 | validation: 0.1258869745320703]
	TIME [epoch: 1.35 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12651867916012474		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.12651867916012474 | validation: 0.15120455402924535]
	TIME [epoch: 1.35 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11562575503047938		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.11562575503047938 | validation: 0.11936178686478138]
	TIME [epoch: 1.36 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11508418102088816		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.11508418102088816 | validation: 0.16657921494472094]
	TIME [epoch: 1.35 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11981826779745611		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.11981826779745611 | validation: 0.1163728484836434]
	TIME [epoch: 1.35 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13602012113209788		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.13602012113209788 | validation: 0.2600278091802645]
	TIME [epoch: 1.35 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20241893256473126		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.20241893256473126 | validation: 0.12904730151418273]
	TIME [epoch: 1.35 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17611097415543675		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.17611097415543675 | validation: 0.30608842209104026]
	TIME [epoch: 1.35 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24871298827007998		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.24871298827007998 | validation: 0.2605740371072452]
	TIME [epoch: 1.35 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3090881652191754		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.3090881652191754 | validation: 0.19942444054917435]
	TIME [epoch: 1.35 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.175858079724797		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.175858079724797 | validation: 0.11957828528575574]
	TIME [epoch: 1.35 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12736394089720965		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.12736394089720965 | validation: 0.22892055186848995]
	TIME [epoch: 1.35 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17628839012006253		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.17628839012006253 | validation: 0.1086558972747225]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14164009206975056		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.14164009206975056 | validation: 0.15545086126001492]
	TIME [epoch: 1.35 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11251173492875782		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.11251173492875782 | validation: 0.11522920845047456]
	TIME [epoch: 1.35 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10665698689082462		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.10665698689082462 | validation: 0.16440722561311097]
	TIME [epoch: 1.35 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11604612805753876		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.11604612805753876 | validation: 0.12244864703350139]
	TIME [epoch: 1.35 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12087020251308009		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.12087020251308009 | validation: 0.2598252060174879]
	TIME [epoch: 1.35 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20994971097163792		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.20994971097163792 | validation: 0.21447325657707347]
	TIME [epoch: 1.35 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3068127033420249		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.3068127033420249 | validation: 0.28406716397664494]
	TIME [epoch: 1.35 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24364265345944483		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.24364265345944483 | validation: 0.11822100615617344]
	TIME [epoch: 1.35 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11910921534634226		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.11910921534634226 | validation: 0.13609457901210426]
	TIME [epoch: 1.35 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09568607357690503		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.09568607357690503 | validation: 0.14012592750365235]
	TIME [epoch: 1.35 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10066095826583854		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.10066095826583854 | validation: 0.12163997338667132]
	TIME [epoch: 1.36 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11246907860188621		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.11246907860188621 | validation: 0.19646814076646696]
	TIME [epoch: 1.35 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15729128843878773		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.15729128843878773 | validation: 0.1645772259460654]
	TIME [epoch: 1.35 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22289094239776874		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.22289094239776874 | validation: 0.3222955977664624]
	TIME [epoch: 1.35 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2465410792826116		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.2465410792826116 | validation: 0.11862466079223682]
	TIME [epoch: 1.35 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11539079616027045		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.11539079616027045 | validation: 0.10693136311680351]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11650586405325379		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.11650586405325379 | validation: 0.1840133312620703]
	TIME [epoch: 1.35 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1376032494997149		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.1376032494997149 | validation: 0.12164871576854977]
	TIME [epoch: 1.35 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17473199583290558		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.17473199583290558 | validation: 0.2480817548978231]
	TIME [epoch: 1.35 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2681585825885265		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.2681585825885265 | validation: 0.11414946514370808]
	TIME [epoch: 1.35 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10197587405524285		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.10197587405524285 | validation: 0.10452788099514412]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_316.pth
	Model improved!!!
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14545511034635603		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.14545511034635603 | validation: 0.2659572596753554]
	TIME [epoch: 1.35 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20311148141907182		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.20311148141907182 | validation: 0.16576112663661347]
	TIME [epoch: 1.35 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1579836446633913		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.1579836446633913 | validation: 0.19115509742564618]
	TIME [epoch: 1.35 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20627379545258478		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.20627379545258478 | validation: 0.14777003005007755]
	TIME [epoch: 1.35 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15132452640298763		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.15132452640298763 | validation: 0.22362300560127046]
	TIME [epoch: 1.35 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16490828637103813		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.16490828637103813 | validation: 0.1146539603054459]
	TIME [epoch: 1.35 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11054072692423349		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.11054072692423349 | validation: 0.14803561944636967]
	TIME [epoch: 1.35 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10990310868050512		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.10990310868050512 | validation: 0.11846745827202745]
	TIME [epoch: 1.35 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09902192380428035		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.09902192380428035 | validation: 0.12717393440999478]
	TIME [epoch: 1.35 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10567122055493787		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.10567122055493787 | validation: 0.12485292942699287]
	TIME [epoch: 1.35 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12084688451254756		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.12084688451254756 | validation: 0.17459093470884934]
	TIME [epoch: 1.35 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1733686108999857		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.1733686108999857 | validation: 0.16160549573392363]
	TIME [epoch: 1.35 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19788250470272215		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.19788250470272215 | validation: 0.24550188183453203]
	TIME [epoch: 1.35 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19292019130995225		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.19292019130995225 | validation: 0.10972884531575129]
	TIME [epoch: 1.35 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10306106343765008		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.10306106343765008 | validation: 0.12269006241361131]
	TIME [epoch: 1.35 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08171541469288193		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.08171541469288193 | validation: 0.10336428109988316]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_332.pth
	Model improved!!!
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09181578428351969		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.09181578428351969 | validation: 0.20505409182100115]
	TIME [epoch: 1.35 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1712370809047814		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.1712370809047814 | validation: 0.11062565013743969]
	TIME [epoch: 1.35 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15563774852095844		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.15563774852095844 | validation: 0.16161499688228137]
	TIME [epoch: 1.35 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1315475415748729		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.1315475415748729 | validation: 0.11350843783166038]
	TIME [epoch: 1.35 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08708665129905194		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.08708665129905194 | validation: 0.09842944773235163]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_337.pth
	Model improved!!!
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11024516635303179		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.11024516635303179 | validation: 0.25044330795117187]
	TIME [epoch: 1.34 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20221060164582558		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.20221060164582558 | validation: 0.15256445055884635]
	TIME [epoch: 1.34 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17622943059832913		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.17622943059832913 | validation: 0.20267545203848256]
	TIME [epoch: 1.34 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20288990748918406		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.20288990748918406 | validation: 0.13750316809308397]
	TIME [epoch: 1.34 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1534846402988086		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.1534846402988086 | validation: 0.14346819390994206]
	TIME [epoch: 1.34 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11324110708940667		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.11324110708940667 | validation: 0.09143737633627959]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09781929260968522		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.09781929260968522 | validation: 0.11293853140943]
	TIME [epoch: 1.34 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08788238997980453		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.08788238997980453 | validation: 0.10398431043057883]
	TIME [epoch: 1.34 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08071958402739064		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.08071958402739064 | validation: 0.10431201970338487]
	TIME [epoch: 1.34 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07651963853438516		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.07651963853438516 | validation: 0.10917275090756325]
	TIME [epoch: 1.34 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08550164999747864		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.08550164999747864 | validation: 0.13429066166070136]
	TIME [epoch: 1.35 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11140054699545139		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.11140054699545139 | validation: 0.1382385435206516]
	TIME [epoch: 1.34 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15742133558426832		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.15742133558426832 | validation: 0.16707718180815145]
	TIME [epoch: 1.34 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16809254032210713		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.16809254032210713 | validation: 0.16744942446684963]
	TIME [epoch: 1.34 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15202960772704183		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.15202960772704183 | validation: 0.11601192206130942]
	TIME [epoch: 1.34 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17387071523219835		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.17387071523219835 | validation: 0.3246878678828543]
	TIME [epoch: 1.34 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2563823440559913		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.2563823440559913 | validation: 0.0965234555899194]
	TIME [epoch: 1.34 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10385592016491614		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.10385592016491614 | validation: 0.12352506403090012]
	TIME [epoch: 1.34 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16933820476009664		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.16933820476009664 | validation: 0.27089295214096343]
	TIME [epoch: 1.35 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2120012802320547		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.2120012802320547 | validation: 0.133909333183277]
	TIME [epoch: 1.34 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10286822461577544		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.10286822461577544 | validation: 0.10354902833251245]
	TIME [epoch: 1.35 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15874882732555104		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.15874882732555104 | validation: 0.18338797025453873]
	TIME [epoch: 1.34 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13647572221548313		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.13647572221548313 | validation: 0.1200566138090887]
	TIME [epoch: 1.34 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09319904238054179		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.09319904238054179 | validation: 0.10404043657823414]
	TIME [epoch: 1.34 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10731919115109398		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.10731919115109398 | validation: 0.13926365433656654]
	TIME [epoch: 1.34 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1094766975776751		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.1094766975776751 | validation: 0.09014338352040215]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_363.pth
	Model improved!!!
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08189902332318064		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.08189902332318064 | validation: 0.09692556682255399]
	TIME [epoch: 1.34 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07437340333433637		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.07437340333433637 | validation: 0.09522023529834185]
	TIME [epoch: 1.34 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07916516910392184		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.07916516910392184 | validation: 0.10964029558403086]
	TIME [epoch: 1.34 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09593321002859981		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.09593321002859981 | validation: 0.13732122581102946]
	TIME [epoch: 1.34 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15154147994252085		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.15154147994252085 | validation: 0.26496268374649773]
	TIME [epoch: 1.34 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23473717529820345		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.23473717529820345 | validation: 0.14760901927887327]
	TIME [epoch: 1.34 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1526538629174857		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.1526538629174857 | validation: 0.10144529389720014]
	TIME [epoch: 1.35 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09767569624048976		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.09767569624048976 | validation: 0.11823471704392474]
	TIME [epoch: 1.34 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09144797314966688		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.09144797314966688 | validation: 0.21924480011864944]
	TIME [epoch: 1.34 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26759727264424166		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.26759727264424166 | validation: 0.14103142867394067]
	TIME [epoch: 1.34 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15095329376796313		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.15095329376796313 | validation: 0.21809124474692726]
	TIME [epoch: 1.34 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18943510054713728		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.18943510054713728 | validation: 0.14532542435396942]
	TIME [epoch: 1.34 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12168809970461432		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.12168809970461432 | validation: 0.11600739997080999]
	TIME [epoch: 1.34 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08497778295111257		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.08497778295111257 | validation: 0.08550052987488274]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_377.pth
	Model improved!!!
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07895024020227054		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.07895024020227054 | validation: 0.1191338694427757]
	TIME [epoch: 1.35 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08790324211504412		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.08790324211504412 | validation: 0.08788848317968719]
	TIME [epoch: 1.35 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0841722652380859		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.0841722652380859 | validation: 0.11172519374125417]
	TIME [epoch: 1.35 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07333924542826685		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.07333924542826685 | validation: 0.08795019681997868]
	TIME [epoch: 1.35 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07934050132883509		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.07934050132883509 | validation: 0.15953281848651435]
	TIME [epoch: 1.35 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12283962173306222		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.12283962173306222 | validation: 0.10288886056955114]
	TIME [epoch: 1.34 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10660350789672475		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.10660350789672475 | validation: 0.14118168149450372]
	TIME [epoch: 1.34 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1378303162467303		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.1378303162467303 | validation: 0.1384530874707759]
	TIME [epoch: 1.34 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1334730923877153		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.1334730923877153 | validation: 0.1259586712089695]
	TIME [epoch: 1.34 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09589262245233938		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.09589262245233938 | validation: 0.09765388002822978]
	TIME [epoch: 1.34 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07946771847821102		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.07946771847821102 | validation: 0.08911452115708257]
	TIME [epoch: 1.34 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07492435696764556		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.07492435696764556 | validation: 0.07609134235841487]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_389.pth
	Model improved!!!
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09811939946545409		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.09811939946545409 | validation: 0.18520142060939304]
	TIME [epoch: 1.35 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1717129512959562		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.1717129512959562 | validation: 0.08919863767398975]
	TIME [epoch: 1.36 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08900920094839697		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.08900920094839697 | validation: 0.10630006181615662]
	TIME [epoch: 1.35 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08946572670749227		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.08946572670749227 | validation: 0.11144595598372926]
	TIME [epoch: 1.35 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09329054061523152		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.09329054061523152 | validation: 0.10618903521206056]
	TIME [epoch: 1.35 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11665136376757441		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.11665136376757441 | validation: 0.2044249536456471]
	TIME [epoch: 1.35 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17093289458890823		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.17093289458890823 | validation: 0.09611906390052766]
	TIME [epoch: 1.35 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08672230093532825		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.08672230093532825 | validation: 0.08897231754132978]
	TIME [epoch: 1.35 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10225533592923937		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.10225533592923937 | validation: 0.1450793400057325]
	TIME [epoch: 1.35 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10810875236662806		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.10810875236662806 | validation: 0.09336683928875217]
	TIME [epoch: 1.35 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07599976282363417		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.07599976282363417 | validation: 0.10602162726719522]
	TIME [epoch: 1.35 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08095723891619112		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.08095723891619112 | validation: 0.09995999489713506]
	TIME [epoch: 1.34 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08929219304686707		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.08929219304686707 | validation: 0.1283683229154664]
	TIME [epoch: 1.35 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10631219574766322		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.10631219574766322 | validation: 0.10491797079744024]
	TIME [epoch: 1.34 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09661852996858192		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.09661852996858192 | validation: 0.11095072519720764]
	TIME [epoch: 1.34 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09442126593192147		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.09442126593192147 | validation: 0.1042159517352272]
	TIME [epoch: 1.34 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08126956158657529		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.08126956158657529 | validation: 0.10313211284910138]
	TIME [epoch: 1.34 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17153360980908303		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.17153360980908303 | validation: 0.11168329895562397]
	TIME [epoch: 1.35 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12676000856735864		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.12676000856735864 | validation: 0.1657913607233411]
	TIME [epoch: 1.34 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12134227941482989		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.12134227941482989 | validation: 0.09915021205482923]
	TIME [epoch: 1.34 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07779256932558117		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.07779256932558117 | validation: 0.09600416093779202]
	TIME [epoch: 1.34 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07417570300212394		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.07417570300212394 | validation: 0.10723148761974488]
	TIME [epoch: 1.34 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07513716730828943		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.07513716730828943 | validation: 0.09584954100646581]
	TIME [epoch: 1.34 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07732608704909716		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.07732608704909716 | validation: 0.12903564968813794]
	TIME [epoch: 1.35 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09738449655144608		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.09738449655144608 | validation: 0.10179441461870019]
	TIME [epoch: 1.34 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10539456869760981		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.10539456869760981 | validation: 0.15427463982685685]
	TIME [epoch: 1.34 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12146589464853953		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.12146589464853953 | validation: 0.09197208746923921]
	TIME [epoch: 1.34 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08193429199522662		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.08193429199522662 | validation: 0.08426101158408394]
	TIME [epoch: 1.34 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0725390759278881		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.0725390759278881 | validation: 0.10247863826508666]
	TIME [epoch: 1.34 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07351315386053268		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.07351315386053268 | validation: 0.12070634911195662]
	TIME [epoch: 1.34 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08829129607686607		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.08829129607686607 | validation: 0.09924106799612052]
	TIME [epoch: 1.34 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15235513073495432		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.15235513073495432 | validation: 0.211544670907124]
	TIME [epoch: 1.35 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18195937768333817		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.18195937768333817 | validation: 0.09896273805846394]
	TIME [epoch: 1.34 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0813632004902347		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.0813632004902347 | validation: 0.0845006312242833]
	TIME [epoch: 1.35 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1187822282883071		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.1187822282883071 | validation: 0.12985298281157656]
	TIME [epoch: 1.34 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11903729746720115		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.11903729746720115 | validation: 0.08069941050695673]
	TIME [epoch: 1.34 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06996311019273922		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.06996311019273922 | validation: 0.09585792501156287]
	TIME [epoch: 1.34 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09337599108011603		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.09337599108011603 | validation: 0.13995897487564185]
	TIME [epoch: 1.34 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10724381756650807		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.10724381756650807 | validation: 0.1178140244906219]
	TIME [epoch: 1.34 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10557211785995567		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.10557211785995567 | validation: 0.0902518002010287]
	TIME [epoch: 1.34 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10333365060991931		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.10333365060991931 | validation: 0.08272650399815124]
	TIME [epoch: 1.34 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05976074065471007		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.05976074065471007 | validation: 0.08771948763713798]
	TIME [epoch: 1.34 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059983955385037147		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.059983955385037147 | validation: 0.08811332537786767]
	TIME [epoch: 1.34 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060202479479816114		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.060202479479816114 | validation: 0.09998113224411936]
	TIME [epoch: 1.34 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0676503751837683		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.0676503751837683 | validation: 0.09232016791203167]
	TIME [epoch: 1.34 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10027059490429213		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.10027059490429213 | validation: 0.19807101956964993]
	TIME [epoch: 1.35 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14946105818876962		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.14946105818876962 | validation: 0.10951145994263133]
	TIME [epoch: 1.34 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09402908194903714		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.09402908194903714 | validation: 0.07442644442995792]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_437.pth
	Model improved!!!
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09789831725814124		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.09789831725814124 | validation: 0.09565223168239809]
	TIME [epoch: 1.35 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06732510648544499		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.06732510648544499 | validation: 0.08315388533017115]
	TIME [epoch: 1.35 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05676298511523174		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.05676298511523174 | validation: 0.07614816739091265]
	TIME [epoch: 1.34 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06130277857243623		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.06130277857243623 | validation: 0.08972873946414106]
	TIME [epoch: 1.35 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06015484731275612		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.06015484731275612 | validation: 0.08185608537145803]
	TIME [epoch: 1.35 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06152598197203612		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.06152598197203612 | validation: 0.08844457504745418]
	TIME [epoch: 1.35 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05695632690253946		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.05695632690253946 | validation: 0.0866775098053143]
	TIME [epoch: 1.35 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0780843347912028		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.0780843347912028 | validation: 0.12528586800812133]
	TIME [epoch: 1.35 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12221104184651309		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.12221104184651309 | validation: 0.22148936288327611]
	TIME [epoch: 1.35 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18174702554436095		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.18174702554436095 | validation: 0.0828129003356697]
	TIME [epoch: 1.35 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06057739545059953		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.06057739545059953 | validation: 0.07153750272890928]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_448.pth
	Model improved!!!
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08725608146336562		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.08725608146336562 | validation: 0.13271453752839757]
	TIME [epoch: 1.34 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1036717526733046		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.1036717526733046 | validation: 0.08308890301115354]
	TIME [epoch: 1.34 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06412369279191428		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.06412369279191428 | validation: 0.07657100702321946]
	TIME [epoch: 1.35 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07677411497151863		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.07677411497151863 | validation: 0.11998152450162856]
	TIME [epoch: 1.35 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0937121181655391		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.0937121181655391 | validation: 0.08127094307711773]
	TIME [epoch: 1.35 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0694384636944012		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.0694384636944012 | validation: 0.07826731179098957]
	TIME [epoch: 1.35 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0558054217159307		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.0558054217159307 | validation: 0.08022269869769359]
	TIME [epoch: 1.35 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05335417131935454		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.05335417131935454 | validation: 0.08135336643766611]
	TIME [epoch: 1.35 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06439492871422		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.06439492871422 | validation: 0.12818795205519634]
	TIME [epoch: 1.35 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10310016143150903		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.10310016143150903 | validation: 0.11182256272790383]
	TIME [epoch: 1.35 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10313481792579082		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.10313481792579082 | validation: 0.12581893627172974]
	TIME [epoch: 1.35 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10481174306544375		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.10481174306544375 | validation: 0.09172473696162163]
	TIME [epoch: 1.35 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0647175680209045		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.0647175680209045 | validation: 0.06877981693313005]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_461.pth
	Model improved!!!
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05650243810173326		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.05650243810173326 | validation: 0.08398777325334172]
	TIME [epoch: 1.35 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051723532124913874		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.051723532124913874 | validation: 0.07022821640103798]
	TIME [epoch: 1.35 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05466300446713522		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.05466300446713522 | validation: 0.1235094945886534]
	TIME [epoch: 1.35 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11118967397303152		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.11118967397303152 | validation: 0.08595159475468794]
	TIME [epoch: 1.35 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10506827682363662		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.10506827682363662 | validation: 0.08971623724609283]
	TIME [epoch: 1.35 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07670134713500718		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.07670134713500718 | validation: 0.09565439499805926]
	TIME [epoch: 1.35 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07436551028627483		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.07436551028627483 | validation: 0.09649817929467394]
	TIME [epoch: 1.34 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07943846665381389		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.07943846665381389 | validation: 0.10755008238597093]
	TIME [epoch: 1.35 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0918768465877483		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.0918768465877483 | validation: 0.0975560917244451]
	TIME [epoch: 1.34 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07142405462891964		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.07142405462891964 | validation: 0.08845455603069535]
	TIME [epoch: 1.34 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06397149444681859		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.06397149444681859 | validation: 0.06513397876122218]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_472.pth
	Model improved!!!
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05068364806125761		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.05068364806125761 | validation: 0.07649676201793015]
	TIME [epoch: 1.35 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04948923759784764		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.04948923759784764 | validation: 0.06893183401001782]
	TIME [epoch: 1.35 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046716794996841005		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.046716794996841005 | validation: 0.07402643101255348]
	TIME [epoch: 1.35 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050193196956318485		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.050193196956318485 | validation: 0.06303527136167068]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_476.pth
	Model improved!!!
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05852791430747921		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.05852791430747921 | validation: 0.07572472825380704]
	TIME [epoch: 1.35 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05321343596981088		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.05321343596981088 | validation: 0.08184794709305888]
	TIME [epoch: 1.34 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05827953237427936		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.05827953237427936 | validation: 0.0869594067187873]
	TIME [epoch: 1.35 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08842580192400253		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.08842580192400253 | validation: 0.20211335647628292]
	TIME [epoch: 1.34 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1584279432788016		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.1584279432788016 | validation: 0.059749957816841164]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_481.pth
	Model improved!!!
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05710883366673307		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.05710883366673307 | validation: 0.07034516922418328]
	TIME [epoch: 1.35 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08148708107932766		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.08148708107932766 | validation: 0.17703444022332235]
	TIME [epoch: 1.35 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16956237829849868		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.16956237829849868 | validation: 0.08146046931893551]
	TIME [epoch: 1.35 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06435356376268804		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.06435356376268804 | validation: 0.09409652425146822]
	TIME [epoch: 1.35 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1108784953789158		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.1108784953789158 | validation: 0.11313138152293077]
	TIME [epoch: 1.35 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08253906389666686		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.08253906389666686 | validation: 0.07113978236473825]
	TIME [epoch: 1.35 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054075565392418506		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.054075565392418506 | validation: 0.08166115365699919]
	TIME [epoch: 1.35 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07580420838499889		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.07580420838499889 | validation: 0.08015125242082162]
	TIME [epoch: 1.35 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060327924537891826		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.060327924537891826 | validation: 0.07514552015420078]
	TIME [epoch: 1.35 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05104953034781622		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.05104953034781622 | validation: 0.05968242148473449]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_491.pth
	Model improved!!!
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05066135444577491		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.05066135444577491 | validation: 0.06788382283899315]
	TIME [epoch: 1.35 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048880943818637446		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.048880943818637446 | validation: 0.06417641164734543]
	TIME [epoch: 1.35 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05269273751796937		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.05269273751796937 | validation: 0.10187714436145479]
	TIME [epoch: 1.35 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07574684071275867		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.07574684071275867 | validation: 0.0816381660050063]
	TIME [epoch: 1.35 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0842287208046773		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.0842287208046773 | validation: 0.09563600093756525]
	TIME [epoch: 1.35 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08044100089826298		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.08044100089826298 | validation: 0.07226610511963164]
	TIME [epoch: 1.35 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049050286259283454		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.049050286259283454 | validation: 0.06614102896415353]
	TIME [epoch: 1.35 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048134124814209776		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.048134124814209776 | validation: 0.07614421497699968]
	TIME [epoch: 1.35 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0456194804082816		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.0456194804082816 | validation: 0.05895744792762744]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_500.pth
	Model improved!!!
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04770380316058244		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.04770380316058244 | validation: 0.0753400472535255]
	TIME [epoch: 174 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04872973076876578		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.04872973076876578 | validation: 0.06709478404565494]
	TIME [epoch: 2.69 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06018284562970764		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.06018284562970764 | validation: 0.11290819429322081]
	TIME [epoch: 2.66 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08328472409439446		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.08328472409439446 | validation: 0.07250084206703483]
	TIME [epoch: 2.67 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06882123416404051		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.06882123416404051 | validation: 0.07954057528764462]
	TIME [epoch: 2.67 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055850237089365075		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.055850237089365075 | validation: 0.07416027775813105]
	TIME [epoch: 2.66 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04347507294779012		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.04347507294779012 | validation: 0.05775660146955759]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_507.pth
	Model improved!!!
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04779681350642667		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.04779681350642667 | validation: 0.0948540231145627]
	TIME [epoch: 2.67 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07954705233959578		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.07954705233959578 | validation: 0.06888326819888703]
	TIME [epoch: 2.66 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0711277897230606		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.0711277897230606 | validation: 0.06467059405751933]
	TIME [epoch: 2.66 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048580329201751704		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.048580329201751704 | validation: 0.0571028182885009]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_511.pth
	Model improved!!!
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0429536677560911		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.0429536677560911 | validation: 0.052510921710218955]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_512.pth
	Model improved!!!
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04159115066198689		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.04159115066198689 | validation: 0.07241766027719317]
	TIME [epoch: 2.67 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05048977626778984		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.05048977626778984 | validation: 0.059306893831577334]
	TIME [epoch: 2.66 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05634085626196399		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.05634085626196399 | validation: 0.07239426579123691]
	TIME [epoch: 2.66 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05387166323774285		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.05387166323774285 | validation: 0.059652631914345146]
	TIME [epoch: 2.66 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04580162578802811		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.04580162578802811 | validation: 0.06718651226837373]
	TIME [epoch: 2.66 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04802088836984437		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.04802088836984437 | validation: 0.09207968009005175]
	TIME [epoch: 2.67 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0717826036429931		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.0717826036429931 | validation: 0.10599118432239973]
	TIME [epoch: 2.66 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10954201720416835		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.10954201720416835 | validation: 0.07614382503941947]
	TIME [epoch: 2.66 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060815836654977855		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.060815836654977855 | validation: 0.05444838588977411]
	TIME [epoch: 2.66 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039830423395331306		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.039830423395331306 | validation: 0.05636562806905711]
	TIME [epoch: 2.66 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039741748658981495		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.039741748658981495 | validation: 0.06658028743288581]
	TIME [epoch: 2.66 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03941992857513877		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.03941992857513877 | validation: 0.052844519714124044]
	TIME [epoch: 2.67 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03746112110615358		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.03746112110615358 | validation: 0.06249617453719139]
	TIME [epoch: 2.66 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03625531432680701		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.03625531432680701 | validation: 0.06457123798355789]
	TIME [epoch: 2.66 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04268472324885225		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.04268472324885225 | validation: 0.05947663842366205]
	TIME [epoch: 2.66 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049636685003965564		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.049636685003965564 | validation: 0.12062565070235823]
	TIME [epoch: 2.66 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09203582843943586		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.09203582843943586 | validation: 0.05658948499323718]
	TIME [epoch: 2.66 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06551205337294909		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.06551205337294909 | validation: 0.06536905903773178]
	TIME [epoch: 2.66 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04186011423640908		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.04186011423640908 | validation: 0.06402131582434216]
	TIME [epoch: 2.66 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039224063699172064		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.039224063699172064 | validation: 0.05837816548750822]
	TIME [epoch: 2.66 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03740042825274296		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.03740042825274296 | validation: 0.055530676048459675]
	TIME [epoch: 2.66 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03396169075984464		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.03396169075984464 | validation: 0.06516334253653176]
	TIME [epoch: 2.66 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038346958961633516		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.038346958961633516 | validation: 0.054140728132716566]
	TIME [epoch: 2.67 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047177211069074805		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.047177211069074805 | validation: 0.08662659726931558]
	TIME [epoch: 2.66 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.068832081505614		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.068832081505614 | validation: 0.06093576082061778]
	TIME [epoch: 2.66 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06897135731252767		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.06897135731252767 | validation: 0.08577430617024784]
	TIME [epoch: 2.66 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05540810680166794		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.05540810680166794 | validation: 0.07147966896636467]
	TIME [epoch: 2.66 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04770188781576305		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.04770188781576305 | validation: 0.08353132524859025]
	TIME [epoch: 2.66 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07685485334170432		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.07685485334170432 | validation: 0.10830375624656892]
	TIME [epoch: 2.66 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08252222095299472		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.08252222095299472 | validation: 0.06092499916913167]
	TIME [epoch: 2.66 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03790684702607121		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.03790684702607121 | validation: 0.058760016676237176]
	TIME [epoch: 2.66 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0445364963932945		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.0445364963932945 | validation: 0.07103517895654228]
	TIME [epoch: 2.66 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04394048783007314		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.04394048783007314 | validation: 0.05241541613041113]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_545.pth
	Model improved!!!
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04158582973236442		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.04158582973236442 | validation: 0.06113827203641468]
	TIME [epoch: 2.67 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03611331781889558		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.03611331781889558 | validation: 0.054978911667993714]
	TIME [epoch: 2.66 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03537953606060363		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.03537953606060363 | validation: 0.05585788044192951]
	TIME [epoch: 2.66 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03389248240281545		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.03389248240281545 | validation: 0.05451261306786186]
	TIME [epoch: 2.66 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03240532367586838		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.03240532367586838 | validation: 0.05652771001138345]
	TIME [epoch: 2.66 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037864502521711633		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.037864502521711633 | validation: 0.07170265490280327]
	TIME [epoch: 2.66 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0537646282499904		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.0537646282499904 | validation: 0.06025319145361015]
	TIME [epoch: 2.66 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056429586252504986		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.056429586252504986 | validation: 0.08607301008854859]
	TIME [epoch: 2.66 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0568407546129869		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.0568407546129869 | validation: 0.06351996692236915]
	TIME [epoch: 2.66 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04205756145372853		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.04205756145372853 | validation: 0.05483299943564002]
	TIME [epoch: 2.66 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05064876150050575		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.05064876150050575 | validation: 0.08032089113473734]
	TIME [epoch: 2.66 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06087628811789316		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.06087628811789316 | validation: 0.051982610829342205]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_557.pth
	Model improved!!!
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046283502112016046		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.046283502112016046 | validation: 0.05445947357616546]
	TIME [epoch: 2.66 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028982775646962233		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.028982775646962233 | validation: 0.057063272735055064]
	TIME [epoch: 2.66 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02910998748545201		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.02910998748545201 | validation: 0.04306976820006726]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_560.pth
	Model improved!!!
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037859789666758355		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.037859789666758355 | validation: 0.07117066534214617]
	TIME [epoch: 2.66 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058030762477280944		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.058030762477280944 | validation: 0.056475962546539776]
	TIME [epoch: 2.66 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055763164516948724		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.055763164516948724 | validation: 0.07200743814722887]
	TIME [epoch: 2.66 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04826595671387762		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.04826595671387762 | validation: 0.06979945571075476]
	TIME [epoch: 2.67 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03905156767177404		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.03905156767177404 | validation: 0.056134239147460234]
	TIME [epoch: 2.66 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04052067391751479		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.04052067391751479 | validation: 0.06593858077652158]
	TIME [epoch: 2.66 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041185169813918836		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.041185169813918836 | validation: 0.04759420465146255]
	TIME [epoch: 2.66 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03296735727600516		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.03296735727600516 | validation: 0.06336391887845115]
	TIME [epoch: 2.67 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03073257175765435		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.03073257175765435 | validation: 0.050371347887695796]
	TIME [epoch: 2.66 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030241194860468282		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.030241194860468282 | validation: 0.05427704913641338]
	TIME [epoch: 2.66 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02743463014619866		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.02743463014619866 | validation: 0.04931572973977871]
	TIME [epoch: 2.66 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02643559263439684		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.02643559263439684 | validation: 0.05387399275513654]
	TIME [epoch: 2.66 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027218929802139403		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.027218929802139403 | validation: 0.05107262686326104]
	TIME [epoch: 2.66 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02839766514069222		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.02839766514069222 | validation: 0.06076867216969402]
	TIME [epoch: 2.66 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032169513781620244		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.032169513781620244 | validation: 0.05020912373605695]
	TIME [epoch: 2.67 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04497622730931181		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.04497622730931181 | validation: 0.08488246878498801]
	TIME [epoch: 2.67 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05953386573986883		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.05953386573986883 | validation: 0.05113895768796628]
	TIME [epoch: 2.66 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049106925999307514		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.049106925999307514 | validation: 0.07958765172687476]
	TIME [epoch: 2.66 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06155874501858889		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.06155874501858889 | validation: 0.07987647025527443]
	TIME [epoch: 2.67 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0627523446641591		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.0627523446641591 | validation: 0.052476832418253884]
	TIME [epoch: 2.66 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04631989662694035		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.04631989662694035 | validation: 0.06325881108129519]
	TIME [epoch: 2.66 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031130186085013667		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.031130186085013667 | validation: 0.045557009689091314]
	TIME [epoch: 2.67 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028085452587519672		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.028085452587519672 | validation: 0.04925107517270218]
	TIME [epoch: 2.67 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026407632579464508		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.026407632579464508 | validation: 0.04908664274928243]
	TIME [epoch: 2.66 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026748118633837217		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.026748118633837217 | validation: 0.04940538074767591]
	TIME [epoch: 2.66 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027197118057611647		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.027197118057611647 | validation: 0.04657040471851899]
	TIME [epoch: 2.67 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02767132818835715		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.02767132818835715 | validation: 0.04727453640146084]
	TIME [epoch: 2.66 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025892649385162187		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.025892649385162187 | validation: 0.04679657208030605]
	TIME [epoch: 2.67 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02838303594437267		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.02838303594437267 | validation: 0.05979659174676192]
	TIME [epoch: 2.66 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04273412271748146		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.04273412271748146 | validation: 0.06417777865131717]
	TIME [epoch: 2.67 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060303719580198784		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.060303719580198784 | validation: 0.07064542959291056]
	TIME [epoch: 2.66 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057126371862510406		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.057126371862510406 | validation: 0.045800109721008266]
	TIME [epoch: 2.66 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02877446689253653		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.02877446689253653 | validation: 0.042224819790648176]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_593.pth
	Model improved!!!
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03760672622810527		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.03760672622810527 | validation: 0.06947452695689299]
	TIME [epoch: 2.66 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04579413176144323		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.04579413176144323 | validation: 0.04256533314101022]
	TIME [epoch: 2.66 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03683363725985576		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.03683363725985576 | validation: 0.05747140635310179]
	TIME [epoch: 2.66 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03381582886890913		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.03381582886890913 | validation: 0.0470428343934283]
	TIME [epoch: 2.66 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03066811915527679		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.03066811915527679 | validation: 0.05249590568991561]
	TIME [epoch: 2.66 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03353120568501922		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.03353120568501922 | validation: 0.056040421958217915]
	TIME [epoch: 2.66 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036969761765550835		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.036969761765550835 | validation: 0.04720518444836609]
	TIME [epoch: 2.66 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027515965095783534		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.027515965095783534 | validation: 0.042171338603716316]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_601.pth
	Model improved!!!
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024068653493854973		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.024068653493854973 | validation: 0.047397516272489705]
	TIME [epoch: 2.66 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024246502618738522		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.024246502618738522 | validation: 0.04345619639676404]
	TIME [epoch: 2.66 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024279990210786227		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.024279990210786227 | validation: 0.051160460112925216]
	TIME [epoch: 2.66 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02491378360739737		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.02491378360739737 | validation: 0.04850701341954677]
	TIME [epoch: 2.66 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030007049861887784		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.030007049861887784 | validation: 0.05942318627510425]
	TIME [epoch: 2.66 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03662637202185001		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.03662637202185001 | validation: 0.04647699250250155]
	TIME [epoch: 2.66 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03983470019129847		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.03983470019129847 | validation: 0.05071933167930216]
	TIME [epoch: 2.66 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03061942478673407		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.03061942478673407 | validation: 0.048931572953779194]
	TIME [epoch: 2.66 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028972485014126265		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.028972485014126265 | validation: 0.05887487315761795]
	TIME [epoch: 2.66 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04012965107113394		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.04012965107113394 | validation: 0.06147950175036231]
	TIME [epoch: 2.66 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045782530636649964		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.045782530636649964 | validation: 0.05584464386538315]
	TIME [epoch: 2.67 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04111861587029207		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.04111861587029207 | validation: 0.044712950358801934]
	TIME [epoch: 2.67 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024525968172420518		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.024525968172420518 | validation: 0.042276106895955706]
	TIME [epoch: 2.66 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021507334323872144		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.021507334323872144 | validation: 0.047269260312567764]
	TIME [epoch: 2.66 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028348550301123834		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.028348550301123834 | validation: 0.04175854429647663]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_616.pth
	Model improved!!!
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031096580495356744		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.031096580495356744 | validation: 0.05480339046919222]
	TIME [epoch: 2.66 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034578293439215736		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.034578293439215736 | validation: 0.0430265576786775]
	TIME [epoch: 2.66 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0383700498007647		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.0383700498007647 | validation: 0.06349290287537751]
	TIME [epoch: 2.66 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042795588388403055		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.042795588388403055 | validation: 0.032599600980717916]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_620.pth
	Model improved!!!
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031089132667807988		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.031089132667807988 | validation: 0.05398255981623273]
	TIME [epoch: 2.66 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036050854143854436		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.036050854143854436 | validation: 0.04965730658308908]
	TIME [epoch: 2.67 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03392349261592114		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.03392349261592114 | validation: 0.04515258110933004]
	TIME [epoch: 2.66 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020118572396730414		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.020118572396730414 | validation: 0.03772343980930041]
	TIME [epoch: 2.66 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022095375581513182		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.022095375581513182 | validation: 0.04757960148738423]
	TIME [epoch: 2.65 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02604449622414812		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.02604449622414812 | validation: 0.03957153947557971]
	TIME [epoch: 2.65 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024234008750364567		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.024234008750364567 | validation: 0.04713865875467469]
	TIME [epoch: 2.65 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02529135730474647		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.02529135730474647 | validation: 0.034914383844877044]
	TIME [epoch: 2.65 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025053414774702347		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.025053414774702347 | validation: 0.04134827462763008]
	TIME [epoch: 2.65 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023784480832141127		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.023784480832141127 | validation: 0.03436687041488319]
	TIME [epoch: 2.65 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0208920429849543		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.0208920429849543 | validation: 0.0408274071427798]
	TIME [epoch: 2.65 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022214892475549385		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.022214892475549385 | validation: 0.03918185639083321]
	TIME [epoch: 2.65 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027500487826340594		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.027500487826340594 | validation: 0.05965333876781627]
	TIME [epoch: 2.65 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04108299213698232		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.04108299213698232 | validation: 0.04791086558212607]
	TIME [epoch: 2.65 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038965988282384054		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.038965988282384054 | validation: 0.04428430991691165]
	TIME [epoch: 2.65 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027152630816893533		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.027152630816893533 | validation: 0.048985724435078704]
	TIME [epoch: 2.65 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03312775813775332		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.03312775813775332 | validation: 0.034409196075192154]
	TIME [epoch: 2.65 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03191185016657458		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.03191185016657458 | validation: 0.05338550480945536]
	TIME [epoch: 2.67 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024869455971711665		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.024869455971711665 | validation: 0.03631489571746035]
	TIME [epoch: 2.68 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0171353641573166		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.0171353641573166 | validation: 0.03851689338139794]
	TIME [epoch: 2.67 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01874739284861136		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.01874739284861136 | validation: 0.04533792193519065]
	TIME [epoch: 2.67 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019597637412259724		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.019597637412259724 | validation: 0.03544517663841101]
	TIME [epoch: 2.67 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019948529673455225		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.019948529673455225 | validation: 0.04185533739126617]
	TIME [epoch: 2.68 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020324835087058134		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.020324835087058134 | validation: 0.044020776155223595]
	TIME [epoch: 2.67 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024426370999048378		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.024426370999048378 | validation: 0.049269420079875295]
	TIME [epoch: 2.68 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03463503067567585		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.03463503067567585 | validation: 0.04255193054350506]
	TIME [epoch: 2.67 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028506396026218957		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.028506396026218957 | validation: 0.04241414006398478]
	TIME [epoch: 2.67 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026571501755056057		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.026571501755056057 | validation: 0.04292574301282657]
	TIME [epoch: 2.67 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02764102122283343		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.02764102122283343 | validation: 0.03710774241959305]
	TIME [epoch: 2.67 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03330302134135045		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.03330302134135045 | validation: 0.049063282724954305]
	TIME [epoch: 2.67 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02767466849912806		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.02767466849912806 | validation: 0.034515949321442085]
	TIME [epoch: 2.67 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022204555883741043		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.022204555883741043 | validation: 0.04620551687608662]
	TIME [epoch: 2.68 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020261417673542805		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.020261417673542805 | validation: 0.038272627114668646]
	TIME [epoch: 2.67 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02181932182721491		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.02181932182721491 | validation: 0.03909469611321159]
	TIME [epoch: 2.67 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024630075924242635		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.024630075924242635 | validation: 0.04071953081930496]
	TIME [epoch: 2.67 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027190429346722486		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.027190429346722486 | validation: 0.04478418561870541]
	TIME [epoch: 2.68 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025460848529545714		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.025460848529545714 | validation: 0.04017627559361338]
	TIME [epoch: 2.67 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019044223449528327		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.019044223449528327 | validation: 0.036244139617436645]
	TIME [epoch: 2.67 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01678883309020603		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.01678883309020603 | validation: 0.035505767994107755]
	TIME [epoch: 2.67 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017911919280379846		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.017911919280379846 | validation: 0.0387593531560714]
	TIME [epoch: 2.68 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026227667677182893		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.026227667677182893 | validation: 0.04124750415037968]
	TIME [epoch: 2.67 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028321468586154693		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.028321468586154693 | validation: 0.04565476275248397]
	TIME [epoch: 2.67 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029320052968859606		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.029320052968859606 | validation: 0.038764880810644456]
	TIME [epoch: 2.67 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02339835850044505		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.02339835850044505 | validation: 0.04098685134987332]
	TIME [epoch: 2.67 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0224114671431224		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.0224114671431224 | validation: 0.032310258963950865]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_665.pth
	Model improved!!!
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020634576230447444		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.020634576230447444 | validation: 0.04949496185753563]
	TIME [epoch: 2.65 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023230224220821282		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.023230224220821282 | validation: 0.03979056851729671]
	TIME [epoch: 2.65 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02655227670912631		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.02655227670912631 | validation: 0.04494202599267669]
	TIME [epoch: 2.65 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023115794881644672		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.023115794881644672 | validation: 0.03678741708335986]
	TIME [epoch: 2.65 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020107722917062986		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.020107722917062986 | validation: 0.04028241991376333]
	TIME [epoch: 2.65 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01730424387079442		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.01730424387079442 | validation: 0.03966808463888086]
	TIME [epoch: 2.65 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02036541925814503		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.02036541925814503 | validation: 0.03191003324234406]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_672.pth
	Model improved!!!
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019781942895269286		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.019781942895269286 | validation: 0.034462056226388975]
	TIME [epoch: 2.67 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01985006464988646		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.01985006464988646 | validation: 0.033905608496331785]
	TIME [epoch: 2.67 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023672912536640087		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.023672912536640087 | validation: 0.05226794810020651]
	TIME [epoch: 2.67 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028656310777475706		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.028656310777475706 | validation: 0.03565418888324973]
	TIME [epoch: 2.67 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019495758130002188		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.019495758130002188 | validation: 0.029475617563858282]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_677.pth
	Model improved!!!
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019684354615027027		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.019684354615027027 | validation: 0.03405621640462797]
	TIME [epoch: 2.67 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015584695206532483		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.015584695206532483 | validation: 0.02943448672817205]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_679.pth
	Model improved!!!
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017295312609253478		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.017295312609253478 | validation: 0.03500200962456806]
	TIME [epoch: 2.67 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015160156907166378		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.015160156907166378 | validation: 0.033672934163545]
	TIME [epoch: 2.67 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015933861190065707		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.015933861190065707 | validation: 0.036710161475568624]
	TIME [epoch: 2.67 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019685105421764656		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.019685105421764656 | validation: 0.03341123403173574]
	TIME [epoch: 2.67 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022933133104829737		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.022933133104829737 | validation: 0.040264784811028846]
	TIME [epoch: 2.67 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029160803947599515		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.029160803947599515 | validation: 0.033862127373588694]
	TIME [epoch: 2.67 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02940275955263676		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.02940275955263676 | validation: 0.0360993448606372]
	TIME [epoch: 2.67 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016398196266698275		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.016398196266698275 | validation: 0.03907901291280126]
	TIME [epoch: 2.67 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014519935606603827		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.014519935606603827 | validation: 0.03555892625431243]
	TIME [epoch: 2.68 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021252900402744555		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.021252900402744555 | validation: 0.050729166713138964]
	TIME [epoch: 2.67 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02452249502889814		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.02452249502889814 | validation: 0.03772209516301908]
	TIME [epoch: 2.67 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022946483393554252		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.022946483393554252 | validation: 0.03590371153730241]
	TIME [epoch: 2.67 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020315861284914433		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.020315861284914433 | validation: 0.04280783687029768]
	TIME [epoch: 2.67 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022245980510457636		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.022245980510457636 | validation: 0.0342855225751012]
	TIME [epoch: 2.67 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023514248863426675		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.023514248863426675 | validation: 0.03618928810415263]
	TIME [epoch: 2.67 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019945019167959294		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.019945019167959294 | validation: 0.037569324697074]
	TIME [epoch: 2.67 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014651480412901945		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.014651480412901945 | validation: 0.027759649907514797]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_696.pth
	Model improved!!!
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01378488282083181		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.01378488282083181 | validation: 0.036980870953064064]
	TIME [epoch: 2.67 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014127897008935153		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.014127897008935153 | validation: 0.027032380947508074]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_698.pth
	Model improved!!!
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014683830333840561		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.014683830333840561 | validation: 0.036451907230398505]
	TIME [epoch: 2.68 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017037981940878556		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.017037981940878556 | validation: 0.03051536368914175]
	TIME [epoch: 2.67 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017202182089980193		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.017202182089980193 | validation: 0.05037473554335512]
	TIME [epoch: 2.67 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02210657656762605		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.02210657656762605 | validation: 0.027651986187659506]
	TIME [epoch: 2.67 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021908812599125898		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.021908812599125898 | validation: 0.04115802962655144]
	TIME [epoch: 2.67 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017749942480299288		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.017749942480299288 | validation: 0.033568075928269096]
	TIME [epoch: 2.67 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01604186293695467		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.01604186293695467 | validation: 0.044552998085243434]
	TIME [epoch: 2.67 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024441035773285998		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.024441035773285998 | validation: 0.04001646897363571]
	TIME [epoch: 2.67 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028434433868791846		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.028434433868791846 | validation: 0.04682758069556782]
	TIME [epoch: 2.67 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027579617013655897		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.027579617013655897 | validation: 0.02990364533084261]
	TIME [epoch: 2.67 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014706370679195134		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.014706370679195134 | validation: 0.03003626061494903]
	TIME [epoch: 2.67 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012267479214651551		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.012267479214651551 | validation: 0.02930658534480182]
	TIME [epoch: 2.68 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014231398839705776		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.014231398839705776 | validation: 0.03481044985584143]
	TIME [epoch: 2.67 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014668670764104486		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.014668670764104486 | validation: 0.03015817688406243]
	TIME [epoch: 2.67 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013992546174829507		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.013992546174829507 | validation: 0.035378153200463114]
	TIME [epoch: 2.67 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013790629125375903		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.013790629125375903 | validation: 0.03189724969878458]
	TIME [epoch: 2.67 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017714375497832		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.017714375497832 | validation: 0.03716698425100693]
	TIME [epoch: 2.67 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020564862770592773		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.020564862770592773 | validation: 0.03777938073334489]
	TIME [epoch: 2.67 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027420862882327226		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.027420862882327226 | validation: 0.03183561882812561]
	TIME [epoch: 2.67 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016953777147367847		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.016953777147367847 | validation: 0.03208571298547582]
	TIME [epoch: 2.67 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016832921206726466		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.016832921206726466 | validation: 0.02510694748644964]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_719.pth
	Model improved!!!
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015621527546915617		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.015621527546915617 | validation: 0.03888128586939297]
	TIME [epoch: 2.67 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016790463216138335		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.016790463216138335 | validation: 0.030925722994004902]
	TIME [epoch: 2.67 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014847444996204967		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.014847444996204967 | validation: 0.029095297827983948]
	TIME [epoch: 2.66 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014806048477441948		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.014806048477441948 | validation: 0.029496388342464597]
	TIME [epoch: 2.67 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016836274286111685		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.016836274286111685 | validation: 0.03680794406865623]
	TIME [epoch: 2.67 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01968088184972561		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.01968088184972561 | validation: 0.03370055207442323]
	TIME [epoch: 2.67 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022228538050978512		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.022228538050978512 | validation: 0.03114417097274297]
	TIME [epoch: 2.67 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016079530511711498		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.016079530511711498 | validation: 0.02819231667227259]
	TIME [epoch: 2.67 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013228139367894557		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.013228139367894557 | validation: 0.03047775777514168]
	TIME [epoch: 2.67 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012643675843853883		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.012643675843853883 | validation: 0.029454447520699246]
	TIME [epoch: 2.67 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01290325191181072		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.01290325191181072 | validation: 0.02521324811446947]
	TIME [epoch: 2.67 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01367967583430956		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.01367967583430956 | validation: 0.035870531876521505]
	TIME [epoch: 2.67 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016363689176003428		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.016363689176003428 | validation: 0.027483346956034806]
	TIME [epoch: 2.67 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01834189384568008		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.01834189384568008 | validation: 0.033899047679028065]
	TIME [epoch: 2.67 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020524279224471278		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.020524279224471278 | validation: 0.029970971394504078]
	TIME [epoch: 2.66 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014652899393399485		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.014652899393399485 | validation: 0.03435151349479761]
	TIME [epoch: 2.67 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012113240031020744		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.012113240031020744 | validation: 0.03534711316566216]
	TIME [epoch: 2.67 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014735532416696373		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.014735532416696373 | validation: 0.03747947195004838]
	TIME [epoch: 2.67 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021267938470981313		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.021267938470981313 | validation: 0.04364232713741422]
	TIME [epoch: 2.66 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02911082015056607		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.02911082015056607 | validation: 0.027394125969153395]
	TIME [epoch: 2.67 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016146930861064567		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.016146930861064567 | validation: 0.03161834075114813]
	TIME [epoch: 2.67 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012105166488468069		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.012105166488468069 | validation: 0.031183611309531546]
	TIME [epoch: 2.67 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01510133232588783		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.01510133232588783 | validation: 0.027493806659360322]
	TIME [epoch: 2.67 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015129561485469288		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.015129561485469288 | validation: 0.030303052830953803]
	TIME [epoch: 2.67 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012672269650849721		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.012672269650849721 | validation: 0.02749814353843342]
	TIME [epoch: 2.69 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011700727360648205		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.011700727360648205 | validation: 0.03344525762884052]
	TIME [epoch: 2.67 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011379058914404995		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.011379058914404995 | validation: 0.02365715074052336]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_746.pth
	Model improved!!!
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01145090820839084		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.01145090820839084 | validation: 0.03432602369863171]
	TIME [epoch: 2.67 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01611516190131108		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.01611516190131108 | validation: 0.02915011955805391]
	TIME [epoch: 2.67 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01961665590197886		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.01961665590197886 | validation: 0.034568370156248765]
	TIME [epoch: 2.67 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015900815216817535		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.015900815216817535 | validation: 0.029972686032320298]
	TIME [epoch: 2.67 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012712651365350493		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.012712651365350493 | validation: 0.028950478056028486]
	TIME [epoch: 2.67 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011023336332912557		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.011023336332912557 | validation: 0.02443443949855525]
	TIME [epoch: 2.67 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010422581794446002		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.010422581794446002 | validation: 0.026436947284445258]
	TIME [epoch: 2.67 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010569489428365029		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.010569489428365029 | validation: 0.02684899305446156]
	TIME [epoch: 2.68 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014638302776805204		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.014638302776805204 | validation: 0.03721055971629731]
	TIME [epoch: 2.67 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02656755406021205		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.02656755406021205 | validation: 0.028061980030142555]
	TIME [epoch: 2.67 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029021866102797153		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.029021866102797153 | validation: 0.027729703088474212]
	TIME [epoch: 2.67 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017284256575018992		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.017284256575018992 | validation: 0.024410629057987035]
	TIME [epoch: 2.67 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011262175166954767		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.011262175166954767 | validation: 0.03150634931978783]
	TIME [epoch: 2.67 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010950621298504847		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.010950621298504847 | validation: 0.02867488904300726]
	TIME [epoch: 2.67 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010407965192985686		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.010407965192985686 | validation: 0.027652328973053086]
	TIME [epoch: 2.67 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01017162992739531		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.01017162992739531 | validation: 0.03139944087720869]
	TIME [epoch: 2.67 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010966263078235583		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.010966263078235583 | validation: 0.027327389189774044]
	TIME [epoch: 2.67 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010313860998458586		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.010313860998458586 | validation: 0.030863145571326467]
	TIME [epoch: 2.67 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011639587190770928		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.011639587190770928 | validation: 0.03592227458469662]
	TIME [epoch: 2.68 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014034811104480786		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.014034811104480786 | validation: 0.028253115279123366]
	TIME [epoch: 2.67 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018530399582469704		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.018530399582469704 | validation: 0.04048903502613607]
	TIME [epoch: 2.67 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022935597535474674		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.022935597535474674 | validation: 0.03227705263748273]
	TIME [epoch: 2.67 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01625396427490607		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.01625396427490607 | validation: 0.027944711173067106]
	TIME [epoch: 2.67 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009801172430076631		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.009801172430076631 | validation: 0.029902292225406125]
	TIME [epoch: 2.67 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012071087564141261		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.012071087564141261 | validation: 0.027431102688056243]
	TIME [epoch: 2.67 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01591399128773994		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.01591399128773994 | validation: 0.0320873289505539]
	TIME [epoch: 2.67 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012830071040760295		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.012830071040760295 | validation: 0.03100547523051951]
	TIME [epoch: 2.67 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016074338513360684		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.016074338513360684 | validation: 0.026507923161229675]
	TIME [epoch: 2.67 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020772554920490766		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.020772554920490766 | validation: 0.029287815793731076]
	TIME [epoch: 2.67 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01660548311389867		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.01660548311389867 | validation: 0.02436858683437785]
	TIME [epoch: 2.67 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012236137636043712		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.012236137636043712 | validation: 0.02656298317098308]
	TIME [epoch: 2.67 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010726014342590214		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.010726014342590214 | validation: 0.029611467519103086]
	TIME [epoch: 2.67 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012282933815790032		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.012282933815790032 | validation: 0.02468323211371053]
	TIME [epoch: 2.67 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011009852932330146		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.011009852932330146 | validation: 0.030305482339303594]
	TIME [epoch: 2.67 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011604870808617319		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.011604870808617319 | validation: 0.02664258944291258]
	TIME [epoch: 2.67 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01576045159823871		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.01576045159823871 | validation: 0.026783098947055606]
	TIME [epoch: 2.67 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014497215972680345		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.014497215972680345 | validation: 0.03589612086377334]
	TIME [epoch: 2.67 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01720892017092466		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.01720892017092466 | validation: 0.027249542339220646]
	TIME [epoch: 2.67 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012752739526686636		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.012752739526686636 | validation: 0.0324667956757191]
	TIME [epoch: 2.67 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010960088987945852		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.010960088987945852 | validation: 0.027551556449264825]
	TIME [epoch: 2.67 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009734545948479938		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.009734545948479938 | validation: 0.027506428613236947]
	TIME [epoch: 2.67 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010845128042704202		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.010845128042704202 | validation: 0.030599649941493823]
	TIME [epoch: 2.67 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010643134034373598		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.010643134034373598 | validation: 0.027371477547108215]
	TIME [epoch: 2.67 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009733582369729874		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.009733582369729874 | validation: 0.028460330386351776]
	TIME [epoch: 2.67 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010902651222400557		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.010902651222400557 | validation: 0.02656753695299473]
	TIME [epoch: 2.67 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013194023796599		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.013194023796599 | validation: 0.03300236495617054]
	TIME [epoch: 2.67 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014824778706710817		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.014824778706710817 | validation: 0.025345217255691256]
	TIME [epoch: 2.67 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015372782967545795		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.015372782967545795 | validation: 0.032029708357705036]
	TIME [epoch: 2.67 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01196798659278197		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.01196798659278197 | validation: 0.024236714923755842]
	TIME [epoch: 2.67 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00949959620622486		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.00949959620622486 | validation: 0.02792460963046567]
	TIME [epoch: 2.67 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011177808554373987		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.011177808554373987 | validation: 0.02691717769713016]
	TIME [epoch: 2.67 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01169239534904746		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.01169239534904746 | validation: 0.032787540882842615]
	TIME [epoch: 2.68 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012075191454199528		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.012075191454199528 | validation: 0.026050993352994312]
	TIME [epoch: 2.67 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012451088880837284		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.012451088880837284 | validation: 0.027509611740092]
	TIME [epoch: 2.67 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013311579809456346		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.013311579809456346 | validation: 0.03221454507046432]
	TIME [epoch: 2.67 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015236205534683349		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.015236205534683349 | validation: 0.02925186267265073]
	TIME [epoch: 2.67 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018232902463634418		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.018232902463634418 | validation: 0.02720034763495314]
	TIME [epoch: 2.67 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013059749731018378		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.013059749731018378 | validation: 0.02810288768602033]
	TIME [epoch: 2.67 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00992241304232559		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.00992241304232559 | validation: 0.024091244707388174]
	TIME [epoch: 2.67 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01000452909624057		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.01000452909624057 | validation: 0.025954250486827812]
	TIME [epoch: 2.67 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009492002555625571		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.009492002555625571 | validation: 0.026273494484437922]
	TIME [epoch: 2.67 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00916935015156221		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.00916935015156221 | validation: 0.02416409055383081]
	TIME [epoch: 2.67 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008331763982898532		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.008331763982898532 | validation: 0.026184872251622828]
	TIME [epoch: 2.67 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009612640359366811		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.009612640359366811 | validation: 0.02681546393754436]
	TIME [epoch: 2.67 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009946768017559641		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.009946768017559641 | validation: 0.02989456072136413]
	TIME [epoch: 2.67 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015429541305390423		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.015429541305390423 | validation: 0.029971934671557923]
	TIME [epoch: 2.66 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018604887752803858		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.018604887752803858 | validation: 0.02955168583656591]
	TIME [epoch: 2.67 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015592107639385448		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.015592107639385448 | validation: 0.021718251836520232]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_814.pth
	Model improved!!!
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011020831717088309		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.011020831717088309 | validation: 0.03243602808786377]
	TIME [epoch: 2.67 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0149590924459562		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.0149590924459562 | validation: 0.02362699014285732]
	TIME [epoch: 2.67 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013358257754208713		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.013358257754208713 | validation: 0.02544492078202325]
	TIME [epoch: 2.67 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00987582384050775		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.00987582384050775 | validation: 0.025580572903773982]
	TIME [epoch: 2.67 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00971584159857128		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.00971584159857128 | validation: 0.018715749580796626]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_819.pth
	Model improved!!!
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011220954951408122		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.011220954951408122 | validation: 0.028027538054955992]
	TIME [epoch: 2.67 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01021424900067591		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.01021424900067591 | validation: 0.023069648515407118]
	TIME [epoch: 2.67 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009362753308045743		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.009362753308045743 | validation: 0.023223078140053144]
	TIME [epoch: 2.67 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008242738211574183		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.008242738211574183 | validation: 0.023782723312825563]
	TIME [epoch: 2.66 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009178211702492759		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.009178211702492759 | validation: 0.022259655715919424]
	TIME [epoch: 2.67 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009717457543322221		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.009717457543322221 | validation: 0.03065974901582276]
	TIME [epoch: 2.67 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01212735364425057		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.01212735364425057 | validation: 0.027148304276332993]
	TIME [epoch: 2.66 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013741091170202429		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.013741091170202429 | validation: 0.027501550871523364]
	TIME [epoch: 2.66 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016663532161737226		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.016663532161737226 | validation: 0.029852171684925544]
	TIME [epoch: 2.67 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010952381259741344		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.010952381259741344 | validation: 0.024694846330836063]
	TIME [epoch: 2.67 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008323441094239045		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.008323441094239045 | validation: 0.028027135180991425]
	TIME [epoch: 2.66 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007510761719874588		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.007510761719874588 | validation: 0.024345003460875182]
	TIME [epoch: 2.67 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008892065263074447		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.008892065263074447 | validation: 0.025280113625763545]
	TIME [epoch: 2.67 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007962862610311894		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.007962862610311894 | validation: 0.02133885226910667]
	TIME [epoch: 2.66 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00920998026259438		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.00920998026259438 | validation: 0.023260818629917735]
	TIME [epoch: 2.67 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009541268079900043		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.009541268079900043 | validation: 0.0283751882430067]
	TIME [epoch: 2.67 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01206193457837383		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.01206193457837383 | validation: 0.030488222790914888]
	TIME [epoch: 2.67 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017394943258552006		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.017394943258552006 | validation: 0.028955436754687883]
	TIME [epoch: 2.66 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018474717071227716		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.018474717071227716 | validation: 0.026785151010121955]
	TIME [epoch: 2.67 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008908548603175928		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.008908548603175928 | validation: 0.02976943059216044]
	TIME [epoch: 2.66 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008784832961413667		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.008784832961413667 | validation: 0.027389485862419495]
	TIME [epoch: 2.67 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008007212739862333		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.008007212739862333 | validation: 0.023483509565149076]
	TIME [epoch: 2.66 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007628798831968278		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.007628798831968278 | validation: 0.022302635841540075]
	TIME [epoch: 2.67 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008576824637371097		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.008576824637371097 | validation: 0.02760559926783414]
	TIME [epoch: 2.66 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010143878272939657		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.010143878272939657 | validation: 0.023023034982204083]
	TIME [epoch: 2.67 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012313591037886838		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.012313591037886838 | validation: 0.030890539820991617]
	TIME [epoch: 2.66 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013171984682746654		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.013171984682746654 | validation: 0.023013431355958937]
	TIME [epoch: 2.67 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009445151942444604		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.009445151942444604 | validation: 0.027488039300406344]
	TIME [epoch: 2.66 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008373386924755278		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.008373386924755278 | validation: 0.024518728025815586]
	TIME [epoch: 2.67 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007402880612069518		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.007402880612069518 | validation: 0.022720151065393004]
	TIME [epoch: 2.67 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010254523097495027		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.010254523097495027 | validation: 0.023968581447561643]
	TIME [epoch: 2.67 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012459757033834265		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.012459757033834265 | validation: 0.029027787268337202]
	TIME [epoch: 2.66 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014292358128367726		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.014292358128367726 | validation: 0.028775574561036656]
	TIME [epoch: 2.67 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012825491774078324		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.012825491774078324 | validation: 0.027565305502088702]
	TIME [epoch: 2.67 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00907097467907762		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.00907097467907762 | validation: 0.023321248955804355]
	TIME [epoch: 2.67 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007226499794072625		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.007226499794072625 | validation: 0.02268949856165461]
	TIME [epoch: 2.66 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010269821403730919		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.010269821403730919 | validation: 0.026385145880504404]
	TIME [epoch: 2.67 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01015054265685937		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.01015054265685937 | validation: 0.01857119531198539]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_857.pth
	Model improved!!!
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009346649415532473		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.009346649415532473 | validation: 0.025552101864896173]
	TIME [epoch: 2.67 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008012085273184847		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.008012085273184847 | validation: 0.02318425817224992]
	TIME [epoch: 2.67 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008320700725646078		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.008320700725646078 | validation: 0.025387766437516127]
	TIME [epoch: 2.66 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007303420317601421		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.007303420317601421 | validation: 0.02596200408244083]
	TIME [epoch: 2.67 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007790967874136323		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.007790967874136323 | validation: 0.02297886566982519]
	TIME [epoch: 2.67 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009841081219630793		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.009841081219630793 | validation: 0.03054441602951956]
	TIME [epoch: 2.67 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012906548093318471		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.012906548093318471 | validation: 0.023952396729505165]
	TIME [epoch: 2.67 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014612824255860602		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.014612824255860602 | validation: 0.022968736895486078]
	TIME [epoch: 2.67 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008575695243549768		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.008575695243549768 | validation: 0.0245168443774785]
	TIME [epoch: 2.67 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007778225599779269		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.007778225599779269 | validation: 0.024460146622887438]
	TIME [epoch: 2.67 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007515060654187054		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.007515060654187054 | validation: 0.025632146190309436]
	TIME [epoch: 2.66 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010435994260350759		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.010435994260350759 | validation: 0.02788414180548108]
	TIME [epoch: 2.67 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01164608973447645		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.01164608973447645 | validation: 0.02579682735360811]
	TIME [epoch: 2.66 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011428066362200702		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.011428066362200702 | validation: 0.027080700025737647]
	TIME [epoch: 2.66 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009176089335369403		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.009176089335369403 | validation: 0.02762944897364457]
	TIME [epoch: 2.66 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008464997604639915		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.008464997604639915 | validation: 0.02075496175760068]
	TIME [epoch: 2.66 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00891830210282207		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.00891830210282207 | validation: 0.02703451909982815]
	TIME [epoch: 2.66 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008899603451565208		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.008899603451565208 | validation: 0.023688214021522914]
	TIME [epoch: 2.67 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009001582934668125		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.009001582934668125 | validation: 0.02706464481272608]
	TIME [epoch: 2.66 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0072954114451591655		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.0072954114451591655 | validation: 0.024252704722366392]
	TIME [epoch: 2.66 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006893061026548726		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.006893061026548726 | validation: 0.024581778369192488]
	TIME [epoch: 2.66 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008588303677135676		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.008588303677135676 | validation: 0.02616722849831367]
	TIME [epoch: 2.67 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01283757487331009		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.01283757487331009 | validation: 0.027781143244392337]
	TIME [epoch: 2.66 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012866118085953593		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.012866118085953593 | validation: 0.02397848355502106]
	TIME [epoch: 2.66 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013126200644429487		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.013126200644429487 | validation: 0.0228673676345018]
	TIME [epoch: 2.66 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00984508628983248		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.00984508628983248 | validation: 0.025032521370643535]
	TIME [epoch: 2.66 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008155790098777068		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.008155790098777068 | validation: 0.017768305090132698]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_884.pth
	Model improved!!!
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008684214681627743		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.008684214681627743 | validation: 0.023902482427881833]
	TIME [epoch: 2.67 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007372951201141073		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.007372951201141073 | validation: 0.02225826194535794]
	TIME [epoch: 2.67 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008162968553316562		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.008162968553316562 | validation: 0.02449142330710973]
	TIME [epoch: 2.67 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007223793864841608		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.007223793864841608 | validation: 0.02437177850793312]
	TIME [epoch: 2.66 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0070438231792706385		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.0070438231792706385 | validation: 0.0197615151370267]
	TIME [epoch: 2.66 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00916048694465571		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.00916048694465571 | validation: 0.02811958817860698]
	TIME [epoch: 2.66 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009834817903649208		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.009834817903649208 | validation: 0.0272308975539621]
	TIME [epoch: 2.66 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012557111546855291		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.012557111546855291 | validation: 0.025159850665680373]
	TIME [epoch: 2.67 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011660259964183038		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.011660259964183038 | validation: 0.026671254494593868]
	TIME [epoch: 2.66 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008333881987570733		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.008333881987570733 | validation: 0.018760549986297993]
	TIME [epoch: 2.66 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007040506509270961		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.007040506509270961 | validation: 0.02548175828835765]
	TIME [epoch: 2.66 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0073903037243349635		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.0073903037243349635 | validation: 0.021493983008774]
	TIME [epoch: 2.67 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007452926495047034		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.007452926495047034 | validation: 0.024512363259204962]
	TIME [epoch: 2.66 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007744407363573159		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.007744407363573159 | validation: 0.024584740629469127]
	TIME [epoch: 2.66 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006344286921424671		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.006344286921424671 | validation: 0.028532200123708207]
	TIME [epoch: 2.66 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007949084015870397		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.007949084015870397 | validation: 0.021064513510923047]
	TIME [epoch: 2.66 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009011432654960707		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.009011432654960707 | validation: 0.032582056041965304]
	TIME [epoch: 2.66 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0117011591072407		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.0117011591072407 | validation: 0.019906094158231413]
	TIME [epoch: 2.66 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009817042325278027		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.009817042325278027 | validation: 0.020735132483660813]
	TIME [epoch: 2.66 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006763158117458723		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.006763158117458723 | validation: 0.02722596465003495]
	TIME [epoch: 2.66 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006815900204243937		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.006815900204243937 | validation: 0.022716233047090396]
	TIME [epoch: 2.65 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006844351624901839		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.006844351624901839 | validation: 0.023944093516857636]
	TIME [epoch: 2.66 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006480890634906866		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.006480890634906866 | validation: 0.02314830795415508]
	TIME [epoch: 2.66 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0066163900819282626		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.0066163900819282626 | validation: 0.022314059508412268]
	TIME [epoch: 2.66 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007037512271023145		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.007037512271023145 | validation: 0.022667700241098744]
	TIME [epoch: 2.66 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009220803759896727		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.009220803759896727 | validation: 0.027186523417504882]
	TIME [epoch: 2.65 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011712713402544473		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.011712713402544473 | validation: 0.03161674326149146]
	TIME [epoch: 2.66 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020696719784572153		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.020696719784572153 | validation: 0.017799348027009967]
	TIME [epoch: 2.66 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009673009178195593		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.009673009178195593 | validation: 0.025114494173684544]
	TIME [epoch: 2.66 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009085403609926391		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.009085403609926391 | validation: 0.023000374949380478]
	TIME [epoch: 2.65 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009826564174126267		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.009826564174126267 | validation: 0.025948541787078318]
	TIME [epoch: 2.66 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007422721895240989		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.007422721895240989 | validation: 0.027862116956378993]
	TIME [epoch: 2.65 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006347496993799919		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.006347496993799919 | validation: 0.02087448049252917]
	TIME [epoch: 2.66 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006997681114044803		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.006997681114044803 | validation: 0.026185357804182866]
	TIME [epoch: 2.65 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006713714325752203		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.006713714325752203 | validation: 0.02115129137790213]
	TIME [epoch: 2.66 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006718493259425733		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.006718493259425733 | validation: 0.022656024169577307]
	TIME [epoch: 2.65 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00686757233980495		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.00686757233980495 | validation: 0.0199393709068984]
	TIME [epoch: 2.66 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006340851574954391		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.006340851574954391 | validation: 0.023321026022579096]
	TIME [epoch: 2.65 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006409296990799571		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.006409296990799571 | validation: 0.024572048213670584]
	TIME [epoch: 2.65 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007127423354100895		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.007127423354100895 | validation: 0.023526772369351856]
	TIME [epoch: 2.66 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010014671702034404		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.010014671702034404 | validation: 0.029947809738862654]
	TIME [epoch: 2.66 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01625352444476995		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.01625352444476995 | validation: 0.021446498692383322]
	TIME [epoch: 2.66 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010021706709787075		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.010021706709787075 | validation: 0.02244131652344855]
	TIME [epoch: 2.65 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0066334285524385205		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.0066334285524385205 | validation: 0.021295939051302226]
	TIME [epoch: 2.66 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007147001343875386		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.007147001343875386 | validation: 0.021896091778772976]
	TIME [epoch: 2.65 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007229634870056272		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.007229634870056272 | validation: 0.025137345848322334]
	TIME [epoch: 2.66 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009354584534398388		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.009354584534398388 | validation: 0.025115557798237112]
	TIME [epoch: 2.65 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013796804827651492		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.013796804827651492 | validation: 0.024059808932506133]
	TIME [epoch: 2.65 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009388769633251408		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.009388769633251408 | validation: 0.022427564317021132]
	TIME [epoch: 2.66 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0064665264614819275		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.0064665264614819275 | validation: 0.01962368275925429]
	TIME [epoch: 2.65 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007698109057509897		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.007698109057509897 | validation: 0.02447634413129165]
	TIME [epoch: 2.66 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0067782601267079405		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.0067782601267079405 | validation: 0.02080722335703753]
	TIME [epoch: 2.66 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006723787933355556		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.006723787933355556 | validation: 0.02344621085588862]
	TIME [epoch: 2.65 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006408710487915772		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.006408710487915772 | validation: 0.023379101572567486]
	TIME [epoch: 2.65 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006520460125579959		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.006520460125579959 | validation: 0.024328535537762087]
	TIME [epoch: 2.65 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0065808934552569285		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.0065808934552569285 | validation: 0.02048870750473533]
	TIME [epoch: 2.66 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006322144038325321		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.006322144038325321 | validation: 0.023447352180927607]
	TIME [epoch: 2.65 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007246197560861443		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.007246197560861443 | validation: 0.025266838247269097]
	TIME [epoch: 2.66 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01018909601795999		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.01018909601795999 | validation: 0.0219378171465635]
	TIME [epoch: 2.65 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010993473385264057		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.010993473385264057 | validation: 0.02196921710604981]
	TIME [epoch: 2.65 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008277604438453854		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.008277604438453854 | validation: 0.02506738933699273]
	TIME [epoch: 2.68 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007376874378649831		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.007376874378649831 | validation: 0.01881138926628937]
	TIME [epoch: 2.66 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006787468988853487		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.006787468988853487 | validation: 0.019161934518014147]
	TIME [epoch: 2.66 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0070395258558326535		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.0070395258558326535 | validation: 0.02356898041592588]
	TIME [epoch: 2.66 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007804131317705765		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.007804131317705765 | validation: 0.022297637416190598]
	TIME [epoch: 2.66 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008962261700739422		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.008962261700739422 | validation: 0.023825060750411922]
	TIME [epoch: 2.66 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007349793941611349		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.007349793941611349 | validation: 0.01763733864171432]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_951.pth
	Model improved!!!
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006685318302008478		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.006685318302008478 | validation: 0.0224902666360689]
	TIME [epoch: 2.66 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007345610072964873		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.007345610072964873 | validation: 0.022175780047004545]
	TIME [epoch: 2.66 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006296190008402707		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.006296190008402707 | validation: 0.019800931352450247]
	TIME [epoch: 2.66 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007979368425255517		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.007979368425255517 | validation: 0.02292203327249888]
	TIME [epoch: 2.66 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008716138914805428		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.008716138914805428 | validation: 0.023661470871863134]
	TIME [epoch: 2.65 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010444348494496145		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.010444348494496145 | validation: 0.020414935400112147]
	TIME [epoch: 2.66 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007801677129102913		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.007801677129102913 | validation: 0.02007493796045188]
	TIME [epoch: 2.66 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007327606183123618		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.007327606183123618 | validation: 0.021887975532893178]
	TIME [epoch: 2.66 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0059581058800890355		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.0059581058800890355 | validation: 0.02535148859184463]
	TIME [epoch: 2.65 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006246388620459695		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.006246388620459695 | validation: 0.021932543056524114]
	TIME [epoch: 2.66 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006411542374291979		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.006411542374291979 | validation: 0.017199564473193974]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_962.pth
	Model improved!!!
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008088130209264861		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.008088130209264861 | validation: 0.02647335027647847]
	TIME [epoch: 2.66 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008967543286744085		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.008967543286744085 | validation: 0.017707033939853057]
	TIME [epoch: 2.66 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0086755456799329		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.0086755456799329 | validation: 0.024467379001641167]
	TIME [epoch: 2.65 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008724152088352373		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.008724152088352373 | validation: 0.030785087773132935]
	TIME [epoch: 2.65 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011114975706012028		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.011114975706012028 | validation: 0.019055984019076034]
	TIME [epoch: 2.65 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007874785640562765		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.007874785640562765 | validation: 0.021439629379396054]
	TIME [epoch: 2.66 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006455968246919377		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.006455968246919377 | validation: 0.020739590883840853]
	TIME [epoch: 2.66 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006338938807156527		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.006338938807156527 | validation: 0.020868030767480686]
	TIME [epoch: 2.66 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006397579758460607		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.006397579758460607 | validation: 0.018069065887008597]
	TIME [epoch: 2.66 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0071436798128524685		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.0071436798128524685 | validation: 0.02064377991757682]
	TIME [epoch: 2.65 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006614865065281861		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.006614865065281861 | validation: 0.02098765433699015]
	TIME [epoch: 2.65 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006818556850432271		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.006818556850432271 | validation: 0.025075689161698902]
	TIME [epoch: 2.65 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007294671822167531		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.007294671822167531 | validation: 0.020343905251954755]
	TIME [epoch: 2.66 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006561946371624734		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.006561946371624734 | validation: 0.02349416657563749]
	TIME [epoch: 2.66 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0067330732104367245		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.0067330732104367245 | validation: 0.021879009274116335]
	TIME [epoch: 2.67 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006756753890981642		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.006756753890981642 | validation: 0.02018621281562487]
	TIME [epoch: 2.66 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007089675708228469		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.007089675708228469 | validation: 0.018631029807316835]
	TIME [epoch: 2.66 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009998338867031827		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.009998338867031827 | validation: 0.02573708985432791]
	TIME [epoch: 2.66 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010106289983560706		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.010106289983560706 | validation: 0.022803535043121482]
	TIME [epoch: 2.66 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008102056821843846		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.008102056821843846 | validation: 0.021767109292625264]
	TIME [epoch: 2.65 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006903773494076353		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.006903773494076353 | validation: 0.021290924870913386]
	TIME [epoch: 2.64 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006363567565028694		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.006363567565028694 | validation: 0.020107624967192306]
	TIME [epoch: 2.64 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006383773497762217		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.006383773497762217 | validation: 0.022239834793323022]
	TIME [epoch: 2.65 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007145100365702555		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.007145100365702555 | validation: 0.01969779896428958]
	TIME [epoch: 2.64 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0072984313799311275		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.0072984313799311275 | validation: 0.024588420106120848]
	TIME [epoch: 2.64 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007305203240911953		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.007305203240911953 | validation: 0.018824094539097638]
	TIME [epoch: 2.64 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0071734260278029935		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.0071734260278029935 | validation: 0.0246460507425775]
	TIME [epoch: 2.65 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007649065619742319		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.007649065619742319 | validation: 0.020047376270642982]
	TIME [epoch: 2.64 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006487987617439206		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.006487987617439206 | validation: 0.020816379119796294]
	TIME [epoch: 2.65 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005783750452819327		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.005783750452819327 | validation: 0.02451592251546524]
	TIME [epoch: 2.65 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006267666620078378		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.006267666620078378 | validation: 0.02098456338362501]
	TIME [epoch: 2.65 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005284357865024114		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.005284357865024114 | validation: 0.01819294894168846]
	TIME [epoch: 2.66 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0053736432192815495		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.0053736432192815495 | validation: 0.023711591153827094]
	TIME [epoch: 2.65 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00663982440407114		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.00663982440407114 | validation: 0.021177108582634985]
	TIME [epoch: 2.64 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00890548657668991		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.00890548657668991 | validation: 0.024676229064632883]
	TIME [epoch: 2.65 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012493155304457784		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.012493155304457784 | validation: 0.02219553880718469]
	TIME [epoch: 2.65 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009078563995119608		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.009078563995119608 | validation: 0.02220167339173611]
	TIME [epoch: 2.65 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006065702977078198		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.006065702977078198 | validation: 0.02078313340247322]
	TIME [epoch: 2.64 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006500715192071045		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.006500715192071045 | validation: 0.01983388445169808]
	TIME [epoch: 173 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006123818753553305		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.006123818753553305 | validation: 0.02233164858542691]
	TIME [epoch: 5.68 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006103297799059233		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.006103297799059233 | validation: 0.020473117840724545]
	TIME [epoch: 5.67 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0059827350470921335		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.0059827350470921335 | validation: 0.018999884637066422]
	TIME [epoch: 5.67 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005519019457620329		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.005519019457620329 | validation: 0.020419832407443418]
	TIME [epoch: 5.67 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006717534326653349		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.006717534326653349 | validation: 0.021696564953886402]
	TIME [epoch: 5.67 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007962653508134862		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.007962653508134862 | validation: 0.021538004793031876]
	TIME [epoch: 5.67 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008718332418802707		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.008718332418802707 | validation: 0.02235148324680916]
	TIME [epoch: 5.67 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0067972046364670425		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.0067972046364670425 | validation: 0.019437484945140637]
	TIME [epoch: 5.67 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005812830850263062		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.005812830850263062 | validation: 0.026273266613829994]
	TIME [epoch: 5.67 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007206386017884075		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.007206386017884075 | validation: 0.022288250300601775]
	TIME [epoch: 5.7 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008468181325008253		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.008468181325008253 | validation: 0.02158156746149592]
	TIME [epoch: 5.69 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006543291428332705		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.006543291428332705 | validation: 0.018928760242171674]
	TIME [epoch: 5.67 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005519395021039215		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.005519395021039215 | validation: 0.023174638372841807]
	TIME [epoch: 5.66 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006051454042687221		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.006051454042687221 | validation: 0.019454158868857277]
	TIME [epoch: 5.67 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0064214152826983905		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.0064214152826983905 | validation: 0.017381504569181163]
	TIME [epoch: 5.67 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006108607237934045		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.006108607237934045 | validation: 0.019894072815925836]
	TIME [epoch: 5.67 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006432909131234348		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.006432909131234348 | validation: 0.019095236495038904]
	TIME [epoch: 5.66 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00665115384426829		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.00665115384426829 | validation: 0.023625068657683757]
	TIME [epoch: 5.67 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005954236653826146		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.005954236653826146 | validation: 0.017431312607564566]
	TIME [epoch: 5.67 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0064520258087389796		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.0064520258087389796 | validation: 0.024888969174060683]
	TIME [epoch: 5.66 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005201967849710323		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.005201967849710323 | validation: 0.019986731308799147]
	TIME [epoch: 5.67 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005787061489065897		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.005787061489065897 | validation: 0.02462767729763148]
	TIME [epoch: 5.67 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008518350035810425		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.008518350035810425 | validation: 0.01879662021421935]
	TIME [epoch: 5.66 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008001120963759555		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.008001120963759555 | validation: 0.02244747127221616]
	TIME [epoch: 5.67 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00513447136784295		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.00513447136784295 | validation: 0.016928565582935717]
	TIME [epoch: 5.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_1026.pth
	Model improved!!!
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006072810329298692		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.006072810329298692 | validation: 0.020890168780778096]
	TIME [epoch: 5.69 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00647180189396765		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.00647180189396765 | validation: 0.02355097914008432]
	TIME [epoch: 5.69 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006318370041494137		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.006318370041494137 | validation: 0.01841444739659718]
	TIME [epoch: 5.67 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006636838896948212		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.006636838896948212 | validation: 0.0226306177508402]
	TIME [epoch: 5.68 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0065312022783438044		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.0065312022783438044 | validation: 0.018280470880377233]
	TIME [epoch: 5.67 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006517789854972653		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.006517789854972653 | validation: 0.02249411269259558]
	TIME [epoch: 5.68 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006124189293551676		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.006124189293551676 | validation: 0.0169389838458739]
	TIME [epoch: 5.67 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0072433694634812555		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.0072433694634812555 | validation: 0.02419381996248714]
	TIME [epoch: 5.67 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006169243952619652		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.006169243952619652 | validation: 0.021046288532158244]
	TIME [epoch: 5.67 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005814124973012276		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.005814124973012276 | validation: 0.02140007938850921]
	TIME [epoch: 5.67 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007603754367895539		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.007603754367895539 | validation: 0.024980086601599373]
	TIME [epoch: 5.67 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008738864154200979		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.008738864154200979 | validation: 0.023080118870249168]
	TIME [epoch: 5.68 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009512011247970091		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.009512011247970091 | validation: 0.023731897381964785]
	TIME [epoch: 5.68 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007168996476149556		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.007168996476149556 | validation: 0.016694887944382463]
	TIME [epoch: 5.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_1040.pth
	Model improved!!!
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005979632340118086		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.005979632340118086 | validation: 0.025184386535614257]
	TIME [epoch: 5.67 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006597174833499764		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.006597174833499764 | validation: 0.022814670057261843]
	TIME [epoch: 5.67 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00591768918027912		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.00591768918027912 | validation: 0.02230397930076853]
	TIME [epoch: 5.67 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005382209804013633		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.005382209804013633 | validation: 0.021272630155211317]
	TIME [epoch: 5.69 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006198145564884283		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.006198145564884283 | validation: 0.020300603967523468]
	TIME [epoch: 5.67 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006900212740529286		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.006900212740529286 | validation: 0.02258007006577728]
	TIME [epoch: 5.68 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008085559144730954		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.008085559144730954 | validation: 0.017684948521703294]
	TIME [epoch: 5.68 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006030748175127598		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.006030748175127598 | validation: 0.025306226751888084]
	TIME [epoch: 5.67 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0055737642101330856		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.0055737642101330856 | validation: 0.02089185733113058]
	TIME [epoch: 5.67 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005811434976574974		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.005811434976574974 | validation: 0.017329617769404393]
	TIME [epoch: 5.68 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006274970390930166		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.006274970390930166 | validation: 0.01974899320710788]
	TIME [epoch: 5.67 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0053016766454314634		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.0053016766454314634 | validation: 0.01947079170419316]
	TIME [epoch: 5.67 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005473443569141541		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.005473443569141541 | validation: 0.02190301407517331]
	TIME [epoch: 5.68 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0059049443603724555		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.0059049443603724555 | validation: 0.020698460945783072]
	TIME [epoch: 5.67 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0058479296756912815		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.0058479296756912815 | validation: 0.02615416650325495]
	TIME [epoch: 5.67 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007059942476313497		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.007059942476313497 | validation: 0.018729727176262267]
	TIME [epoch: 5.67 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005562652118459739		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.005562652118459739 | validation: 0.02321070439805725]
	TIME [epoch: 5.67 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005901305479233185		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.005901305479233185 | validation: 0.022830663320003098]
	TIME [epoch: 5.67 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0059888522535153		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.0059888522535153 | validation: 0.017986072620295303]
	TIME [epoch: 5.68 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007049074438707912		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.007049074438707912 | validation: 0.026590261935187588]
	TIME [epoch: 5.67 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007281624317123696		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.007281624317123696 | validation: 0.023153713419365077]
	TIME [epoch: 5.68 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006465761285620395		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.006465761285620395 | validation: 0.021530864178801946]
	TIME [epoch: 5.67 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007972155985602304		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.007972155985602304 | validation: 0.021203611449792537]
	TIME [epoch: 5.67 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006087532774071463		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.006087532774071463 | validation: 0.023166426742383096]
	TIME [epoch: 5.67 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006148625811892192		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.006148625811892192 | validation: 0.020402892897212057]
	TIME [epoch: 5.67 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0057007596451332685		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.0057007596451332685 | validation: 0.023364871769388507]
	TIME [epoch: 5.67 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00603313298132053		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.00603313298132053 | validation: 0.017852992746151732]
	TIME [epoch: 5.67 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00695437214027157		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.00695437214027157 | validation: 0.02390536394282318]
	TIME [epoch: 5.67 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005778976086572223		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.005778976086572223 | validation: 0.0214811282998286]
	TIME [epoch: 5.67 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006084633697324831		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.006084633697324831 | validation: 0.023627728392847436]
	TIME [epoch: 5.67 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0060143878074444684		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.0060143878074444684 | validation: 0.024605462854995864]
	TIME [epoch: 5.68 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007362359565435612		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.007362359565435612 | validation: 0.025231353468205077]
	TIME [epoch: 5.67 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007594153602240135		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.007594153602240135 | validation: 0.021512539923718724]
	TIME [epoch: 5.67 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006264648415469816		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.006264648415469816 | validation: 0.01605094013599048]
	TIME [epoch: 5.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_1074.pth
	Model improved!!!
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005571153456874663		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.005571153456874663 | validation: 0.023073144985381513]
	TIME [epoch: 5.68 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005984708804670664		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.005984708804670664 | validation: 0.0231085284720442]
	TIME [epoch: 5.67 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005882914425704047		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.005882914425704047 | validation: 0.02118946771336837]
	TIME [epoch: 5.69 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006397072371235817		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.006397072371235817 | validation: 0.020848571357590565]
	TIME [epoch: 5.67 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0053926231137952875		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.0053926231137952875 | validation: 0.019633766871649805]
	TIME [epoch: 5.67 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005699012933822034		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.005699012933822034 | validation: 0.01891219974978239]
	TIME [epoch: 5.67 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005780445552412748		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.005780445552412748 | validation: 0.020948368675215035]
	TIME [epoch: 5.67 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005383613756253689		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.005383613756253689 | validation: 0.01824143792618078]
	TIME [epoch: 5.67 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005331763241276477		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.005331763241276477 | validation: 0.01889163884118881]
	TIME [epoch: 5.67 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005405542827162795		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.005405542827162795 | validation: 0.019340744271046362]
	TIME [epoch: 5.67 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005467827169948913		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.005467827169948913 | validation: 0.018645483928772168]
	TIME [epoch: 5.67 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005666793402410031		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.005666793402410031 | validation: 0.020767255410557975]
	TIME [epoch: 5.67 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00652215909167645		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.00652215909167645 | validation: 0.021558196596785307]
	TIME [epoch: 5.67 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0080916002851954		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.0080916002851954 | validation: 0.021476901372572778]
	TIME [epoch: 5.67 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0069063474879404564		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.0069063474879404564 | validation: 0.021415670000571298]
	TIME [epoch: 5.66 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005311474405300645		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.005311474405300645 | validation: 0.02015606056810373]
	TIME [epoch: 5.67 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005962973107065998		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.005962973107065998 | validation: 0.018018566961021134]
	TIME [epoch: 5.66 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005918981287938132		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.005918981287938132 | validation: 0.02056085009420009]
	TIME [epoch: 5.67 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005939619099588998		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.005939619099588998 | validation: 0.022658921830185898]
	TIME [epoch: 5.67 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005423838758901474		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.005423838758901474 | validation: 0.025348513997673395]
	TIME [epoch: 5.67 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006459790585855505		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.006459790585855505 | validation: 0.021634958685443708]
	TIME [epoch: 5.67 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007781021992818682		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.007781021992818682 | validation: 0.020117769195055347]
	TIME [epoch: 5.67 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006107348969874431		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.006107348969874431 | validation: 0.016986652829150375]
	TIME [epoch: 5.66 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005296207323931197		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.005296207323931197 | validation: 0.02449009406963587]
	TIME [epoch: 5.67 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00590267674169622		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.00590267674169622 | validation: 0.018176007416928475]
	TIME [epoch: 5.67 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005629051857837391		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.005629051857837391 | validation: 0.023823643661432515]
	TIME [epoch: 5.68 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006046862230334675		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.006046862230334675 | validation: 0.02429113928473553]
	TIME [epoch: 5.66 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005840639297927117		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.005840639297927117 | validation: 0.020565468409723155]
	TIME [epoch: 5.67 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006614794946922433		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.006614794946922433 | validation: 0.02137288196395433]
	TIME [epoch: 5.67 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005481383344662667		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.005481383344662667 | validation: 0.019977572918038533]
	TIME [epoch: 5.66 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005746595120681897		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.005746595120681897 | validation: 0.02236979896555871]
	TIME [epoch: 5.67 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006018664287113349		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.006018664287113349 | validation: 0.02220505983062118]
	TIME [epoch: 5.67 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005994793909689991		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.005994793909689991 | validation: 0.022966864922004296]
	TIME [epoch: 5.66 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00586454286014197		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.00586454286014197 | validation: 0.020790759943098425]
	TIME [epoch: 5.67 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005571114296668094		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.005571114296668094 | validation: 0.019428654715707528]
	TIME [epoch: 5.66 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006139236867634639		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.006139236867634639 | validation: 0.02176775931269577]
	TIME [epoch: 5.66 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006026375296761033		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.006026375296761033 | validation: 0.022403740370079563]
	TIME [epoch: 5.7 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006365885352870691		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.006365885352870691 | validation: 0.01980867975567635]
	TIME [epoch: 5.66 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00473716226383704		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.00473716226383704 | validation: 0.019839382889112113]
	TIME [epoch: 5.67 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005807671343054257		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.005807671343054257 | validation: 0.021293449838952674]
	TIME [epoch: 5.66 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005042932655934547		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.005042932655934547 | validation: 0.021052164217725336]
	TIME [epoch: 5.67 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005214012761983502		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.005214012761983502 | validation: 0.020872248017297913]
	TIME [epoch: 5.67 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004920698401753738		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.004920698401753738 | validation: 0.021463871777237922]
	TIME [epoch: 5.67 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004873524693289062		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.004873524693289062 | validation: 0.018333789806485767]
	TIME [epoch: 5.67 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0050063789159266545		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.0050063789159266545 | validation: 0.020796746339858688]
	TIME [epoch: 5.67 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005213910145439642		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.005213910145439642 | validation: 0.02113112702943311]
	TIME [epoch: 5.67 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005592654588073476		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.005592654588073476 | validation: 0.023651768700531418]
	TIME [epoch: 5.67 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00602030967778423		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.00602030967778423 | validation: 0.023027040597128092]
	TIME [epoch: 5.67 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006357765857799838		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.006357765857799838 | validation: 0.020267516856714452]
	TIME [epoch: 5.67 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005958723494407972		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.005958723494407972 | validation: 0.023957305367187355]
	TIME [epoch: 5.67 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006572169015918186		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.006572169015918186 | validation: 0.019136592838063984]
	TIME [epoch: 5.67 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004787129439940732		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.004787129439940732 | validation: 0.01840233743321981]
	TIME [epoch: 5.67 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005143892936143424		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.005143892936143424 | validation: 0.024575866654581037]
	TIME [epoch: 5.67 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0057157584103651885		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.0057157584103651885 | validation: 0.02140240603487196]
	TIME [epoch: 5.67 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005097747059903117		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.005097747059903117 | validation: 0.02096838037333253]
	TIME [epoch: 5.67 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0068606332627475595		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.0068606332627475595 | validation: 0.02329282722328472]
	TIME [epoch: 5.67 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00593523192905127		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.00593523192905127 | validation: 0.02209421946464274]
	TIME [epoch: 5.67 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00508902140664065		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.00508902140664065 | validation: 0.020351556915541538]
	TIME [epoch: 5.66 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004504829980969103		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.004504829980969103 | validation: 0.02199417964808631]
	TIME [epoch: 5.67 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004623358843648949		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.004623358843648949 | validation: 0.02001346648201459]
	TIME [epoch: 5.66 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005631163296625408		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.005631163296625408 | validation: 0.01712546569172384]
	TIME [epoch: 5.67 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0056848826543037315		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.0056848826543037315 | validation: 0.021987473718141328]
	TIME [epoch: 5.66 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004758205982593482		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.004758205982593482 | validation: 0.020437430448024454]
	TIME [epoch: 5.67 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005689904473662928		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.005689904473662928 | validation: 0.022294938234506845]
	TIME [epoch: 5.66 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005586949748170813		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.005586949748170813 | validation: 0.0216520004197461]
	TIME [epoch: 5.67 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0052815266232361414		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.0052815266232361414 | validation: 0.01825607992274365]
	TIME [epoch: 5.66 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004993199333170071		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.004993199333170071 | validation: 0.018993129695071644]
	TIME [epoch: 5.67 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005434214063326286		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.005434214063326286 | validation: 0.017789094288697026]
	TIME [epoch: 5.66 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005911800257507542		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.005911800257507542 | validation: 0.02373812040029072]
	TIME [epoch: 5.67 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00788575632774927		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.00788575632774927 | validation: 0.022589261443794265]
	TIME [epoch: 5.67 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005328960382412457		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.005328960382412457 | validation: 0.020818815607558806]
	TIME [epoch: 5.67 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004847995402833356		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.004847995402833356 | validation: 0.01987140860382546]
	TIME [epoch: 5.66 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004935535460197296		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.004935535460197296 | validation: 0.016491538068077516]
	TIME [epoch: 5.67 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005849216076946844		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.005849216076946844 | validation: 0.019015036993898796]
	TIME [epoch: 5.67 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006301573905222711		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.006301573905222711 | validation: 0.021545541180188522]
	TIME [epoch: 5.67 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009156711042421717		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.009156711042421717 | validation: 0.021916045202278345]
	TIME [epoch: 5.66 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005001267420288768		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.005001267420288768 | validation: 0.021284864048699694]
	TIME [epoch: 5.67 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005430466889796246		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.005430466889796246 | validation: 0.015911174989354515]
	TIME [epoch: 5.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_1152.pth
	Model improved!!!
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005368060199340794		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.005368060199340794 | validation: 0.02238441504826867]
	TIME [epoch: 5.67 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0059941847229455685		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.0059941847229455685 | validation: 0.018630166814754722]
	TIME [epoch: 5.66 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0053396880754261146		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.0053396880754261146 | validation: 0.016837652948926973]
	TIME [epoch: 5.67 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004436188156973386		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.004436188156973386 | validation: 0.020899088989909543]
	TIME [epoch: 5.66 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004474796911737029		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.004474796911737029 | validation: 0.02286222988147868]
	TIME [epoch: 5.67 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005209472098066066		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.005209472098066066 | validation: 0.01754184291570283]
	TIME [epoch: 5.67 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005454141965447808		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.005454141965447808 | validation: 0.01844605883799766]
	TIME [epoch: 5.66 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004682202782340981		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.004682202782340981 | validation: 0.022774754531669297]
	TIME [epoch: 5.67 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006101654722737251		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.006101654722737251 | validation: 0.019956653897067037]
	TIME [epoch: 5.68 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004924376410735704		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.004924376410735704 | validation: 0.01995870692468861]
	TIME [epoch: 5.67 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005180003261960164		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.005180003261960164 | validation: 0.0185336181688432]
	TIME [epoch: 5.67 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005194968430893982		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.005194968430893982 | validation: 0.022060237193325352]
	TIME [epoch: 5.66 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005664546203660235		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.005664546203660235 | validation: 0.0201886393629353]
	TIME [epoch: 5.67 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005326663994750505		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.005326663994750505 | validation: 0.02131020032875689]
	TIME [epoch: 5.67 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005486087420496091		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.005486087420496091 | validation: 0.022900532654585727]
	TIME [epoch: 5.67 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006136883328722811		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.006136883328722811 | validation: 0.019436605449078072]
	TIME [epoch: 5.66 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005414321280993233		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.005414321280993233 | validation: 0.021245578096333918]
	TIME [epoch: 5.67 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00540087457824622		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.00540087457824622 | validation: 0.022125108239973268]
	TIME [epoch: 5.66 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00574486730882712		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.00574486730882712 | validation: 0.020015228031040246]
	TIME [epoch: 5.67 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005572502989503982		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.005572502989503982 | validation: 0.020748962937255113]
	TIME [epoch: 5.66 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005467946507279855		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.005467946507279855 | validation: 0.02305583050708231]
	TIME [epoch: 5.67 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005794515230212984		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.005794515230212984 | validation: 0.019426156690993592]
	TIME [epoch: 5.67 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004621610054646703		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.004621610054646703 | validation: 0.019207079929444426]
	TIME [epoch: 5.67 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0050677239759233175		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.0050677239759233175 | validation: 0.024736000449520512]
	TIME [epoch: 5.67 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005326634418889134		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.005326634418889134 | validation: 0.01813471139734133]
	TIME [epoch: 5.67 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005921746295202206		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.005921746295202206 | validation: 0.019304458393167467]
	TIME [epoch: 5.66 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004670280873277084		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.004670280873277084 | validation: 0.01819179870450386]
	TIME [epoch: 5.67 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005146836593893409		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.005146836593893409 | validation: 0.02161639628779899]
	TIME [epoch: 5.66 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005416298436174487		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.005416298436174487 | validation: 0.021566872623085744]
	TIME [epoch: 5.66 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00482418764853975		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.00482418764853975 | validation: 0.01968194152167049]
	TIME [epoch: 5.66 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005239974872048914		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.005239974872048914 | validation: 0.020877787235034397]
	TIME [epoch: 5.66 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004967447618526455		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.004967447618526455 | validation: 0.020640172521526935]
	TIME [epoch: 5.67 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005714442440072214		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.005714442440072214 | validation: 0.02396764934079837]
	TIME [epoch: 5.66 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008033609743584382		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.008033609743584382 | validation: 0.02308648751975159]
	TIME [epoch: 5.66 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005331692956124496		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.005331692956124496 | validation: 0.022626563740505614]
	TIME [epoch: 5.67 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004333922721134318		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.004333922721134318 | validation: 0.01843788081023474]
	TIME [epoch: 5.66 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005366670476892845		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.005366670476892845 | validation: 0.023398192847062217]
	TIME [epoch: 5.66 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004724590611844959		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.004724590611844959 | validation: 0.015354913226328516]
	TIME [epoch: 5.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_1190.pth
	Model improved!!!
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005023384435114984		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.005023384435114984 | validation: 0.021682316893040857]
	TIME [epoch: 5.66 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004532972918861491		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.004532972918861491 | validation: 0.02140796916892257]
	TIME [epoch: 5.66 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005487321617667964		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.005487321617667964 | validation: 0.017220351972200343]
	TIME [epoch: 5.67 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005187417878070164		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.005187417878070164 | validation: 0.019929756782171095]
	TIME [epoch: 5.69 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004724977836369508		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.004724977836369508 | validation: 0.020862643701378827]
	TIME [epoch: 5.67 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005008726166505811		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.005008726166505811 | validation: 0.019700638263915805]
	TIME [epoch: 5.66 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0047891107822224345		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.0047891107822224345 | validation: 0.019668147139976878]
	TIME [epoch: 5.66 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004595609515130368		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.004595609515130368 | validation: 0.016107702897862553]
	TIME [epoch: 5.67 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005343209554736335		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.005343209554736335 | validation: 0.01774600493357431]
	TIME [epoch: 5.67 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004994718495314494		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.004994718495314494 | validation: 0.021219573034391773]
	TIME [epoch: 5.67 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0056099011909299295		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.0056099011909299295 | validation: 0.015210193132029082]
	TIME [epoch: 5.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_1201.pth
	Model improved!!!
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004942669563986674		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.004942669563986674 | validation: 0.01766692665534091]
	TIME [epoch: 5.67 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005835976846706808		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.005835976846706808 | validation: 0.02279782848321851]
	TIME [epoch: 5.66 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005661674433646504		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.005661674433646504 | validation: 0.02081537597180897]
	TIME [epoch: 5.66 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004712965556526565		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.004712965556526565 | validation: 0.019731447830297445]
	TIME [epoch: 5.67 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00516833733715109		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.00516833733715109 | validation: 0.019313054843909917]
	TIME [epoch: 5.66 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0052455980845054605		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.0052455980845054605 | validation: 0.02066376612468024]
	TIME [epoch: 5.67 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005373583598413516		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.005373583598413516 | validation: 0.019652155169968457]
	TIME [epoch: 5.67 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0064499309684719576		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.0064499309684719576 | validation: 0.019987923806840037]
	TIME [epoch: 5.67 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0050082659726924895		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.0050082659726924895 | validation: 0.022030584545649313]
	TIME [epoch: 5.69 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004943166210232652		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.004943166210232652 | validation: 0.02144462644246017]
	TIME [epoch: 5.66 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004992489132379612		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.004992489132379612 | validation: 0.019431081080826806]
	TIME [epoch: 5.67 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0047476099668175165		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.0047476099668175165 | validation: 0.021867321712758982]
	TIME [epoch: 5.66 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005200414953928734		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.005200414953928734 | validation: 0.018890233100075083]
	TIME [epoch: 5.66 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0049190008441237065		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.0049190008441237065 | validation: 0.01959853809436607]
	TIME [epoch: 5.66 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005548955841871841		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.005548955841871841 | validation: 0.02127001220284158]
	TIME [epoch: 5.67 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0056099003573554665		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.0056099003573554665 | validation: 0.019429485573919825]
	TIME [epoch: 5.67 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005282997232719977		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.005282997232719977 | validation: 0.02123872731897332]
	TIME [epoch: 5.67 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0048703264544770145		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.0048703264544770145 | validation: 0.01998409427882828]
	TIME [epoch: 5.67 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005210490367307379		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.005210490367307379 | validation: 0.017727514159653093]
	TIME [epoch: 5.67 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005936237757775782		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.005936237757775782 | validation: 0.021550439733634588]
	TIME [epoch: 5.67 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007015414933113389		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.007015414933113389 | validation: 0.01826141241638234]
	TIME [epoch: 5.67 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00505088573715238		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.00505088573715238 | validation: 0.017565574275567132]
	TIME [epoch: 5.67 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00537240807531532		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.00537240807531532 | validation: 0.017569868231574304]
	TIME [epoch: 5.67 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004695598434075843		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.004695598434075843 | validation: 0.020520959520441265]
	TIME [epoch: 5.66 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0051479843566413774		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.0051479843566413774 | validation: 0.018865561568590396]
	TIME [epoch: 5.67 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00518924601044074		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.00518924601044074 | validation: 0.01861301919731231]
	TIME [epoch: 5.67 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004968702023977038		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.004968702023977038 | validation: 0.018092704133066708]
	TIME [epoch: 5.67 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0051876023078933865		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.0051876023078933865 | validation: 0.020110179267527718]
	TIME [epoch: 5.66 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004401871706737825		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.004401871706737825 | validation: 0.01802744200883252]
	TIME [epoch: 5.67 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005786189693988744		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.005786189693988744 | validation: 0.021702696729501137]
	TIME [epoch: 5.67 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005247032549833919		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.005247032549833919 | validation: 0.023048545227080275]
	TIME [epoch: 5.66 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006535247441777699		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.006535247441777699 | validation: 0.0171649301401413]
	TIME [epoch: 5.66 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005392187839714924		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.005392187839714924 | validation: 0.01730614883780528]
	TIME [epoch: 5.67 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005340897520925216		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.005340897520925216 | validation: 0.02557028022943686]
	TIME [epoch: 5.67 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005688621677992953		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.005688621677992953 | validation: 0.021197486234173057]
	TIME [epoch: 5.67 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00480647697785356		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.00480647697785356 | validation: 0.018503849964388]
	TIME [epoch: 5.67 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004705409729624981		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.004705409729624981 | validation: 0.01937853217989327]
	TIME [epoch: 5.67 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0058079968326796875		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.0058079968326796875 | validation: 0.022812212409443013]
	TIME [epoch: 5.67 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006167671304799008		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.006167671304799008 | validation: 0.018769495219781475]
	TIME [epoch: 5.67 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005512800888107079		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.005512800888107079 | validation: 0.0194120637843241]
	TIME [epoch: 5.66 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005666095905358834		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.005666095905358834 | validation: 0.020697725540437417]
	TIME [epoch: 5.67 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005678589636569318		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.005678589636569318 | validation: 0.02033263124148682]
	TIME [epoch: 5.66 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004986539682458908		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.004986539682458908 | validation: 0.019050883657566842]
	TIME [epoch: 5.67 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004764137822473658		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.004764137822473658 | validation: 0.01825313340726864]
	TIME [epoch: 5.66 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0049579501636450845		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.0049579501636450845 | validation: 0.019669964773418248]
	TIME [epoch: 5.66 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005096654326183565		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.005096654326183565 | validation: 0.019412345918138597]
	TIME [epoch: 5.66 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005209361614001338		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.005209361614001338 | validation: 0.02073637312251636]
	TIME [epoch: 5.66 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004680437637128372		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.004680437637128372 | validation: 0.017317498641315664]
	TIME [epoch: 5.66 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005050349041908537		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.005050349041908537 | validation: 0.02034208353233562]
	TIME [epoch: 5.67 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005616597462091162		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.005616597462091162 | validation: 0.019460500791069247]
	TIME [epoch: 5.66 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005764112718128452		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.005764112718128452 | validation: 0.02042429603221202]
	TIME [epoch: 5.67 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004538093652901745		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.004538093652901745 | validation: 0.019513338651365775]
	TIME [epoch: 5.67 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004248017150658472		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.004248017150658472 | validation: 0.021104062078573484]
	TIME [epoch: 5.66 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004813963662484259		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.004813963662484259 | validation: 0.02006195532755849]
	TIME [epoch: 5.66 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004338075172556661		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.004338075172556661 | validation: 0.019428355034298652]
	TIME [epoch: 5.66 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005190626980966424		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.005190626980966424 | validation: 0.023267064109282976]
	TIME [epoch: 5.66 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004853934249487697		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.004853934249487697 | validation: 0.02286611848714705]
	TIME [epoch: 5.66 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004873262524475353		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.004873262524475353 | validation: 0.019529323242059405]
	TIME [epoch: 5.66 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00540943054798565		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.00540943054798565 | validation: 0.019998686557822155]
	TIME [epoch: 5.66 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005139799330569317		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.005139799330569317 | validation: 0.020559883522606494]
	TIME [epoch: 5.69 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005128196421224462		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.005128196421224462 | validation: 0.017715580885440296]
	TIME [epoch: 5.66 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004672233245470674		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.004672233245470674 | validation: 0.01698199529043255]
	TIME [epoch: 5.66 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004538513558374107		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.004538513558374107 | validation: 0.020233318528233513]
	TIME [epoch: 5.66 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004323132284448435		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.004323132284448435 | validation: 0.02009139053062028]
	TIME [epoch: 5.66 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0051456936211534375		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.0051456936211534375 | validation: 0.01708220533962005]
	TIME [epoch: 5.66 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0060494786044310266		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.0060494786044310266 | validation: 0.02301580334264415]
	TIME [epoch: 5.66 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006770983321437745		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.006770983321437745 | validation: 0.01765258543786613]
	TIME [epoch: 5.67 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005028875192498108		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.005028875192498108 | validation: 0.020582093315033214]
	TIME [epoch: 5.66 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005389708057793963		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.005389708057793963 | validation: 0.019980942765528465]
	TIME [epoch: 5.66 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00515885524854693		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.00515885524854693 | validation: 0.01879632583920874]
	TIME [epoch: 5.66 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004523397253483936		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.004523397253483936 | validation: 0.018585193779878495]
	TIME [epoch: 5.67 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004483499056652394		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.004483499056652394 | validation: 0.016266548788998105]
	TIME [epoch: 5.66 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004854111358501179		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.004854111358501179 | validation: 0.020161486196320647]
	TIME [epoch: 5.66 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005270264403870164		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.005270264403870164 | validation: 0.019262052508304775]
	TIME [epoch: 5.66 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004928290782605041		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.004928290782605041 | validation: 0.02145109885449035]
	TIME [epoch: 5.66 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004248591980298464		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.004248591980298464 | validation: 0.020047564710225854]
	TIME [epoch: 5.66 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0048554455453113455		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.0048554455453113455 | validation: 0.020596542570995403]
	TIME [epoch: 5.66 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004676025763876308		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.004676025763876308 | validation: 0.018604801011151428]
	TIME [epoch: 5.66 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004812039809167226		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.004812039809167226 | validation: 0.01684736822239269]
	TIME [epoch: 5.65 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00521005912323125		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.00521005912323125 | validation: 0.020205373434445728]
	TIME [epoch: 5.67 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004514198316922506		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.004514198316922506 | validation: 0.022496718658536353]
	TIME [epoch: 5.67 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0049073522962375235		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.0049073522962375235 | validation: 0.01871034086593434]
	TIME [epoch: 5.66 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004299172237618305		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.004299172237618305 | validation: 0.018820689415251324]
	TIME [epoch: 5.67 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00488230303695999		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.00488230303695999 | validation: 0.016970342645451088]
	TIME [epoch: 5.66 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005222344261422365		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.005222344261422365 | validation: 0.017376646044162846]
	TIME [epoch: 5.67 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005052119463943005		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.005052119463943005 | validation: 0.019611066960266224]
	TIME [epoch: 5.67 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004815239596577254		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.004815239596577254 | validation: 0.02056351801360721]
	TIME [epoch: 5.66 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005015265836951568		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.005015265836951568 | validation: 0.019831801412615237]
	TIME [epoch: 5.66 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005168048780897669		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.005168048780897669 | validation: 0.023356824419422953]
	TIME [epoch: 5.67 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005907877642896122		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.005907877642896122 | validation: 0.019702146174832694]
	TIME [epoch: 5.66 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0051142239119117685		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.0051142239119117685 | validation: 0.01789414345756296]
	TIME [epoch: 5.68 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005308822348148279		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.005308822348148279 | validation: 0.021079931648343486]
	TIME [epoch: 5.66 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005159923032221363		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.005159923032221363 | validation: 0.01905143232986528]
	TIME [epoch: 5.68 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00464144349189912		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.00464144349189912 | validation: 0.019000759201925944]
	TIME [epoch: 5.66 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005129887816187395		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.005129887816187395 | validation: 0.017955733652896776]
	TIME [epoch: 5.66 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0043419260258961345		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.0043419260258961345 | validation: 0.022428301988068146]
	TIME [epoch: 5.66 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004854299577542749		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.004854299577542749 | validation: 0.022758253823607213]
	TIME [epoch: 5.66 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0055170412707288554		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.0055170412707288554 | validation: 0.02169776406395657]
	TIME [epoch: 5.66 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005307073108841602		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.005307073108841602 | validation: 0.026001731204427548]
	TIME [epoch: 5.67 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004782614648761816		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.004782614648761816 | validation: 0.02177829771987766]
	TIME [epoch: 5.66 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005301109469488763		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.005301109469488763 | validation: 0.017401777341226034]
	TIME [epoch: 5.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_3_v_mmd1_1302.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 4358.178 seconds.
