Args:
Namespace(name='model_phi1_4a_distortion_v2_2_v_mmd1', outdir='out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_2/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_2/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 457320687

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.937592887283354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.937592887283354 | validation: 4.256886433343389]
	TIME [epoch: 171 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.324252798354103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.324252798354103 | validation: 4.261917546711067]
	TIME [epoch: 0.806 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3373965155899254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3373965155899254 | validation: 3.9709398087595704]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.891425508775772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.891425508775772 | validation: 3.6074494511492343]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5709687939455312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5709687939455312 | validation: 3.401826373282002]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2651508041143065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2651508041143065 | validation: 3.3486375822136925]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.199532824533758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.199532824533758 | validation: 3.3116610505853674]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3036373893318696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3036373893318696 | validation: 2.9875936081219083]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8783321045062895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8783321045062895 | validation: 3.03618076953229]
	TIME [epoch: 0.69 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9213604074840944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9213604074840944 | validation: 3.051806009263662]
	TIME [epoch: 0.687 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1101351753385176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1101351753385176 | validation: 2.6096512233220555]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8573954773972998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8573954773972998 | validation: 2.670849911587636]
	TIME [epoch: 0.692 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.618964940573692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.618964940573692 | validation: 2.555082357838916]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5398864134253853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5398864134253853 | validation: 2.4412469678930484]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5490746560954325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5490746560954325 | validation: 2.3936509674406863]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5132615701762064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5132615701762064 | validation: 2.2788476753460842]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.494128469301312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.494128469301312 | validation: 2.2522048954366936]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.487484157371101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.487484157371101 | validation: 2.2040109922386337]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.470670209502636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.470670209502636 | validation: 2.1803481393581494]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4611206957845306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4611206957845306 | validation: 2.129694702966956]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4625111269027258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4625111269027258 | validation: 2.122524251182052]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4503896434868524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4503896434868524 | validation: 2.071962268176527]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4434893245032878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4434893245032878 | validation: 2.1690818739709283]
	TIME [epoch: 0.693 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4782288799080965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4782288799080965 | validation: 2.1744797186522877]
	TIME [epoch: 0.691 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5844939986589988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5844939986589988 | validation: 2.3566371432440643]
	TIME [epoch: 0.69 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5642623058529188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5642623058529188 | validation: 2.0121635493808974]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4188006486119569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4188006486119569 | validation: 2.0277070295819173]
	TIME [epoch: 0.692 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4519996051339512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4519996051339512 | validation: 2.1132633020083333]
	TIME [epoch: 0.69 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4707630216552017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4707630216552017 | validation: 1.9672203919131193]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.405944096736112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.405944096736112 | validation: 1.9305768606522862]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.384216866059526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.384216866059526 | validation: 1.9333613180470544]
	TIME [epoch: 0.694 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3845551799246487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3845551799246487 | validation: 1.8996518432060363]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3792675138820858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3792675138820858 | validation: 1.9612405073355235]
	TIME [epoch: 0.696 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3885931798402549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3885931798402549 | validation: 1.874776375007678]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3952650356725338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3952650356725338 | validation: 2.0874080795326657]
	TIME [epoch: 0.69 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4470613381565276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4470613381565276 | validation: 1.8707591258722418]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3898262054473196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3898262054473196 | validation: 1.8734354792525534]
	TIME [epoch: 0.692 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3483745141524275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3483745141524275 | validation: 1.7628376412157998]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3132881394653253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3132881394653253 | validation: 1.7575035744773204]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2991450756216258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2991450756216258 | validation: 1.7074507969367596]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2866355263060876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2866355263060876 | validation: 1.7299352717097607]
	TIME [epoch: 0.687 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2812580176736506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2812580176736506 | validation: 1.724647495664108]
	TIME [epoch: 0.687 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3656662211507193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3656662211507193 | validation: 2.2472233828062183]
	TIME [epoch: 0.686 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6100376359909654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6100376359909654 | validation: 1.6602451911900538]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2788237145306904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2788237145306904 | validation: 1.602836674364294]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2363193915221071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2363193915221071 | validation: 1.6721258073569771]
	TIME [epoch: 0.697 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2559202626291281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2559202626291281 | validation: 1.565251150781576]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2319156361453638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2319156361453638 | validation: 1.5929718119938796]
	TIME [epoch: 0.692 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2206021383493788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2206021383493788 | validation: 1.489128009658786]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.198121171098463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.198121171098463 | validation: 1.5874987015300666]
	TIME [epoch: 0.694 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.219166535722373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.219166535722373 | validation: 1.4714763175403935]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.267841599306111		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.267841599306111 | validation: 1.7717160141063801]
	TIME [epoch: 0.693 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3441530696766706		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.3441530696766706 | validation: 1.4034325366653488]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1799586618029358		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.1799586618029358 | validation: 1.347345774598995]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.110476932302945		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.110476932302945 | validation: 1.2981056946309817]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0835568421705284		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.0835568421705284 | validation: 1.2144514627414846]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0727395851744959		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.0727395851744959 | validation: 1.4122469855626747]
	TIME [epoch: 0.691 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.132016644673712		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.132016644673712 | validation: 1.3840843155877316]
	TIME [epoch: 0.69 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3911715532072617		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.3911715532072617 | validation: 1.6971254577505812]
	TIME [epoch: 0.692 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2880847372788689		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.2880847372788689 | validation: 1.2050047935273032]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9867107053737594		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 0.9867107053737594 | validation: 1.1253340813896113]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0375631240813428		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.0375631240813428 | validation: 1.0832192653597352]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9547731873262025		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 0.9547731873262025 | validation: 0.893582143850064]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8754003356609766		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 0.8754003356609766 | validation: 0.8252655818025257]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8017958497370671		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 0.8017958497370671 | validation: 0.6990206215755835]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7784855078946873		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 0.7784855078946873 | validation: 2.244174513987474]
	TIME [epoch: 0.69 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6845538098808674		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.6845538098808674 | validation: 0.7481318827224641]
	TIME [epoch: 0.692 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7361914211202648		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 0.7361914211202648 | validation: 0.6962877369743846]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9066905982397273		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 0.9066905982397273 | validation: 1.7220690264604777]
	TIME [epoch: 0.692 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3017050832586288		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.3017050832586288 | validation: 0.8730474616839164]
	TIME [epoch: 0.688 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7751338860552294		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 0.7751338860552294 | validation: 1.1593659013797077]
	TIME [epoch: 0.687 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1926680936431306		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.1926680936431306 | validation: 0.720327667496174]
	TIME [epoch: 0.686 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.68197799737482		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 0.68197799737482 | validation: 0.7602272513657168]
	TIME [epoch: 0.687 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7076450605401297		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 0.7076450605401297 | validation: 0.6208156606705562]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7380616001050612		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 0.7380616001050612 | validation: 0.8171974499962158]
	TIME [epoch: 0.701 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7010914276549991		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 0.7010914276549991 | validation: 0.5547233559957612]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6491752995028597		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 0.6491752995028597 | validation: 0.7898710065415295]
	TIME [epoch: 0.688 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6647080831346084		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 0.6647080831346084 | validation: 0.538069673953261]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7068075168653737		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 0.7068075168653737 | validation: 0.7897170388987598]
	TIME [epoch: 0.69 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6623799101131782		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 0.6623799101131782 | validation: 0.49981586765421293]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6111501268647822		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 0.6111501268647822 | validation: 0.7046257301596044]
	TIME [epoch: 0.694 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6027549570802636		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 0.6027549570802636 | validation: 0.47566695945504966]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.636010004336783		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 0.636010004336783 | validation: 0.7219957806316941]
	TIME [epoch: 0.696 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6046570879927313		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 0.6046570879927313 | validation: 0.4611308311635325]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5915339848235756		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 0.5915339848235756 | validation: 0.7103275124635753]
	TIME [epoch: 0.694 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5782673466836972		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 0.5782673466836972 | validation: 0.4613981347345284]
	TIME [epoch: 0.691 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6347701277931014		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 0.6347701277931014 | validation: 0.6088706404417522]
	TIME [epoch: 0.692 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5193829963301887		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 0.5193829963301887 | validation: 0.4296260150689413]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4771297372957153		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 0.4771297372957153 | validation: 0.6506254215685658]
	TIME [epoch: 0.691 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5193727629805064		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.5193727629805064 | validation: 0.4584448693692183]
	TIME [epoch: 0.696 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7046921692555449		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 0.7046921692555449 | validation: 0.5676924468858466]
	TIME [epoch: 0.695 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5197847394405839		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.5197847394405839 | validation: 0.613701656619115]
	TIME [epoch: 0.694 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4918592303240144		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.4918592303240144 | validation: 0.6199886418919195]
	TIME [epoch: 0.694 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7808866933583951		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 0.7808866933583951 | validation: 0.5243480045693846]
	TIME [epoch: 0.693 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4509768865078415		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 0.4509768865078415 | validation: 0.4528995707601222]
	TIME [epoch: 0.694 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43100231860697674		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 0.43100231860697674 | validation: 0.4124616831901602]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48942766719058306		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.48942766719058306 | validation: 0.9640518344244872]
	TIME [epoch: 0.688 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6701410962058113		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.6701410962058113 | validation: 0.5060112956418149]
	TIME [epoch: 0.69 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7732029655181322		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.7732029655181322 | validation: 0.40111463755146987]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.587709636250698		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.587709636250698 | validation: 0.6496577760792894]
	TIME [epoch: 0.691 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5224218992264039		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.5224218992264039 | validation: 0.4510385815140735]
	TIME [epoch: 0.689 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5574980762107457		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.5574980762107457 | validation: 0.5832044461302169]
	TIME [epoch: 0.687 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45822858388573906		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.45822858388573906 | validation: 0.36019494106140587]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47039890438154564		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.47039890438154564 | validation: 0.4831149159348927]
	TIME [epoch: 0.691 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4123748425911832		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.4123748425911832 | validation: 0.368147916487529]
	TIME [epoch: 0.687 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39649639307933193		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 0.39649639307933193 | validation: 0.5697703257952876]
	TIME [epoch: 0.688 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42609901612759066		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.42609901612759066 | validation: 0.3673952937198681]
	TIME [epoch: 0.688 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6092773309474343		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.6092773309474343 | validation: 0.5124780199911675]
	TIME [epoch: 0.688 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42341007601140834		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.42341007601140834 | validation: 0.670146546702827]
	TIME [epoch: 0.699 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46263582199013953		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 0.46263582199013953 | validation: 0.648681751665097]
	TIME [epoch: 0.687 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8321796155438571		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.8321796155438571 | validation: 0.35232215851469456]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4702112622247857		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.4702112622247857 | validation: 1.1369306869105305]
	TIME [epoch: 0.689 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8337864300788924		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.8337864300788924 | validation: 0.3493818182468707]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4455862380397362		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.4455862380397362 | validation: 0.37359852080832906]
	TIME [epoch: 0.691 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41132867618376995		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.41132867618376995 | validation: 0.47224503289256625]
	TIME [epoch: 0.688 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4153492172414563		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.4153492172414563 | validation: 0.34240516863227194]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4133049432873728		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.4133049432873728 | validation: 0.48436794686675544]
	TIME [epoch: 0.691 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40068063072792237		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.40068063072792237 | validation: 0.3427979399517577]
	TIME [epoch: 0.687 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4015860853533301		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.4015860853533301 | validation: 0.575420914090599]
	TIME [epoch: 0.689 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4221293410960113		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.4221293410960113 | validation: 0.3607969386939622]
	TIME [epoch: 0.688 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5551938829809242		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.5551938829809242 | validation: 0.4323404125642791]
	TIME [epoch: 0.689 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3675697525171455		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.3675697525171455 | validation: 0.6061202878099491]
	TIME [epoch: 0.688 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4191638157489301		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.4191638157489301 | validation: 0.4754747124391141]
	TIME [epoch: 0.688 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6733927229228333		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.6733927229228333 | validation: 0.42795970068563244]
	TIME [epoch: 0.688 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5007489927302536		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.5007489927302536 | validation: 0.6606737390987346]
	TIME [epoch: 0.688 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4540345318058928		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.4540345318058928 | validation: 0.3782159628618855]
	TIME [epoch: 0.688 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5024689837932708		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.5024689837932708 | validation: 0.4589627964235935]
	TIME [epoch: 0.689 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3615628266452836		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.3615628266452836 | validation: 0.3736840695165308]
	TIME [epoch: 0.688 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.347036910697781		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.347036910697781 | validation: 0.3953612978283726]
	TIME [epoch: 0.689 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3350005600804994		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.3350005600804994 | validation: 0.3711453126430029]
	TIME [epoch: 0.688 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33200762740401885		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.33200762740401885 | validation: 0.5283690730394834]
	TIME [epoch: 0.688 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35161791872526715		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.35161791872526715 | validation: 0.3835714861395541]
	TIME [epoch: 0.689 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5614315061127407		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.5614315061127407 | validation: 0.5110600492548013]
	TIME [epoch: 0.689 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.396770565733887		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.396770565733887 | validation: 0.8938143136868683]
	TIME [epoch: 0.688 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.623248988665386		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.623248988665386 | validation: 0.7627593827044654]
	TIME [epoch: 0.688 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.909914148560408		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.909914148560408 | validation: 0.47615829859923003]
	TIME [epoch: 0.689 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6786583962537348		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.6786583962537348 | validation: 0.4585893436953521]
	TIME [epoch: 0.69 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3866600149571723		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.3866600149571723 | validation: 0.4226397287993886]
	TIME [epoch: 0.688 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37665237653484696		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.37665237653484696 | validation: 0.362043523584779]
	TIME [epoch: 0.688 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39016090278116056		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.39016090278116056 | validation: 0.41518085450121467]
	TIME [epoch: 0.689 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3459958673625804		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.3459958673625804 | validation: 0.3406590387158794]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3404059914053076		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.3404059914053076 | validation: 0.37347063080566334]
	TIME [epoch: 0.689 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3268690511922891		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.3268690511922891 | validation: 0.36950413474715077]
	TIME [epoch: 0.688 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3184663106989715		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.3184663106989715 | validation: 0.3565931446685192]
	TIME [epoch: 0.69 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31611126663344413		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.31611126663344413 | validation: 0.35775215459054643]
	TIME [epoch: 0.688 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3142147805197039		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.3142147805197039 | validation: 0.4629638643555531]
	TIME [epoch: 0.687 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.318508674708405		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.318508674708405 | validation: 0.2993126059620475]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42153028475277987		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.42153028475277987 | validation: 0.9598976955364815]
	TIME [epoch: 0.698 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5633969153824989		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.5633969153824989 | validation: 0.5038450670796266]
	TIME [epoch: 0.687 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7735054482246081		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.7735054482246081 | validation: 0.4442001688795574]
	TIME [epoch: 0.687 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7215010657887538		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.7215010657887538 | validation: 0.4964881983397598]
	TIME [epoch: 0.689 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6126744501553963		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.6126744501553963 | validation: 0.46948065507995396]
	TIME [epoch: 0.687 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4005066748072825		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.4005066748072825 | validation: 0.7081508254499147]
	TIME [epoch: 0.688 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4511868024102796		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.4511868024102796 | validation: 0.5097401017566661]
	TIME [epoch: 0.688 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6146491147304626		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.6146491147304626 | validation: 0.3620955752871728]
	TIME [epoch: 0.688 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3534956018380165		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.3534956018380165 | validation: 0.7802079050039543]
	TIME [epoch: 0.687 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5035792777391048		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.5035792777391048 | validation: 0.31234044188921867]
	TIME [epoch: 0.687 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43303290634337144		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.43303290634337144 | validation: 0.35525078298936563]
	TIME [epoch: 0.687 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35428731724408663		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.35428731724408663 | validation: 0.4865013949031921]
	TIME [epoch: 0.69 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37093582583814266		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.37093582583814266 | validation: 0.3277158769505484]
	TIME [epoch: 0.688 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3344566209988635		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.3344566209988635 | validation: 0.3664187426887207]
	TIME [epoch: 0.687 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31851859143474737		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.31851859143474737 | validation: 0.4035838427447718]
	TIME [epoch: 0.687 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31667420190670675		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.31667420190670675 | validation: 0.32224012033218163]
	TIME [epoch: 0.691 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3132122725273097		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.3132122725273097 | validation: 0.4245403202947218]
	TIME [epoch: 0.687 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30872034554941724		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.30872034554941724 | validation: 0.30698605639359633]
	TIME [epoch: 0.687 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3174862536085272		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.3174862536085272 | validation: 0.6341933542197529]
	TIME [epoch: 0.687 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34583155335670174		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.34583155335670174 | validation: 0.31878733338035076]
	TIME [epoch: 0.689 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4825798627462816		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.4825798627462816 | validation: 0.38633568959775905]
	TIME [epoch: 0.689 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3200932688906574		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.3200932688906574 | validation: 0.7949884771296329]
	TIME [epoch: 0.688 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45982303086638004		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.45982303086638004 | validation: 0.5199607484843872]
	TIME [epoch: 0.686 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6911333682146921		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.6911333682146921 | validation: 0.34536353097626177]
	TIME [epoch: 0.69 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.52932984093046		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.52932984093046 | validation: 0.49081873093109113]
	TIME [epoch: 0.687 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3856796591309373		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.3856796591309373 | validation: 0.5284095023677438]
	TIME [epoch: 0.687 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3825898763754978		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.3825898763754978 | validation: 0.34950576681982853]
	TIME [epoch: 0.687 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3415800019279868		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.3415800019279868 | validation: 0.36453082940517856]
	TIME [epoch: 0.688 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3100482821137523		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.3100482821137523 | validation: 0.4240631375965291]
	TIME [epoch: 0.687 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3122637071357394		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.3122637071357394 | validation: 0.3341661277736025]
	TIME [epoch: 0.687 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29998321133860373		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.29998321133860373 | validation: 0.40057280435110915]
	TIME [epoch: 0.687 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2929607305384903		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.2929607305384903 | validation: 0.3367004191315803]
	TIME [epoch: 0.688 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28654808403217236		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.28654808403217236 | validation: 0.3986510649523878]
	TIME [epoch: 0.687 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28597830269322894		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.28597830269322894 | validation: 0.31937594050133256]
	TIME [epoch: 0.686 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29212591196644894		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.29212591196644894 | validation: 0.4921251122040758]
	TIME [epoch: 0.687 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2894852837873569		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.2894852837873569 | validation: 0.29265505552166177]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_183.pth
	Model improved!!!
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3169361609779678		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.3169361609779678 | validation: 0.7438207041499455]
	TIME [epoch: 0.688 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3678506950614211		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.3678506950614211 | validation: 0.3871853658375028]
	TIME [epoch: 0.687 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5737383792511568		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.5737383792511568 | validation: 0.28752692659207985]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33414085432863105		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.33414085432863105 | validation: 0.9086316131265048]
	TIME [epoch: 0.688 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.536281226398678		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.536281226398678 | validation: 0.31389448643287365]
	TIME [epoch: 0.688 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45451963890551084		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.45451963890551084 | validation: 0.3092969170442276]
	TIME [epoch: 0.697 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3975651128367339		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.3975651128367339 | validation: 0.4415877024395565]
	TIME [epoch: 0.687 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3131298420015578		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.3131298420015578 | validation: 0.3651316490536388]
	TIME [epoch: 0.687 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30355260743696016		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.30355260743696016 | validation: 0.3285240244037276]
	TIME [epoch: 0.687 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2933933376232703		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.2933933376232703 | validation: 0.36989568458826777]
	TIME [epoch: 0.687 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2804523767860683		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.2804523767860683 | validation: 0.3704074492431442]
	TIME [epoch: 0.691 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2778495403281266		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.2778495403281266 | validation: 0.33263988571833464]
	TIME [epoch: 0.687 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27436542427368943		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.27436542427368943 | validation: 0.35236943238430407]
	TIME [epoch: 0.687 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2695305252933008		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.2695305252933008 | validation: 0.3823927160709373]
	TIME [epoch: 0.686 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2688920292777479		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.2688920292777479 | validation: 0.33352790308322317]
	TIME [epoch: 0.688 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26884862293670564		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.26884862293670564 | validation: 0.3987431780019845]
	TIME [epoch: 0.687 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26803887119664915		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.26803887119664915 | validation: 0.31644579656911975]
	TIME [epoch: 0.687 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2675720933636437		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.2675720933636437 | validation: 0.4533252130603018]
	TIME [epoch: 173 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27599821162773647		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.27599821162773647 | validation: 0.2959367035981094]
	TIME [epoch: 1.35 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31438451651910465		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.31438451651910465 | validation: 0.5681899062466116]
	TIME [epoch: 1.34 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30757413721185306		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.30757413721185306 | validation: 0.28010881395996945]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_204.pth
	Model improved!!!
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.366471668081996		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.366471668081996 | validation: 0.47002199613820317]
	TIME [epoch: 1.34 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.276765888164284		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.276765888164284 | validation: 0.3430777948646808]
	TIME [epoch: 1.34 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2575393665016323		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.2575393665016323 | validation: 0.39031168056014254]
	TIME [epoch: 1.34 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26012017455267306		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.26012017455267306 | validation: 0.35807788895446047]
	TIME [epoch: 1.34 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2649095061869469		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.2649095061869469 | validation: 0.3466213992140302]
	TIME [epoch: 1.34 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2776187427451079		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.2776187427451079 | validation: 0.4690570583640626]
	TIME [epoch: 1.34 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2743984350445878		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.2743984350445878 | validation: 0.27261496645054123]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33006647035270376		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.33006647035270376 | validation: 0.6970831501993006]
	TIME [epoch: 1.36 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33983875406248304		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.33983875406248304 | validation: 0.2722719094779314]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38719641128645976		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.38719641128645976 | validation: 0.3547491799292618]
	TIME [epoch: 1.35 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2651763609348346		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.2651763609348346 | validation: 0.596602738979492]
	TIME [epoch: 1.35 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3248059148583407		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.3248059148583407 | validation: 0.30296976143271387]
	TIME [epoch: 1.35 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.346608673576379		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.346608673576379 | validation: 0.3657074367106521]
	TIME [epoch: 1.36 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25558741622780434		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.25558741622780434 | validation: 0.43271427303945975]
	TIME [epoch: 1.34 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2807146678582066		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.2807146678582066 | validation: 0.2893099434817568]
	TIME [epoch: 1.34 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29509543308917335		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.29509543308917335 | validation: 0.3674592597208883]
	TIME [epoch: 1.34 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25092660894460167		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.25092660894460167 | validation: 0.4108224053302756]
	TIME [epoch: 1.35 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25817998254870383		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.25817998254870383 | validation: 0.3056053993070573]
	TIME [epoch: 1.34 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2661260139627378		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.2661260139627378 | validation: 0.4153350969545748]
	TIME [epoch: 1.34 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25507450121328773		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.25507450121328773 | validation: 0.32732243640975645]
	TIME [epoch: 1.34 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25655143004513625		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.25655143004513625 | validation: 0.3920588428869716]
	TIME [epoch: 1.34 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25461002611013656		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.25461002611013656 | validation: 0.32533307984823995]
	TIME [epoch: 1.34 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26068946708227764		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.26068946708227764 | validation: 0.4270996127137471]
	TIME [epoch: 1.34 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2756977398160219		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.2756977398160219 | validation: 0.2819687704936281]
	TIME [epoch: 1.34 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30546584435994917		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.30546584435994917 | validation: 0.5542953972670638]
	TIME [epoch: 1.34 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29078660361837605		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.29078660361837605 | validation: 0.25642758328300114]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_230.pth
	Model improved!!!
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34746354874664725		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.34746354874664725 | validation: 0.40206044416407033]
	TIME [epoch: 1.35 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26131776004519663		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.26131776004519663 | validation: 0.5053070415786561]
	TIME [epoch: 1.35 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26826571148700423		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.26826571148700423 | validation: 0.29180517884730434]
	TIME [epoch: 1.35 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3038574549693363		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.3038574549693363 | validation: 0.432428923321694]
	TIME [epoch: 1.35 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25805450860935786		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.25805450860935786 | validation: 0.34563486882859135]
	TIME [epoch: 1.35 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24585777035493755		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.24585777035493755 | validation: 0.3452350344922162]
	TIME [epoch: 1.35 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24329713825661461		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.24329713825661461 | validation: 0.3761038258930163]
	TIME [epoch: 1.35 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24013728451177646		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.24013728451177646 | validation: 0.2995590593882981]
	TIME [epoch: 1.35 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25309314391118326		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.25309314391118326 | validation: 0.4290819307227282]
	TIME [epoch: 1.35 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2504817942179265		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.2504817942179265 | validation: 0.30529721998299597]
	TIME [epoch: 1.35 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.276134759576157		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.276134759576157 | validation: 0.4990312157651342]
	TIME [epoch: 1.35 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2769491102576364		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.2769491102576364 | validation: 0.2709643364570021]
	TIME [epoch: 1.35 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3075615969735462		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.3075615969735462 | validation: 0.43537396471675394]
	TIME [epoch: 1.36 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25422713519741946		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.25422713519741946 | validation: 0.3149330066998619]
	TIME [epoch: 1.35 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24178379923893828		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.24178379923893828 | validation: 0.37176893712851417]
	TIME [epoch: 1.35 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24524057212958858		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.24524057212958858 | validation: 0.3452585634445904]
	TIME [epoch: 1.35 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2473538486870648		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.2473538486870648 | validation: 0.35701950887530887]
	TIME [epoch: 1.35 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24436470687621398		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.24436470687621398 | validation: 0.33929500655080896]
	TIME [epoch: 1.35 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23599714438290878		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.23599714438290878 | validation: 0.34567344281307477]
	TIME [epoch: 1.35 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2372673845182237		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.2372673845182237 | validation: 0.36547208745080306]
	TIME [epoch: 1.35 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23624431697199058		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.23624431697199058 | validation: 0.29700265325579916]
	TIME [epoch: 1.35 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24610356748682727		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.24610356748682727 | validation: 0.5391102957062991]
	TIME [epoch: 1.35 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2836360501205233		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.2836360501205233 | validation: 0.28565568151680737]
	TIME [epoch: 1.35 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43991274859864077		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.43991274859864077 | validation: 0.369304700801765]
	TIME [epoch: 1.35 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2542955600972913		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.2542955600972913 | validation: 0.5785778351657326]
	TIME [epoch: 1.35 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29871308439309785		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.29871308439309785 | validation: 0.2746069986176082]
	TIME [epoch: 1.35 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3663224486211491		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.3663224486211491 | validation: 0.31522250417836606]
	TIME [epoch: 1.35 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24992694431367948		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.24992694431367948 | validation: 0.4746160650354789]
	TIME [epoch: 1.35 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27116561588254956		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.27116561588254956 | validation: 0.2932523538496161]
	TIME [epoch: 1.35 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24601020672191154		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.24601020672191154 | validation: 0.33482284124424927]
	TIME [epoch: 1.35 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23745648985980553		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.23745648985980553 | validation: 0.38098408541657935]
	TIME [epoch: 1.35 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23705882903926204		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.23705882903926204 | validation: 0.31567642470542995]
	TIME [epoch: 1.35 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23198711433544134		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.23198711433544134 | validation: 0.3451210834677987]
	TIME [epoch: 1.35 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2325323665420698		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.2325323665420698 | validation: 0.33408525377242243]
	TIME [epoch: 1.35 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22968073300818495		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.22968073300818495 | validation: 0.32454081131860746]
	TIME [epoch: 1.36 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23365551797583436		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.23365551797583436 | validation: 0.34840128781453944]
	TIME [epoch: 1.35 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23576996789979923		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.23576996789979923 | validation: 0.33625324790724154]
	TIME [epoch: 1.35 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25017130118748265		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.25017130118748265 | validation: 0.33933936384533675]
	TIME [epoch: 1.35 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2566562816027275		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.2566562816027275 | validation: 0.3488900605577785]
	TIME [epoch: 1.35 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2429643505094824		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.2429643505094824 | validation: 0.3330543681206764]
	TIME [epoch: 1.35 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22772071657129936		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.22772071657129936 | validation: 0.3038161123592434]
	TIME [epoch: 1.35 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22770364460915077		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.22770364460915077 | validation: 0.3755083984479008]
	TIME [epoch: 1.35 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23141230666040769		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.23141230666040769 | validation: 0.24869535068152954]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_273.pth
	Model improved!!!
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25080651595230785		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.25080651595230785 | validation: 0.5981951181805061]
	TIME [epoch: 1.35 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3044878336962617		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.3044878336962617 | validation: 0.2490668436830964]
	TIME [epoch: 1.35 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38203058085359715		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.38203058085359715 | validation: 0.27367093846964896]
	TIME [epoch: 1.35 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2574274937005874		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.2574274937005874 | validation: 0.5241670204438963]
	TIME [epoch: 1.35 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2753925475374936		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.2753925475374936 | validation: 0.28832822222976245]
	TIME [epoch: 1.35 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2528199335935386		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.2528199335935386 | validation: 0.32384324674999043]
	TIME [epoch: 1.35 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23439923037911617		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.23439923037911617 | validation: 0.3695459643531013]
	TIME [epoch: 1.35 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23029629810342264		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.23029629810342264 | validation: 0.2985626176410251]
	TIME [epoch: 1.35 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22502467155543598		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.22502467155543598 | validation: 0.3261195937022118]
	TIME [epoch: 1.35 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22200295719499707		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.22200295719499707 | validation: 0.33001452012995336]
	TIME [epoch: 1.35 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22541896610715642		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.22541896610715642 | validation: 0.30135332557011996]
	TIME [epoch: 1.35 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22107554933989504		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.22107554933989504 | validation: 0.3297213679697213]
	TIME [epoch: 1.35 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22011199151775543		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.22011199151775543 | validation: 0.3052091362514118]
	TIME [epoch: 1.36 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22411340886295864		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.22411340886295864 | validation: 0.335350911021348]
	TIME [epoch: 1.35 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22828437479594038		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.22828437479594038 | validation: 0.2942824391787104]
	TIME [epoch: 1.35 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23148707373150387		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.23148707373150387 | validation: 0.3487341725378723]
	TIME [epoch: 1.35 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23078153198685677		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.23078153198685677 | validation: 0.28703430005861114]
	TIME [epoch: 1.35 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23424740867058874		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.23424740867058874 | validation: 0.32942205771741073]
	TIME [epoch: 1.35 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23138071558340406		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.23138071558340406 | validation: 0.3164161864556714]
	TIME [epoch: 1.35 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.225031173505322		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.225031173505322 | validation: 0.2739269524659161]
	TIME [epoch: 1.35 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22891640228352497		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.22891640228352497 | validation: 0.4736378186267944]
	TIME [epoch: 1.35 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25333314585509864		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.25333314585509864 | validation: 0.2242016413323641]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36433236364728805		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.36433236364728805 | validation: 0.3519006266497473]
	TIME [epoch: 1.35 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22982340928992406		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.22982340928992406 | validation: 0.42826937369245777]
	TIME [epoch: 1.35 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2437823840109112		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.2437823840109112 | validation: 0.23323228231172002]
	TIME [epoch: 1.35 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28082363892007495		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.28082363892007495 | validation: 0.32380514860473547]
	TIME [epoch: 1.35 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22371621675166103		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.22371621675166103 | validation: 0.3756223747530644]
	TIME [epoch: 1.35 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22937729840195795		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.22937729840195795 | validation: 0.2599112260878598]
	TIME [epoch: 1.35 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22562951168746576		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.22562951168746576 | validation: 0.31371747086904134]
	TIME [epoch: 1.35 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21538104416784853		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.21538104416784853 | validation: 0.3239216265317549]
	TIME [epoch: 1.34 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21907088476491246		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.21907088476491246 | validation: 0.27979708909720524]
	TIME [epoch: 1.34 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21590404273772848		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.21590404273772848 | validation: 0.31476499394824164]
	TIME [epoch: 1.34 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21478669017576807		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.21478669017576807 | validation: 0.2688826306753969]
	TIME [epoch: 1.34 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22307676692099535		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.22307676692099535 | validation: 0.33440346310781816]
	TIME [epoch: 1.35 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2337169756735606		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.2337169756735606 | validation: 0.25427936793249856]
	TIME [epoch: 1.34 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24827790075256434		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.24827790075256434 | validation: 0.3498065878701159]
	TIME [epoch: 1.34 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2241044040402637		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.2241044040402637 | validation: 0.2633798416658353]
	TIME [epoch: 1.34 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21860509643962772		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.21860509643962772 | validation: 0.34231908595193633]
	TIME [epoch: 1.35 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21882029496000313		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.21882029496000313 | validation: 0.25077792195564963]
	TIME [epoch: 1.34 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21672755040573946		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.21672755040573946 | validation: 0.3739759259585332]
	TIME [epoch: 1.35 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22486130468969606		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.22486130468969606 | validation: 0.21472560913942715]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_314.pth
	Model improved!!!
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2490961705193756		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.2490961705193756 | validation: 0.4097841310742606]
	TIME [epoch: 1.35 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2416611350924984		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.2416611350924984 | validation: 0.22820241637777197]
	TIME [epoch: 1.35 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.223647688309011		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.223647688309011 | validation: 0.3122757198738677]
	TIME [epoch: 1.35 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21226218337081623		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.21226218337081623 | validation: 0.2899618959594265]
	TIME [epoch: 1.35 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21417051723973587		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.21417051723973587 | validation: 0.28797496447733434]
	TIME [epoch: 1.35 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21352276583678073		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.21352276583678073 | validation: 0.280595974515873]
	TIME [epoch: 1.35 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2148525337046305		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.2148525337046305 | validation: 0.2972588286630183]
	TIME [epoch: 1.35 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2141137719522245		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.2141137719522245 | validation: 0.25270156009120276]
	TIME [epoch: 1.35 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2110530880789488		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.2110530880789488 | validation: 0.342241672503875]
	TIME [epoch: 1.35 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21981050732779073		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.21981050732779073 | validation: 0.19153695312006855]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_324.pth
	Model improved!!!
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2558518425208349		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.2558518425208349 | validation: 0.4473269846894603]
	TIME [epoch: 1.35 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2537325387065725		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.2537325387065725 | validation: 0.20989215085341387]
	TIME [epoch: 1.35 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23245526617111584		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.23245526617111584 | validation: 0.29210386403187244]
	TIME [epoch: 1.35 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2061462944682499		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.2061462944682499 | validation: 0.3272508792569345]
	TIME [epoch: 1.35 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20946238857543426		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.20946238857543426 | validation: 0.2393788972070739]
	TIME [epoch: 1.35 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20985709206265024		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.20985709206265024 | validation: 0.30586234061058515]
	TIME [epoch: 1.34 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20409976182933962		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.20409976182933962 | validation: 0.2670441788963382]
	TIME [epoch: 1.35 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2039433798455682		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.2039433798455682 | validation: 0.28584633807909615]
	TIME [epoch: 1.34 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20441624220086962		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.20441624220086962 | validation: 0.24762581230984756]
	TIME [epoch: 1.34 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21989725723066822		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.21989725723066822 | validation: 0.3342649325166209]
	TIME [epoch: 1.35 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23503959847335146		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.23503959847335146 | validation: 0.251887487295915]
	TIME [epoch: 1.35 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2207588825385409		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.2207588825385409 | validation: 0.30271109354604286]
	TIME [epoch: 1.35 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20238849557968272		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.20238849557968272 | validation: 0.24885911055304133]
	TIME [epoch: 1.35 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20456502396694703		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.20456502396694703 | validation: 0.30551750005401995]
	TIME [epoch: 1.35 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2106096047610982		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.2106096047610982 | validation: 0.18925187686821351]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_339.pth
	Model improved!!!
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2437002924863902		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.2437002924863902 | validation: 0.4651823945604305]
	TIME [epoch: 1.35 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.270876915119371		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.270876915119371 | validation: 0.17786244774427729]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_341.pth
	Model improved!!!
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2630601390959833		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.2630601390959833 | validation: 0.26679283578202995]
	TIME [epoch: 1.35 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20753748766378338		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.20753748766378338 | validation: 0.35109783896301394]
	TIME [epoch: 1.35 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2196382488172555		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.2196382488172555 | validation: 0.23617070238062793]
	TIME [epoch: 1.35 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20907058922684996		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.20907058922684996 | validation: 0.2740401492990293]
	TIME [epoch: 1.35 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1999791848648758		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.1999791848648758 | validation: 0.2859600247771971]
	TIME [epoch: 1.37 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20144268105479132		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.20144268105479132 | validation: 0.25161494402142637]
	TIME [epoch: 1.35 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20004074388585236		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.20004074388585236 | validation: 0.25770656133871217]
	TIME [epoch: 1.35 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20253312777776442		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.20253312777776442 | validation: 0.25918015178410636]
	TIME [epoch: 1.36 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20139497206804602		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.20139497206804602 | validation: 0.2840602296138452]
	TIME [epoch: 1.35 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2029195150803752		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.2029195150803752 | validation: 0.24337980195198927]
	TIME [epoch: 1.35 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20727444358703154		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.20727444358703154 | validation: 0.31019512454547793]
	TIME [epoch: 1.35 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2066562231776678		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.2066562231776678 | validation: 0.2259753369738438]
	TIME [epoch: 1.35 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20615378931807837		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.20615378931807837 | validation: 0.31156174859200875]
	TIME [epoch: 1.35 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20075921375935651		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.20075921375935651 | validation: 0.21263487578988088]
	TIME [epoch: 1.35 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20434753966555433		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.20434753966555433 | validation: 0.3651563063794456]
	TIME [epoch: 1.35 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22156957767913646		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.22156957767913646 | validation: 0.16733408807615843]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_357.pth
	Model improved!!!
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25741733975285436		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.25741733975285436 | validation: 0.3210557521050058]
	TIME [epoch: 1.34 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21183928682708217		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.21183928682708217 | validation: 0.2578863308143742]
	TIME [epoch: 1.34 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19716098745322008		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.19716098745322008 | validation: 0.23277867150876436]
	TIME [epoch: 1.35 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1989800021212568		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.1989800021212568 | validation: 0.2961265874046352]
	TIME [epoch: 1.34 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.199092882240411		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.199092882240411 | validation: 0.21795019852343267]
	TIME [epoch: 1.35 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19677810146591163		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.19677810146591163 | validation: 0.2883429491995642]
	TIME [epoch: 1.34 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19633709355167056		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.19633709355167056 | validation: 0.2229370819395769]
	TIME [epoch: 1.34 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19420153990867736		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.19420153990867736 | validation: 0.29299339788354656]
	TIME [epoch: 1.34 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19989774121212692		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.19989774121212692 | validation: 0.21099126783761674]
	TIME [epoch: 1.34 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20093678708385335		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.20093678708385335 | validation: 0.33177018473410097]
	TIME [epoch: 1.34 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21098701338765594		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.21098701338765594 | validation: 0.20661890034648822]
	TIME [epoch: 1.35 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23327211619158392		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.23327211619158392 | validation: 0.3284688137757306]
	TIME [epoch: 1.34 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21697706776659698		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.21697706776659698 | validation: 0.22550987374850184]
	TIME [epoch: 1.35 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1944440766184831		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.1944440766184831 | validation: 0.27839642279834426]
	TIME [epoch: 1.35 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.196093250929976		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.196093250929976 | validation: 0.2188681133782406]
	TIME [epoch: 1.35 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20271911752736052		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.20271911752736052 | validation: 0.3042505389435517]
	TIME [epoch: 1.34 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21194943885298373		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.21194943885298373 | validation: 0.17097361107694406]
	TIME [epoch: 1.35 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24322215192453436		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.24322215192453436 | validation: 0.31691492033937085]
	TIME [epoch: 1.34 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20990357935057652		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.20990357935057652 | validation: 0.21204028471797717]
	TIME [epoch: 1.34 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1960699244932022		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.1960699244932022 | validation: 0.260880883827336]
	TIME [epoch: 1.34 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1909200295441363		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.1909200295441363 | validation: 0.24298305173245427]
	TIME [epoch: 1.35 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18971280965171541		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.18971280965171541 | validation: 0.2261906425195496]
	TIME [epoch: 1.34 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18712189135107352		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.18712189135107352 | validation: 0.2565648704748994]
	TIME [epoch: 1.35 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18839439544463993		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.18839439544463993 | validation: 0.21141069226978781]
	TIME [epoch: 1.34 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1913308477970736		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.1913308477970736 | validation: 0.30138633660817965]
	TIME [epoch: 1.35 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1947019477929279		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.1947019477929279 | validation: 0.17303128320618844]
	TIME [epoch: 1.34 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21225461614610883		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.21225461614610883 | validation: 0.3665813614958041]
	TIME [epoch: 1.35 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22084697089511515		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.22084697089511515 | validation: 0.16342536114374506]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_385.pth
	Model improved!!!
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21387570075604173		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.21387570075604173 | validation: 0.2746129879556867]
	TIME [epoch: 1.36 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18961166490697132		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.18961166490697132 | validation: 0.25800607557338645]
	TIME [epoch: 1.35 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19152663179789556		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.19152663179789556 | validation: 0.20766003435680025]
	TIME [epoch: 1.35 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19848293900821445		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.19848293900821445 | validation: 0.25867392888234025]
	TIME [epoch: 1.35 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19513696256588156		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.19513696256588156 | validation: 0.23275791573200097]
	TIME [epoch: 1.35 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18814111177854964		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.18814111177854964 | validation: 0.22979460550307595]
	TIME [epoch: 1.35 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.189894490548562		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.189894490548562 | validation: 0.2709261337684378]
	TIME [epoch: 1.36 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19380597531147975		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.19380597531147975 | validation: 0.19971998456742623]
	TIME [epoch: 1.36 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20656032853364398		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.20656032853364398 | validation: 0.32265097926186187]
	TIME [epoch: 1.35 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2196749720926337		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.2196749720926337 | validation: 0.19599989726713926]
	TIME [epoch: 1.35 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2006043456053849		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.2006043456053849 | validation: 0.27497866052046377]
	TIME [epoch: 1.35 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18677345171057247		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.18677345171057247 | validation: 0.2095864541881043]
	TIME [epoch: 1.35 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18713929792272566		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.18713929792272566 | validation: 0.24773302388243615]
	TIME [epoch: 1.35 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18880046150858198		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.18880046150858198 | validation: 0.20638530458263227]
	TIME [epoch: 1.35 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19198651008697235		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.19198651008697235 | validation: 0.2713476604012847]
	TIME [epoch: 1.35 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1919286481632983		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.1919286481632983 | validation: 0.17004498221030806]
	TIME [epoch: 1.35 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2051106359148712		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.2051106359148712 | validation: 0.33411561428546754]
	TIME [epoch: 1.35 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21449458575383518		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.21449458575383518 | validation: 0.1530272934338182]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_403.pth
	Model improved!!!
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21123641386898137		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.21123641386898137 | validation: 0.2582929110198349]
	TIME [epoch: 1.34 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18831607787653074		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.18831607787653074 | validation: 0.2354542590333372]
	TIME [epoch: 1.34 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18371365875589743		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.18371365875589743 | validation: 0.2101684222240172]
	TIME [epoch: 1.34 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18412182268991495		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.18412182268991495 | validation: 0.23421725251194442]
	TIME [epoch: 1.34 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1821576866139106		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.1821576866139106 | validation: 0.21577415393668664]
	TIME [epoch: 1.34 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18384411280333532		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.18384411280333532 | validation: 0.2110442410796758]
	TIME [epoch: 1.34 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19202765233115823		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.19202765233115823 | validation: 0.28466275386953294]
	TIME [epoch: 1.34 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20314223149553712		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.20314223149553712 | validation: 0.1876496594911818]
	TIME [epoch: 1.34 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20433609115950518		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.20433609115950518 | validation: 0.29959055957273645]
	TIME [epoch: 1.34 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19406292945321352		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.19406292945321352 | validation: 0.1886642725338016]
	TIME [epoch: 1.34 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1859451890436086		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.1859451890436086 | validation: 0.2556560459768591]
	TIME [epoch: 1.35 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18527367397140646		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.18527367397140646 | validation: 0.19934466550576346]
	TIME [epoch: 1.34 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18736571580156267		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.18736571580156267 | validation: 0.25439225684619676]
	TIME [epoch: 1.34 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19251080578723645		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.19251080578723645 | validation: 0.1891709419666453]
	TIME [epoch: 1.34 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19652084402402417		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.19652084402402417 | validation: 0.266687537581378]
	TIME [epoch: 1.34 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19293245938837347		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.19293245938837347 | validation: 0.16324787414140352]
	TIME [epoch: 1.34 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19603369133176649		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.19603369133176649 | validation: 0.29140309286036664]
	TIME [epoch: 1.34 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19174880813723405		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.19174880813723405 | validation: 0.1837027128759262]
	TIME [epoch: 1.34 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18914806909061907		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.18914806909061907 | validation: 0.24993104289374746]
	TIME [epoch: 1.34 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1855492858111352		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.1855492858111352 | validation: 0.2115152323912878]
	TIME [epoch: 1.34 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1884694036375938		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.1884694036375938 | validation: 0.2192709555813732]
	TIME [epoch: 1.34 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18494253490656334		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.18494253490656334 | validation: 0.21656472525547066]
	TIME [epoch: 1.34 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.182585712092312		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.182585712092312 | validation: 0.2074129426188375]
	TIME [epoch: 1.34 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17978656338117835		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.17978656338117835 | validation: 0.23021894334705795]
	TIME [epoch: 1.34 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1816911661264271		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.1816911661264271 | validation: 0.1728454138032479]
	TIME [epoch: 1.34 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18790214072070513		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.18790214072070513 | validation: 0.2831132438319424]
	TIME [epoch: 1.34 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1953991694490398		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.1953991694490398 | validation: 0.14193256534740337]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_430.pth
	Model improved!!!
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20534807633515		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.20534807633515 | validation: 0.29640331574654777]
	TIME [epoch: 1.35 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19358948106595164		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.19358948106595164 | validation: 0.19064371845204664]
	TIME [epoch: 1.35 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18615445735599742		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.18615445735599742 | validation: 0.22086955184894852]
	TIME [epoch: 1.35 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18235604109210043		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.18235604109210043 | validation: 0.21119445691096028]
	TIME [epoch: 1.35 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18621282146816648		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.18621282146816648 | validation: 0.22016943198632247]
	TIME [epoch: 1.36 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18841156124389205		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.18841156124389205 | validation: 0.21188654685542035]
	TIME [epoch: 1.35 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18603087697967763		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.18603087697967763 | validation: 0.22026605443373778]
	TIME [epoch: 1.35 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18118914527350857		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.18118914527350857 | validation: 0.21019753903130287]
	TIME [epoch: 1.35 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17553690193967839		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.17553690193967839 | validation: 0.20868763297886722]
	TIME [epoch: 1.35 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1752681624338365		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.1752681624338365 | validation: 0.2098717244102487]
	TIME [epoch: 1.35 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17831239228904913		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.17831239228904913 | validation: 0.1961138866076464]
	TIME [epoch: 1.35 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17752122224976583		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.17752122224976583 | validation: 0.22815905437606948]
	TIME [epoch: 1.35 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18580059360209322		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.18580059360209322 | validation: 0.1979927322369075]
	TIME [epoch: 1.35 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.188088807104484		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.188088807104484 | validation: 0.2352279184619861]
	TIME [epoch: 1.35 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1904813818682392		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.1904813818682392 | validation: 0.1688012749554629]
	TIME [epoch: 1.35 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18998492503651748		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.18998492503651748 | validation: 0.2989135743966533]
	TIME [epoch: 1.35 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20290397866232687		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.20290397866232687 | validation: 0.14156129288393482]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_447.pth
	Model improved!!!
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21379468442970584		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.21379468442970584 | validation: 0.27054027639539363]
	TIME [epoch: 1.35 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19142811615193558		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.19142811615193558 | validation: 0.19416368695596312]
	TIME [epoch: 1.35 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17995767411002628		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.17995767411002628 | validation: 0.2003854536899147]
	TIME [epoch: 1.35 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1774907730212766		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.1774907730212766 | validation: 0.22668686096761462]
	TIME [epoch: 1.35 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17464899475842802		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.17464899475842802 | validation: 0.18042266542565663]
	TIME [epoch: 1.35 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17194706914906271		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.17194706914906271 | validation: 0.2101706536405037]
	TIME [epoch: 1.35 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.174497100216475		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.174497100216475 | validation: 0.18450265515556336]
	TIME [epoch: 1.35 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.173868183250486		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.173868183250486 | validation: 0.2227750609539092]
	TIME [epoch: 1.35 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17704656243927028		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.17704656243927028 | validation: 0.1687937183807815]
	TIME [epoch: 1.35 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1808917543212425		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.1808917543212425 | validation: 0.277845767484078]
	TIME [epoch: 1.36 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1881776350151494		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.1881776350151494 | validation: 0.14841803533702166]
	TIME [epoch: 1.35 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19933735028275906		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.19933735028275906 | validation: 0.2818560901640585]
	TIME [epoch: 1.35 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1880013630479401		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.1880013630479401 | validation: 0.16284306648720623]
	TIME [epoch: 1.35 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1753963221489991		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.1753963221489991 | validation: 0.20134026577116787]
	TIME [epoch: 1.35 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17052279749377827		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.17052279749377827 | validation: 0.20950407999970203]
	TIME [epoch: 1.35 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1716045826479869		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.1716045826479869 | validation: 0.19791168481645308]
	TIME [epoch: 1.35 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17967070064018878		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.17967070064018878 | validation: 0.20534178892384822]
	TIME [epoch: 1.35 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18300379002196565		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.18300379002196565 | validation: 0.23821284401251017]
	TIME [epoch: 1.35 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1937723563976762		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.1937723563976762 | validation: 0.16146309155753258]
	TIME [epoch: 1.35 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19500877664661737		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.19500877664661737 | validation: 0.23898451625023878]
	TIME [epoch: 1.35 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17962406596073735		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.17962406596073735 | validation: 0.16223502145388066]
	TIME [epoch: 1.35 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17375278002881694		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.17375278002881694 | validation: 0.2375725793847397]
	TIME [epoch: 1.35 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17478313128246672		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.17478313128246672 | validation: 0.17364331540363487]
	TIME [epoch: 1.35 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17669502117865343		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.17669502117865343 | validation: 0.23907912528778896]
	TIME [epoch: 1.35 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17708781141258967		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.17708781141258967 | validation: 0.1623819529041705]
	TIME [epoch: 1.35 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1776881262224066		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.1776881262224066 | validation: 0.24429039435909147]
	TIME [epoch: 1.35 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17792267182779425		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.17792267182779425 | validation: 0.15237774846778335]
	TIME [epoch: 1.35 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17977695628060783		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.17977695628060783 | validation: 0.260281696208978]
	TIME [epoch: 1.35 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1792848149148535		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.1792848149148535 | validation: 0.14054736903460774]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_476.pth
	Model improved!!!
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18040108088734513		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.18040108088734513 | validation: 0.2418590594926624]
	TIME [epoch: 1.34 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18078059343515981		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.18078059343515981 | validation: 0.1594589562426396]
	TIME [epoch: 1.35 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1770400744827213		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.1770400744827213 | validation: 0.2096143572674789]
	TIME [epoch: 1.34 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1748959301240874		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.1748959301240874 | validation: 0.19658402459176305]
	TIME [epoch: 1.34 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17175715583823947		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.17175715583823947 | validation: 0.18826034751748688]
	TIME [epoch: 1.36 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1716474134982834		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.1716474134982834 | validation: 0.2090455815395323]
	TIME [epoch: 1.34 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17342112383283984		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.17342112383283984 | validation: 0.1998797605386936]
	TIME [epoch: 1.34 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18301229679935657		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.18301229679935657 | validation: 0.23047084942048485]
	TIME [epoch: 1.34 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19387646886649823		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.19387646886649823 | validation: 0.1862597080292527]
	TIME [epoch: 1.34 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18071491314725138		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.18071491314725138 | validation: 0.20779909683113806]
	TIME [epoch: 1.34 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16951197057059575		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.16951197057059575 | validation: 0.17834010294938585]
	TIME [epoch: 1.34 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16843025615357404		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.16843025615357404 | validation: 0.22045866913996265]
	TIME [epoch: 1.34 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1720083075909139		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.1720083075909139 | validation: 0.15011813521853307]
	TIME [epoch: 1.34 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17763339483900104		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.17763339483900104 | validation: 0.26946899256102524]
	TIME [epoch: 1.34 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18383266817218005		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.18383266817218005 | validation: 0.13244043511392992]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_491.pth
	Model improved!!!
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19376365987447636		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.19376365987447636 | validation: 0.24411723383983316]
	TIME [epoch: 1.35 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17895382524510006		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.17895382524510006 | validation: 0.1786342324325065]
	TIME [epoch: 1.35 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.169944602815959		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.169944602815959 | validation: 0.18445623400561148]
	TIME [epoch: 1.35 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1685812124021049		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.1685812124021049 | validation: 0.20189890552485013]
	TIME [epoch: 1.35 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17107894335461693		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.17107894335461693 | validation: 0.17379942959041608]
	TIME [epoch: 1.35 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17286481129897496		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.17286481129897496 | validation: 0.19128927635814416]
	TIME [epoch: 1.35 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17161557555170276		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.17161557555170276 | validation: 0.1839813282082089]
	TIME [epoch: 1.35 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17024035705518167		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.17024035705518167 | validation: 0.1828818312577849]
	TIME [epoch: 1.35 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17093300035614944		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.17093300035614944 | validation: 0.2159427004018716]
	TIME [epoch: 1.36 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17567967761526454		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.17567967761526454 | validation: 0.1618134349521405]
	TIME [epoch: 176 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1773416555660629		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.1773416555660629 | validation: 0.273798085481697]
	TIME [epoch: 2.67 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18635884496257824		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.18635884496257824 | validation: 0.12104384087993819]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_503.pth
	Model improved!!!
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18477347093658555		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.18477347093658555 | validation: 0.243286983769364]
	TIME [epoch: 2.65 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17511714803138398		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.17511714803138398 | validation: 0.16413469996435792]
	TIME [epoch: 2.65 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16922291717726703		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.16922291717726703 | validation: 0.18353858574332546]
	TIME [epoch: 2.65 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1670907008889273		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.1670907008889273 | validation: 0.19129886042603222]
	TIME [epoch: 2.65 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1658731332420097		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.1658731332420097 | validation: 0.165779484131256]
	TIME [epoch: 2.65 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16711685180384006		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.16711685180384006 | validation: 0.20697814716021545]
	TIME [epoch: 2.65 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16602973513418376		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.16602973513418376 | validation: 0.1615371043754067]
	TIME [epoch: 2.65 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16707035564768766		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.16707035564768766 | validation: 0.22161377012730576]
	TIME [epoch: 2.65 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16967713740555054		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.16967713740555054 | validation: 0.15758656947252525]
	TIME [epoch: 2.65 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16980382086951953		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.16980382086951953 | validation: 0.239611973638303]
	TIME [epoch: 2.66 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1746610707450378		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.1746610707450378 | validation: 0.14723548284071555]
	TIME [epoch: 2.65 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17687954553533666		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.17687954553533666 | validation: 0.24122495539351452]
	TIME [epoch: 2.65 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17524638890074168		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.17524638890074168 | validation: 0.14523922637427736]
	TIME [epoch: 2.65 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16981952421710797		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.16981952421710797 | validation: 0.2279842857997172]
	TIME [epoch: 2.65 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1698173854245867		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.1698173854245867 | validation: 0.1447498405887397]
	TIME [epoch: 2.65 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1728433963795171		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.1728433963795171 | validation: 0.24616943127014707]
	TIME [epoch: 2.65 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1772165876221554		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.1772165876221554 | validation: 0.137786102873616]
	TIME [epoch: 2.65 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18682540207123263		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.18682540207123263 | validation: 0.2101527725376625]
	TIME [epoch: 2.65 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17286545018383437		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.17286545018383437 | validation: 0.17559938532453534]
	TIME [epoch: 2.65 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16364151939223143		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.16364151939223143 | validation: 0.17168882703944252]
	TIME [epoch: 2.65 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16120430098498578		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.16120430098498578 | validation: 0.20853102538300505]
	TIME [epoch: 2.65 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16428183008468403		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.16428183008468403 | validation: 0.14749096935457334]
	TIME [epoch: 2.66 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16647771537199674		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.16647771537199674 | validation: 0.23814615587937898]
	TIME [epoch: 2.65 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17127617323731628		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.17127617323731628 | validation: 0.14467456991589267]
	TIME [epoch: 2.65 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17801300996917246		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.17801300996917246 | validation: 0.24159099745293178]
	TIME [epoch: 2.65 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18113070100423187		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.18113070100423187 | validation: 0.15387378511427502]
	TIME [epoch: 2.64 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17031644782097766		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.17031644782097766 | validation: 0.18282110495496837]
	TIME [epoch: 2.65 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16141049412484335		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.16141049412484335 | validation: 0.18910030153828636]
	TIME [epoch: 2.65 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1619134595571079		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.1619134595571079 | validation: 0.17015999039230875]
	TIME [epoch: 2.65 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1631517146073724		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.1631517146073724 | validation: 0.1929919513434345]
	TIME [epoch: 2.65 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1603945168653385		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.1603945168653385 | validation: 0.14430105414530758]
	TIME [epoch: 2.65 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16669304289071313		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.16669304289071313 | validation: 0.23537523861715592]
	TIME [epoch: 2.65 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17233921784279418		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.17233921784279418 | validation: 0.11936578617979149]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_536.pth
	Model improved!!!
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18409680073077467		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.18409680073077467 | validation: 0.2305885167425574]
	TIME [epoch: 2.65 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17135793498587767		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.17135793498587767 | validation: 0.16538077662279682]
	TIME [epoch: 2.65 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16572362644546254		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.16572362644546254 | validation: 0.15115814196834423]
	TIME [epoch: 2.65 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16424758326313416		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.16424758326313416 | validation: 0.22196224335714676]
	TIME [epoch: 2.65 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17237604023526168		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.17237604023526168 | validation: 0.15682304834613922]
	TIME [epoch: 2.65 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17737022819133877		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.17737022819133877 | validation: 0.22407908200980234]
	TIME [epoch: 2.65 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1738180825102335		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.1738180825102335 | validation: 0.16861095910607726]
	TIME [epoch: 2.65 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16060703827093112		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.16060703827093112 | validation: 0.17796072969320842]
	TIME [epoch: 2.65 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15887376615092233		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.15887376615092233 | validation: 0.19040969283036854]
	TIME [epoch: 2.65 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16051235994333687		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.16051235994333687 | validation: 0.15292836230881166]
	TIME [epoch: 2.65 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16401738277589442		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.16401738277589442 | validation: 0.213707321365137]
	TIME [epoch: 2.66 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16376020578421993		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.16376020578421993 | validation: 0.1321579384015433]
	TIME [epoch: 2.65 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17181059582004707		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.17181059582004707 | validation: 0.24520392523590595]
	TIME [epoch: 2.65 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17187696059427335		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.17187696059427335 | validation: 0.13630600987706284]
	TIME [epoch: 2.65 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16974731935327353		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.16974731935327353 | validation: 0.19145729403626047]
	TIME [epoch: 2.65 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16317173911711375		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.16317173911711375 | validation: 0.18243440119965593]
	TIME [epoch: 2.65 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16031413232837413		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.16031413232837413 | validation: 0.15716041050493582]
	TIME [epoch: 2.65 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16349854309647072		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.16349854309647072 | validation: 0.21057339502701622]
	TIME [epoch: 2.65 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17399529018434132		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.17399529018434132 | validation: 0.15230090558292245]
	TIME [epoch: 2.65 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17357062323545988		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.17357062323545988 | validation: 0.20640715616613356]
	TIME [epoch: 2.65 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16790557063408781		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.16790557063408781 | validation: 0.15509702736990882]
	TIME [epoch: 2.65 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15967805935263016		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.15967805935263016 | validation: 0.1914925855134324]
	TIME [epoch: 2.66 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1600245178337084		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.1600245178337084 | validation: 0.13966216257175712]
	TIME [epoch: 2.65 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16590582319021147		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.16590582319021147 | validation: 0.22710052899487865]
	TIME [epoch: 2.65 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16924081649689213		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.16924081649689213 | validation: 0.13380051583374306]
	TIME [epoch: 2.65 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16781956630198352		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.16781956630198352 | validation: 0.19857451992661146]
	TIME [epoch: 2.65 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1616248112466051		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.1616248112466051 | validation: 0.15919922470993367]
	TIME [epoch: 2.65 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1579109913772694		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.1579109913772694 | validation: 0.17514341438481518]
	TIME [epoch: 2.66 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16064843982845606		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.16064843982845606 | validation: 0.16916643638557216]
	TIME [epoch: 2.65 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1611212194882819		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.1611212194882819 | validation: 0.17548855852680023]
	TIME [epoch: 2.66 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16096186466779883		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.16096186466779883 | validation: 0.16004951802050763]
	TIME [epoch: 2.65 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16577070800963342		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.16577070800963342 | validation: 0.20433763791573792]
	TIME [epoch: 2.65 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16873400943460368		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.16873400943460368 | validation: 0.14128567983308846]
	TIME [epoch: 2.66 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1673306244745382		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.1673306244745382 | validation: 0.2289613880168343]
	TIME [epoch: 2.65 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16769950143888657		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.16769950143888657 | validation: 0.12379662280360236]
	TIME [epoch: 2.65 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16704722352556048		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.16704722352556048 | validation: 0.2199296278232854]
	TIME [epoch: 2.65 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16483034690724427		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.16483034690724427 | validation: 0.13977636744244235]
	TIME [epoch: 2.65 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1613299625972217		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.1613299625972217 | validation: 0.17489179800655905]
	TIME [epoch: 2.65 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1564502820047501		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.1564502820047501 | validation: 0.18247616967571711]
	TIME [epoch: 2.65 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15816134741416646		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.15816134741416646 | validation: 0.14613458870970236]
	TIME [epoch: 2.66 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15812760218014849		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.15812760218014849 | validation: 0.1985962454687579]
	TIME [epoch: 2.66 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15687076909901015		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.15687076909901015 | validation: 0.14933602521107642]
	TIME [epoch: 2.66 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16063940967188153		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.16063940967188153 | validation: 0.2128552383347697]
	TIME [epoch: 2.65 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16353988892506433		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.16353988892506433 | validation: 0.1312439291075388]
	TIME [epoch: 2.66 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17054871700928032		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.17054871700928032 | validation: 0.23172162564355345]
	TIME [epoch: 2.65 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16670212725096076		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.16670212725096076 | validation: 0.13924460342581416]
	TIME [epoch: 2.65 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1589267902128025		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.1589267902128025 | validation: 0.18432862575854136]
	TIME [epoch: 2.65 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15416173641732053		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.15416173641732053 | validation: 0.15995305697468287]
	TIME [epoch: 2.65 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.156615984890376		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.156615984890376 | validation: 0.17412336278486046]
	TIME [epoch: 2.65 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15799441372295167		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.15799441372295167 | validation: 0.16901256992646424]
	TIME [epoch: 2.65 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16073264480636268		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.16073264480636268 | validation: 0.1771801725531438]
	TIME [epoch: 2.65 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16345284734301985		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.16345284734301985 | validation: 0.160572475789068]
	TIME [epoch: 2.65 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16456748657332138		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.16456748657332138 | validation: 0.18741171321616107]
	TIME [epoch: 2.65 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1632135974028867		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.1632135974028867 | validation: 0.13623829476861987]
	TIME [epoch: 2.65 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15897461016562983		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.15897461016562983 | validation: 0.20196044811172517]
	TIME [epoch: 2.65 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15872254642115022		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.15872254642115022 | validation: 0.124663984573823]
	TIME [epoch: 2.66 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16241015585440632		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.16241015585440632 | validation: 0.22973027372350072]
	TIME [epoch: 2.65 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16584659473574218		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.16584659473574218 | validation: 0.1302918810271044]
	TIME [epoch: 2.65 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16397881911990672		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.16397881911990672 | validation: 0.19177836771745824]
	TIME [epoch: 2.65 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1564917195610788		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.1564917195610788 | validation: 0.1562315582444836]
	TIME [epoch: 2.65 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1530473373630178		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.1530473373630178 | validation: 0.17168457375263854]
	TIME [epoch: 2.65 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15303616189638158		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.15303616189638158 | validation: 0.169960638570584]
	TIME [epoch: 2.65 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1559249772166109		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.1559249772166109 | validation: 0.1565178841257717]
	TIME [epoch: 2.65 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.149594981317372		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.149594981317372 | validation: 0.16788349477741604]
	TIME [epoch: 2.65 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15340859299735016		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.15340859299735016 | validation: 0.16433809968892416]
	TIME [epoch: 2.65 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15131699178039532		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.15131699178039532 | validation: 0.16995257636326455]
	TIME [epoch: 2.66 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15725849896905236		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.15725849896905236 | validation: 0.16940377974814172]
	TIME [epoch: 2.65 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16649868201439624		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.16649868201439624 | validation: 0.18156385766140215]
	TIME [epoch: 2.65 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17048957460110103		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.17048957460110103 | validation: 0.16095717792129816]
	TIME [epoch: 2.65 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15737564091812112		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.15737564091812112 | validation: 0.16106687063869032]
	TIME [epoch: 2.65 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15392392706622393		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.15392392706622393 | validation: 0.17132142338777057]
	TIME [epoch: 2.65 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15260371941681594		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.15260371941681594 | validation: 0.1399940098078538]
	TIME [epoch: 2.65 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15488701878192696		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.15488701878192696 | validation: 0.21342842245712113]
	TIME [epoch: 2.65 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1592907077560182		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.1592907077560182 | validation: 0.10879834597371314]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_610.pth
	Model improved!!!
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17141053317176		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.17141053317176 | validation: 0.2531086702507099]
	TIME [epoch: 2.65 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1754198443055117		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.1754198443055117 | validation: 0.13599126979109508]
	TIME [epoch: 2.65 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15880396890979964		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.15880396890979964 | validation: 0.15489991791030713]
	TIME [epoch: 2.66 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15197903047723396		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.15197903047723396 | validation: 0.1858631370195664]
	TIME [epoch: 2.65 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15228114457100295		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.15228114457100295 | validation: 0.15129630601797872]
	TIME [epoch: 2.65 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1536731811536043		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.1536731811536043 | validation: 0.17442529127719725]
	TIME [epoch: 2.65 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15266944368271665		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.15266944368271665 | validation: 0.15493419551582008]
	TIME [epoch: 2.69 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15182206050176766		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.15182206050176766 | validation: 0.18337691610413415]
	TIME [epoch: 2.65 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15177185019178308		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.15177185019178308 | validation: 0.1457353648707899]
	TIME [epoch: 2.65 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15321734506042387		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.15321734506042387 | validation: 0.19161917924425698]
	TIME [epoch: 2.65 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15575855832177699		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.15575855832177699 | validation: 0.13882335639946455]
	TIME [epoch: 2.65 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1560951635840374		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.1560951635840374 | validation: 0.18999046200220984]
	TIME [epoch: 2.65 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15427866526307468		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.15427866526307468 | validation: 0.14879924524912444]
	TIME [epoch: 2.65 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15214994059353845		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.15214994059353845 | validation: 0.1809607012665827]
	TIME [epoch: 2.65 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15317807482110704		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.15317807482110704 | validation: 0.14479349057018695]
	TIME [epoch: 2.66 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1537277504971537		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.1537277504971537 | validation: 0.20454841399407214]
	TIME [epoch: 2.65 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15694228078070022		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.15694228078070022 | validation: 0.10504396089782445]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_627.pth
	Model improved!!!
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16940721103713416		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.16940721103713416 | validation: 0.22499510585671267]
	TIME [epoch: 2.65 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17307387462149607		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.17307387462149607 | validation: 0.13573237191425905]
	TIME [epoch: 2.65 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15391637569266778		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.15391637569266778 | validation: 0.14392154379011043]
	TIME [epoch: 2.65 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14961734700543253		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.14961734700543253 | validation: 0.18680550370630095]
	TIME [epoch: 2.65 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14862846826992335		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.14862846826992335 | validation: 0.15321490265347382]
	TIME [epoch: 2.65 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14907661634730457		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.14907661634730457 | validation: 0.17910328377885687]
	TIME [epoch: 2.65 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15070113300575474		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.15070113300575474 | validation: 0.15792889408092894]
	TIME [epoch: 2.65 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15026523185896615		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.15026523185896615 | validation: 0.17329274500641945]
	TIME [epoch: 2.66 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.155910893056241		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.155910893056241 | validation: 0.15630034157876357]
	TIME [epoch: 2.65 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15846225519702314		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.15846225519702314 | validation: 0.1848889594949266]
	TIME [epoch: 2.65 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15434432652839664		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.15434432652839664 | validation: 0.14098498663590178]
	TIME [epoch: 2.65 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.149419982244355		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.149419982244355 | validation: 0.19304948881008005]
	TIME [epoch: 2.65 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14948211153891566		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.14948211153891566 | validation: 0.12849833387296686]
	TIME [epoch: 2.65 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1526156231899088		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.1526156231899088 | validation: 0.19587362486207632]
	TIME [epoch: 2.65 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15495467680923242		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.15495467680923242 | validation: 0.12247939922422955]
	TIME [epoch: 2.65 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15672950126170132		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.15672950126170132 | validation: 0.20393853721252314]
	TIME [epoch: 2.65 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1524757749459469		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.1524757749459469 | validation: 0.1402853640149145]
	TIME [epoch: 2.65 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15009512365742775		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.15009512365742775 | validation: 0.17369164556891434]
	TIME [epoch: 2.65 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14791705847579892		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.14791705847579892 | validation: 0.15553947504696453]
	TIME [epoch: 2.65 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1496116678316754		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.1496116678316754 | validation: 0.17595123090578377]
	TIME [epoch: 2.66 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14806192667262955		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.14806192667262955 | validation: 0.15327162509731626]
	TIME [epoch: 2.66 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15129540875867123		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.15129540875867123 | validation: 0.17925450390030992]
	TIME [epoch: 2.65 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15032414150575618		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.15032414150575618 | validation: 0.1500141075907698]
	TIME [epoch: 2.65 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15035777814491647		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.15035777814491647 | validation: 0.17582221199230555]
	TIME [epoch: 2.65 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14863462532797356		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.14863462532797356 | validation: 0.13785993771630498]
	TIME [epoch: 2.65 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15059328955125345		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.15059328955125345 | validation: 0.19712238860531162]
	TIME [epoch: 2.65 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1502763737537796		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.1502763737537796 | validation: 0.11969946597872859]
	TIME [epoch: 2.66 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15713333533756044		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.15713333533756044 | validation: 0.22525175637493047]
	TIME [epoch: 2.65 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16268435679696502		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.16268435679696502 | validation: 0.11878108969176089]
	TIME [epoch: 2.65 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15234384400035098		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.15234384400035098 | validation: 0.1590711763882967]
	TIME [epoch: 2.65 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14329785043933047		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.14329785043933047 | validation: 0.1915646401591081]
	TIME [epoch: 2.66 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15016096745760876		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.15016096745760876 | validation: 0.1349989715600903]
	TIME [epoch: 2.65 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1485754313033418		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.1485754313033418 | validation: 0.1845984597879923]
	TIME [epoch: 2.65 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14808124806260026		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.14808124806260026 | validation: 0.14904252012748015]
	TIME [epoch: 2.65 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14624047048951963		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.14624047048951963 | validation: 0.15840156817739684]
	TIME [epoch: 2.65 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14878927996911703		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.14878927996911703 | validation: 0.15833933796784094]
	TIME [epoch: 2.65 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14913045189993515		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.14913045189993515 | validation: 0.16465083163915917]
	TIME [epoch: 2.65 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.150096782434413		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.150096782434413 | validation: 0.15234724722486057]
	TIME [epoch: 2.65 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14646718603706282		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.14646718603706282 | validation: 0.17148741847667695]
	TIME [epoch: 2.65 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14455899235313033		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.14455899235313033 | validation: 0.1448181779660156]
	TIME [epoch: 2.65 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14321729203795164		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.14321729203795164 | validation: 0.1652020319394632]
	TIME [epoch: 2.65 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14300565847237479		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.14300565847237479 | validation: 0.134323231363916]
	TIME [epoch: 2.66 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14663141292031337		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.14663141292031337 | validation: 0.22036602214157608]
	TIME [epoch: 2.65 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15906353211801957		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.15906353211801957 | validation: 0.10417451236879452]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_671.pth
	Model improved!!!
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16921419880137123		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.16921419880137123 | validation: 0.19772777702081756]
	TIME [epoch: 2.65 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1527947122724408		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.1527947122724408 | validation: 0.15716745164615803]
	TIME [epoch: 2.65 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1437821048290559		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.1437821048290559 | validation: 0.14074600127654432]
	TIME [epoch: 2.66 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14649493500098415		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.14649493500098415 | validation: 0.18423671611289705]
	TIME [epoch: 2.65 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14656085433295304		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.14656085433295304 | validation: 0.14527787428421224]
	TIME [epoch: 2.66 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14201892432900473		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.14201892432900473 | validation: 0.14984675270526981]
	TIME [epoch: 2.65 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1420158149215404		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.1420158149215404 | validation: 0.15453633383136772]
	TIME [epoch: 2.66 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14266963226378054		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.14266963226378054 | validation: 0.15422859593601923]
	TIME [epoch: 2.65 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14395409927669767		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.14395409927669767 | validation: 0.16585699363235862]
	TIME [epoch: 2.67 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1469989552037668		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.1469989552037668 | validation: 0.144602073878442]
	TIME [epoch: 2.65 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15219661710977994		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.15219661710977994 | validation: 0.1892431371914125]
	TIME [epoch: 2.65 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15241069320500902		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.15241069320500902 | validation: 0.13546670069017158]
	TIME [epoch: 2.65 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14684496600488797		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.14684496600488797 | validation: 0.1798019023777956]
	TIME [epoch: 2.65 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14528956740687127		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.14528956740687127 | validation: 0.12703294594636325]
	TIME [epoch: 2.65 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14533321468849725		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.14533321468849725 | validation: 0.17980819278619709]
	TIME [epoch: 2.65 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14751341977826674		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.14751341977826674 | validation: 0.12280053240394127]
	TIME [epoch: 2.65 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14709407037012043		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.14709407037012043 | validation: 0.18047459511927444]
	TIME [epoch: 2.65 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14456935831178552		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.14456935831178552 | validation: 0.14345590575715392]
	TIME [epoch: 2.65 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14253179130430982		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.14253179130430982 | validation: 0.16440736095088254]
	TIME [epoch: 2.65 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14272762554152935		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.14272762554152935 | validation: 0.1508393240656032]
	TIME [epoch: 2.66 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1454234929428927		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.1454234929428927 | validation: 0.16826547372827105]
	TIME [epoch: 2.65 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14527754865555248		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.14527754865555248 | validation: 0.1454694332464928]
	TIME [epoch: 2.65 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14579388411069258		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.14579388411069258 | validation: 0.16315765420657696]
	TIME [epoch: 2.65 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14371963686794315		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.14371963686794315 | validation: 0.14875882934347864]
	TIME [epoch: 2.65 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1421071634320355		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.1421071634320355 | validation: 0.1662871960242298]
	TIME [epoch: 2.65 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14068669412774093		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.14068669412774093 | validation: 0.13907452035885134]
	TIME [epoch: 2.65 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14130807598270098		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.14130807598270098 | validation: 0.1944246765719073]
	TIME [epoch: 2.65 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1473402191472718		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.1473402191472718 | validation: 0.11175155727255733]
	TIME [epoch: 2.65 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15769196192123666		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.15769196192123666 | validation: 0.22796931137491627]
	TIME [epoch: 2.65 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15887022320910846		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.15887022320910846 | validation: 0.1275036004483454]
	TIME [epoch: 2.65 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14342801561753799		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.14342801561753799 | validation: 0.15086927320802523]
	TIME [epoch: 2.67 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.140066166948948		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.140066166948948 | validation: 0.18168647953442876]
	TIME [epoch: 2.65 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14176226870166242		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.14176226870166242 | validation: 0.1435625210666674]
	TIME [epoch: 2.65 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1394652944048367		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.1394652944048367 | validation: 0.16583276024623955]
	TIME [epoch: 2.65 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14095084654200446		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.14095084654200446 | validation: 0.14248769098280653]
	TIME [epoch: 2.65 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14092600051751858		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.14092600051751858 | validation: 0.1469291433219223]
	TIME [epoch: 2.65 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1387167638638094		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.1387167638638094 | validation: 0.1621901436549946]
	TIME [epoch: 2.65 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13929296143420708		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.13929296143420708 | validation: 0.1361838693589103]
	TIME [epoch: 2.65 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14201088065759124		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.14201088065759124 | validation: 0.17596021805989193]
	TIME [epoch: 2.65 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14201157254492056		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.14201157254492056 | validation: 0.12122188840981044]
	TIME [epoch: 2.65 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14782222463182007		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.14782222463182007 | validation: 0.20318580880270806]
	TIME [epoch: 2.65 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15094372027355815		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.15094372027355815 | validation: 0.12365798221034736]
	TIME [epoch: 2.66 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14738220566661628		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.14738220566661628 | validation: 0.17851355876119476]
	TIME [epoch: 2.65 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1436483646482828		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.1436483646482828 | validation: 0.14740622402861275]
	TIME [epoch: 2.65 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14058070665029948		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.14058070665029948 | validation: 0.1430251929099837]
	TIME [epoch: 2.65 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14353539807014543		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.14353539807014543 | validation: 0.17934594360668255]
	TIME [epoch: 2.65 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14660568402852497		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.14660568402852497 | validation: 0.12306745405146996]
	TIME [epoch: 2.65 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14224861156988308		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.14224861156988308 | validation: 0.17092992511633592]
	TIME [epoch: 2.65 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13904898094033927		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.13904898094033927 | validation: 0.14499631264956117]
	TIME [epoch: 2.65 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13940822257299743		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.13940822257299743 | validation: 0.16936551548721415]
	TIME [epoch: 2.65 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14300300532790672		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.14300300532790672 | validation: 0.13947186516289883]
	TIME [epoch: 2.65 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13959313969685402		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.13959313969685402 | validation: 0.16909451081248777]
	TIME [epoch: 2.65 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1419533729184763		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.1419533729184763 | validation: 0.13495503731415903]
	TIME [epoch: 2.65 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1410337808549936		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.1410337808549936 | validation: 0.1867144781754228]
	TIME [epoch: 2.66 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14336275748119426		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.14336275748119426 | validation: 0.11504813550469942]
	TIME [epoch: 2.65 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15059575456674684		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.15059575456674684 | validation: 0.19611611168546844]
	TIME [epoch: 2.65 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1467512060585731		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.1467512060585731 | validation: 0.13500347422806805]
	TIME [epoch: 2.65 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.140234413652179		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.140234413652179 | validation: 0.15426911448541936]
	TIME [epoch: 2.65 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1376183714574683		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.1376183714574683 | validation: 0.16597753872804738]
	TIME [epoch: 2.65 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1388197816800242		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.1388197816800242 | validation: 0.14849968818644493]
	TIME [epoch: 2.65 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13812484930229982		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.13812484930229982 | validation: 0.15006062064079323]
	TIME [epoch: 2.65 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13640451848828317		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.13640451848828317 | validation: 0.15159020180833435]
	TIME [epoch: 2.65 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14023550571523966		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.14023550571523966 | validation: 0.13380131076847945]
	TIME [epoch: 2.65 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14015831713009466		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.14015831713009466 | validation: 0.19140256191035762]
	TIME [epoch: 2.65 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14291354616472185		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.14291354616472185 | validation: 0.1187769032745793]
	TIME [epoch: 2.66 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1445121346664627		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.1445121346664627 | validation: 0.1806579924458125]
	TIME [epoch: 2.65 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1419920144074701		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.1419920144074701 | validation: 0.13666092122471163]
	TIME [epoch: 2.65 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13842736894939117		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.13842736894939117 | validation: 0.15786415819559385]
	TIME [epoch: 2.65 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13604931469388734		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.13604931469388734 | validation: 0.1455542641442403]
	TIME [epoch: 2.65 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13637090744048408		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.13637090744048408 | validation: 0.14678464572124655]
	TIME [epoch: 2.65 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1346808543815477		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.1346808543815477 | validation: 0.15738721104965592]
	TIME [epoch: 2.65 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13642699902537145		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.13642699902537145 | validation: 0.13185886099546762]
	TIME [epoch: 2.65 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1394833310630297		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.1394833310630297 | validation: 0.17560265559646893]
	TIME [epoch: 2.65 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14690129113661954		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.14690129113661954 | validation: 0.11945529787688457]
	TIME [epoch: 2.65 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14937839155778287		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.14937839155778287 | validation: 0.16545622969322943]
	TIME [epoch: 2.65 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14070915106754162		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.14070915106754162 | validation: 0.1479705184397707]
	TIME [epoch: 2.66 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13624369005635295		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.13624369005635295 | validation: 0.15840018751253332]
	TIME [epoch: 2.65 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13796435700664145		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.13796435700664145 | validation: 0.15247957110863083]
	TIME [epoch: 2.65 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1380884149670747		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.1380884149670747 | validation: 0.14980094705758415]
	TIME [epoch: 2.65 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13496043669690044		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.13496043669690044 | validation: 0.14107998427311774]
	TIME [epoch: 2.65 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1338325284090231		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.1338325284090231 | validation: 0.15793712760736428]
	TIME [epoch: 2.65 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13362766287945835		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.13362766287945835 | validation: 0.12737311359356873]
	TIME [epoch: 2.65 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13489473523450898		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.13489473523450898 | validation: 0.19459040038228315]
	TIME [epoch: 2.66 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14283009346967948		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.14283009346967948 | validation: 0.11456497025453644]
	TIME [epoch: 2.65 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1507605709643762		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.1507605709643762 | validation: 0.19472120241225338]
	TIME [epoch: 2.65 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14574143548323357		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.14574143548323357 | validation: 0.13769451941838798]
	TIME [epoch: 2.65 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13512794964177893		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.13512794964177893 | validation: 0.1346735260130318]
	TIME [epoch: 2.66 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1351113857693159		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.1351113857693159 | validation: 0.16686602241590964]
	TIME [epoch: 2.65 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1358243400385617		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.1358243400385617 | validation: 0.13615109872933073]
	TIME [epoch: 2.65 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13757220332781597		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.13757220332781597 | validation: 0.157555032352863]
	TIME [epoch: 2.65 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13472962539771666		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.13472962539771666 | validation: 0.14407460623771143]
	TIME [epoch: 2.65 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13635431786184016		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.13635431786184016 | validation: 0.1464671937194634]
	TIME [epoch: 2.65 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1374457116029101		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.1374457116029101 | validation: 0.14624849973154863]
	TIME [epoch: 2.65 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13664122683640237		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.13664122683640237 | validation: 0.14533173151313591]
	TIME [epoch: 2.65 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1398286330134913		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.1398286330134913 | validation: 0.14509224144445576]
	TIME [epoch: 2.65 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1380811169710694		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.1380811169710694 | validation: 0.16265953632218322]
	TIME [epoch: 2.65 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13725834228163072		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.13725834228163072 | validation: 0.13488539218171697]
	TIME [epoch: 2.65 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13526872250498276		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.13526872250498276 | validation: 0.17531284121986854]
	TIME [epoch: 2.66 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1370449270239977		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.1370449270239977 | validation: 0.10689525483599237]
	TIME [epoch: 2.65 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1447813810772614		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.1447813810772614 | validation: 0.19794244511009387]
	TIME [epoch: 2.65 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1462541544206414		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.1462541544206414 | validation: 0.12886508219387946]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_2_v_mmd1_772.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 1849.302 seconds.
