Args:
Namespace(name='model_phi1_4a_distortion3_v_mmd1', outdir='out/model_training/model_phi1_4a_distortion3_v_mmd1', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion3/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion3/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2334987757

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.505297237082443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.505297237082443 | validation: 5.521242243485428]
	TIME [epoch: 127 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.4204803129040355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.4204803129040355 | validation: 5.11828521019163]
	TIME [epoch: 0.482 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.337379342770312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.337379342770312 | validation: 4.7971715629091305]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.160489185833485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.160489185833485 | validation: 4.125932418575158]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.869324485614351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.869324485614351 | validation: 2.7113094041925025]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.406143647203189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.406143647203189 | validation: 6.013137175610485]
	TIME [epoch: 0.477 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.476370474892653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.476370474892653 | validation: 6.158920535826412]
	TIME [epoch: 0.474 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.656471627513492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.656471627513492 | validation: 5.363503471299139]
	TIME [epoch: 0.475 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.106186143007656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.106186143007656 | validation: 4.980134155325068]
	TIME [epoch: 0.474 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.934228781268005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.934228781268005 | validation: 4.714862787748451]
	TIME [epoch: 0.476 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.746616388179469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.746616388179469 | validation: 4.3438646680071]
	TIME [epoch: 0.475 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.577505352007838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.577505352007838 | validation: 3.471599938411305]
	TIME [epoch: 0.474 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.154142658031852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.154142658031852 | validation: 2.7452562267683565]
	TIME [epoch: 0.474 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.027996296065487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.027996296065487 | validation: 2.5616184117315375]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.412900005602694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.412900005602694 | validation: 2.3836855101908125]
	TIME [epoch: 0.477 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1943007539751602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1943007539751602 | validation: 3.5506296907712724]
	TIME [epoch: 0.475 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.1155747360888295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1155747360888295 | validation: 2.678633338878966]
	TIME [epoch: 0.474 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5693696559729777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5693696559729777 | validation: 2.9676520482136564]
	TIME [epoch: 0.475 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.482747131195524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.482747131195524 | validation: 2.0089673668065244]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0714333694051787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0714333694051787 | validation: 1.8120476103027383]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.974446206782575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.974446206782575 | validation: 3.229611143706332]
	TIME [epoch: 0.475 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.504607540246165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.504607540246165 | validation: 2.026929698483656]
	TIME [epoch: 0.475 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.059804413534781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.059804413534781 | validation: 1.8197267239065882]
	TIME [epoch: 0.473 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8574258552291947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8574258552291947 | validation: 2.0971481087181454]
	TIME [epoch: 0.473 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.968704068498703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.968704068498703 | validation: 1.9552991888270173]
	TIME [epoch: 0.473 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.812729964219667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.812729964219667 | validation: 2.3897879822766988]
	TIME [epoch: 0.473 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.027487966230308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.027487966230308 | validation: 1.8822756715836846]
	TIME [epoch: 0.473 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.855516015570537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.855516015570537 | validation: 1.6655727632401849]
	TIME [epoch: 0.473 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.673006586663539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.673006586663539 | validation: 2.135089737391401]
	TIME [epoch: 0.479 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6675892473769602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6675892473769602 | validation: 2.215096227555128]
	TIME [epoch: 0.474 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.933615184222333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.933615184222333 | validation: 1.625658325611864]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4613808763742013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4613808763742013 | validation: 2.870524455607679]
	TIME [epoch: 0.474 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9407019201770153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9407019201770153 | validation: 1.7290344523606163]
	TIME [epoch: 0.473 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5329157397392676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5329157397392676 | validation: 1.7679017090737628]
	TIME [epoch: 0.474 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2991980854044134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2991980854044134 | validation: 1.586758401260378]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.168123145263647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.168123145263647 | validation: 2.1680828402774828]
	TIME [epoch: 0.475 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4085858852778226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4085858852778226 | validation: 1.7885387059028255]
	TIME [epoch: 0.474 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.70387862161736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.70387862161736 | validation: 1.9601623981532066]
	TIME [epoch: 0.475 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2138948851466003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2138948851466003 | validation: 1.8010522396791218]
	TIME [epoch: 0.474 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.002172337280371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.002172337280371 | validation: 1.870164390843005]
	TIME [epoch: 0.474 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1503771337707196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1503771337707196 | validation: 2.0718823480461626]
	TIME [epoch: 0.474 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.155512621844607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.155512621844607 | validation: 1.803620792575451]
	TIME [epoch: 0.475 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9637350926215402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9637350926215402 | validation: 1.7142868327832006]
	TIME [epoch: 0.475 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7386523942910597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7386523942910597 | validation: 1.6310582280021049]
	TIME [epoch: 0.474 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.625815305970484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.625815305970484 | validation: 1.2144121853487424]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0361790332840024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0361790332840024 | validation: 2.551953855631078]
	TIME [epoch: 0.475 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4162760601028714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4162760601028714 | validation: 1.6290352943350852]
	TIME [epoch: 0.476 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6551885044429424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6551885044429424 | validation: 1.3800765457617528]
	TIME [epoch: 0.474 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8753182968562698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8753182968562698 | validation: 1.725355412339451]
	TIME [epoch: 0.474 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6311020472195175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6311020472195175 | validation: 1.366369437006699]
	TIME [epoch: 0.474 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4525451087553751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4525451087553751 | validation: 1.7667365380454456]
	TIME [epoch: 0.475 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5431788470281178		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.5431788470281178 | validation: 1.3053894400866832]
	TIME [epoch: 0.474 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5131691842540336		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.5131691842540336 | validation: 1.6788773304296383]
	TIME [epoch: 0.474 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5563644349562145		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.5563644349562145 | validation: 1.1776698984200586]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.354866914355767		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.354866914355767 | validation: 1.4336112982159666]
	TIME [epoch: 0.475 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4058451792519295		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.4058451792519295 | validation: 1.4284785311969372]
	TIME [epoch: 0.475 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5705614761554656		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.5705614761554656 | validation: 1.4858379262183017]
	TIME [epoch: 0.475 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.576433776158763		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.576433776158763 | validation: 1.2665860777850364]
	TIME [epoch: 0.475 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2587167048770689		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.2587167048770689 | validation: 1.124072912026444]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1638378120527244		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.1638378120527244 | validation: 1.1706909208943492]
	TIME [epoch: 0.476 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1768437051816198		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.1768437051816198 | validation: 1.1793504388555112]
	TIME [epoch: 0.474 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.240191472481066		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.240191472481066 | validation: 1.2968553521134536]
	TIME [epoch: 0.474 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3767416258220089		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.3767416258220089 | validation: 1.2982130361423885]
	TIME [epoch: 0.475 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3646027012107949		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.3646027012107949 | validation: 1.0069302967788925]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2009855964754763		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.2009855964754763 | validation: 1.2802945568269486]
	TIME [epoch: 0.474 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2503721497090003		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.2503721497090003 | validation: 0.87030966831835]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2898924208788998		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.2898924208788998 | validation: 1.7507520089939335]
	TIME [epoch: 0.475 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5006162689667388		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.5006162689667388 | validation: 0.9516852956208738]
	TIME [epoch: 0.474 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.221106959513587		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.221106959513587 | validation: 1.0933446593378702]
	TIME [epoch: 0.474 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.096184071979337		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.096184071979337 | validation: 0.9299134021075487]
	TIME [epoch: 0.474 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0757604179814948		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.0757604179814948 | validation: 1.0644695938220246]
	TIME [epoch: 0.475 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0844502244676508		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.0844502244676508 | validation: 0.8998461299590327]
	TIME [epoch: 0.473 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1572391496387429		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.1572391496387429 | validation: 1.2494220667246552]
	TIME [epoch: 0.474 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2998739398896266		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.2998739398896266 | validation: 1.0733763980570927]
	TIME [epoch: 0.475 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2269814914382104		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.2269814914382104 | validation: 1.0978846028326241]
	TIME [epoch: 0.475 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2211062290763999		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.2211062290763999 | validation: 1.262110831353513]
	TIME [epoch: 0.478 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1868235716331714		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.1868235716331714 | validation: 1.0003839480723065]
	TIME [epoch: 0.475 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1101072562376786		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.1101072562376786 | validation: 1.0084432636095575]
	TIME [epoch: 0.475 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0881245559576815		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.0881245559576815 | validation: 0.99895188875369]
	TIME [epoch: 0.475 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1254693818226627		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.1254693818226627 | validation: 1.0323621307085407]
	TIME [epoch: 0.475 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1280500399347704		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.1280500399347704 | validation: 1.0777738659345062]
	TIME [epoch: 0.476 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.143597679449587		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.143597679449587 | validation: 0.8993154962896697]
	TIME [epoch: 0.475 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1300358440334122		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.1300358440334122 | validation: 1.1532857613939385]
	TIME [epoch: 0.475 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1535977928428127		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.1535977928428127 | validation: 0.8168696606533237]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1733066016146336		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.1733066016146336 | validation: 1.488093138118812]
	TIME [epoch: 0.483 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2875453799121517		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.2875453799121517 | validation: 0.825313773090112]
	TIME [epoch: 0.476 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1440577508281689		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.1440577508281689 | validation: 1.0375699944312502]
	TIME [epoch: 0.474 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0504022310733128		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.0504022310733128 | validation: 0.8641689695700973]
	TIME [epoch: 0.474 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9966298579814378		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 0.9966298579814378 | validation: 0.9199094565867629]
	TIME [epoch: 0.474 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9983477428077532		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.9983477428077532 | validation: 0.929245229922311]
	TIME [epoch: 0.475 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0310247856272312		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.0310247856272312 | validation: 1.018237946800722]
	TIME [epoch: 0.474 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1425794685463662		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.1425794685463662 | validation: 0.8424413624947079]
	TIME [epoch: 0.474 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0547385414429338		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.0547385414429338 | validation: 0.9864140715960752]
	TIME [epoch: 0.473 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0573985046609498		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.0573985046609498 | validation: 0.8138703104565949]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0717846105849498		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.0717846105849498 | validation: 0.9598650744705144]
	TIME [epoch: 0.474 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.03633752716899		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.03633752716899 | validation: 0.830310310600461]
	TIME [epoch: 0.472 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0162666608639228		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.0162666608639228 | validation: 1.0674007054036403]
	TIME [epoch: 0.474 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1681256065182968		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.1681256065182968 | validation: 1.2364322456399017]
	TIME [epoch: 0.475 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1095765897989491		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.1095765897989491 | validation: 0.7375970062964781]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9777109375090458		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.9777109375090458 | validation: 1.0087255325723246]
	TIME [epoch: 0.476 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0400027134151852		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.0400027134151852 | validation: 0.8849433290143298]
	TIME [epoch: 0.475 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0420450073629954		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.0420450073629954 | validation: 1.0843717211095134]
	TIME [epoch: 0.474 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0379157125028011		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.0379157125028011 | validation: 0.8256022223744903]
	TIME [epoch: 0.475 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.057622293358133		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.057622293358133 | validation: 1.0882770884001525]
	TIME [epoch: 0.474 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0676662185026253		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.0676662185026253 | validation: 0.7124403034402849]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1152768543737206		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.1152768543737206 | validation: 0.9964985171395265]
	TIME [epoch: 0.475 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0762262553289392		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.0762262553289392 | validation: 0.7565226696339006]
	TIME [epoch: 0.473 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0282321866235509		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.0282321866235509 | validation: 0.9033299620538314]
	TIME [epoch: 0.473 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9943111492534734		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.9943111492534734 | validation: 0.8557176741561415]
	TIME [epoch: 0.473 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0075405126956107		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.0075405126956107 | validation: 0.9158332729331378]
	TIME [epoch: 0.473 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0215756603258448		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.0215756603258448 | validation: 0.9754086024575322]
	TIME [epoch: 0.472 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0878857921184226		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.0878857921184226 | validation: 0.9892174227902291]
	TIME [epoch: 0.474 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.092432799113935		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.092432799113935 | validation: 0.8603896111914381]
	TIME [epoch: 0.474 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9231910508446849		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.9231910508446849 | validation: 0.8248090148641105]
	TIME [epoch: 0.474 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9167592879058484		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.9167592879058484 | validation: 0.7930956987221943]
	TIME [epoch: 0.473 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9634494356930893		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.9634494356930893 | validation: 0.9959763694753984]
	TIME [epoch: 0.473 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9912773216332411		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.9912773216332411 | validation: 0.7685208237807606]
	TIME [epoch: 0.473 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0057667369351004		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.0057667369351004 | validation: 0.851415330711575]
	TIME [epoch: 0.474 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9311781845505259		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.9311781845505259 | validation: 0.7549053449643037]
	TIME [epoch: 0.474 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8649359187921435		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.8649359187921435 | validation: 0.765134956425876]
	TIME [epoch: 0.477 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8743422386215657		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.8743422386215657 | validation: 1.1061296464219827]
	TIME [epoch: 0.473 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0227537747648625		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.0227537747648625 | validation: 1.041068197223769]
	TIME [epoch: 0.474 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.231401138045004		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.231401138045004 | validation: 1.0491113255281495]
	TIME [epoch: 0.475 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0262813200286485		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.0262813200286485 | validation: 0.7676819769331685]
	TIME [epoch: 0.474 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0052992142161485		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.0052992142161485 | validation: 0.98697889714787]
	TIME [epoch: 0.474 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.093665740724005		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.093665740724005 | validation: 0.8605113940959432]
	TIME [epoch: 0.474 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0816179917295592		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.0816179917295592 | validation: 0.9527022937536818]
	TIME [epoch: 0.474 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0297015954308575		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.0297015954308575 | validation: 0.7413238509531253]
	TIME [epoch: 0.474 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0132051628691172		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.0132051628691172 | validation: 0.978799379924899]
	TIME [epoch: 0.474 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0213248507511727		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.0213248507511727 | validation: 0.715520795018878]
	TIME [epoch: 0.474 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.014307831716622		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.014307831716622 | validation: 1.0452913543842557]
	TIME [epoch: 0.474 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.035951072417746		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.035951072417746 | validation: 0.7693839071626297]
	TIME [epoch: 0.474 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9839153605135941		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.9839153605135941 | validation: 0.8386506057981085]
	TIME [epoch: 0.474 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9397246674040096		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.9397246674040096 | validation: 0.7269543683076805]
	TIME [epoch: 0.473 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.915686173071881		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.915686173071881 | validation: 0.7725466155403655]
	TIME [epoch: 0.473 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8740688992734807		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.8740688992734807 | validation: 1.165970381087668]
	TIME [epoch: 0.473 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1509792459671735		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.1509792459671735 | validation: 1.191015940247564]
	TIME [epoch: 0.474 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3133147232944935		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.3133147232944935 | validation: 1.130875187579795]
	TIME [epoch: 0.475 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0725312367311253		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.0725312367311253 | validation: 0.927142989015284]
	TIME [epoch: 0.473 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0110629991090447		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.0110629991090447 | validation: 0.8588872462852365]
	TIME [epoch: 0.474 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0066934154268572		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.0066934154268572 | validation: 0.7704974504913276]
	TIME [epoch: 0.474 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9371772302285988		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.9371772302285988 | validation: 0.801626333753016]
	TIME [epoch: 0.476 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9374526113568296		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.9374526113568296 | validation: 0.7741661341267089]
	TIME [epoch: 0.474 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9011401286096543		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.9011401286096543 | validation: 0.7553909103802022]
	TIME [epoch: 0.479 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8458342159443203		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.8458342159443203 | validation: 0.8370205837587824]
	TIME [epoch: 0.474 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8670342546472487		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.8670342546472487 | validation: 0.7599001739967565]
	TIME [epoch: 0.474 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9215219805003438		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.9215219805003438 | validation: 0.83755114862972]
	TIME [epoch: 0.475 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8808468450179919		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.8808468450179919 | validation: 0.7107200058972873]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9288620522461412		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.9288620522461412 | validation: 0.8752234280949223]
	TIME [epoch: 0.474 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9208905590237145		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.9208905590237145 | validation: 1.0107318961820007]
	TIME [epoch: 0.474 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0056585304992902		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.0056585304992902 | validation: 0.9577584482116084]
	TIME [epoch: 0.474 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1230321618356254		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.1230321618356254 | validation: 0.8226906706615159]
	TIME [epoch: 0.473 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8653349461882579		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.8653349461882579 | validation: 0.8065291404805672]
	TIME [epoch: 0.474 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.882686902435824		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.882686902435824 | validation: 0.7757887455744159]
	TIME [epoch: 0.473 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8833650794122949		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.8833650794122949 | validation: 0.8646484584073912]
	TIME [epoch: 0.473 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8744342796154854		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.8744342796154854 | validation: 0.7289400758954505]
	TIME [epoch: 0.473 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8731820870306323		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.8731820870306323 | validation: 0.7692697393994191]
	TIME [epoch: 0.474 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8133118735649191		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.8133118735649191 | validation: 0.7267084507476481]
	TIME [epoch: 0.474 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8002145328114071		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.8002145328114071 | validation: 0.7213632542675846]
	TIME [epoch: 0.475 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8111255854619518		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.8111255854619518 | validation: 0.7326490960281693]
	TIME [epoch: 0.474 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.849354337574697		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.849354337574697 | validation: 0.8962121653372108]
	TIME [epoch: 0.472 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9149622042672929		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.9149622042672929 | validation: 0.906665784610186]
	TIME [epoch: 0.473 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0165492623373558		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.0165492623373558 | validation: 0.8577723027548159]
	TIME [epoch: 0.474 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8059287284331557		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.8059287284331557 | validation: 0.7884106750028955]
	TIME [epoch: 0.474 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8341814682614449		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.8341814682614449 | validation: 0.7517746188708951]
	TIME [epoch: 0.474 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.968028380067777		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.968028380067777 | validation: 0.7550758779504898]
	TIME [epoch: 0.473 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8518100496360538		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.8518100496360538 | validation: 0.8220056368123411]
	TIME [epoch: 0.473 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.830945040459229		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.830945040459229 | validation: 0.8154223514981699]
	TIME [epoch: 0.474 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8952761810037191		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.8952761810037191 | validation: 0.910691042019108]
	TIME [epoch: 0.475 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8385705367529621		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.8385705367529621 | validation: 0.6410880901210595]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.841667184197542		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.841667184197542 | validation: 0.8635964679021566]
	TIME [epoch: 0.474 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8120256086370653		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.8120256086370653 | validation: 0.6459766023508101]
	TIME [epoch: 0.474 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7900690371905797		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.7900690371905797 | validation: 0.8465162937478145]
	TIME [epoch: 0.474 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.790381097463708		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.790381097463708 | validation: 0.6578742525475348]
	TIME [epoch: 0.473 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7951651690604766		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.7951651690604766 | validation: 0.8716558883472033]
	TIME [epoch: 0.473 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8139753361663327		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.8139753361663327 | validation: 0.8143947550392314]
	TIME [epoch: 0.474 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.916538072945078		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.916538072945078 | validation: 0.6996871681068987]
	TIME [epoch: 0.474 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.818724777844222		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.818724777844222 | validation: 0.7645508783492508]
	TIME [epoch: 0.474 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7952398361725308		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.7952398361725308 | validation: 0.6393415283860956]
	TIME [epoch: 0.473 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7392786599434104		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.7392786599434104 | validation: 0.6954429152248358]
	TIME [epoch: 0.474 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7254497257669046		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.7254497257669046 | validation: 0.6628702770234203]
	TIME [epoch: 0.474 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7222794630539605		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.7222794630539605 | validation: 0.8122815715242041]
	TIME [epoch: 0.474 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7639770147503202		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.7639770147503202 | validation: 0.782454232625979]
	TIME [epoch: 0.473 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.921809662569632		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.921809662569632 | validation: 0.7775009760602297]
	TIME [epoch: 0.474 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7929572938409898		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.7929572938409898 | validation: 0.6554208119805103]
	TIME [epoch: 0.473 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.783054226901524		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.783054226901524 | validation: 0.8333274234880993]
	TIME [epoch: 0.474 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7866623297578182		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.7866623297578182 | validation: 0.71347538293309]
	TIME [epoch: 0.475 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8621951055557949		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.8621951055557949 | validation: 0.6925777842911348]
	TIME [epoch: 0.475 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7342122499370148		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.7342122499370148 | validation: 0.6649710128163433]
	TIME [epoch: 0.474 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7229683410857868		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.7229683410857868 | validation: 0.7085326098475679]
	TIME [epoch: 0.474 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8150596419409797		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.8150596419409797 | validation: 0.9766440165567714]
	TIME [epoch: 0.475 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8426996399714765		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.8426996399714765 | validation: 0.6762596225650045]
	TIME [epoch: 0.474 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8078885805873988		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.8078885805873988 | validation: 0.7066841359622558]
	TIME [epoch: 0.474 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7170525280185441		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.7170525280185441 | validation: 0.6701465253058707]
	TIME [epoch: 0.473 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7022784570536382		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.7022784570536382 | validation: 0.6046771813212446]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7011397686521912		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.7011397686521912 | validation: 0.7613791740988309]
	TIME [epoch: 0.475 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7031812293946709		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.7031812293946709 | validation: 0.5393395765691152]
	TIME [epoch: 0.473 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_197.pth
	Model improved!!!
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7148440834514455		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.7148440834514455 | validation: 0.8102366367572705]
	TIME [epoch: 0.473 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7339704905152042		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.7339704905152042 | validation: 0.6986379950279211]
	TIME [epoch: 0.472 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8640093929450461		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.8640093929450461 | validation: 0.8047896676008164]
	TIME [epoch: 0.473 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7665389874607814		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.7665389874607814 | validation: 0.6461758996390959]
	TIME [epoch: 134 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7240425319322605		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.7240425319322605 | validation: 0.6843091930365071]
	TIME [epoch: 0.934 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7231164632730768		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.7231164632730768 | validation: 0.6074762699042913]
	TIME [epoch: 0.926 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.773532176295682		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.773532176295682 | validation: 0.7738963602236745]
	TIME [epoch: 0.927 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7340304283289593		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.7340304283289593 | validation: 0.580493586922607]
	TIME [epoch: 0.927 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6972124391138431		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.6972124391138431 | validation: 0.7274263443550487]
	TIME [epoch: 0.925 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6745935017147945		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.6745935017147945 | validation: 0.6061804424151627]
	TIME [epoch: 0.926 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7158889206535429		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.7158889206535429 | validation: 0.7613357523566131]
	TIME [epoch: 0.925 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7416183916647039		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.7416183916647039 | validation: 0.6701644052816018]
	TIME [epoch: 0.926 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7897397582950474		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.7897397582950474 | validation: 0.6674963302596901]
	TIME [epoch: 0.925 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6687091901101805		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.6687091901101805 | validation: 0.5797778192467242]
	TIME [epoch: 0.925 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6387180814782893		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.6387180814782893 | validation: 0.6233820334671538]
	TIME [epoch: 0.925 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6459443691334975		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.6459443691334975 | validation: 0.6686113559980155]
	TIME [epoch: 0.926 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6696897014398714		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.6696897014398714 | validation: 0.6760529825967461]
	TIME [epoch: 0.926 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8031199050106426		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.8031199050106426 | validation: 0.7697109891174908]
	TIME [epoch: 0.928 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7098265478042611		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.7098265478042611 | validation: 0.49617807696848937]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7097870125945602		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.7097870125945602 | validation: 0.7755474396017327]
	TIME [epoch: 0.926 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.712822086445091		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.712822086445091 | validation: 0.6401362558736426]
	TIME [epoch: 0.932 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7649554229872257		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.7649554229872257 | validation: 0.6690588386923424]
	TIME [epoch: 0.925 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6839373848119695		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.6839373848119695 | validation: 0.5711182356844182]
	TIME [epoch: 0.926 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6538517432979488		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.6538517432979488 | validation: 0.6761681629282214]
	TIME [epoch: 0.926 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6599721545307402		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.6599721545307402 | validation: 0.5812387279555147]
	TIME [epoch: 0.925 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.673993819291523		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.673993819291523 | validation: 0.8258663639855313]
	TIME [epoch: 0.926 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7334165460552885		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.7334165460552885 | validation: 0.6308551384613494]
	TIME [epoch: 0.926 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7423525233908174		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.7423525233908174 | validation: 0.592174836158804]
	TIME [epoch: 0.927 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6275651064727775		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.6275651064727775 | validation: 0.6535458577012898]
	TIME [epoch: 0.926 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6300548673973139		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.6300548673973139 | validation: 0.5866195835276321]
	TIME [epoch: 0.928 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6932805585045315		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.6932805585045315 | validation: 0.7680013654611315]
	TIME [epoch: 0.926 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7362312804003449		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.7362312804003449 | validation: 0.6325571219688355]
	TIME [epoch: 0.927 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7336270508883346		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.7336270508883346 | validation: 0.5722556319085403]
	TIME [epoch: 0.927 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6057882351045997		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.6057882351045997 | validation: 0.6236369226897571]
	TIME [epoch: 0.925 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6202307800189293		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.6202307800189293 | validation: 0.5850929654279059]
	TIME [epoch: 0.926 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6959404509458341		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.6959404509458341 | validation: 0.7548741896407041]
	TIME [epoch: 0.925 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6799588244341871		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.6799588244341871 | validation: 0.5413571397068939]
	TIME [epoch: 0.926 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6508430923014968		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.6508430923014968 | validation: 0.6193633766691597]
	TIME [epoch: 0.926 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6176222869704663		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.6176222869704663 | validation: 0.5143128284759321]
	TIME [epoch: 0.927 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6088147482839896		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.6088147482839896 | validation: 0.7048295870153307]
	TIME [epoch: 0.926 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6607276193298318		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.6607276193298318 | validation: 0.5701621040908509]
	TIME [epoch: 0.943 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.687947733005195		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.687947733005195 | validation: 0.6481342441782916]
	TIME [epoch: 0.926 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.633013782255725		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.633013782255725 | validation: 0.5771715057122743]
	TIME [epoch: 0.925 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6254657277128749		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.6254657277128749 | validation: 0.581783605872029]
	TIME [epoch: 0.926 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6125161708576549		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.6125161708576549 | validation: 0.5644413475551043]
	TIME [epoch: 0.925 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6021589131604693		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.6021589131604693 | validation: 0.5948424352688259]
	TIME [epoch: 0.926 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5986990978624293		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.5986990978624293 | validation: 0.5504119079655813]
	TIME [epoch: 0.926 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6578405361359956		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.6578405361359956 | validation: 0.6476910984585301]
	TIME [epoch: 0.926 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6317254092679951		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.6317254092679951 | validation: 0.5194718405586025]
	TIME [epoch: 0.926 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6619715439395745		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.6619715439395745 | validation: 0.6664455282933629]
	TIME [epoch: 0.926 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6099205535323309		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.6099205535323309 | validation: 0.4916486496352607]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5957633146212606		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.5957633146212606 | validation: 0.6129909332554134]
	TIME [epoch: 0.926 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.600590692526492		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.600590692526492 | validation: 0.6106239416754052]
	TIME [epoch: 0.93 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6731791619458397		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.6731791619458397 | validation: 0.6074798938593889]
	TIME [epoch: 0.925 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6151525937442565		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.6151525937442565 | validation: 0.5451083118757544]
	TIME [epoch: 0.926 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5813530996126376		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.5813530996126376 | validation: 0.5152380423448202]
	TIME [epoch: 0.926 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5533927063163085		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.5533927063163085 | validation: 0.5599788056380638]
	TIME [epoch: 0.926 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5423975657755559		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.5423975657755559 | validation: 0.4832296641679206]
	TIME [epoch: 0.928 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_255.pth
	Model improved!!!
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5554125549364067		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.5554125549364067 | validation: 0.6240747737188399]
	TIME [epoch: 0.925 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5959337673731847		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.5959337673731847 | validation: 0.5691644435100282]
	TIME [epoch: 0.925 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7437805847107065		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.7437805847107065 | validation: 0.5895363149990169]
	TIME [epoch: 0.924 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5872742748751425		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.5872742748751425 | validation: 0.5101526913868357]
	TIME [epoch: 0.924 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5375773627334037		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.5375773627334037 | validation: 0.4902175988508643]
	TIME [epoch: 0.927 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.538437382191795		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.538437382191795 | validation: 0.5464180019227465]
	TIME [epoch: 0.925 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5343813420600573		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.5343813420600573 | validation: 0.4531557376509257]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_262.pth
	Model improved!!!
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5380765552740597		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.5380765552740597 | validation: 0.6540043749858105]
	TIME [epoch: 0.924 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5854097789559916		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.5854097789559916 | validation: 0.4445952866201819]
	TIME [epoch: 0.924 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_264.pth
	Model improved!!!
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6230731736631541		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.6230731736631541 | validation: 0.8168263884511506]
	TIME [epoch: 0.925 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8002084851195561		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.8002084851195561 | validation: 0.7571619566527267]
	TIME [epoch: 0.924 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7623366433087958		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.7623366433087958 | validation: 0.5334820779015165]
	TIME [epoch: 0.923 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5506432370449525		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.5506432370449525 | validation: 0.6978827694458767]
	TIME [epoch: 0.924 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7252614526068599		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.7252614526068599 | validation: 0.5507400289587006]
	TIME [epoch: 0.924 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6869684012468323		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.6869684012468323 | validation: 0.5458460260348664]
	TIME [epoch: 0.925 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5280113555624981		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.5280113555624981 | validation: 0.5913185206038486]
	TIME [epoch: 0.925 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5671232342573056		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.5671232342573056 | validation: 0.5088372523920506]
	TIME [epoch: 0.924 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5871180528430029		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.5871180528430029 | validation: 0.5231739327728198]
	TIME [epoch: 0.924 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5189683382861848		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.5189683382861848 | validation: 0.5165556747100218]
	TIME [epoch: 0.924 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5094900302019996		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.5094900302019996 | validation: 0.4620297343532247]
	TIME [epoch: 0.924 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5373713889483756		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.5373713889483756 | validation: 0.5809059136378204]
	TIME [epoch: 0.924 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5505758309236355		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.5505758309236355 | validation: 0.5979271840067971]
	TIME [epoch: 0.924 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5865652215355049		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.5865652215355049 | validation: 0.5332646110770336]
	TIME [epoch: 0.923 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.547699825845468		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.547699825845468 | validation: 0.5793191511411949]
	TIME [epoch: 0.923 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5345787672044211		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.5345787672044211 | validation: 0.5465361472974062]
	TIME [epoch: 0.924 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5387072324045928		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.5387072324045928 | validation: 0.5235146262275278]
	TIME [epoch: 0.929 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5631922363812878		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.5631922363812878 | validation: 0.5482904789879148]
	TIME [epoch: 0.924 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5397113946717452		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.5397113946717452 | validation: 0.5018590535350355]
	TIME [epoch: 0.924 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5399699608984977		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.5399699608984977 | validation: 0.5390709651059874]
	TIME [epoch: 0.923 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.512874305075437		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.512874305075437 | validation: 0.5297592044935673]
	TIME [epoch: 0.924 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.505981113509142		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.505981113509142 | validation: 0.5226055984866216]
	TIME [epoch: 0.923 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.544230389586905		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.544230389586905 | validation: 0.5494527930545682]
	TIME [epoch: 0.924 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5549234605208311		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.5549234605208311 | validation: 0.5636913282057636]
	TIME [epoch: 0.923 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5145584544432974		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.5145584544432974 | validation: 0.4970501874601032]
	TIME [epoch: 0.924 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5597045102584738		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.5597045102584738 | validation: 0.5805606211569029]
	TIME [epoch: 0.924 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5463441950442468		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.5463441950442468 | validation: 0.526012592537999]
	TIME [epoch: 0.924 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5498321924701915		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.5498321924701915 | validation: 0.520032764741317]
	TIME [epoch: 0.923 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4877105613445545		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.4877105613445545 | validation: 0.42911925127529327]
	TIME [epoch: 0.923 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_293.pth
	Model improved!!!
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5009967752182096		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.5009967752182096 | validation: 0.618360531938804]
	TIME [epoch: 0.924 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5181776658319777		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.5181776658319777 | validation: 0.45928060050614494]
	TIME [epoch: 0.923 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47019847527437636		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.47019847527437636 | validation: 0.47846899056114633]
	TIME [epoch: 0.924 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47852120770891865		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.47852120770891865 | validation: 0.5975631920657367]
	TIME [epoch: 0.924 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5243391199888036		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.5243391199888036 | validation: 0.5311483032773598]
	TIME [epoch: 0.926 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5714753303591097		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.5714753303591097 | validation: 0.5472709032886082]
	TIME [epoch: 0.924 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6081012192005466		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.6081012192005466 | validation: 0.6732638516794586]
	TIME [epoch: 0.926 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6143062273452234		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.6143062273452234 | validation: 0.5459347304313944]
	TIME [epoch: 0.927 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7282117414540127		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.7282117414540127 | validation: 0.4445923542792618]
	TIME [epoch: 0.926 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6305509772898656		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.6305509772898656 | validation: 0.5600941148541753]
	TIME [epoch: 0.927 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5335807161970633		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.5335807161970633 | validation: 0.5539515566837422]
	TIME [epoch: 0.926 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47747710370619106		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.47747710370619106 | validation: 0.492916767606374]
	TIME [epoch: 0.927 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.480592529762134		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.480592529762134 | validation: 0.47284184256482525]
	TIME [epoch: 0.925 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47325957322760376		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.47325957322760376 | validation: 0.5330432740045764]
	TIME [epoch: 0.928 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4624283805983273		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.4624283805983273 | validation: 0.5107825572877461]
	TIME [epoch: 0.925 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.509940608352984		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.509940608352984 | validation: 0.5936343652410984]
	TIME [epoch: 0.924 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5897455881038938		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.5897455881038938 | validation: 0.4830109710092174]
	TIME [epoch: 0.923 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46622134163214585		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.46622134163214585 | validation: 0.5361961697342867]
	TIME [epoch: 0.924 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4700435310830374		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.4700435310830374 | validation: 0.5527757723139208]
	TIME [epoch: 0.928 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5142183278992529		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.5142183278992529 | validation: 0.6291100231351543]
	TIME [epoch: 0.924 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5975876208700299		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.5975876208700299 | validation: 0.5528345572637332]
	TIME [epoch: 0.924 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49996305024690757		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.49996305024690757 | validation: 0.49379663603161356]
	TIME [epoch: 0.924 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4618028294085508		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.4618028294085508 | validation: 0.4962029886022021]
	TIME [epoch: 0.924 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43713879416490303		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.43713879416490303 | validation: 0.5107106555676978]
	TIME [epoch: 0.924 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43720920119980833		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.43720920119980833 | validation: 0.47865305430114485]
	TIME [epoch: 0.924 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4816553973888925		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.4816553973888925 | validation: 0.5999107160458154]
	TIME [epoch: 0.924 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49967040178295097		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.49967040178295097 | validation: 0.5061311265490155]
	TIME [epoch: 0.923 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4656762089620436		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.4656762089620436 | validation: 0.5491963696233759]
	TIME [epoch: 0.924 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.479912070717062		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.479912070717062 | validation: 0.5134443880612808]
	TIME [epoch: 0.944 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4464570269648812		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.4464570269648812 | validation: 0.4868176530248617]
	TIME [epoch: 0.924 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44344593032889185		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.44344593032889185 | validation: 0.5310626430525514]
	TIME [epoch: 0.924 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4621358243867793		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.4621358243867793 | validation: 0.4757292537129632]
	TIME [epoch: 0.923 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4714339068449071		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.4714339068449071 | validation: 0.5551412033932092]
	TIME [epoch: 0.924 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49083865556147455		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.49083865556147455 | validation: 0.6752559274821159]
	TIME [epoch: 0.923 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5845721187962732		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.5845721187962732 | validation: 0.4588124651750498]
	TIME [epoch: 0.924 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4582828404240009		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.4582828404240009 | validation: 0.49536682317067965]
	TIME [epoch: 0.924 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42432871660538385		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.42432871660538385 | validation: 0.5492616150265455]
	TIME [epoch: 0.924 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4603634201598267		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.4603634201598267 | validation: 0.49930487643314597]
	TIME [epoch: 0.924 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4859027367044557		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.4859027367044557 | validation: 0.5236581720056143]
	TIME [epoch: 0.923 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5215880360188183		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.5215880360188183 | validation: 0.5532178359930192]
	TIME [epoch: 0.924 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4868752632481009		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.4868752632481009 | validation: 0.4557579169991634]
	TIME [epoch: 0.923 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4217761636451715		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.4217761636451715 | validation: 0.48626983773661725]
	TIME [epoch: 0.924 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42289418179282967		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.42289418179282967 | validation: 0.611797509005034]
	TIME [epoch: 0.923 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47962400804761873		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.47962400804761873 | validation: 0.46043087454663606]
	TIME [epoch: 0.924 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4237010494244672		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.4237010494244672 | validation: 0.5086917703363573]
	TIME [epoch: 0.923 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40375113960888676		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.40375113960888676 | validation: 0.49806154874788844]
	TIME [epoch: 0.924 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40212966099048675		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.40212966099048675 | validation: 0.49102840926091496]
	TIME [epoch: 0.923 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39110198221498343		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.39110198221498343 | validation: 0.5369240196841234]
	TIME [epoch: 0.924 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42000086175781504		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.42000086175781504 | validation: 0.5151921294240601]
	TIME [epoch: 0.923 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4396993781843366		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.4396993781843366 | validation: 0.6258474792506694]
	TIME [epoch: 0.925 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6017151545690395		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.6017151545690395 | validation: 0.611677744896181]
	TIME [epoch: 0.926 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5395574266106995		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.5395574266106995 | validation: 0.43399018129498246]
	TIME [epoch: 0.928 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4456982010201145		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.4456982010201145 | validation: 0.5765512568852764]
	TIME [epoch: 0.922 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4228718407008513		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.4228718407008513 | validation: 0.4835043480773113]
	TIME [epoch: 0.924 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43712753034560947		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.43712753034560947 | validation: 0.60270276399983]
	TIME [epoch: 0.923 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5039041114336175		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.5039041114336175 | validation: 0.4546003784009791]
	TIME [epoch: 0.925 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43112087672771926		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.43112087672771926 | validation: 0.5877862399908599]
	TIME [epoch: 0.922 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42513152584897845		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.42513152584897845 | validation: 0.55900512097808]
	TIME [epoch: 0.925 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5907266327126729		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.5907266327126729 | validation: 0.3935100427818052]
	TIME [epoch: 0.923 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_352.pth
	Model improved!!!
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4853794540516584		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.4853794540516584 | validation: 0.6055853225252265]
	TIME [epoch: 0.926 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4381042687311223		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.4381042687311223 | validation: 0.5208704050018987]
	TIME [epoch: 0.925 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3953791190897077		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.3953791190897077 | validation: 0.42911793996105196]
	TIME [epoch: 0.926 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41220240237027667		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.41220240237027667 | validation: 0.5290706541196609]
	TIME [epoch: 0.925 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3995169639062146		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.3995169639062146 | validation: 0.5277605710077985]
	TIME [epoch: 0.927 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40252813571936796		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.40252813571936796 | validation: 0.45796870352369234]
	TIME [epoch: 0.924 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.383884388165355		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.383884388165355 | validation: 0.513654480825984]
	TIME [epoch: 0.924 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3969701093565389		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.3969701093565389 | validation: 0.5124742738778695]
	TIME [epoch: 0.924 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43703238003679673		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.43703238003679673 | validation: 0.552274213551715]
	TIME [epoch: 0.923 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4955647875979049		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.4955647875979049 | validation: 0.5238439140453303]
	TIME [epoch: 0.923 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38374392207784697		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.38374392207784697 | validation: 0.479996052716189]
	TIME [epoch: 0.924 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37001505442196375		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.37001505442196375 | validation: 0.4860902060231103]
	TIME [epoch: 0.923 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3797725627195581		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.3797725627195581 | validation: 0.5581335524559143]
	TIME [epoch: 0.924 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44006073100706417		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.44006073100706417 | validation: 0.49639354864842344]
	TIME [epoch: 0.925 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48969459702824486		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.48969459702824486 | validation: 0.5175809259497935]
	TIME [epoch: 0.923 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3765406923170434		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.3765406923170434 | validation: 0.49805621560175867]
	TIME [epoch: 0.925 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35816601898447015		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.35816601898447015 | validation: 0.47355955959424423]
	TIME [epoch: 0.924 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37249383644376793		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.37249383644376793 | validation: 0.6944309590242922]
	TIME [epoch: 0.925 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5317117272536883		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.5317117272536883 | validation: 0.5795782114808151]
	TIME [epoch: 0.923 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4200873061642343		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.4200873061642343 | validation: 0.3771221889073995]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_372.pth
	Model improved!!!
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47367606520365174		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.47367606520365174 | validation: 0.5183822575024584]
	TIME [epoch: 0.925 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3705773890025679		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.3705773890025679 | validation: 0.5456170943876572]
	TIME [epoch: 0.925 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37546382153656693		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.37546382153656693 | validation: 0.41675511889359246]
	TIME [epoch: 0.926 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3823575781541848		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.3823575781541848 | validation: 0.5114919549032925]
	TIME [epoch: 0.932 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3579085601487413		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.3579085601487413 | validation: 0.48811699412865317]
	TIME [epoch: 0.927 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3477046410958913		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.3477046410958913 | validation: 0.45771528270832695]
	TIME [epoch: 0.924 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36163205800397824		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.36163205800397824 | validation: 0.511153643170033]
	TIME [epoch: 0.925 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3503453137590435		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.3503453137590435 | validation: 0.46024605238589616]
	TIME [epoch: 0.924 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3504693289185339		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.3504693289185339 | validation: 0.6439150914096718]
	TIME [epoch: 0.925 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5206557185369782		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.5206557185369782 | validation: 0.5452580672274155]
	TIME [epoch: 0.923 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42398036675901907		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.42398036675901907 | validation: 0.5276464661656805]
	TIME [epoch: 0.925 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4548759593148791		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.4548759593148791 | validation: 0.4717112199397273]
	TIME [epoch: 0.923 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34716684209243953		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.34716684209243953 | validation: 0.40181444956782725]
	TIME [epoch: 0.925 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3806377214309296		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.3806377214309296 | validation: 0.5682902751274613]
	TIME [epoch: 0.924 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3783442721617311		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.3783442721617311 | validation: 0.49494483707823794]
	TIME [epoch: 0.924 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4318980362108532		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.4318980362108532 | validation: 0.5946082065269179]
	TIME [epoch: 0.925 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5398425414617417		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.5398425414617417 | validation: 0.5545169343224021]
	TIME [epoch: 0.925 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3584021022083153		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.3584021022083153 | validation: 0.3866483380945015]
	TIME [epoch: 0.926 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36832799425421725		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.36832799425421725 | validation: 0.529949660612169]
	TIME [epoch: 0.926 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34810590335985664		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.34810590335985664 | validation: 0.4538506205778505]
	TIME [epoch: 0.925 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33039942458546007		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.33039942458546007 | validation: 0.5432803482587034]
	TIME [epoch: 0.924 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36995807424381594		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.36995807424381594 | validation: 0.474262439017596]
	TIME [epoch: 0.925 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3698642114478271		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.3698642114478271 | validation: 0.5263151116294161]
	TIME [epoch: 0.923 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38935075206806397		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.38935075206806397 | validation: 0.4791189694245905]
	TIME [epoch: 0.926 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32979321916970394		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.32979321916970394 | validation: 0.5071652724005136]
	TIME [epoch: 0.924 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3479454044746422		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.3479454044746422 | validation: 0.45319305324981285]
	TIME [epoch: 0.925 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42850351580720825		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.42850351580720825 | validation: 0.590750295159123]
	TIME [epoch: 0.925 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4327874511339123		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.4327874511339123 | validation: 0.5458981922345902]
	TIME [epoch: 0.925 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36405594963819177		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.36405594963819177 | validation: 0.3861537470337529]
	TIME [epoch: 0.925 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3617510142860557		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.3617510142860557 | validation: 0.48638882919173576]
	TIME [epoch: 0.925 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32064116188576125		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.32064116188576125 | validation: 0.4890173598968133]
	TIME [epoch: 0.924 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4266456191946421		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.4266456191946421 | validation: 0.5127286155690562]
	TIME [epoch: 0.924 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.395275168217417		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.395275168217417 | validation: 0.4838668690791023]
	TIME [epoch: 0.924 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3275927786670698		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.3275927786670698 | validation: 0.5184390541059346]
	TIME [epoch: 0.924 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.353614307040896		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.353614307040896 | validation: 0.4411291780876778]
	TIME [epoch: 0.937 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32129966729257337		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.32129966729257337 | validation: 0.5394597885976493]
	TIME [epoch: 0.927 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3328394539682274		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.3328394539682274 | validation: 0.43672549151886414]
	TIME [epoch: 0.923 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36026067896857655		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.36026067896857655 | validation: 0.571878322554047]
	TIME [epoch: 0.923 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44579285713389327		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.44579285713389327 | validation: 0.571603563779786]
	TIME [epoch: 0.924 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46658786724102586		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.46658786724102586 | validation: 0.43082376202454764]
	TIME [epoch: 0.924 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3180882950897632		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.3180882950897632 | validation: 0.5660547373486245]
	TIME [epoch: 0.923 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5394581029814126		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.5394581029814126 | validation: 0.45389411663510926]
	TIME [epoch: 0.926 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3244090231244096		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.3244090231244096 | validation: 0.5875286598578934]
	TIME [epoch: 0.926 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3771841048820897		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.3771841048820897 | validation: 0.44466399640651455]
	TIME [epoch: 0.925 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34323372189614554		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.34323372189614554 | validation: 0.4889396844242677]
	TIME [epoch: 0.924 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.313779061124418		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.313779061124418 | validation: 0.4440796478949327]
	TIME [epoch: 0.922 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.313226654227228		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.313226654227228 | validation: 0.49214493253597436]
	TIME [epoch: 0.925 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3052690842409514		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.3052690842409514 | validation: 0.48138935184853465]
	TIME [epoch: 0.923 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29981456543651375		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.29981456543651375 | validation: 0.46769792684106637]
	TIME [epoch: 0.924 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3030625705286561		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.3030625705286561 | validation: 0.4469112091791588]
	TIME [epoch: 0.922 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2909368955731906		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.2909368955731906 | validation: 0.5114628555009945]
	TIME [epoch: 0.923 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32919878394210567		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.32919878394210567 | validation: 0.5188071998672538]
	TIME [epoch: 0.923 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42701991374636394		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.42701991374636394 | validation: 0.5249875195707346]
	TIME [epoch: 0.923 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45679004983856286		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.45679004983856286 | validation: 0.4638684477230493]
	TIME [epoch: 0.925 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3086867696164572		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.3086867696164572 | validation: 0.4806383133321339]
	TIME [epoch: 0.923 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3658787251896228		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.3658787251896228 | validation: 0.5252905043628331]
	TIME [epoch: 0.926 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3604787563297647		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.3604787563297647 | validation: 1.1434931442996303]
	TIME [epoch: 0.926 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.77863606072786		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.77863606072786 | validation: 1.223901709584453]
	TIME [epoch: 0.926 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8136560996501989		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.8136560996501989 | validation: 1.1208885544871785]
	TIME [epoch: 0.925 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7550432583180664		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.7550432583180664 | validation: 0.9858877317821654]
	TIME [epoch: 0.926 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7204003658367623		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.7204003658367623 | validation: 0.7684771095727921]
	TIME [epoch: 0.926 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5657439463161156		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.5657439463161156 | validation: 0.5640977343649586]
	TIME [epoch: 0.927 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35465062130639224		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.35465062130639224 | validation: 0.3971449838891147]
	TIME [epoch: 0.925 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3484980278150526		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.3484980278150526 | validation: 0.48131478341672973]
	TIME [epoch: 0.927 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3272611672141825		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.3272611672141825 | validation: 0.49834353129455017]
	TIME [epoch: 0.926 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3145665573140809		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.3145665573140809 | validation: 0.47895205067440716]
	TIME [epoch: 0.923 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30081987442026764		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.30081987442026764 | validation: 0.48558206423347094]
	TIME [epoch: 0.922 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3122259545224241		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.3122259545224241 | validation: 0.4494564329982428]
	TIME [epoch: 0.928 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3284718047810064		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.3284718047810064 | validation: 0.5118746649771542]
	TIME [epoch: 0.923 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3146720324607985		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.3146720324607985 | validation: 0.46540281426867924]
	TIME [epoch: 0.924 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2896965698478165		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.2896965698478165 | validation: 0.4748145581199854]
	TIME [epoch: 0.923 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31495118130583344		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.31495118130583344 | validation: 0.5574201712253601]
	TIME [epoch: 0.924 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3717332860092261		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.3717332860092261 | validation: 0.4101659403143474]
	TIME [epoch: 0.923 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3265554102190534		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.3265554102190534 | validation: 0.49798823321612784]
	TIME [epoch: 0.925 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3110683040915716		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.3110683040915716 | validation: 0.5494935432810273]
	TIME [epoch: 0.926 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33589545952567995		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.33589545952567995 | validation: 0.4419579566261915]
	TIME [epoch: 0.925 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3041147213580405		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.3041147213580405 | validation: 0.49249372358018273]
	TIME [epoch: 0.925 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2962536945187902		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.2962536945187902 | validation: 0.5006538481061001]
	TIME [epoch: 0.924 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2963544317031312		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.2963544317031312 | validation: 0.43039067397078795]
	TIME [epoch: 0.925 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2954764058106528		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.2954764058106528 | validation: 0.5555537745022384]
	TIME [epoch: 0.924 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34242467546495275		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.34242467546495275 | validation: 0.45937222098329633]
	TIME [epoch: 0.926 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3050444536095212		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.3050444536095212 | validation: 0.4114298573136175]
	TIME [epoch: 0.927 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3711854414217867		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.3711854414217867 | validation: 0.6185757201589057]
	TIME [epoch: 0.926 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46682444468646805		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.46682444468646805 | validation: 0.5596104306991075]
	TIME [epoch: 0.926 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33766534494033046		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.33766534494033046 | validation: 0.4748201687599564]
	TIME [epoch: 0.925 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.403599543602315		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.403599543602315 | validation: 0.42327742253862777]
	TIME [epoch: 0.926 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3420830473339964		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.3420830473339964 | validation: 0.5082461650208858]
	TIME [epoch: 0.925 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28082473314184936		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.28082473314184936 | validation: 0.47128444516887635]
	TIME [epoch: 0.927 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27378745070423344		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.27378745070423344 | validation: 0.47134837434943333]
	TIME [epoch: 0.925 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3217699332835426		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.3217699332835426 | validation: 0.4593032416405385]
	TIME [epoch: 0.925 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28097793697093815		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.28097793697093815 | validation: 0.4899165335112097]
	TIME [epoch: 0.927 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26868211511681656		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.26868211511681656 | validation: 0.46289868824685104]
	TIME [epoch: 0.924 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2815593635928404		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.2815593635928404 | validation: 0.5340118197622435]
	TIME [epoch: 0.927 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37108982377586364		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.37108982377586364 | validation: 0.5072771975663644]
	TIME [epoch: 0.924 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.370029185674981		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.370029185674981 | validation: 0.4023434129038388]
	TIME [epoch: 0.925 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2890358003717676		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.2890358003717676 | validation: 0.5478309382751461]
	TIME [epoch: 0.924 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44230834968416044		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.44230834968416044 | validation: 0.5114305550895408]
	TIME [epoch: 0.925 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3334809509796583		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.3334809509796583 | validation: 0.407439992911349]
	TIME [epoch: 0.927 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2932229759089634		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.2932229759089634 | validation: 0.4503205016255658]
	TIME [epoch: 0.925 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2728628431001721		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.2728628431001721 | validation: 0.46934820176459485]
	TIME [epoch: 0.927 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2673469421101571		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.2673469421101571 | validation: 0.459817367953675]
	TIME [epoch: 0.923 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26658923282075386		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.26658923282075386 | validation: 0.43678348662464006]
	TIME [epoch: 0.922 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27056466319609		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.27056466319609 | validation: 0.49894199057909233]
	TIME [epoch: 0.922 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2847181425482289		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.2847181425482289 | validation: 0.3930527193728368]
	TIME [epoch: 0.923 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2902633801982411		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.2902633801982411 | validation: 0.5142064918368862]
	TIME [epoch: 0.925 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3201518581284452		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.3201518581284452 | validation: 0.5244877167490836]
	TIME [epoch: 0.923 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5048254635249464		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.5048254635249464 | validation: 0.39537925342035857]
	TIME [epoch: 0.925 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3060975355417513		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.3060975355417513 | validation: 0.5952702194483771]
	TIME [epoch: 0.922 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3901704758624118		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.3901704758624118 | validation: 0.48561970902083806]
	TIME [epoch: 0.923 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28259047979276525		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.28259047979276525 | validation: 0.44661067591910153]
	TIME [epoch: 0.924 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30175141468342237		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.30175141468342237 | validation: 0.44744679794833]
	TIME [epoch: 0.923 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2873378559062929		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.2873378559062929 | validation: 0.49207970507269094]
	TIME [epoch: 0.924 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26354794377092927		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.26354794377092927 | validation: 0.4303636382748174]
	TIME [epoch: 0.923 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2673796815740402		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.2673796815740402 | validation: 0.46820383966586965]
	TIME [epoch: 0.923 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2630646208761831		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.2630646208761831 | validation: 0.471002159698333]
	TIME [epoch: 0.923 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27905582920736877		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.27905582920736877 | validation: 0.44878222749093233]
	TIME [epoch: 0.923 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3076941091478103		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.3076941091478103 | validation: 0.4623191736760013]
	TIME [epoch: 0.922 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2680208281178054		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.2680208281178054 | validation: 0.49476671904384073]
	TIME [epoch: 0.923 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2697827024881354		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.2697827024881354 | validation: 0.42143430688231504]
	TIME [epoch: 0.924 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26028751684889867		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.26028751684889867 | validation: 0.5373745270643723]
	TIME [epoch: 0.924 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30173704026893755		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.30173704026893755 | validation: 0.4420472980307598]
	TIME [epoch: 0.924 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.304885549310289		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.304885549310289 | validation: 0.4145577382941641]
	TIME [epoch: 0.927 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26594576795591374		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.26594576795591374 | validation: 0.5298369687185466]
	TIME [epoch: 0.925 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28015397474893816		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.28015397474893816 | validation: 0.4101074263360496]
	TIME [epoch: 0.923 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26394294922266154		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.26394294922266154 | validation: 0.45783016674802773]
	TIME [epoch: 0.925 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2605703983094888		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.2605703983094888 | validation: 0.5166324674458566]
	TIME [epoch: 0.923 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3169286802797806		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.3169286802797806 | validation: 0.4429953215923057]
	TIME [epoch: 0.923 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28069114496908365		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.28069114496908365 | validation: 0.4590931853348787]
	TIME [epoch: 0.925 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2851225771779696		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.2851225771779696 | validation: 0.46084828589669846]
	TIME [epoch: 137 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30230062195147045		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.30230062195147045 | validation: 0.5081056859391254]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion3_v_mmd1_20250421_122430/states/model_phi1_4a_distortion3_v_mmd1_502.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 792.428 seconds.
