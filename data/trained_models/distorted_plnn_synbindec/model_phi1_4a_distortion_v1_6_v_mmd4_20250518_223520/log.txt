Args:
Namespace(name='model_phi1_4a_distortion_v1_6_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_6/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_6/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.05487167835235596, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2492311961

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.313522336495806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.313522336495806 | validation: 6.134055870363881]
	TIME [epoch: 162 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.997995044940026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.997995044940026 | validation: 6.294624647709859]
	TIME [epoch: 0.774 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.82668465894737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.82668465894737 | validation: 5.685737906769275]
	TIME [epoch: 0.959 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.58041200380731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.58041200380731 | validation: 6.025716715620059]
	TIME [epoch: 0.693 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.96188133545498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.96188133545498 | validation: 5.8147405388414635]
	TIME [epoch: 0.691 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.776075114703483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.776075114703483 | validation: 5.5146822303652465]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.438514711377609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.438514711377609 | validation: 5.730211499679424]
	TIME [epoch: 0.692 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.599676945992878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.599676945992878 | validation: 5.501601552113735]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.402074850223376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.402074850223376 | validation: 5.558851724888668]
	TIME [epoch: 0.693 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.40933290056416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.40933290056416 | validation: 5.431955060982665]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.326050576624199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.326050576624199 | validation: 5.343009523732035]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.284958729002547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.284958729002547 | validation: 5.340208302616753]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.259792825058778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.259792825058778 | validation: 5.3215326900627815]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.224277119273963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.224277119273963 | validation: 5.3429311804526725]
	TIME [epoch: 0.697 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.178567209829872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.178567209829872 | validation: 5.351771666628342]
	TIME [epoch: 0.697 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.129505787549707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.129505787549707 | validation: 5.258572584689493]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.07184555177179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.07184555177179 | validation: 5.465725438846618]
	TIME [epoch: 0.698 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.020388829844719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.020388829844719 | validation: 5.104618861428422]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.048939377957764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.048939377957764 | validation: 5.386060546022538]
	TIME [epoch: 0.695 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.826274362093257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.826274362093257 | validation: 5.816085402869906]
	TIME [epoch: 0.694 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.415049899746942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.415049899746942 | validation: 5.115555350989583]
	TIME [epoch: 0.693 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.201816904738604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.201816904738604 | validation: 5.0903763823135275]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.131529751965379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.131529751965379 | validation: 4.972977529169354]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.7057141345224505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7057141345224505 | validation: 5.539606374584135]
	TIME [epoch: 0.696 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.617403439842112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.617403439842112 | validation: 5.037155581486449]
	TIME [epoch: 0.692 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.550586964276536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.550586964276536 | validation: 5.043330348836378]
	TIME [epoch: 0.695 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.241070336686384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.241070336686384 | validation: 5.5418119083804385]
	TIME [epoch: 0.693 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.762042204981715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.762042204981715 | validation: 4.991544266845585]
	TIME [epoch: 0.691 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.022507305576689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.022507305576689 | validation: 4.859447334715917]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.724796158970853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.724796158970853 | validation: 4.8663336401380946]
	TIME [epoch: 0.695 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.051503183132284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.051503183132284 | validation: 5.643676156678624]
	TIME [epoch: 0.689 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.653790163168774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.653790163168774 | validation: 4.501563298016331]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.162044795857536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.162044795857536 | validation: 4.501063387910825]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.968438047571684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.968438047571684 | validation: 4.908592857168211]
	TIME [epoch: 0.696 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.79957732927951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.79957732927951 | validation: 4.4310988642236735]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.721790709552576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.721790709552576 | validation: 4.512917639982525]
	TIME [epoch: 0.697 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5964524277700547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5964524277700547 | validation: 4.70919381473994]
	TIME [epoch: 0.695 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5475112761740815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5475112761740815 | validation: 4.463208522574534]
	TIME [epoch: 0.696 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7846223718584113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7846223718584113 | validation: 5.064004432610508]
	TIME [epoch: 0.695 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0137247846746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0137247846746 | validation: 4.4499178800609265]
	TIME [epoch: 0.695 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8407777847709235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8407777847709235 | validation: 4.454734655200737]
	TIME [epoch: 0.696 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3760256222843616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3760256222843616 | validation: 4.775473457049759]
	TIME [epoch: 0.696 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.564441122814191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.564441122814191 | validation: 4.443404062782955]
	TIME [epoch: 0.696 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.86579862398945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.86579862398945 | validation: 4.2965103901486525]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3117790934159337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3117790934159337 | validation: 4.972554381805218]
	TIME [epoch: 0.698 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.818961234931776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.818961234931776 | validation: 4.246874325859342]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4801388635177326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4801388635177326 | validation: 4.293133191629501]
	TIME [epoch: 0.696 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.172591191578174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.172591191578174 | validation: 4.4619385538922804]
	TIME [epoch: 0.696 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.240436138507754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.240436138507754 | validation: 4.201149920009841]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4605731697032236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4605731697032236 | validation: 4.40531881254488]
	TIME [epoch: 0.696 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1635511773841443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1635511773841443 | validation: 4.08907471116507]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.136795085631508		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.136795085631508 | validation: 4.414498423354513]
	TIME [epoch: 0.694 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.14497423875948		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 3.14497423875948 | validation: 4.0655287413394126]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.339091234760035		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 3.339091234760035 | validation: 4.321670710409395]
	TIME [epoch: 0.697 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.081203498582746		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 3.081203498582746 | validation: 3.9866335142584006]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1347342871580675		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 3.1347342871580675 | validation: 4.355616242177944]
	TIME [epoch: 0.698 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1064072822857147		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 3.1064072822857147 | validation: 3.9495330184561173]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2152191839990945		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 3.2152191839990945 | validation: 4.139416570166112]
	TIME [epoch: 0.695 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9101634742445506		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 2.9101634742445506 | validation: 3.8727141705982335]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8716960743551945		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 2.8716960743551945 | validation: 4.084196067788142]
	TIME [epoch: 0.696 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.902237426822036		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 2.902237426822036 | validation: 3.834723745430237]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2142460980929095		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 3.2142460980929095 | validation: 4.042276868595933]
	TIME [epoch: 0.695 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.844634518405616		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 2.844634518405616 | validation: 3.7092260180342045]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8566594394409313		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 2.8566594394409313 | validation: 4.047270858744333]
	TIME [epoch: 0.692 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.914517224754705		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 2.914517224754705 | validation: 3.6618957768992413]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0212462606961754		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 3.0212462606961754 | validation: 3.7681540226325896]
	TIME [epoch: 0.692 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.663968715673351		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 2.663968715673351 | validation: 3.5127009899700234]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.62395314043235		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 2.62395314043235 | validation: 3.718516192380832]
	TIME [epoch: 0.693 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.678384249849332		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 2.678384249849332 | validation: 3.3067937729741885]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9270085513110975		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 2.9270085513110975 | validation: 3.5759899994316804]
	TIME [epoch: 0.691 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5775808289593214		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 2.5775808289593214 | validation: 3.0887031656624084]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5036871445365882		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 2.5036871445365882 | validation: 3.2702136632752885]
	TIME [epoch: 0.695 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4499544812210607		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 2.4499544812210607 | validation: 2.6680268805326346]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4382733995125068		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 2.4382733995125068 | validation: 2.817150112414425]
	TIME [epoch: 0.693 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.253093041537168		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 2.253093041537168 | validation: 1.9587666027685877]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9961759849823768		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.9961759849823768 | validation: 1.8749671838823887]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7511412847332912		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.7511412847332912 | validation: 1.7576796351815276]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7506802519113962		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.7506802519113962 | validation: 3.0375083026725003]
	TIME [epoch: 0.693 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.252544994342983		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 2.252544994342983 | validation: 2.673781832433096]
	TIME [epoch: 0.694 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0610926234076024		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 2.0610926234076024 | validation: 1.4703515966654979]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5081289831797995		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.5081289831797995 | validation: 1.9455933295848729]
	TIME [epoch: 0.691 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7752336853669846		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.7752336853669846 | validation: 1.9212081694418324]
	TIME [epoch: 0.691 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6941517398812471		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.6941517398812471 | validation: 1.6707137955953817]
	TIME [epoch: 0.689 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.565966035921806		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.565966035921806 | validation: 1.405421556643264]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5469058224436572		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.5469058224436572 | validation: 1.3790272792947602]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4764237699010931		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.4764237699010931 | validation: 1.4459193444269354]
	TIME [epoch: 0.691 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4510286422969407		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.4510286422969407 | validation: 1.4114282576854682]
	TIME [epoch: 0.689 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.431569631847331		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.431569631847331 | validation: 1.2103540896993596]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4604070847667772		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.4604070847667772 | validation: 1.4721719341745203]
	TIME [epoch: 0.694 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.549869150600232		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.549869150600232 | validation: 1.2905232071509678]
	TIME [epoch: 0.694 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8763455059621743		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.8763455059621743 | validation: 1.481372616852278]
	TIME [epoch: 0.693 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3946652245898412		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.3946652245898412 | validation: 1.1230747554870508]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3573093165099457		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.3573093165099457 | validation: 1.73869914038869]
	TIME [epoch: 0.694 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.506835716045145		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.506835716045145 | validation: 1.27856368890839]
	TIME [epoch: 0.694 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6254442570714809		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.6254442570714809 | validation: 1.558963704750417]
	TIME [epoch: 0.694 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5371267954903727		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.5371267954903727 | validation: 1.2058424955195788]
	TIME [epoch: 0.692 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4002371664224516		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.4002371664224516 | validation: 1.5000643241019915]
	TIME [epoch: 0.691 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4096873971515698		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.4096873971515698 | validation: 1.4081568294033184]
	TIME [epoch: 0.692 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.412528000403631		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.412528000403631 | validation: 1.274767379318675]
	TIME [epoch: 0.692 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2942046566355554		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.2942046566355554 | validation: 1.0988353577406522]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.118455408728145		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.118455408728145 | validation: 1.199054586106195]
	TIME [epoch: 0.691 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1100223062007186		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.1100223062007186 | validation: 1.0875220506370147]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.104088858170721		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.104088858170721 | validation: 1.2284933806105567]
	TIME [epoch: 0.694 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1708305393000498		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.1708305393000498 | validation: 1.0675076197199154]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3119432584533588		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.3119432584533588 | validation: 1.369700354282024]
	TIME [epoch: 0.695 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.351806539888513		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.351806539888513 | validation: 1.0669235151045207]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0601329682958451		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.0601329682958451 | validation: 1.1097439236864297]
	TIME [epoch: 0.693 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0491333341412696		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.0491333341412696 | validation: 1.1057985035693254]
	TIME [epoch: 0.689 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0686419653457189		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.0686419653457189 | validation: 1.1795240697815617]
	TIME [epoch: 0.69 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0837803661836045		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.0837803661836045 | validation: 0.9918284597813429]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1217505925352467		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.1217505925352467 | validation: 1.23065054291565]
	TIME [epoch: 0.69 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.180826337304158		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.180826337304158 | validation: 0.9771367648795458]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0434903239331956		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.0434903239331956 | validation: 1.052426087898188]
	TIME [epoch: 0.691 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.037513263415435		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.037513263415435 | validation: 1.021045596689287]
	TIME [epoch: 0.689 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9934933760038481		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.9934933760038481 | validation: 1.005173859634657]
	TIME [epoch: 0.689 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0082028353410308		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.0082028353410308 | validation: 1.0119913775016494]
	TIME [epoch: 0.69 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9623769803834		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.9623769803834 | validation: 0.9652857657058732]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9366292521069045		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.9366292521069045 | validation: 0.861259603608648]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9847978465807006		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.9847978465807006 | validation: 1.2460669664183301]
	TIME [epoch: 0.691 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2668842315318494		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.2668842315318494 | validation: 0.9035077736545344]
	TIME [epoch: 0.691 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0368715975272584		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.0368715975272584 | validation: 1.0126329806648149]
	TIME [epoch: 0.689 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9503893558260476		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.9503893558260476 | validation: 0.8564971442006194]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9214717046720604		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.9214717046720604 | validation: 1.0124891039320105]
	TIME [epoch: 0.697 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.95003283549543		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.95003283549543 | validation: 0.9391463260913655]
	TIME [epoch: 0.693 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.012637202573899		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.012637202573899 | validation: 1.0487578202039902]
	TIME [epoch: 0.694 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0540435619445019		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.0540435619445019 | validation: 0.8173257802308023]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9846927497296253		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.9846927497296253 | validation: 0.939136514963767]
	TIME [epoch: 0.695 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0101336031662085		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.0101336031662085 | validation: 0.8389215242561421]
	TIME [epoch: 0.693 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8795877058014958		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.8795877058014958 | validation: 0.8341799954669128]
	TIME [epoch: 0.692 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8941084391487405		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.8941084391487405 | validation: 0.9960588882844349]
	TIME [epoch: 0.693 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9239020007873616		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.9239020007873616 | validation: 0.8542227722559139]
	TIME [epoch: 0.691 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9353615017980041		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.9353615017980041 | validation: 0.9410202110725566]
	TIME [epoch: 0.692 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9186015527644767		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.9186015527644767 | validation: 0.7799961105707193]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9645531609365138		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.9645531609365138 | validation: 0.9681704430537711]
	TIME [epoch: 0.692 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0263832874720433		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.0263832874720433 | validation: 0.7156569796431002]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8058791635162723		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.8058791635162723 | validation: 0.7509543960904204]
	TIME [epoch: 0.694 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7896630832966011		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.7896630832966011 | validation: 0.7421060283310741]
	TIME [epoch: 0.69 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8187283272892598		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.8187283272892598 | validation: 0.9325678826884479]
	TIME [epoch: 0.689 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8996085119431078		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.8996085119431078 | validation: 0.9529909723535357]
	TIME [epoch: 0.69 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.964904367679332		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.964904367679332 | validation: 0.785659576493334]
	TIME [epoch: 0.69 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8103309333688534		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.8103309333688534 | validation: 0.7186521273341357]
	TIME [epoch: 0.688 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7724771598743968		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.7724771598743968 | validation: 0.6794388769055941]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.804045253844578		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.804045253844578 | validation: 1.011435139590751]
	TIME [epoch: 0.694 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.076404363548175		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.076404363548175 | validation: 0.8567969584596643]
	TIME [epoch: 0.692 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.088055880393933		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.088055880393933 | validation: 0.8295663442549301]
	TIME [epoch: 0.691 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8608055687908731		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.8608055687908731 | validation: 0.651420314596266]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7814017248830842		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.7814017248830842 | validation: 0.7503007004155093]
	TIME [epoch: 0.692 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7671736798805077		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.7671736798805077 | validation: 0.6804766541175402]
	TIME [epoch: 0.689 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7813299650726472		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.7813299650726472 | validation: 0.7909504093831469]
	TIME [epoch: 0.688 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7936558868286278		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.7936558868286278 | validation: 0.6924627935024769]
	TIME [epoch: 0.689 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8636634905566354		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.8636634905566354 | validation: 0.903387492065221]
	TIME [epoch: 0.689 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8935168495141508		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.8935168495141508 | validation: 0.8300015031760541]
	TIME [epoch: 0.69 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9791840646764223		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.9791840646764223 | validation: 0.9146936048128289]
	TIME [epoch: 0.689 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9784727932413132		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.9784727932413132 | validation: 0.6447278935864917]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7455205510070404		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.7455205510070404 | validation: 0.6438080347855959]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7278322228676259		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.7278322228676259 | validation: 0.7117643765240631]
	TIME [epoch: 0.693 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7368694702250642		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.7368694702250642 | validation: 0.674681042383263]
	TIME [epoch: 0.691 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7841903325362262		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.7841903325362262 | validation: 0.8122597681505527]
	TIME [epoch: 0.689 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8594548297324058		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.8594548297324058 | validation: 0.6559055621459537]
	TIME [epoch: 0.694 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8787521652231662		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.8787521652231662 | validation: 0.7938261857834134]
	TIME [epoch: 0.689 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8892530306137937		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.8892530306137937 | validation: 0.6403830932207264]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_161.pth
	Model improved!!!
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8162607799517665		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.8162607799517665 | validation: 0.8720538135014867]
	TIME [epoch: 0.691 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.822875566853125		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.822875566853125 | validation: 0.8481529788631597]
	TIME [epoch: 0.69 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8679969510137321		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.8679969510137321 | validation: 0.6999782075750245]
	TIME [epoch: 0.688 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7861626052874918		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.7861626052874918 | validation: 0.7181250272537696]
	TIME [epoch: 0.688 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8049598872116454		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.8049598872116454 | validation: 0.6552676376113888]
	TIME [epoch: 0.688 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7854645453805006		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.7854645453805006 | validation: 0.7114198477459119]
	TIME [epoch: 0.689 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7930827348815691		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.7930827348815691 | validation: 0.6066495527215539]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.773759357285722		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.773759357285722 | validation: 0.7701213945387569]
	TIME [epoch: 0.69 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8260548216530557		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.8260548216530557 | validation: 0.6644795219637646]
	TIME [epoch: 0.689 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8390045720888702		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.8390045720888702 | validation: 0.7379842123693243]
	TIME [epoch: 0.687 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7595156376144886		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.7595156376144886 | validation: 0.630891913561801]
	TIME [epoch: 0.688 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7462710340877746		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.7462710340877746 | validation: 0.6926445117553517]
	TIME [epoch: 0.689 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7173606891350995		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.7173606891350995 | validation: 0.6893763632555143]
	TIME [epoch: 0.693 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7620255209258677		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.7620255209258677 | validation: 0.7421853498734153]
	TIME [epoch: 0.688 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7689847566678825		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.7689847566678825 | validation: 0.7124315466268769]
	TIME [epoch: 0.688 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7972628289826651		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.7972628289826651 | validation: 0.6562002599478373]
	TIME [epoch: 0.688 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7880700857978943		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.7880700857978943 | validation: 0.7504692622421208]
	TIME [epoch: 0.689 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8546246093387083		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.8546246093387083 | validation: 0.6089498654852984]
	TIME [epoch: 0.689 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7618208954875186		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.7618208954875186 | validation: 0.7686846793420836]
	TIME [epoch: 0.688 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7899545996448087		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.7899545996448087 | validation: 0.6465106654084954]
	TIME [epoch: 0.687 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8404468759609628		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.8404468759609628 | validation: 0.7302266886828129]
	TIME [epoch: 0.69 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7410781580317932		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.7410781580317932 | validation: 0.6253220918740374]
	TIME [epoch: 0.689 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7377422723987748		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.7377422723987748 | validation: 0.6981102823194183]
	TIME [epoch: 0.69 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7219781760159235		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.7219781760159235 | validation: 0.614593604142065]
	TIME [epoch: 0.69 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7437274223104511		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.7437274223104511 | validation: 0.7583815855526101]
	TIME [epoch: 0.689 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7927554920485572		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.7927554920485572 | validation: 0.6335353655270929]
	TIME [epoch: 0.688 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.804646332500878		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.804646332500878 | validation: 0.6921158483193155]
	TIME [epoch: 0.689 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7366376681779838		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.7366376681779838 | validation: 0.5719703021460794]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7086434805220455		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.7086434805220455 | validation: 0.6942336403267332]
	TIME [epoch: 0.692 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7281820839103392		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.7281820839103392 | validation: 0.6053176172675043]
	TIME [epoch: 0.689 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7306932286658098		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.7306932286658098 | validation: 0.7039040015110395]
	TIME [epoch: 0.689 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7241668908438866		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.7241668908438866 | validation: 0.7024832877194062]
	TIME [epoch: 0.689 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7767322996279573		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.7767322996279573 | validation: 0.7642497485503688]
	TIME [epoch: 0.69 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7861476294274801		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.7861476294274801 | validation: 0.718112624002751]
	TIME [epoch: 0.689 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7717214763939884		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.7717214763939884 | validation: 0.5945168022178585]
	TIME [epoch: 0.688 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7248482919162885		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.7248482919162885 | validation: 0.6667461353488889]
	TIME [epoch: 0.689 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7350140737683113		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.7350140737683113 | validation: 0.6221303493949027]
	TIME [epoch: 0.689 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7736832698134642		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.7736832698134642 | validation: 0.778217910165244]
	TIME [epoch: 0.688 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7704628794833663		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.7704628794833663 | validation: 0.577854484539534]
	TIME [epoch: 0.697 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7229614357056763		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.7229614357056763 | validation: 0.6177383197853674]
	TIME [epoch: 168 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6626806424913775		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.6626806424913775 | validation: 0.5870439521438432]
	TIME [epoch: 1.38 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6602386452409161		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.6602386452409161 | validation: 0.5600076936436463]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6570585307830576		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.6570585307830576 | validation: 0.5831278826635975]
	TIME [epoch: 1.36 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.666403367581369		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.666403367581369 | validation: 0.584785356052287]
	TIME [epoch: 1.36 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6920485339652703		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.6920485339652703 | validation: 0.7009230167802254]
	TIME [epoch: 1.36 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7711741294430036		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.7711741294430036 | validation: 0.7097303938154168]
	TIME [epoch: 1.36 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8122578054312314		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.8122578054312314 | validation: 0.7332818840712146]
	TIME [epoch: 1.36 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.764525310027955		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.764525310027955 | validation: 0.6093652271388841]
	TIME [epoch: 1.36 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6664983021223202		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.6664983021223202 | validation: 0.5652544645025319]
	TIME [epoch: 1.36 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6753771476919207		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.6753771476919207 | validation: 0.660454417030133]
	TIME [epoch: 1.36 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7491396360266106		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.7491396360266106 | validation: 0.5678382699949804]
	TIME [epoch: 1.36 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7032520116456729		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.7032520116456729 | validation: 0.5886993843502988]
	TIME [epoch: 1.36 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6566682795187664		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.6566682795187664 | validation: 0.5917257297229597]
	TIME [epoch: 1.36 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6624908473725318		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.6624908473725318 | validation: 0.6243636382996436]
	TIME [epoch: 1.36 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6722748534426668		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.6722748534426668 | validation: 0.6444650177122822]
	TIME [epoch: 1.36 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6897233796812137		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.6897233796812137 | validation: 0.6545111296294952]
	TIME [epoch: 1.36 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6695455741297398		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.6695455741297398 | validation: 0.6060825818595488]
	TIME [epoch: 1.36 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6760115942188578		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.6760115942188578 | validation: 0.619427330301325]
	TIME [epoch: 1.36 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6601270008472728		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.6601270008472728 | validation: 0.6071774130491449]
	TIME [epoch: 1.36 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6689393843265762		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.6689393843265762 | validation: 0.6383710286447374]
	TIME [epoch: 1.36 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6814611159888181		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.6814611159888181 | validation: 0.5760502796012449]
	TIME [epoch: 1.36 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.691466863807539		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.691466863807539 | validation: 0.6274102742529906]
	TIME [epoch: 1.36 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6815623882670512		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.6815623882670512 | validation: 0.5448781067028131]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_224.pth
	Model improved!!!
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6678283843690048		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.6678283843690048 | validation: 0.7142245138785567]
	TIME [epoch: 1.36 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.756732888309234		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.756732888309234 | validation: 0.6702946025049813]
	TIME [epoch: 1.36 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8716802853359218		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.8716802853359218 | validation: 0.7193068259863595]
	TIME [epoch: 1.36 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7428644781090498		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.7428644781090498 | validation: 0.7816041589644671]
	TIME [epoch: 1.36 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7624447164325934		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.7624447164325934 | validation: 0.5691698526909467]
	TIME [epoch: 1.36 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6703359381659292		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.6703359381659292 | validation: 0.6249447598898775]
	TIME [epoch: 1.36 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.657232150880714		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.657232150880714 | validation: 0.6183208647861298]
	TIME [epoch: 1.36 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6403797121017928		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.6403797121017928 | validation: 0.5927650952680168]
	TIME [epoch: 1.36 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6293744203630404		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.6293744203630404 | validation: 0.5377983481782834]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_233.pth
	Model improved!!!
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6114281219529997		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.6114281219529997 | validation: 0.5673140990450458]
	TIME [epoch: 1.36 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.613411893647007		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.613411893647007 | validation: 0.5111834799791842]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_235.pth
	Model improved!!!
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.658106660091297		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.658106660091297 | validation: 0.7120623815614594]
	TIME [epoch: 1.36 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7116409517319511		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.7116409517319511 | validation: 0.5757336208580125]
	TIME [epoch: 1.36 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7364413514272746		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.7364413514272746 | validation: 0.6609452045510052]
	TIME [epoch: 1.36 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.65784289814532		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.65784289814532 | validation: 0.6060070312867352]
	TIME [epoch: 1.36 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6341147816947895		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.6341147816947895 | validation: 0.5553136199104614]
	TIME [epoch: 1.36 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6045084631942823		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.6045084631942823 | validation: 0.5626941124552004]
	TIME [epoch: 1.36 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5981353757684245		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.5981353757684245 | validation: 0.5297632976809803]
	TIME [epoch: 1.36 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6032891283856876		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.6032891283856876 | validation: 0.6007285031899771]
	TIME [epoch: 1.36 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6251028039158742		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.6251028039158742 | validation: 0.5377683254259228]
	TIME [epoch: 1.36 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6485966800486704		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.6485966800486704 | validation: 0.6209378658774569]
	TIME [epoch: 1.36 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6221022403456832		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.6221022403456832 | validation: 0.5778005003380327]
	TIME [epoch: 1.36 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.652368390700856		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.652368390700856 | validation: 0.7519842527324044]
	TIME [epoch: 1.36 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7008088910331381		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.7008088910331381 | validation: 0.5823831010995998]
	TIME [epoch: 1.36 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6977711979018991		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.6977711979018991 | validation: 0.6249486067611671]
	TIME [epoch: 1.36 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6168879718416606		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.6168879718416606 | validation: 0.4997139172114956]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_250.pth
	Model improved!!!
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5899050205823881		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.5899050205823881 | validation: 0.5420107623329394]
	TIME [epoch: 1.36 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5651206481898009		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.5651206481898009 | validation: 0.5119077035566636]
	TIME [epoch: 1.36 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5867484934078092		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.5867484934078092 | validation: 0.5910734808911757]
	TIME [epoch: 1.36 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6055144931643329		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.6055144931643329 | validation: 0.606995658947528]
	TIME [epoch: 1.36 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6391768523040484		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.6391768523040484 | validation: 0.6306814112858606]
	TIME [epoch: 1.36 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6369166816072037		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.6369166816072037 | validation: 0.5574917003123426]
	TIME [epoch: 1.36 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5745024086243865		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.5745024086243865 | validation: 0.5249885226428769]
	TIME [epoch: 1.36 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5536231912385161		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.5536231912385161 | validation: 0.5805992479634877]
	TIME [epoch: 1.36 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5570473622582717		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.5570473622582717 | validation: 0.5088111726413713]
	TIME [epoch: 1.35 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6061535665801683		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.6061535665801683 | validation: 0.7181251968983755]
	TIME [epoch: 1.36 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.672650784526325		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.672650784526325 | validation: 0.6119005171992582]
	TIME [epoch: 1.36 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7345853212481211		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.7345853212481211 | validation: 0.6057874348972394]
	TIME [epoch: 1.36 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5863817793852865		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.5863817793852865 | validation: 0.5831974061354076]
	TIME [epoch: 1.35 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5694003233220815		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.5694003233220815 | validation: 0.5320513962055916]
	TIME [epoch: 1.36 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5644058685330945		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.5644058685330945 | validation: 0.5290214885348264]
	TIME [epoch: 1.35 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.551835391103026		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.551835391103026 | validation: 0.5177115228780382]
	TIME [epoch: 1.35 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5277794243899592		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.5277794243899592 | validation: 0.5118499946697511]
	TIME [epoch: 1.35 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5301774459049627		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.5301774459049627 | validation: 0.580965085239851]
	TIME [epoch: 1.35 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5471888085225435		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.5471888085225435 | validation: 0.5115318448742715]
	TIME [epoch: 1.36 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6181426697890264		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.6181426697890264 | validation: 0.5995112487035631]
	TIME [epoch: 1.36 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6048778553757828		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.6048778553757828 | validation: 0.5618431381083626]
	TIME [epoch: 1.36 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6246845560781175		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.6246845560781175 | validation: 0.637810825431213]
	TIME [epoch: 1.36 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.583179547315943		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.583179547315943 | validation: 0.760308070357385]
	TIME [epoch: 1.36 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.655015801490139		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.655015801490139 | validation: 0.5298450000840308]
	TIME [epoch: 1.36 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5126282118599268		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.5126282118599268 | validation: 0.5811709022785797]
	TIME [epoch: 1.36 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5342236308334113		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.5342236308334113 | validation: 0.5263471818996511]
	TIME [epoch: 1.36 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5657165973926798		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.5657165973926798 | validation: 0.6121825733216744]
	TIME [epoch: 1.36 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5482845030116035		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.5482845030116035 | validation: 0.4881491374060179]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49861761179726805		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.49861761179726805 | validation: 0.5231383093174596]
	TIME [epoch: 1.35 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4897878099813351		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.4897878099813351 | validation: 0.4771799145549611]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_280.pth
	Model improved!!!
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5130682834832915		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.5130682834832915 | validation: 0.5937478199618197]
	TIME [epoch: 1.36 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5309426811629714		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.5309426811629714 | validation: 0.47950175345371576]
	TIME [epoch: 1.36 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5384955630929414		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.5384955630929414 | validation: 0.5737815074496296]
	TIME [epoch: 1.35 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5082622391767351		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.5082622391767351 | validation: 0.5025607007485576]
	TIME [epoch: 1.35 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5411942782652729		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.5411942782652729 | validation: 0.5450591222678801]
	TIME [epoch: 1.36 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5281239746769294		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.5281239746769294 | validation: 0.5094566662811756]
	TIME [epoch: 1.35 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4886749356274915		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.4886749356274915 | validation: 0.4910244310434559]
	TIME [epoch: 1.35 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4614394811763093		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.4614394811763093 | validation: 0.5371444990590252]
	TIME [epoch: 1.35 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4874896178510123		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.4874896178510123 | validation: 0.7618613529328031]
	TIME [epoch: 1.35 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5827963353449063		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.5827963353449063 | validation: 0.5547069244374252]
	TIME [epoch: 1.35 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5078915047430942		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.5078915047430942 | validation: 0.4986312099344353]
	TIME [epoch: 1.35 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5162103425326022		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.5162103425326022 | validation: 0.6002744753470145]
	TIME [epoch: 1.35 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5428070808811611		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.5428070808811611 | validation: 0.5178957278939226]
	TIME [epoch: 1.35 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5134314834617099		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.5134314834617099 | validation: 0.5818102299931541]
	TIME [epoch: 1.35 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47822604204165553		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.47822604204165553 | validation: 0.4530322259771012]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49412403427092494		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.49412403427092494 | validation: 0.5124946159299476]
	TIME [epoch: 1.36 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4383760169158511		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.4383760169158511 | validation: 0.4451398133859232]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4294996995907349		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.4294996995907349 | validation: 0.5221172969577996]
	TIME [epoch: 1.35 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43127652169675457		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.43127652169675457 | validation: 0.4931672948732816]
	TIME [epoch: 1.35 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5284888732363754		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.5284888732363754 | validation: 0.5360968333647079]
	TIME [epoch: 1.35 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47256673237483293		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.47256673237483293 | validation: 0.4600473550146982]
	TIME [epoch: 1.35 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4176418259349929		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.4176418259349929 | validation: 0.49090129008516636]
	TIME [epoch: 1.35 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41495148239793656		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.41495148239793656 | validation: 0.43701668136434507]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_303.pth
	Model improved!!!
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42884424942398214		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.42884424942398214 | validation: 0.5687912958308063]
	TIME [epoch: 1.35 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4725421317612628		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.4725421317612628 | validation: 0.4923994640991506]
	TIME [epoch: 1.35 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5209300638730052		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.5209300638730052 | validation: 0.7404311739574715]
	TIME [epoch: 1.35 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5895129737027466		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.5895129737027466 | validation: 0.7061464293812586]
	TIME [epoch: 1.35 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5291821184867831		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.5291821184867831 | validation: 0.4458540078224566]
	TIME [epoch: 1.35 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3914245934213889		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.3914245934213889 | validation: 0.4797802810451146]
	TIME [epoch: 1.35 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3821717126072066		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.3821717126072066 | validation: 0.5167201566645648]
	TIME [epoch: 1.35 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41614278130053656		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.41614278130053656 | validation: 0.503074156189272]
	TIME [epoch: 1.35 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45972372238949377		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.45972372238949377 | validation: 0.5832572217884835]
	TIME [epoch: 1.35 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5382444987910063		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.5382444987910063 | validation: 0.4431147102388101]
	TIME [epoch: 1.35 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.397520352231001		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.397520352231001 | validation: 0.5006509179546038]
	TIME [epoch: 1.35 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38373134945424314		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.38373134945424314 | validation: 0.4500596997649857]
	TIME [epoch: 1.36 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4007832687969403		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.4007832687969403 | validation: 0.5063839913276424]
	TIME [epoch: 1.35 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40050464074767006		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.40050464074767006 | validation: 0.4212828912740452]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_317.pth
	Model improved!!!
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3903980571105282		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.3903980571105282 | validation: 0.5488883459767889]
	TIME [epoch: 1.36 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44599917732045075		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.44599917732045075 | validation: 0.4375635259302607]
	TIME [epoch: 1.36 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4555318767319362		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.4555318767319362 | validation: 0.5110036160040915]
	TIME [epoch: 1.36 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38824113434520013		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.38824113434520013 | validation: 0.44069211684332965]
	TIME [epoch: 1.36 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3701710693068169		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.3701710693068169 | validation: 0.47096440203723655]
	TIME [epoch: 1.36 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.373418817587292		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.373418817587292 | validation: 0.4503178991647554]
	TIME [epoch: 1.36 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4285606459597574		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.4285606459597574 | validation: 0.5336921991311778]
	TIME [epoch: 1.36 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.422348067079163		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.422348067079163 | validation: 0.5684785797850663]
	TIME [epoch: 1.36 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42557351065351806		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.42557351065351806 | validation: 0.6386159161154323]
	TIME [epoch: 1.36 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4916669336304981		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.4916669336304981 | validation: 0.5258393256958303]
	TIME [epoch: 1.36 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39704561988194903		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.39704561988194903 | validation: 0.45131311660047324]
	TIME [epoch: 1.36 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38384263878359137		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.38384263878359137 | validation: 0.4758818208330077]
	TIME [epoch: 1.36 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3947696223481246		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.3947696223481246 | validation: 0.44774671093897833]
	TIME [epoch: 1.36 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3519301040222689		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.3519301040222689 | validation: 0.42707835433790287]
	TIME [epoch: 1.36 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3386693023844238		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.3386693023844238 | validation: 0.41754986031226926]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_332.pth
	Model improved!!!
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3287860931101973		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.3287860931101973 | validation: 0.4449217743562824]
	TIME [epoch: 1.36 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3277325744046485		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.3277325744046485 | validation: 0.422096701404022]
	TIME [epoch: 1.36 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3639901016556169		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.3639901016556169 | validation: 0.6300768354909535]
	TIME [epoch: 1.36 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47804652306341694		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.47804652306341694 | validation: 0.5025588147833915]
	TIME [epoch: 1.36 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5150715590048934		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.5150715590048934 | validation: 0.4930611709420665]
	TIME [epoch: 1.36 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34213052781739656		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.34213052781739656 | validation: 0.4363770033985116]
	TIME [epoch: 1.36 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31109383012970976		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.31109383012970976 | validation: 0.4230262327499157]
	TIME [epoch: 1.36 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.359323942289206		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.359323942289206 | validation: 0.5718327536929219]
	TIME [epoch: 1.36 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4240274784731325		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.4240274784731325 | validation: 0.4766454538879161]
	TIME [epoch: 1.36 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37881553767611437		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.37881553767611437 | validation: 0.4269864223221067]
	TIME [epoch: 1.36 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3205517137337161		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.3205517137337161 | validation: 0.41339381457601887]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31204931152402227		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.31204931152402227 | validation: 0.4653344329797383]
	TIME [epoch: 1.36 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34808238241086115		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.34808238241086115 | validation: 0.42519127035070714]
	TIME [epoch: 1.36 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3901084089179876		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.3901084089179876 | validation: 0.4844809958665384]
	TIME [epoch: 1.36 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3691710849465305		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.3691710849465305 | validation: 0.4232111524304955]
	TIME [epoch: 1.36 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3239040169617519		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.3239040169617519 | validation: 0.5128995823484205]
	TIME [epoch: 1.36 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3290213903595964		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.3290213903595964 | validation: 0.4382877281312917]
	TIME [epoch: 1.36 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3334992161271113		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.3334992161271113 | validation: 0.5134826692302855]
	TIME [epoch: 1.36 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3334950163663126		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.3334950163663126 | validation: 0.40386807009905534]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_351.pth
	Model improved!!!
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3182268425672628		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.3182268425672628 | validation: 0.44997357113226466]
	TIME [epoch: 1.36 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30383247146690245		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.30383247146690245 | validation: 0.39402365782518733]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_353.pth
	Model improved!!!
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37289131557295224		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.37289131557295224 | validation: 0.5400777208118355]
	TIME [epoch: 1.36 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.438699947534445		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.438699947534445 | validation: 0.4489006082408402]
	TIME [epoch: 1.36 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46805105646526474		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.46805105646526474 | validation: 0.4093425098174975]
	TIME [epoch: 1.36 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.278509253703901		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.278509253703901 | validation: 0.4861754415285511]
	TIME [epoch: 1.36 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33497535068985757		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.33497535068985757 | validation: 0.3982693707395507]
	TIME [epoch: 1.36 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33473942101761955		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.33473942101761955 | validation: 0.4489312711091003]
	TIME [epoch: 1.36 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29388322464324923		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.29388322464324923 | validation: 0.4398902742879635]
	TIME [epoch: 1.36 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.304382200205063		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.304382200205063 | validation: 0.5016350355625633]
	TIME [epoch: 1.36 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3580689890829882		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.3580689890829882 | validation: 0.4967779745608241]
	TIME [epoch: 1.36 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3759524474104318		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.3759524474104318 | validation: 0.40976053051437233]
	TIME [epoch: 1.36 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32861184893328527		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.32861184893328527 | validation: 0.4127775066578419]
	TIME [epoch: 1.36 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27267528763801113		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.27267528763801113 | validation: 0.37592776530135136]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_365.pth
	Model improved!!!
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2643354754845564		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.2643354754845564 | validation: 0.401146309888408]
	TIME [epoch: 1.36 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2633713026863789		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.2633713026863789 | validation: 0.3747081869476674]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_367.pth
	Model improved!!!
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2858533384764021		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.2858533384764021 | validation: 0.460237548179661]
	TIME [epoch: 1.36 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3579088162998507		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.3579088162998507 | validation: 0.421558187176516]
	TIME [epoch: 1.36 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33819740353866345		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.33819740353866345 | validation: 0.5221624489742952]
	TIME [epoch: 1.36 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5800713268046295		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.5800713268046295 | validation: 0.5323690439563759]
	TIME [epoch: 1.36 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5832206993611281		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.5832206993611281 | validation: 0.46996980917860603]
	TIME [epoch: 1.36 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4400877696733461		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.4400877696733461 | validation: 0.46344946756570543]
	TIME [epoch: 1.36 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3656090553809964		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.3656090553809964 | validation: 0.4358892314687102]
	TIME [epoch: 1.36 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3085000425868817		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.3085000425868817 | validation: 0.42511697129068493]
	TIME [epoch: 1.36 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32208667905443816		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.32208667905443816 | validation: 0.48492409940047687]
	TIME [epoch: 1.36 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35075071241821904		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.35075071241821904 | validation: 0.43927142606287833]
	TIME [epoch: 1.36 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36023566891125086		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.36023566891125086 | validation: 0.44356878273338574]
	TIME [epoch: 1.36 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3269358182626261		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.3269358182626261 | validation: 0.36200089547228687]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_379.pth
	Model improved!!!
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2800774186366276		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.2800774186366276 | validation: 0.42225091691675476]
	TIME [epoch: 1.35 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2634036407537691		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.2634036407537691 | validation: 0.33708283478291784]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_381.pth
	Model improved!!!
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25353676503046124		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.25353676503046124 | validation: 0.397583314391263]
	TIME [epoch: 1.36 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2585686703175182		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.2585686703175182 | validation: 0.38270785529490264]
	TIME [epoch: 1.35 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27006970608990816		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.27006970608990816 | validation: 0.45101898036474114]
	TIME [epoch: 1.36 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30795740781886166		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.30795740781886166 | validation: 0.41718020407896983]
	TIME [epoch: 1.35 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31576604950145026		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.31576604950145026 | validation: 0.44550528440750237]
	TIME [epoch: 1.35 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2757923219900151		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.2757923219900151 | validation: 0.36789939240544167]
	TIME [epoch: 1.35 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24901642859732206		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.24901642859732206 | validation: 0.37609858976266203]
	TIME [epoch: 1.35 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24228873765101966		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.24228873765101966 | validation: 0.40911055710034944]
	TIME [epoch: 1.35 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27215867001940847		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.27215867001940847 | validation: 0.37910224661327296]
	TIME [epoch: 1.35 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3330659059184961		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.3330659059184961 | validation: 0.44788575230033345]
	TIME [epoch: 1.35 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3426865281368762		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.3426865281368762 | validation: 0.4088709530419856]
	TIME [epoch: 1.35 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3070695051729948		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.3070695051729948 | validation: 0.3785465912298468]
	TIME [epoch: 1.35 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26094773414556516		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.26094773414556516 | validation: 0.37671480497802295]
	TIME [epoch: 1.36 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23583448532568846		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.23583448532568846 | validation: 0.35800291332957807]
	TIME [epoch: 1.36 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22563606547656492		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.22563606547656492 | validation: 0.35076431883494735]
	TIME [epoch: 1.36 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22028370429642408		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.22028370429642408 | validation: 0.3512778932764682]
	TIME [epoch: 1.35 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2327581928683025		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.2327581928683025 | validation: 0.38302426241439685]
	TIME [epoch: 1.35 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25186950948683945		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.25186950948683945 | validation: 0.4355114644420571]
	TIME [epoch: 1.36 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3007876081667146		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.3007876081667146 | validation: 0.4522655654471371]
	TIME [epoch: 1.36 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34083671783106284		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.34083671783106284 | validation: 0.4054039777488127]
	TIME [epoch: 1.36 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3062135417934337		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.3062135417934337 | validation: 0.32704062194747574]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_402.pth
	Model improved!!!
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2651424948306932		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.2651424948306932 | validation: 0.3731462505896757]
	TIME [epoch: 1.36 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22943301170182742		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.22943301170182742 | validation: 0.3344812816514036]
	TIME [epoch: 1.36 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2168682953607366		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.2168682953607366 | validation: 0.32265115172911707]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_405.pth
	Model improved!!!
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21366627035796193		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.21366627035796193 | validation: 0.37348565026103203]
	TIME [epoch: 1.36 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25024570120189266		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.25024570120189266 | validation: 0.3860460213779326]
	TIME [epoch: 1.35 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34033992818796166		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.34033992818796166 | validation: 0.5413834037760673]
	TIME [epoch: 1.35 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34828853452062136		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.34828853452062136 | validation: 0.40071569439238214]
	TIME [epoch: 1.35 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25854817801753444		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.25854817801753444 | validation: 0.3150730688129652]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_410.pth
	Model improved!!!
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2231344799603667		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.2231344799603667 | validation: 0.35557575605289654]
	TIME [epoch: 1.36 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22850050762573218		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.22850050762573218 | validation: 0.3337771631290179]
	TIME [epoch: 1.36 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2534184598189012		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.2534184598189012 | validation: 0.40499850093321277]
	TIME [epoch: 1.36 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29324043414146167		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.29324043414146167 | validation: 0.31078081012496367]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_414.pth
	Model improved!!!
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2256509429006497		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.2256509429006497 | validation: 0.31776632983674635]
	TIME [epoch: 1.36 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19873304586140772		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.19873304586140772 | validation: 0.36241164678614557]
	TIME [epoch: 1.36 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2061617977870727		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.2061617977870727 | validation: 0.3719857987796877]
	TIME [epoch: 1.36 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24838236719178763		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.24838236719178763 | validation: 0.489633819362965]
	TIME [epoch: 1.36 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3024953035604419		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.3024953035604419 | validation: 0.39615817550118204]
	TIME [epoch: 1.36 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27846538298612905		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.27846538298612905 | validation: 0.321619889173035]
	TIME [epoch: 1.36 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2322274200280693		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.2322274200280693 | validation: 0.34296703018629177]
	TIME [epoch: 1.36 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20640155903904833		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.20640155903904833 | validation: 0.3227848690188183]
	TIME [epoch: 1.36 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20639383632053018		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.20639383632053018 | validation: 0.3466261822373271]
	TIME [epoch: 1.36 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22397895339562524		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.22397895339562524 | validation: 0.2964677414248296]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_424.pth
	Model improved!!!
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2362183016918062		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.2362183016918062 | validation: 0.3482441455956507]
	TIME [epoch: 1.36 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2313379273082022		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.2313379273082022 | validation: 0.3298648482873862]
	TIME [epoch: 1.36 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2687399888849216		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.2687399888849216 | validation: 0.3743195150363847]
	TIME [epoch: 1.36 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23514019406041328		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.23514019406041328 | validation: 0.45970104350478264]
	TIME [epoch: 1.36 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29922373948661757		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.29922373948661757 | validation: 0.37880052769698436]
	TIME [epoch: 1.35 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26998331470642206		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.26998331470642206 | validation: 0.31155867329801257]
	TIME [epoch: 1.36 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20422136231448357		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.20422136231448357 | validation: 0.3076293127388204]
	TIME [epoch: 1.36 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17794911941854655		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.17794911941854655 | validation: 0.30899578873192335]
	TIME [epoch: 1.36 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17729470080513685		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.17729470080513685 | validation: 0.3071444318185275]
	TIME [epoch: 1.36 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17807967058692994		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.17807967058692994 | validation: 0.2990083383523937]
	TIME [epoch: 1.36 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18793260617655705		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.18793260617655705 | validation: 0.33430184765180915]
	TIME [epoch: 1.36 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22467835225563995		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.22467835225563995 | validation: 0.42461641066729405]
	TIME [epoch: 1.36 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3027322162208032		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.3027322162208032 | validation: 0.3933229373750506]
	TIME [epoch: 1.36 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3053089749319799		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.3053089749319799 | validation: 0.33099304327930656]
	TIME [epoch: 1.36 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22495273407193941		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.22495273407193941 | validation: 0.2944150310506986]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_439.pth
	Model improved!!!
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18430740103381196		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.18430740103381196 | validation: 0.31050612644926484]
	TIME [epoch: 1.36 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18036794258231634		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.18036794258231634 | validation: 0.30546466780594883]
	TIME [epoch: 1.36 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20032205821784055		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.20032205821784055 | validation: 0.36610295515091323]
	TIME [epoch: 1.35 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2371793386367115		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.2371793386367115 | validation: 0.33972739483859604]
	TIME [epoch: 1.36 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27187611962490826		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.27187611962490826 | validation: 0.35599438640761333]
	TIME [epoch: 1.36 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22039832040295948		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.22039832040295948 | validation: 0.28335656150822297]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_445.pth
	Model improved!!!
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1718991537142236		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.1718991537142236 | validation: 0.3007925925476488]
	TIME [epoch: 1.36 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17109998442240879		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.17109998442240879 | validation: 0.2802944044154348]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_447.pth
	Model improved!!!
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1673658759598179		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.1673658759598179 | validation: 0.28632827572980457]
	TIME [epoch: 1.36 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1677854351567387		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.1677854351567387 | validation: 0.2855836502213916]
	TIME [epoch: 1.35 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16726166000022755		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.16726166000022755 | validation: 0.2733654811451883]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_450.pth
	Model improved!!!
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16557404761037373		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.16557404761037373 | validation: 0.2805185353161142]
	TIME [epoch: 1.36 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16903200230548102		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.16903200230548102 | validation: 0.3053906448721566]
	TIME [epoch: 1.35 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2088769582648905		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.2088769582648905 | validation: 0.5116605366431307]
	TIME [epoch: 1.36 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38424268281617674		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.38424268281617674 | validation: 0.45168965092533403]
	TIME [epoch: 1.35 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3492995543870058		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.3492995543870058 | validation: 0.2926096238193586]
	TIME [epoch: 1.36 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22014136401292703		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.22014136401292703 | validation: 0.28010175717140706]
	TIME [epoch: 1.35 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18494779252144425		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.18494779252144425 | validation: 0.2734761265774733]
	TIME [epoch: 1.36 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18898469674778512		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.18898469674778512 | validation: 0.3197300155738584]
	TIME [epoch: 1.35 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19357918771212215		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.19357918771212215 | validation: 0.295368528969351]
	TIME [epoch: 1.36 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19627700996997186		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.19627700996997186 | validation: 0.28812453276125893]
	TIME [epoch: 1.36 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17819397899489312		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.17819397899489312 | validation: 0.2991777685453991]
	TIME [epoch: 1.36 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17572670150416222		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.17572670150416222 | validation: 0.28998996793726195]
	TIME [epoch: 1.36 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17309361060983158		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.17309361060983158 | validation: 0.31204271466265426]
	TIME [epoch: 1.36 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1802278051758345		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.1802278051758345 | validation: 0.2892682856193166]
	TIME [epoch: 1.35 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18938608911919233		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.18938608911919233 | validation: 0.3284348346158657]
	TIME [epoch: 1.36 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19760624837990398		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.19760624837990398 | validation: 0.2819962573930816]
	TIME [epoch: 1.36 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19264189231572101		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.19264189231572101 | validation: 0.3160219373098837]
	TIME [epoch: 1.36 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19120742347961175		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.19120742347961175 | validation: 0.2711341770713119]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_468.pth
	Model improved!!!
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16992084404670263		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.16992084404670263 | validation: 0.28310758792639723]
	TIME [epoch: 1.36 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16412486101730836		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.16412486101730836 | validation: 0.24830087987182684]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_470.pth
	Model improved!!!
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16818598980961055		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.16818598980961055 | validation: 0.30601864783636573]
	TIME [epoch: 1.36 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17610545661641638		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.17610545661641638 | validation: 0.240747591696472]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_472.pth
	Model improved!!!
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18785584690497717		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.18785584690497717 | validation: 0.2950117003960879]
	TIME [epoch: 1.36 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19892880720981754		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.19892880720981754 | validation: 0.2909523756944317]
	TIME [epoch: 1.37 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20145426853628878		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.20145426853628878 | validation: 0.42081714483486543]
	TIME [epoch: 1.35 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26125989983724884		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.26125989983724884 | validation: 0.3804651166357753]
	TIME [epoch: 1.35 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23561990487214307		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.23561990487214307 | validation: 0.2595535444453387]
	TIME [epoch: 1.35 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16851498862415235		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.16851498862415235 | validation: 0.22723452352166779]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_478.pth
	Model improved!!!
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14672072082310608		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.14672072082310608 | validation: 0.26964167203382555]
	TIME [epoch: 1.35 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14827962848959397		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.14827962848959397 | validation: 0.2574897642892184]
	TIME [epoch: 1.35 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1446000332107164		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.1446000332107164 | validation: 0.25362542753292167]
	TIME [epoch: 1.35 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1452969627801886		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.1452969627801886 | validation: 0.2760278413324194]
	TIME [epoch: 1.35 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18901978606063397		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.18901978606063397 | validation: 0.30159533413288264]
	TIME [epoch: 1.35 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2562885635529784		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.2562885635529784 | validation: 0.2931090528539129]
	TIME [epoch: 1.35 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1940996027506122		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.1940996027506122 | validation: 0.29265788961135625]
	TIME [epoch: 1.35 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17351136697258837		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.17351136697258837 | validation: 0.27337411952975427]
	TIME [epoch: 1.35 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19028498286921536		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.19028498286921536 | validation: 0.28753274363510645]
	TIME [epoch: 1.35 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17719655956048155		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.17719655956048155 | validation: 0.2519625665159379]
	TIME [epoch: 1.35 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15330265468618345		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.15330265468618345 | validation: 0.2841471578887939]
	TIME [epoch: 1.35 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14582447465082082		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.14582447465082082 | validation: 0.24226424628463292]
	TIME [epoch: 1.35 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15179102374083556		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.15179102374083556 | validation: 0.2593305843005322]
	TIME [epoch: 1.35 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15600475305469816		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.15600475305469816 | validation: 0.2626619333330576]
	TIME [epoch: 1.35 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16255240117098552		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.16255240117098552 | validation: 0.3032803003178032]
	TIME [epoch: 1.35 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1741295484143818		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.1741295484143818 | validation: 0.28105928503127026]
	TIME [epoch: 1.35 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18157122998490366		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.18157122998490366 | validation: 0.29399180210249937]
	TIME [epoch: 1.35 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16735872399451446		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.16735872399451446 | validation: 0.2554912720163864]
	TIME [epoch: 1.35 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14852436839400074		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.14852436839400074 | validation: 0.258657156256945]
	TIME [epoch: 1.35 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14321977145462517		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.14321977145462517 | validation: 0.24395643294030084]
	TIME [epoch: 1.35 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1469092438218348		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.1469092438218348 | validation: 0.24259271187400178]
	TIME [epoch: 1.35 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18721645701245643		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.18721645701245643 | validation: 0.29559123621854066]
	TIME [epoch: 1.35 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2081175621309713		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.2081175621309713 | validation: 0.254703717151237]
	TIME [epoch: 170 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16832121375193823		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.16832121375193823 | validation: 0.24556966134278985]
	TIME [epoch: 2.69 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14665020162884848		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.14665020162884848 | validation: 0.28911899266387764]
	TIME [epoch: 2.68 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1463997883905676		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.1463997883905676 | validation: 0.238128074545776]
	TIME [epoch: 2.68 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1598406210745589		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.1598406210745589 | validation: 0.26668831485208966]
	TIME [epoch: 2.68 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.159090444835846		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.159090444835846 | validation: 0.23048920897811842]
	TIME [epoch: 2.69 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14095924598131643		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.14095924598131643 | validation: 0.2630404728924419]
	TIME [epoch: 2.68 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13768298128716913		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.13768298128716913 | validation: 0.2674563381227945]
	TIME [epoch: 2.69 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16611335506294522		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.16611335506294522 | validation: 0.29952829645596174]
	TIME [epoch: 2.68 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1970463363903818		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.1970463363903818 | validation: 0.28519417176977807]
	TIME [epoch: 2.68 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19894378873781637		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.19894378873781637 | validation: 0.22359224686350665]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_511.pth
	Model improved!!!
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15059017410054776		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.15059017410054776 | validation: 0.21686448630923058]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_512.pth
	Model improved!!!
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1248491158199008		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.1248491158199008 | validation: 0.22898051532861122]
	TIME [epoch: 2.68 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11613587697166583		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.11613587697166583 | validation: 0.20338805517012518]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_514.pth
	Model improved!!!
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11309077931634322		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.11309077931634322 | validation: 0.20731269051973333]
	TIME [epoch: 2.68 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11364759278051363		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.11364759278051363 | validation: 0.20619759964301113]
	TIME [epoch: 2.69 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11850965373008009		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.11850965373008009 | validation: 0.21830404080650612]
	TIME [epoch: 2.68 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12519079257082388		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.12519079257082388 | validation: 0.21480693738206905]
	TIME [epoch: 2.68 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14027770680348284		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.14027770680348284 | validation: 0.24908373492103697]
	TIME [epoch: 2.68 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16634735765108027		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.16634735765108027 | validation: 0.3163304627822095]
	TIME [epoch: 2.69 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20054322996175256		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.20054322996175256 | validation: 0.4063852628841928]
	TIME [epoch: 2.68 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26100976283470984		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.26100976283470984 | validation: 0.28298354108337015]
	TIME [epoch: 2.68 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17054226011367576		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.17054226011367576 | validation: 0.2069692320606774]
	TIME [epoch: 2.68 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15084240548866443		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.15084240548866443 | validation: 0.2093145570597832]
	TIME [epoch: 2.68 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12120217017832854		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.12120217017832854 | validation: 0.20541074119050898]
	TIME [epoch: 2.68 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11203815022527462		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.11203815022527462 | validation: 0.20566068204950716]
	TIME [epoch: 2.68 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1175542329457987		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.1175542329457987 | validation: 0.239486303397833]
	TIME [epoch: 2.68 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1334083892966472		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.1334083892966472 | validation: 0.23655842822636883]
	TIME [epoch: 2.68 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1486847154068646		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.1486847154068646 | validation: 0.2705391026441711]
	TIME [epoch: 2.68 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15780155911588015		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.15780155911588015 | validation: 0.2502314533429512]
	TIME [epoch: 2.68 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16720784918767706		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.16720784918767706 | validation: 0.20590029072568755]
	TIME [epoch: 2.68 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1480935664566241		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.1480935664566241 | validation: 0.2100171926828466]
	TIME [epoch: 2.68 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1328923627238785		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.1328923627238785 | validation: 0.19298403795200136]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_533.pth
	Model improved!!!
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11529010191451043		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.11529010191451043 | validation: 0.192068436291285]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_534.pth
	Model improved!!!
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10509618764698891		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.10509618764698891 | validation: 0.19129000051346057]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_535.pth
	Model improved!!!
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10162255412222783		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.10162255412222783 | validation: 0.18783017400194693]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_536.pth
	Model improved!!!
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10512695290101244		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.10512695290101244 | validation: 0.21292586786270373]
	TIME [epoch: 2.68 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11146996958733851		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.11146996958733851 | validation: 0.22642655318578717]
	TIME [epoch: 2.69 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14779991922736582		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.14779991922736582 | validation: 0.3470713015288336]
	TIME [epoch: 2.68 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22586033907058536		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.22586033907058536 | validation: 0.25197675047154394]
	TIME [epoch: 2.68 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18166235573282985		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.18166235573282985 | validation: 0.19835875414894769]
	TIME [epoch: 2.68 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15041263697404852		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.15041263697404852 | validation: 0.2005349686885042]
	TIME [epoch: 2.68 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1464209951580797		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.1464209951580797 | validation: 0.19396298119219701]
	TIME [epoch: 2.68 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11339847731239826		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.11339847731239826 | validation: 0.19543445767459292]
	TIME [epoch: 2.68 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10284424330600679		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.10284424330600679 | validation: 0.18529706998502754]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_545.pth
	Model improved!!!
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09637721820942446		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.09637721820942446 | validation: 0.18017080206576896]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_546.pth
	Model improved!!!
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10007351192175669		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.10007351192175669 | validation: 0.185125978185283]
	TIME [epoch: 2.67 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10542414067439855		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.10542414067439855 | validation: 0.2201554797513029]
	TIME [epoch: 2.67 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1306425605077373		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.1306425605077373 | validation: 0.28759189484586833]
	TIME [epoch: 2.68 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2015334135987532		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.2015334135987532 | validation: 0.2904218076752689]
	TIME [epoch: 2.68 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18591001666788642		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.18591001666788642 | validation: 0.20577409487483622]
	TIME [epoch: 2.68 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13337731991558974		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.13337731991558974 | validation: 0.17374559968458614]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_552.pth
	Model improved!!!
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10314620273826595		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.10314620273826595 | validation: 0.17786424687918087]
	TIME [epoch: 2.68 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10130433305855828		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.10130433305855828 | validation: 0.18982667082121046]
	TIME [epoch: 2.69 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10320015508739044		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.10320015508739044 | validation: 0.18341690611794734]
	TIME [epoch: 2.68 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11334863705155021		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.11334863705155021 | validation: 0.1936693991094081]
	TIME [epoch: 2.68 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12037312829095274		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.12037312829095274 | validation: 0.19745832399629662]
	TIME [epoch: 2.68 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11621490992341495		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.11621490992341495 | validation: 0.21684919106488154]
	TIME [epoch: 2.68 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1334934840153349		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.1334934840153349 | validation: 0.2845040815955816]
	TIME [epoch: 2.69 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16188187379093086		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.16188187379093086 | validation: 0.22125879485195013]
	TIME [epoch: 2.68 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15021575087108885		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.15021575087108885 | validation: 0.18775419070405938]
	TIME [epoch: 2.68 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1123059242155191		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.1123059242155191 | validation: 0.17184290940677494]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_562.pth
	Model improved!!!
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09056592996476034		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.09056592996476034 | validation: 0.16195600060117346]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_563.pth
	Model improved!!!
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0889906102534076		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.0889906102534076 | validation: 0.17615048320244275]
	TIME [epoch: 2.68 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09662404554582597		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.09662404554582597 | validation: 0.16675051258570683]
	TIME [epoch: 2.68 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11055952254627478		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.11055952254627478 | validation: 0.1949986797731257]
	TIME [epoch: 2.68 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1157302071904494		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.1157302071904494 | validation: 0.1696574157394696]
	TIME [epoch: 2.68 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12016437965175965		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.12016437965175965 | validation: 0.19650537057931491]
	TIME [epoch: 2.68 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1040304064725136		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.1040304064725136 | validation: 0.1819178128581246]
	TIME [epoch: 2.68 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10629135966840242		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.10629135966840242 | validation: 0.25071181638240797]
	TIME [epoch: 2.69 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13809618508939941		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.13809618508939941 | validation: 0.2717930768439643]
	TIME [epoch: 2.68 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18764856986717474		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.18764856986717474 | validation: 0.21734334626561483]
	TIME [epoch: 2.68 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14429500982336002		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.14429500982336002 | validation: 0.18266127937491047]
	TIME [epoch: 2.68 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11132147331156618		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.11132147331156618 | validation: 0.1684890814120955]
	TIME [epoch: 2.68 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09305929748931475		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.09305929748931475 | validation: 0.16106405753951472]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_575.pth
	Model improved!!!
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08716708546693813		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.08716708546693813 | validation: 0.15363607664024184]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_576.pth
	Model improved!!!
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08643217239350844		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.08643217239350844 | validation: 0.15662168681345967]
	TIME [epoch: 2.67 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08810503747799164		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.08810503747799164 | validation: 0.15402615246130563]
	TIME [epoch: 2.66 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09279886044610722		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.09279886044610722 | validation: 0.17829675562779285]
	TIME [epoch: 2.67 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11108889768993514		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.11108889768993514 | validation: 0.1863415448395553]
	TIME [epoch: 2.67 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13990391039421232		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.13990391039421232 | validation: 0.22273516904653465]
	TIME [epoch: 2.66 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15076443329824596		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.15076443329824596 | validation: 0.298147551777542]
	TIME [epoch: 2.66 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1797984870906277		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.1797984870906277 | validation: 0.2025476509756067]
	TIME [epoch: 2.66 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1445346982250525		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.1445346982250525 | validation: 0.15811687526279097]
	TIME [epoch: 2.66 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09037446666094169		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.09037446666094169 | validation: 0.1555949748898826]
	TIME [epoch: 2.66 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08491080924162052		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.08491080924162052 | validation: 0.16030679919888796]
	TIME [epoch: 2.66 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08912190864419788		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.08912190864419788 | validation: 0.18717637351959857]
	TIME [epoch: 2.66 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09550142249990388		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.09550142249990388 | validation: 0.16713380853255527]
	TIME [epoch: 2.66 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10219413059867324		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.10219413059867324 | validation: 0.188343347087566]
	TIME [epoch: 2.66 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10933101560084367		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.10933101560084367 | validation: 0.1878828245937071]
	TIME [epoch: 2.66 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12351763852088457		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.12351763852088457 | validation: 0.18881960834362643]
	TIME [epoch: 2.67 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11148740686420106		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.11148740686420106 | validation: 0.16868487128363358]
	TIME [epoch: 2.67 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10015171102589594		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.10015171102589594 | validation: 0.157001552491133]
	TIME [epoch: 2.67 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09732085956982169		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.09732085956982169 | validation: 0.16319547751062782]
	TIME [epoch: 2.66 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09867074143647064		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.09867074143647064 | validation: 0.14624248036424545]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_595.pth
	Model improved!!!
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09711211915158949		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.09711211915158949 | validation: 0.1467252855619965]
	TIME [epoch: 2.67 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08490435519005039		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.08490435519005039 | validation: 0.14716991371218757]
	TIME [epoch: 2.66 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08027511802095008		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.08027511802095008 | validation: 0.15332533089628778]
	TIME [epoch: 2.67 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0796357376451683		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.0796357376451683 | validation: 0.13360473695510158]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_599.pth
	Model improved!!!
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08615136662479352		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.08615136662479352 | validation: 0.16217657579266775]
	TIME [epoch: 2.68 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09238640374962724		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.09238640374962724 | validation: 0.1437739611284383]
	TIME [epoch: 2.7 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10708598183583828		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.10708598183583828 | validation: 0.16583609579381217]
	TIME [epoch: 2.68 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11001898233399278		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.11001898233399278 | validation: 0.25852120104829884]
	TIME [epoch: 2.68 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16173095803031948		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.16173095803031948 | validation: 0.3159078852300737]
	TIME [epoch: 2.69 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21529439624233163		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.21529439624233163 | validation: 0.14985902370437773]
	TIME [epoch: 2.68 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10254517339990216		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.10254517339990216 | validation: 0.15523084597779874]
	TIME [epoch: 2.69 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09043985090168312		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.09043985090168312 | validation: 0.18089023232997936]
	TIME [epoch: 2.68 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10980971692580975		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.10980971692580975 | validation: 0.18645027798557023]
	TIME [epoch: 2.69 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10872594540709318		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.10872594540709318 | validation: 0.1449879545625153]
	TIME [epoch: 2.68 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08889300849227748		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.08889300849227748 | validation: 0.13905747348342437]
	TIME [epoch: 2.68 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07903770914491771		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.07903770914491771 | validation: 0.1334101475225667]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_611.pth
	Model improved!!!
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07419100499823915		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.07419100499823915 | validation: 0.14188030341732036]
	TIME [epoch: 2.67 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07511499454065543		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.07511499454065543 | validation: 0.13314838950839536]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_613.pth
	Model improved!!!
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07374372910886004		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.07374372910886004 | validation: 0.13029797575838417]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_614.pth
	Model improved!!!
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07426987256184675		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.07426987256184675 | validation: 0.13879252190570043]
	TIME [epoch: 2.68 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08773788984797448		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.08773788984797448 | validation: 0.15622012760704865]
	TIME [epoch: 2.68 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10713513033036719		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.10713513033036719 | validation: 0.19375336197293483]
	TIME [epoch: 2.69 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1405946380573893		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.1405946380573893 | validation: 0.2304347845155201]
	TIME [epoch: 2.68 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15282730175437956		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.15282730175437956 | validation: 0.18786964228169128]
	TIME [epoch: 2.68 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10421339403576947		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.10421339403576947 | validation: 0.13201464412152664]
	TIME [epoch: 2.68 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08187455460183969		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.08187455460183969 | validation: 0.1340749285065419]
	TIME [epoch: 2.68 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07035195474121417		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.07035195474121417 | validation: 0.13106060914884696]
	TIME [epoch: 2.68 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07228419485836293		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.07228419485836293 | validation: 0.13656639576956572]
	TIME [epoch: 2.69 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07291934009425317		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.07291934009425317 | validation: 0.1438734454954934]
	TIME [epoch: 2.68 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08214107218966386		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.08214107218966386 | validation: 0.16690578590781305]
	TIME [epoch: 2.68 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09741896758555002		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.09741896758555002 | validation: 0.20795932865311217]
	TIME [epoch: 2.68 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1262769134003693		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.1262769134003693 | validation: 0.1952509904883346]
	TIME [epoch: 2.68 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1312902246885941		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.1312902246885941 | validation: 0.14404628329827251]
	TIME [epoch: 2.68 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08840314587799586		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.08840314587799586 | validation: 0.12524408341582216]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_629.pth
	Model improved!!!
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.071009755195383		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.071009755195383 | validation: 0.12226642934105532]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_630.pth
	Model improved!!!
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07222027440232724		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.07222027440232724 | validation: 0.14397173316009845]
	TIME [epoch: 2.67 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07411919999542006		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.07411919999542006 | validation: 0.12484090550336101]
	TIME [epoch: 2.67 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08428816576887968		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.08428816576887968 | validation: 0.1543601097321209]
	TIME [epoch: 2.67 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08869511583849522		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.08869511583849522 | validation: 0.12830705564485567]
	TIME [epoch: 2.68 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09126075252343183		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.09126075252343183 | validation: 0.15950056413137767]
	TIME [epoch: 2.67 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08762306069684257		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.08762306069684257 | validation: 0.13694188404921942]
	TIME [epoch: 2.67 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08592752003067572		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.08592752003067572 | validation: 0.16404894884812538]
	TIME [epoch: 2.67 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08889605840869584		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.08889605840869584 | validation: 0.15799816185777021]
	TIME [epoch: 2.67 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10248057420679058		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.10248057420679058 | validation: 0.16972558759745124]
	TIME [epoch: 2.67 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1020056500537224		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.1020056500537224 | validation: 0.14635666403831113]
	TIME [epoch: 2.67 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11174217272197258		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.11174217272197258 | validation: 0.14323016008472905]
	TIME [epoch: 2.67 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09522656257749561		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.09522656257749561 | validation: 0.12726987202702383]
	TIME [epoch: 2.67 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08697208476407402		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.08697208476407402 | validation: 0.1299072371895263]
	TIME [epoch: 2.67 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07419831715935474		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.07419831715935474 | validation: 0.11994401781966962]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_644.pth
	Model improved!!!
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06872711399590234		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.06872711399590234 | validation: 0.11922907278646916]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_645.pth
	Model improved!!!
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06283481847876614		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.06283481847876614 | validation: 0.12277072971873637]
	TIME [epoch: 2.67 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06485839508636283		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.06485839508636283 | validation: 0.12628338244056025]
	TIME [epoch: 2.67 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06963450488892485		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.06963450488892485 | validation: 0.14619720177524914]
	TIME [epoch: 2.67 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08351533876801177		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.08351533876801177 | validation: 0.17872951196859843]
	TIME [epoch: 2.67 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11676779626680212		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.11676779626680212 | validation: 0.1964731875877799]
	TIME [epoch: 2.67 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11779032530467226		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.11779032530467226 | validation: 0.144204347161318]
	TIME [epoch: 2.67 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10136890872255414		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.10136890872255414 | validation: 0.12039771421886197]
	TIME [epoch: 2.67 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06889199637915983		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.06889199637915983 | validation: 0.11824738243330535]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_653.pth
	Model improved!!!
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06453381758805164		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.06453381758805164 | validation: 0.1208902492358758]
	TIME [epoch: 2.68 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06808760462265456		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.06808760462265456 | validation: 0.11890345055441026]
	TIME [epoch: 2.68 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07275098706336267		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.07275098706336267 | validation: 0.13315778448824625]
	TIME [epoch: 2.69 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07819468023450594		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.07819468023450594 | validation: 0.11487450512681503]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_657.pth
	Model improved!!!
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08454696833631338		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.08454696833631338 | validation: 0.11946073049401827]
	TIME [epoch: 2.68 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07296601966539906		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.07296601966539906 | validation: 0.14740449593270302]
	TIME [epoch: 2.67 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07811768114489535		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.07811768114489535 | validation: 0.17932654751385224]
	TIME [epoch: 2.68 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11480275358745486		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.11480275358745486 | validation: 0.19471059332900797]
	TIME [epoch: 2.68 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12520149657338459		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.12520149657338459 | validation: 0.1369347304025911]
	TIME [epoch: 2.68 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09217798379659867		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.09217798379659867 | validation: 0.111507280108552]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_663.pth
	Model improved!!!
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06469343625469269		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.06469343625469269 | validation: 0.11785301625611751]
	TIME [epoch: 2.66 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06028237561549865		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.06028237561549865 | validation: 0.12251584189111303]
	TIME [epoch: 2.66 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07207263066240194		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.07207263066240194 | validation: 0.1384579590233634]
	TIME [epoch: 2.67 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08382083501373036		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.08382083501373036 | validation: 0.1349339559985607]
	TIME [epoch: 2.66 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08557508383539293		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.08557508383539293 | validation: 0.13973988832613257]
	TIME [epoch: 3.53 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08131308677863232		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.08131308677863232 | validation: 0.12068283715275308]
	TIME [epoch: 2.68 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06921914411507554		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.06921914411507554 | validation: 0.12451011782468463]
	TIME [epoch: 2.67 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06365288473227379		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.06365288473227379 | validation: 0.11367820422569784]
	TIME [epoch: 2.67 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06695667798715958		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.06695667798715958 | validation: 0.13601831645793713]
	TIME [epoch: 2.67 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07239753339053813		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.07239753339053813 | validation: 0.12217033665733373]
	TIME [epoch: 2.67 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08001109129083502		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.08001109129083502 | validation: 0.1453731236539233]
	TIME [epoch: 2.67 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08588161479003623		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.08588161479003623 | validation: 0.12076745892625712]
	TIME [epoch: 2.67 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07691973294035512		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.07691973294035512 | validation: 0.14244860096712328]
	TIME [epoch: 2.67 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07039345955885959		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.07039345955885959 | validation: 0.11332316552212435]
	TIME [epoch: 2.67 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06805428685614133		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.06805428685614133 | validation: 0.12288989336040634]
	TIME [epoch: 2.67 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07608304237937438		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.07608304237937438 | validation: 0.15150993950297073]
	TIME [epoch: 2.67 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11139953327395714		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.11139953327395714 | validation: 0.13453485707494658]
	TIME [epoch: 2.67 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10516692477812285		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.10516692477812285 | validation: 0.11879729872963792]
	TIME [epoch: 2.67 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07242527668404164		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.07242527668404164 | validation: 0.1310971351236153]
	TIME [epoch: 2.67 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07003798504558696		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.07003798504558696 | validation: 0.10915381680941519]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_683.pth
	Model improved!!!
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06976189503087264		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.06976189503087264 | validation: 0.11844527212455268]
	TIME [epoch: 2.67 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06216856684656834		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.06216856684656834 | validation: 0.09848842039716742]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_685.pth
	Model improved!!!
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057254492664937054		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.057254492664937054 | validation: 0.10894467478135272]
	TIME [epoch: 2.67 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0569467585548907		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.0569467585548907 | validation: 0.10722092649028073]
	TIME [epoch: 2.67 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0569045609216964		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.0569045609216964 | validation: 0.11162826916262561]
	TIME [epoch: 2.67 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06164248124742984		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.06164248124742984 | validation: 0.1279853790075575]
	TIME [epoch: 2.67 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08261360658944135		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.08261360658944135 | validation: 0.1825144269735089]
	TIME [epoch: 2.67 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12072796061437384		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.12072796061437384 | validation: 0.15464542448848906]
	TIME [epoch: 2.67 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10770403535454785		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.10770403535454785 | validation: 0.10614480565434779]
	TIME [epoch: 2.67 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06705787503013205		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.06705787503013205 | validation: 0.09687592416074264]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_693.pth
	Model improved!!!
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05615334759050094		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.05615334759050094 | validation: 0.11052125068599801]
	TIME [epoch: 2.67 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062258686350741624		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.062258686350741624 | validation: 0.09540482126441152]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_695.pth
	Model improved!!!
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06100247851391258		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.06100247851391258 | validation: 0.09768408386970723]
	TIME [epoch: 2.67 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05404236425882172		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.05404236425882172 | validation: 0.10107920593657083]
	TIME [epoch: 2.67 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055910884457167076		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.055910884457167076 | validation: 0.10345215124954127]
	TIME [epoch: 2.67 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0548965304418577		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.0548965304418577 | validation: 0.10442137684441938]
	TIME [epoch: 2.68 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05831182317736827		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.05831182317736827 | validation: 0.11599617975950823]
	TIME [epoch: 2.67 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07778368732099084		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.07778368732099084 | validation: 0.1717417602279847]
	TIME [epoch: 2.67 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11970218382050657		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.11970218382050657 | validation: 0.1528965712972192]
	TIME [epoch: 2.67 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11255111093199002		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.11255111093199002 | validation: 0.11364277782810493]
	TIME [epoch: 2.67 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06785887192946517		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.06785887192946517 | validation: 0.101048610344025]
	TIME [epoch: 2.67 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06265270959999343		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.06265270959999343 | validation: 0.10803289914544761]
	TIME [epoch: 2.67 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05888179347102658		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.05888179347102658 | validation: 0.10560097514037992]
	TIME [epoch: 2.67 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060581073814769626		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.060581073814769626 | validation: 0.1057728023570649]
	TIME [epoch: 2.67 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06176484101185231		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.06176484101185231 | validation: 0.1220017954252835]
	TIME [epoch: 2.67 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06547100896341665		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.06547100896341665 | validation: 0.1225964790410842]
	TIME [epoch: 2.67 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07264552125406504		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.07264552125406504 | validation: 0.13552274701305397]
	TIME [epoch: 2.67 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07280614386558155		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.07280614386558155 | validation: 0.10984310231677048]
	TIME [epoch: 2.67 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06936389830943998		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.06936389830943998 | validation: 0.11046970681022956]
	TIME [epoch: 2.67 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0606487142559668		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.0606487142559668 | validation: 0.0986053920207843]
	TIME [epoch: 2.67 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05723370058416016		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.05723370058416016 | validation: 0.10060921191838781]
	TIME [epoch: 2.67 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05806235457372397		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.05806235457372397 | validation: 0.10088841305354262]
	TIME [epoch: 2.67 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05792044205050212		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.05792044205050212 | validation: 0.11373019850947537]
	TIME [epoch: 2.67 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06484935043928061		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.06484935043928061 | validation: 0.11284776080509838]
	TIME [epoch: 2.67 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0786047706831023		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.0786047706831023 | validation: 0.13186709758852996]
	TIME [epoch: 2.67 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09669068166749		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.09669068166749 | validation: 0.11990324037051368]
	TIME [epoch: 2.67 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08365110624044018		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.08365110624044018 | validation: 0.10627829240492931]
	TIME [epoch: 2.67 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06028177836366208		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.06028177836366208 | validation: 0.09068257847004943]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_721.pth
	Model improved!!!
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057498986994198704		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.057498986994198704 | validation: 0.10574280666015189]
	TIME [epoch: 2.67 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06327812943612837		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.06327812943612837 | validation: 0.09305670445207964]
	TIME [epoch: 2.67 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062110755426720385		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.062110755426720385 | validation: 0.09898824752084479]
	TIME [epoch: 2.67 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05545501456867232		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.05545501456867232 | validation: 0.0886716959556581]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_725.pth
	Model improved!!!
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05270299585157865		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.05270299585157865 | validation: 0.08966056118939464]
	TIME [epoch: 2.67 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051762437882514674		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.051762437882514674 | validation: 0.08994235215921648]
	TIME [epoch: 2.67 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05136593742900793		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.05136593742900793 | validation: 0.0898734215554175]
	TIME [epoch: 2.67 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05385031597911553		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.05385031597911553 | validation: 0.11714282764584741]
	TIME [epoch: 2.68 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06508873543151604		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.06508873543151604 | validation: 0.15010440746631126]
	TIME [epoch: 2.67 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10247951502100026		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.10247951502100026 | validation: 0.15699896888358228]
	TIME [epoch: 2.67 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09809797691079399		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.09809797691079399 | validation: 0.11276179491073064]
	TIME [epoch: 2.67 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0785406711301532		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.0785406711301532 | validation: 0.09082561840238294]
	TIME [epoch: 2.67 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06461582225909876		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.06461582225909876 | validation: 0.09385902456212486]
	TIME [epoch: 2.67 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05136604625836818		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.05136604625836818 | validation: 0.08894228842501704]
	TIME [epoch: 2.67 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04843685503578973		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.04843685503578973 | validation: 0.09559246751321582]
	TIME [epoch: 2.67 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05167709348230069		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.05167709348230069 | validation: 0.08754503262332322]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_737.pth
	Model improved!!!
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051916110740149186		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.051916110740149186 | validation: 0.09777702002905563]
	TIME [epoch: 2.67 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052132047812869294		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.052132047812869294 | validation: 0.09785077041214486]
	TIME [epoch: 2.67 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05645609585827132		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.05645609585827132 | validation: 0.11982658545016478]
	TIME [epoch: 2.67 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06929959916049963		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.06929959916049963 | validation: 0.12251355992237843]
	TIME [epoch: 2.67 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0895808540526495		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.0895808540526495 | validation: 0.1208466246761524]
	TIME [epoch: 2.67 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07834230314714007		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.07834230314714007 | validation: 0.09662881143495572]
	TIME [epoch: 2.67 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06035557101345051		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.06035557101345051 | validation: 0.0922317434762921]
	TIME [epoch: 2.67 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05036791233182008		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.05036791233182008 | validation: 0.09099496126637298]
	TIME [epoch: 2.67 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05059006697568318		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.05059006697568318 | validation: 0.08390188721434572]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_746.pth
	Model improved!!!
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055501513552492714		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.055501513552492714 | validation: 0.09797421717461353]
	TIME [epoch: 2.67 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05389986157714624		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.05389986157714624 | validation: 0.08822979971226376]
	TIME [epoch: 2.67 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050417754703781674		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.050417754703781674 | validation: 0.09847598673190022]
	TIME [epoch: 2.67 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0522839458156915		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.0522839458156915 | validation: 0.08648074947399685]
	TIME [epoch: 2.67 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05174267261093998		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.05174267261093998 | validation: 0.1181090055143058]
	TIME [epoch: 2.67 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06206527832377142		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.06206527832377142 | validation: 0.12207385189069085]
	TIME [epoch: 2.67 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.090013572239542		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.090013572239542 | validation: 0.13170464092825548]
	TIME [epoch: 2.67 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0894907965211684		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.0894907965211684 | validation: 0.0973442255755777]
	TIME [epoch: 2.67 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0676755961040307		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.0676755961040307 | validation: 0.08701668740731446]
	TIME [epoch: 2.67 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04905351897728105		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.04905351897728105 | validation: 0.08290717566972539]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_756.pth
	Model improved!!!
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04805247590900711		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.04805247590900711 | validation: 0.08557044699741645]
	TIME [epoch: 2.67 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05091566960304367		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.05091566960304367 | validation: 0.08219169860724489]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_758.pth
	Model improved!!!
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04989459886804959		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.04989459886804959 | validation: 0.08737883181037052]
	TIME [epoch: 2.67 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04829905948842658		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.04829905948842658 | validation: 0.07925419520223911]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_760.pth
	Model improved!!!
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05091108297063107		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.05091108297063107 | validation: 0.0908009514385924]
	TIME [epoch: 2.67 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047494623491718166		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.047494623491718166 | validation: 0.09083300616254539]
	TIME [epoch: 2.67 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054562146226165786		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.054562146226165786 | validation: 0.11461247354453566]
	TIME [epoch: 2.67 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06329311251482934		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.06329311251482934 | validation: 0.10759592866092958]
	TIME [epoch: 2.68 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07227283693812274		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.07227283693812274 | validation: 0.1261121238187892]
	TIME [epoch: 2.67 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07153430490717634		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.07153430490717634 | validation: 0.09691992236801734]
	TIME [epoch: 2.67 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0631734564495622		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.0631734564495622 | validation: 0.09387325956186279]
	TIME [epoch: 2.67 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059047698896212655		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.059047698896212655 | validation: 0.08803943387885263]
	TIME [epoch: 2.67 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057076497943053556		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.057076497943053556 | validation: 0.08658185364273037]
	TIME [epoch: 2.67 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05157510238118992		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.05157510238118992 | validation: 0.08512167996167969]
	TIME [epoch: 2.67 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05113861207470949		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.05113861207470949 | validation: 0.08946811354636125]
	TIME [epoch: 2.67 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04844420913541331		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.04844420913541331 | validation: 0.08798486398362879]
	TIME [epoch: 2.67 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0536025274469464		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.0536025274469464 | validation: 0.10290705016452745]
	TIME [epoch: 2.67 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06332996377991862		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.06332996377991862 | validation: 0.10824927319893939]
	TIME [epoch: 2.67 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07149969233452101		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.07149969233452101 | validation: 0.1141939297720013]
	TIME [epoch: 2.67 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06272989572926148		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.06272989572926148 | validation: 0.0847047643128715]
	TIME [epoch: 2.67 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05120691722098243		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.05120691722098243 | validation: 0.0833275831945508]
	TIME [epoch: 2.67 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04717436688141551		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.04717436688141551 | validation: 0.07897649726904805]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_778.pth
	Model improved!!!
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04579723075545226		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.04579723075545226 | validation: 0.08024148208637015]
	TIME [epoch: 2.67 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042229489975491856		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.042229489975491856 | validation: 0.07610992357659092]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_780.pth
	Model improved!!!
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04795733955617208		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.04795733955617208 | validation: 0.09146058693804496]
	TIME [epoch: 2.67 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0461483677005615		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.0461483677005615 | validation: 0.08077212264598824]
	TIME [epoch: 2.67 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04897637079229571		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.04897637079229571 | validation: 0.10235107104045343]
	TIME [epoch: 2.67 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06268372688544312		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.06268372688544312 | validation: 0.11507454374046047]
	TIME [epoch: 2.67 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07688214964124138		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.07688214964124138 | validation: 0.10925797810981833]
	TIME [epoch: 2.68 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05840385198109452		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.05840385198109452 | validation: 0.08216679404748922]
	TIME [epoch: 2.67 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04912806469996499		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.04912806469996499 | validation: 0.07887425500570111]
	TIME [epoch: 2.67 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050713581331781964		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.050713581331781964 | validation: 0.08269795211448683]
	TIME [epoch: 2.67 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05395526667121679		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.05395526667121679 | validation: 0.07755133779499525]
	TIME [epoch: 2.67 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049450144317866526		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.049450144317866526 | validation: 0.0792591062017857]
	TIME [epoch: 2.67 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048617023494428625		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.048617023494428625 | validation: 0.10132629412993165]
	TIME [epoch: 2.67 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05378822986315287		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.05378822986315287 | validation: 0.09727882169490398]
	TIME [epoch: 2.67 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06004122166260173		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.06004122166260173 | validation: 0.10174288585145348]
	TIME [epoch: 2.67 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05729668539867041		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.05729668539867041 | validation: 0.08368717175830856]
	TIME [epoch: 2.67 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05456836365306847		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.05456836365306847 | validation: 0.09077153430577953]
	TIME [epoch: 2.67 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05011589796712771		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.05011589796712771 | validation: 0.08518519557788859]
	TIME [epoch: 2.67 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051075892022503525		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.051075892022503525 | validation: 0.08018307548422729]
	TIME [epoch: 2.67 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04885399697287477		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.04885399697287477 | validation: 0.08028879882285224]
	TIME [epoch: 2.67 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04914319117434363		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.04914319117434363 | validation: 0.08138480733690717]
	TIME [epoch: 2.67 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04597675397704517		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.04597675397704517 | validation: 0.08148001260863233]
	TIME [epoch: 2.67 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046584196776628096		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.046584196776628096 | validation: 0.08492637641684525]
	TIME [epoch: 2.67 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050620715896522776		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.050620715896522776 | validation: 0.08747180234451124]
	TIME [epoch: 2.67 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05423327740052754		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.05423327740052754 | validation: 0.09911211254669483]
	TIME [epoch: 2.67 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05810894541077664		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.05810894541077664 | validation: 0.09404471424203947]
	TIME [epoch: 2.68 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058848687587866946		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.058848687587866946 | validation: 0.08793244944633609]
	TIME [epoch: 2.67 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054169264799763875		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.054169264799763875 | validation: 0.08009406232749276]
	TIME [epoch: 2.67 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048075145888892196		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.048075145888892196 | validation: 0.07878708757147729]
	TIME [epoch: 2.67 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04417041601724537		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.04417041601724537 | validation: 0.07108635953365651]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_808.pth
	Model improved!!!
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046331057478369395		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.046331057478369395 | validation: 0.08757312952026174]
	TIME [epoch: 2.67 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047138499744222705		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.047138499744222705 | validation: 0.07410622405886845]
	TIME [epoch: 2.67 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051575383347421816		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.051575383347421816 | validation: 0.08590185101357065]
	TIME [epoch: 2.67 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04701142609014931		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.04701142609014931 | validation: 0.07325331444437824]
	TIME [epoch: 2.67 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04379744484268012		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.04379744484268012 | validation: 0.0785258366031091]
	TIME [epoch: 2.67 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040825758872711296		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.040825758872711296 | validation: 0.07342424865870542]
	TIME [epoch: 2.67 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04063267183397198		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.04063267183397198 | validation: 0.07604125128968263]
	TIME [epoch: 2.67 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041735073649962595		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.041735073649962595 | validation: 0.07492953999434149]
	TIME [epoch: 2.67 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041631838668176206		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.041631838668176206 | validation: 0.0830139410749074]
	TIME [epoch: 2.67 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04362467717211926		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.04362467717211926 | validation: 0.07157145493498328]
	TIME [epoch: 2.67 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04738642817892962		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.04738642817892962 | validation: 0.1281837430495439]
	TIME [epoch: 2.67 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07103885859389124		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.07103885859389124 | validation: 0.0987654755679548]
	TIME [epoch: 2.67 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07041698536627906		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.07041698536627906 | validation: 0.0940114357580979]
	TIME [epoch: 2.67 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056851756810520675		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.056851756810520675 | validation: 0.07360437573327384]
	TIME [epoch: 2.67 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05624906944129312		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.05624906944129312 | validation: 0.08087626620128868]
	TIME [epoch: 2.67 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05129230724758116		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.05129230724758116 | validation: 0.07370454276913042]
	TIME [epoch: 2.67 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04434194778385527		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.04434194778385527 | validation: 0.07368007588338502]
	TIME [epoch: 2.67 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041120986734914274		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.041120986734914274 | validation: 0.06507271317671101]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_826.pth
	Model improved!!!
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0414156008778665		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.0414156008778665 | validation: 0.07154639575853837]
	TIME [epoch: 2.68 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042277803699095035		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.042277803699095035 | validation: 0.07652176643216664]
	TIME [epoch: 2.67 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04623553933078396		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.04623553933078396 | validation: 0.07739371269665052]
	TIME [epoch: 2.68 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04835146749488554		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.04835146749488554 | validation: 0.08343488389870321]
	TIME [epoch: 2.67 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057345904460948484		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.057345904460948484 | validation: 0.10644068180838669]
	TIME [epoch: 2.67 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0646810470678196		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.0646810470678196 | validation: 0.08577028308794321]
	TIME [epoch: 2.67 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06052286597538337		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.06052286597538337 | validation: 0.08198017470139489]
	TIME [epoch: 2.67 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04787012738284119		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.04787012738284119 | validation: 0.06920379818201833]
	TIME [epoch: 2.68 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039305498797497646		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.039305498797497646 | validation: 0.07208294560436423]
	TIME [epoch: 2.68 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040117969832776046		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.040117969832776046 | validation: 0.07554902923315415]
	TIME [epoch: 2.67 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04020991843740339		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.04020991843740339 | validation: 0.06962518770726815]
	TIME [epoch: 2.67 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04255125482135856		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.04255125482135856 | validation: 0.07684348534485902]
	TIME [epoch: 2.67 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041878162797653225		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.041878162797653225 | validation: 0.0749938505237963]
	TIME [epoch: 2.67 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04406704351612415		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.04406704351612415 | validation: 0.08313859349806194]
	TIME [epoch: 2.67 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042742308965692075		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.042742308965692075 | validation: 0.0744513730454536]
	TIME [epoch: 2.67 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04436019504914843		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.04436019504914843 | validation: 0.0772959614019008]
	TIME [epoch: 2.67 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04720249450454743		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.04720249450454743 | validation: 0.07536291970198225]
	TIME [epoch: 2.67 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04901073366855141		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.04901073366855141 | validation: 0.08486914820976137]
	TIME [epoch: 2.67 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05208033595207353		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.05208033595207353 | validation: 0.07039307959298204]
	TIME [epoch: 2.67 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0485100578655211		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.0485100578655211 | validation: 0.07666367642677291]
	TIME [epoch: 2.67 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04642225630428106		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.04642225630428106 | validation: 0.075680815563754]
	TIME [epoch: 2.67 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045263047691877376		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.045263047691877376 | validation: 0.07264987627488992]
	TIME [epoch: 2.66 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043429469896610555		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.043429469896610555 | validation: 0.06453370504449978]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_849.pth
	Model improved!!!
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0394006693795582		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.0394006693795582 | validation: 0.07724054803652496]
	TIME [epoch: 2.68 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040691550776125456		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.040691550776125456 | validation: 0.07221043438690196]
	TIME [epoch: 2.68 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04013342583973117		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.04013342583973117 | validation: 0.07458894687463119]
	TIME [epoch: 2.68 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038971460681979084		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.038971460681979084 | validation: 0.06706410798777564]
	TIME [epoch: 2.67 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03959642258005718		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.03959642258005718 | validation: 0.07843437564782094]
	TIME [epoch: 2.67 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039565409549145523		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.039565409549145523 | validation: 0.06857623014942354]
	TIME [epoch: 2.68 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0438259285031072		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.0438259285031072 | validation: 0.0981670959208823]
	TIME [epoch: 2.68 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053316666871828615		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.053316666871828615 | validation: 0.08991726552697397]
	TIME [epoch: 2.67 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06276858318507893		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.06276858318507893 | validation: 0.08288667582733462]
	TIME [epoch: 2.67 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05671846467918407		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.05671846467918407 | validation: 0.0722599224801924]
	TIME [epoch: 2.69 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04958891816645077		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.04958891816645077 | validation: 0.06878117495015822]
	TIME [epoch: 2.67 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039938815699363354		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.039938815699363354 | validation: 0.06477588112016464]
	TIME [epoch: 2.68 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03743700985639605		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.03743700985639605 | validation: 0.07009536948713876]
	TIME [epoch: 2.68 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037974585677111296		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.037974585677111296 | validation: 0.06189111783402466]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_863.pth
	Model improved!!!
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038097059914093366		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.038097059914093366 | validation: 0.06321121915731087]
	TIME [epoch: 2.67 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03896859837731961		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.03896859837731961 | validation: 0.06931150281658076]
	TIME [epoch: 2.67 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04005788089961877		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.04005788089961877 | validation: 0.06926707356347558]
	TIME [epoch: 2.68 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04298313235456453		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.04298313235456453 | validation: 0.08490253213520522]
	TIME [epoch: 2.68 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052102360071742675		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.052102360071742675 | validation: 0.07872216603717105]
	TIME [epoch: 2.67 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05603540865658273		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.05603540865658273 | validation: 0.09562093491109948]
	TIME [epoch: 2.67 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051924526797176115		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.051924526797176115 | validation: 0.06761887332251092]
	TIME [epoch: 2.68 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04213089698846759		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.04213089698846759 | validation: 0.06578237166797783]
	TIME [epoch: 2.67 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03641078550764717		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.03641078550764717 | validation: 0.065458604206356]
	TIME [epoch: 2.67 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035540283490700694		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.035540283490700694 | validation: 0.0653304171095307]
	TIME [epoch: 2.68 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0374057077555441		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.0374057077555441 | validation: 0.06947280256812936]
	TIME [epoch: 2.67 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037743895613377994		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.037743895613377994 | validation: 0.06463262841926158]
	TIME [epoch: 2.67 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03903001398196917		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.03903001398196917 | validation: 0.07293820192648941]
	TIME [epoch: 2.67 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0452717633451385		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.0452717633451385 | validation: 0.0720378003025061]
	TIME [epoch: 2.67 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0486246950602771		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.0486246950602771 | validation: 0.07549627715309973]
	TIME [epoch: 2.67 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04760544480808708		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.04760544480808708 | validation: 0.06734487498598199]
	TIME [epoch: 2.67 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044060136482300546		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.044060136482300546 | validation: 0.061856578440050096]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_880.pth
	Model improved!!!
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03642400267572401		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.03642400267572401 | validation: 0.06308666318765313]
	TIME [epoch: 2.67 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03554982239849512		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.03554982239849512 | validation: 0.06286768438823019]
	TIME [epoch: 2.67 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0368733978462211		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.0368733978462211 | validation: 0.05969685405004978]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_883.pth
	Model improved!!!
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036656356163641136		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.036656356163641136 | validation: 0.057802946474541074]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_884.pth
	Model improved!!!
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03764708461129994		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.03764708461129994 | validation: 0.07066832817350631]
	TIME [epoch: 2.67 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03863562782486124		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.03863562782486124 | validation: 0.06819699642917977]
	TIME [epoch: 2.67 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04539697486130782		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.04539697486130782 | validation: 0.09180476386531508]
	TIME [epoch: 2.67 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05131375347908259		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.05131375347908259 | validation: 0.07483529957292485]
	TIME [epoch: 2.67 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04999205703623208		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.04999205703623208 | validation: 0.07468820113160207]
	TIME [epoch: 2.67 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0437933378481641		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.0437933378481641 | validation: 0.0659010607563091]
	TIME [epoch: 2.67 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03860304809108148		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.03860304809108148 | validation: 0.05946099654772045]
	TIME [epoch: 2.67 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03516870188703241		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.03516870188703241 | validation: 0.06281297474701822]
	TIME [epoch: 2.67 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036359206622238		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.036359206622238 | validation: 0.06035768479014056]
	TIME [epoch: 2.67 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03556797451940824		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.03556797451940824 | validation: 0.058732172841939594]
	TIME [epoch: 2.67 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03731828762463679		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.03731828762463679 | validation: 0.06321602132337607]
	TIME [epoch: 2.67 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035350820378959424		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.035350820378959424 | validation: 0.06004440660857377]
	TIME [epoch: 2.67 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038072093241636074		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.038072093241636074 | validation: 0.07031733613939357]
	TIME [epoch: 2.67 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04016064915859598		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.04016064915859598 | validation: 0.07761888455210787]
	TIME [epoch: 2.67 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05445859842072647		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.05445859842072647 | validation: 0.09767940454320208]
	TIME [epoch: 2.67 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06104434664397553		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.06104434664397553 | validation: 0.06611232170755521]
	TIME [epoch: 2.67 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042906828325943905		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.042906828325943905 | validation: 0.06835066326526425]
	TIME [epoch: 2.68 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03442062657626726		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.03442062657626726 | validation: 0.059214694498399306]
	TIME [epoch: 2.67 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03637665911688498		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.03637665911688498 | validation: 0.060885459930676435]
	TIME [epoch: 2.67 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038781123622462325		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.038781123622462325 | validation: 0.061681594208910374]
	TIME [epoch: 2.67 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039518993246908496		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.039518993246908496 | validation: 0.06097044799607872]
	TIME [epoch: 2.67 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03688302057666534		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.03688302057666534 | validation: 0.05827055709965358]
	TIME [epoch: 2.66 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03576541247428782		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.03576541247428782 | validation: 0.059782875719839736]
	TIME [epoch: 2.67 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03609775333136788		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.03609775333136788 | validation: 0.05732905173912744]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_908.pth
	Model improved!!!
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03713980591891004		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.03713980591891004 | validation: 0.056409963647795075]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_909.pth
	Model improved!!!
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03489603649615729		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.03489603649615729 | validation: 0.06088456107931747]
	TIME [epoch: 2.67 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03545636117358256		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.03545636117358256 | validation: 0.060417007881809086]
	TIME [epoch: 2.67 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0361515697598254		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.0361515697598254 | validation: 0.06914401831997204]
	TIME [epoch: 2.67 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03870639922300268		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.03870639922300268 | validation: 0.0756422485789384]
	TIME [epoch: 2.67 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05112601615554088		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.05112601615554088 | validation: 0.09631156112580277]
	TIME [epoch: 2.67 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05832390648718043		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.05832390648718043 | validation: 0.06415672426439088]
	TIME [epoch: 2.67 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04671517576416141		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.04671517576416141 | validation: 0.05745135106035601]
	TIME [epoch: 2.67 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03712137473030881		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.03712137473030881 | validation: 0.062399230234463844]
	TIME [epoch: 2.67 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036837787954377615		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.036837787954377615 | validation: 0.06085335953894935]
	TIME [epoch: 2.67 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03729398419963936		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.03729398419963936 | validation: 0.06341115963318811]
	TIME [epoch: 2.67 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034542514223629604		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.034542514223629604 | validation: 0.0565375002419129]
	TIME [epoch: 2.67 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03431338153322696		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.03431338153322696 | validation: 0.05872288666869795]
	TIME [epoch: 2.67 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034746409155913247		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.034746409155913247 | validation: 0.05892036138603571]
	TIME [epoch: 2.67 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034472479518457326		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.034472479518457326 | validation: 0.0576789072421825]
	TIME [epoch: 2.67 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034246382750998276		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.034246382750998276 | validation: 0.05583301959019424]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_924.pth
	Model improved!!!
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03470015832572819		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.03470015832572819 | validation: 0.056733770643823234]
	TIME [epoch: 2.67 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03753069091869502		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.03753069091869502 | validation: 0.06523057934888234]
	TIME [epoch: 2.67 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043207755270786796		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.043207755270786796 | validation: 0.06917519637537835]
	TIME [epoch: 2.67 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04497205591827177		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.04497205591827177 | validation: 0.06847669385247247]
	TIME [epoch: 2.67 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04778929973497161		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.04778929973497161 | validation: 0.08307049787875717]
	TIME [epoch: 2.67 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0449269575384707		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.0449269575384707 | validation: 0.05810457550892857]
	TIME [epoch: 2.67 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03590502209076617		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.03590502209076617 | validation: 0.05648859065491055]
	TIME [epoch: 2.67 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03197406427685855		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.03197406427685855 | validation: 0.062013703720287264]
	TIME [epoch: 2.67 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033842178622294466		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.033842178622294466 | validation: 0.05627174684597888]
	TIME [epoch: 2.67 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031689036507990063		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.031689036507990063 | validation: 0.054628278877357675]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_934.pth
	Model improved!!!
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03591259528861251		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.03591259528861251 | validation: 0.05656376442604924]
	TIME [epoch: 2.67 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03571772100673515		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.03571772100673515 | validation: 0.06389202059392414]
	TIME [epoch: 2.67 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04011776825233643		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.04011776825233643 | validation: 0.0630285909013176]
	TIME [epoch: 2.67 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047628318888745316		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.047628318888745316 | validation: 0.07416038641561355]
	TIME [epoch: 2.68 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043955776330371975		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.043955776330371975 | validation: 0.05344588943791182]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_939.pth
	Model improved!!!
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035826126161180034		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.035826126161180034 | validation: 0.05766830512213669]
	TIME [epoch: 2.67 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031475664050179715		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.031475664050179715 | validation: 0.0646557821435432]
	TIME [epoch: 2.67 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036083976391015134		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.036083976391015134 | validation: 0.0573826887786016]
	TIME [epoch: 2.67 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03749360026984143		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.03749360026984143 | validation: 0.07065300263645372]
	TIME [epoch: 2.67 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038335923057190996		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.038335923057190996 | validation: 0.053245228982532915]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_944.pth
	Model improved!!!
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03483684910005638		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.03483684910005638 | validation: 0.0618359892744378]
	TIME [epoch: 2.67 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0338034895316978		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.0338034895316978 | validation: 0.05209697491466895]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_946.pth
	Model improved!!!
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03435115730049251		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.03435115730049251 | validation: 0.0600077533552304]
	TIME [epoch: 2.66 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037446015973846475		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.037446015973846475 | validation: 0.06092924411504988]
	TIME [epoch: 2.67 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04152176336220434		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.04152176336220434 | validation: 0.06824179914203486]
	TIME [epoch: 2.67 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03848634494581676		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.03848634494581676 | validation: 0.05422173424473442]
	TIME [epoch: 2.66 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03803859115732584		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.03803859115732584 | validation: 0.06352915126229644]
	TIME [epoch: 2.67 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033293358961917166		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.033293358961917166 | validation: 0.06120697082260451]
	TIME [epoch: 2.67 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03342503039116553		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.03342503039116553 | validation: 0.05763484957549905]
	TIME [epoch: 2.67 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032848142285201255		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.032848142285201255 | validation: 0.055022621861521195]
	TIME [epoch: 2.67 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030179096400658415		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.030179096400658415 | validation: 0.05541614347143223]
	TIME [epoch: 2.67 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03164667893347204		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.03164667893347204 | validation: 0.06055595273273883]
	TIME [epoch: 2.67 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032260249126805494		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.032260249126805494 | validation: 0.050384328404909334]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_957.pth
	Model improved!!!
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0343272229863174		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.0343272229863174 | validation: 0.06317407821141173]
	TIME [epoch: 2.67 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035126647615534815		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.035126647615534815 | validation: 0.06457042641247106]
	TIME [epoch: 2.67 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04248992064287615		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.04248992064287615 | validation: 0.07224806011767282]
	TIME [epoch: 2.67 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0434791664295612		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.0434791664295612 | validation: 0.05332753073728466]
	TIME [epoch: 2.67 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036083212619101766		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.036083212619101766 | validation: 0.056217005274844356]
	TIME [epoch: 2.66 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03275553406201953		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.03275553406201953 | validation: 0.052416944535196845]
	TIME [epoch: 2.66 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03571250204229647		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.03571250204229647 | validation: 0.05441214552597208]
	TIME [epoch: 2.67 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03406691901652196		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.03406691901652196 | validation: 0.051938850089752034]
	TIME [epoch: 2.67 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03151294170423688		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.03151294170423688 | validation: 0.05655594370463155]
	TIME [epoch: 2.67 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0321944841294769		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.0321944841294769 | validation: 0.057126976863077765]
	TIME [epoch: 2.66 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03615576777331926		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.03615576777331926 | validation: 0.07001077502038074]
	TIME [epoch: 2.67 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041044940277377776		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.041044940277377776 | validation: 0.06224664848683166]
	TIME [epoch: 2.66 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042005061568789075		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.042005061568789075 | validation: 0.06169186258909316]
	TIME [epoch: 2.67 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033262603309579405		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.033262603309579405 | validation: 0.055213696225432055]
	TIME [epoch: 2.67 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03244749414126941		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.03244749414126941 | validation: 0.053774158672621065]
	TIME [epoch: 2.67 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030757248851016116		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.030757248851016116 | validation: 0.0517352686019078]
	TIME [epoch: 2.66 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03492754947461038		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.03492754947461038 | validation: 0.05300423158124258]
	TIME [epoch: 2.67 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037801055031672026		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.037801055031672026 | validation: 0.05746401089515482]
	TIME [epoch: 2.66 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03650181019419282		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.03650181019419282 | validation: 0.05769507881138755]
	TIME [epoch: 2.67 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0327759901264315		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.0327759901264315 | validation: 0.05250566334760046]
	TIME [epoch: 2.67 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031358496841304546		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.031358496841304546 | validation: 0.05236970028659607]
	TIME [epoch: 2.66 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032390284202102555		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.032390284202102555 | validation: 0.059045433339424795]
	TIME [epoch: 2.66 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030783073285009746		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.030783073285009746 | validation: 0.05164036902503503]
	TIME [epoch: 2.66 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033426303132890876		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.033426303132890876 | validation: 0.05798299588199916]
	TIME [epoch: 2.67 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03351591842215987		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.03351591842215987 | validation: 0.05457809103932393]
	TIME [epoch: 2.67 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03776106395286499		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.03776106395286499 | validation: 0.06640556197958533]
	TIME [epoch: 2.66 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03858769781960697		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.03858769781960697 | validation: 0.05193291960102436]
	TIME [epoch: 2.67 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03628514913266428		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.03628514913266428 | validation: 0.05485536223211878]
	TIME [epoch: 2.67 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03151210984410155		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.03151210984410155 | validation: 0.056755315726874436]
	TIME [epoch: 2.67 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031889215350789826		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.031889215350789826 | validation: 0.050922815540420864]
	TIME [epoch: 2.67 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029976068252082478		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.029976068252082478 | validation: 0.05336983551282877]
	TIME [epoch: 2.67 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030985601509400135		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.030985601509400135 | validation: 0.05076527312786057]
	TIME [epoch: 2.66 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02995504016802738		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.02995504016802738 | validation: 0.053632981476925634]
	TIME [epoch: 2.66 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029744214598432107		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.029744214598432107 | validation: 0.051099872200372314]
	TIME [epoch: 2.67 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03152038785573823		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.03152038785573823 | validation: 0.05890540115663872]
	TIME [epoch: 2.67 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03470504388860369		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.03470504388860369 | validation: 0.049601206961029355]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_993.pth
	Model improved!!!
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03556548003183973		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.03556548003183973 | validation: 0.06671463032429009]
	TIME [epoch: 2.67 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03463526552259226		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.03463526552259226 | validation: 0.057244043299279214]
	TIME [epoch: 2.68 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03943110914932532		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.03943110914932532 | validation: 0.06852627767776642]
	TIME [epoch: 2.67 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043611575111851465		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.043611575111851465 | validation: 0.05386986973469885]
	TIME [epoch: 2.68 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037189858321337796		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.037189858321337796 | validation: 0.05179144203234401]
	TIME [epoch: 2.67 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03112750362026121		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.03112750362026121 | validation: 0.0514035611517105]
	TIME [epoch: 2.67 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031168106024946872		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.031168106024946872 | validation: 0.052980512694423054]
	TIME [epoch: 2.68 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03149064017568184		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.03149064017568184 | validation: 0.04947142762128616]
	TIME [epoch: 174 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1001.pth
	Model improved!!!
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03066911657318089		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.03066911657318089 | validation: 0.05557288752882103]
	TIME [epoch: 5.72 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031764298227380215		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.031764298227380215 | validation: 0.05409826019471961]
	TIME [epoch: 5.7 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03478653752555294		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.03478653752555294 | validation: 0.05739067340634716]
	TIME [epoch: 5.71 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03485818457433653		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.03485818457433653 | validation: 0.046960255224635775]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1005.pth
	Model improved!!!
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03448758452055856		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.03448758452055856 | validation: 0.05683428113822711]
	TIME [epoch: 5.71 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031100790520515568		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.031100790520515568 | validation: 0.04887110652254281]
	TIME [epoch: 5.71 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03133501585618987		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.03133501585618987 | validation: 0.0529354194858431]
	TIME [epoch: 5.71 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03079883939307842		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.03079883939307842 | validation: 0.04665367397311328]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1009.pth
	Model improved!!!
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02976150764442896		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.02976150764442896 | validation: 0.05544988086426333]
	TIME [epoch: 5.71 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029863291415845827		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.029863291415845827 | validation: 0.0497836049282175]
	TIME [epoch: 5.7 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03073287938080638		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.03073287938080638 | validation: 0.052090382894241574]
	TIME [epoch: 5.7 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029854923199662143		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.029854923199662143 | validation: 0.04928733432163057]
	TIME [epoch: 5.7 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030243921560933893		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.030243921560933893 | validation: 0.05706621876211968]
	TIME [epoch: 5.71 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031119754403421816		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.031119754403421816 | validation: 0.05339624413289421]
	TIME [epoch: 5.7 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032542741216317984		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.032542741216317984 | validation: 0.061956373056954506]
	TIME [epoch: 5.7 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03215103281036249		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.03215103281036249 | validation: 0.05215760215662557]
	TIME [epoch: 5.7 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03882674946160265		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.03882674946160265 | validation: 0.05653034606114715]
	TIME [epoch: 5.7 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037770291045542054		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.037770291045542054 | validation: 0.0492247366565825]
	TIME [epoch: 5.7 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03200035060542605		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.03200035060542605 | validation: 0.05045573926843392]
	TIME [epoch: 5.7 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03171034735153584		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.03171034735153584 | validation: 0.04837545378642924]
	TIME [epoch: 5.7 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03059473587614141		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.03059473587614141 | validation: 0.05378754509032166]
	TIME [epoch: 5.7 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030462862067514553		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.030462862067514553 | validation: 0.049479101751222304]
	TIME [epoch: 5.7 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031212642802858247		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.031212642802858247 | validation: 0.04750830902359846]
	TIME [epoch: 5.71 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03244059892590637		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.03244059892590637 | validation: 0.050763411146739046]
	TIME [epoch: 5.7 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03214347088354611		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.03214347088354611 | validation: 0.04851313433774218]
	TIME [epoch: 5.7 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030558104479220142		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.030558104479220142 | validation: 0.05465240320494463]
	TIME [epoch: 5.7 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034059151994398275		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.034059151994398275 | validation: 0.05034813158949191]
	TIME [epoch: 5.7 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03588886570245333		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.03588886570245333 | validation: 0.057477858990951904]
	TIME [epoch: 5.7 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03293361566036738		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.03293361566036738 | validation: 0.04927908363125664]
	TIME [epoch: 5.71 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028799578781673954		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.028799578781673954 | validation: 0.04654822012874909]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1031.pth
	Model improved!!!
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029376459690115885		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.029376459690115885 | validation: 0.047006667594413236]
	TIME [epoch: 5.7 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028060405749147747		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.028060405749147747 | validation: 0.04891149628565159]
	TIME [epoch: 5.7 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028454750990306758		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.028454750990306758 | validation: 0.046332292237287465]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1034.pth
	Model improved!!!
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031181192230793037		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.031181192230793037 | validation: 0.04622738674767107]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1035.pth
	Model improved!!!
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03014655789816464		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.03014655789816464 | validation: 0.05084784422872858]
	TIME [epoch: 5.7 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03108348398273584		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.03108348398273584 | validation: 0.05069948483225816]
	TIME [epoch: 5.71 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03392797054215409		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.03392797054215409 | validation: 0.05984845854970753]
	TIME [epoch: 5.7 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03565331495392607		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.03565331495392607 | validation: 0.05571226483398768]
	TIME [epoch: 5.7 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03761866021528791		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.03761866021528791 | validation: 0.052662253510945334]
	TIME [epoch: 5.71 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028856115110680265		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.028856115110680265 | validation: 0.046733124537754]
	TIME [epoch: 5.7 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029855485155355785		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.029855485155355785 | validation: 0.04505016107632544]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1042.pth
	Model improved!!!
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028165621428195644		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.028165621428195644 | validation: 0.054014966906356335]
	TIME [epoch: 5.7 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029111100486043828		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.029111100486043828 | validation: 0.04638740541397131]
	TIME [epoch: 5.7 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029234644416002036		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.029234644416002036 | validation: 0.0460336670689373]
	TIME [epoch: 5.71 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03113903695028613		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.03113903695028613 | validation: 0.04976625174938376]
	TIME [epoch: 5.7 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02685014881557251		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.02685014881557251 | validation: 0.041427172937766654]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1047.pth
	Model improved!!!
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028462033656090294		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.028462033656090294 | validation: 0.0504977614238723]
	TIME [epoch: 5.7 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028717008508166338		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.028717008508166338 | validation: 0.04679834373845812]
	TIME [epoch: 5.7 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027533857586841952		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.027533857586841952 | validation: 0.045521654325911945]
	TIME [epoch: 5.71 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030091192202689375		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.030091192202689375 | validation: 0.05086607035057942]
	TIME [epoch: 5.7 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029010675385549228		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.029010675385549228 | validation: 0.04595739344230393]
	TIME [epoch: 5.7 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03099689719731267		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.03099689719731267 | validation: 0.06244296710815481]
	TIME [epoch: 5.7 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03174794351252846		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.03174794351252846 | validation: 0.0555330819001877]
	TIME [epoch: 5.7 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0395709212986036		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.0395709212986036 | validation: 0.057234376453564044]
	TIME [epoch: 5.71 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036922172175027024		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.036922172175027024 | validation: 0.04143976199131442]
	TIME [epoch: 5.71 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031024512308436152		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.031024512308436152 | validation: 0.04430234881003142]
	TIME [epoch: 5.7 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028418850763394374		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.028418850763394374 | validation: 0.04597668458657294]
	TIME [epoch: 5.71 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030095391748788575		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.030095391748788575 | validation: 0.044939941679436914]
	TIME [epoch: 5.7 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02814686408894171		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.02814686408894171 | validation: 0.04759513742953608]
	TIME [epoch: 5.71 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02699417046334715		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.02699417046334715 | validation: 0.04693630757879341]
	TIME [epoch: 5.71 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02866368912724627		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.02866368912724627 | validation: 0.045806315251534016]
	TIME [epoch: 5.71 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030015248810525086		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.030015248810525086 | validation: 0.05017832813628185]
	TIME [epoch: 5.7 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029404117049920592		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.029404117049920592 | validation: 0.04593564672232578]
	TIME [epoch: 5.7 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03003591832672008		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.03003591832672008 | validation: 0.05204344217406383]
	TIME [epoch: 5.7 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031387935269009536		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.031387935269009536 | validation: 0.03959131276887702]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1066.pth
	Model improved!!!
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029338509190104542		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.029338509190104542 | validation: 0.0438236339668212]
	TIME [epoch: 5.7 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02840554340569276		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.02840554340569276 | validation: 0.04478510451278146]
	TIME [epoch: 5.71 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02685097552927283		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.02685097552927283 | validation: 0.044857946534815764]
	TIME [epoch: 5.71 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027880597816301478		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.027880597816301478 | validation: 0.043267111683037734]
	TIME [epoch: 5.7 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027751051484475223		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.027751051484475223 | validation: 0.05772366133897964]
	TIME [epoch: 5.71 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03191156718510352		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.03191156718510352 | validation: 0.04732478425330289]
	TIME [epoch: 5.7 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03307419707886421		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.03307419707886421 | validation: 0.05312084060020015]
	TIME [epoch: 5.7 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03300962943311191		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.03300962943311191 | validation: 0.04497237164440763]
	TIME [epoch: 5.7 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029894579325637424		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.029894579325637424 | validation: 0.04494959349963321]
	TIME [epoch: 5.7 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028593593590348743		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.028593593590348743 | validation: 0.04602667960886049]
	TIME [epoch: 5.71 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027392994752114284		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.027392994752114284 | validation: 0.045629841755507286]
	TIME [epoch: 5.7 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027553069775192226		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.027553069775192226 | validation: 0.04941709877099945]
	TIME [epoch: 5.71 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02753923546223243		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.02753923546223243 | validation: 0.04474574517788349]
	TIME [epoch: 5.7 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028472683288883926		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.028472683288883926 | validation: 0.04677381535653857]
	TIME [epoch: 5.7 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029385249388103007		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.029385249388103007 | validation: 0.0483045609661521]
	TIME [epoch: 5.71 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029851588548943857		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.029851588548943857 | validation: 0.04772523022674498]
	TIME [epoch: 5.7 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03219643705846765		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.03219643705846765 | validation: 0.050479708806189606]
	TIME [epoch: 5.7 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030234346893093643		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.030234346893093643 | validation: 0.046563809438461716]
	TIME [epoch: 5.71 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026492569372964337		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.026492569372964337 | validation: 0.04295576003062084]
	TIME [epoch: 5.7 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02703876372388823		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.02703876372388823 | validation: 0.04723281765132409]
	TIME [epoch: 5.7 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028717727180102938		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.028717727180102938 | validation: 0.0431887557640285]
	TIME [epoch: 5.71 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02718899852029919		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.02718899852029919 | validation: 0.04870618104753136]
	TIME [epoch: 5.7 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03103947080500126		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.03103947080500126 | validation: 0.04726913316767593]
	TIME [epoch: 5.7 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03219621272540099		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.03219621272540099 | validation: 0.04559898386679369]
	TIME [epoch: 5.7 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02699086537908713		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.02699086537908713 | validation: 0.04287294403617793]
	TIME [epoch: 5.69 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02750444153531644		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.02750444153531644 | validation: 0.042646995461323105]
	TIME [epoch: 5.71 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029307160306995678		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.029307160306995678 | validation: 0.04791174679557658]
	TIME [epoch: 5.7 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029454015859831095		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.029454015859831095 | validation: 0.042399585932708476]
	TIME [epoch: 5.7 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027530827701379598		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.027530827701379598 | validation: 0.048402463557923894]
	TIME [epoch: 5.69 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02762713338918134		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.02762713338918134 | validation: 0.04682752536568383]
	TIME [epoch: 5.7 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026654648806891584		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.026654648806891584 | validation: 0.04065940088386449]
	TIME [epoch: 5.7 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02790222134507389		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.02790222134507389 | validation: 0.04613697073447142]
	TIME [epoch: 5.7 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028078385761988373		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.028078385761988373 | validation: 0.0416475748530184]
	TIME [epoch: 5.7 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026555969146328025		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.026555969146328025 | validation: 0.044847736284732945]
	TIME [epoch: 5.7 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02600135068371146		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.02600135068371146 | validation: 0.04062103584101565]
	TIME [epoch: 5.7 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02602310267291627		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.02602310267291627 | validation: 0.04380154955993505]
	TIME [epoch: 5.7 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027764731085535756		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.027764731085535756 | validation: 0.04288280386098853]
	TIME [epoch: 5.7 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030268278834427283		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.030268278834427283 | validation: 0.05417500746352053]
	TIME [epoch: 5.7 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0320484597198201		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.0320484597198201 | validation: 0.045346107160144825]
	TIME [epoch: 5.7 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027786002622174844		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.027786002622174844 | validation: 0.04558001728194965]
	TIME [epoch: 5.7 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026511813578556646		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.026511813578556646 | validation: 0.05148798228676704]
	TIME [epoch: 5.7 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030092380341238203		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.030092380341238203 | validation: 0.04410145904368695]
	TIME [epoch: 5.7 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030785310726607288		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.030785310726607288 | validation: 0.04089752454618607]
	TIME [epoch: 5.71 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026985241172136708		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.026985241172136708 | validation: 0.04334092596982802]
	TIME [epoch: 5.7 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028416647895437394		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.028416647895437394 | validation: 0.051710633881521376]
	TIME [epoch: 5.7 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03119462132121961		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.03119462132121961 | validation: 0.045067073036340145]
	TIME [epoch: 5.7 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026629361033078735		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.026629361033078735 | validation: 0.047345974480540565]
	TIME [epoch: 5.71 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02808081841415965		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.02808081841415965 | validation: 0.0438690460769439]
	TIME [epoch: 5.7 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026095843386257568		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.026095843386257568 | validation: 0.040539173935276486]
	TIME [epoch: 5.71 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025008493489145397		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.025008493489145397 | validation: 0.04436400601913241]
	TIME [epoch: 5.7 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027016989071018606		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.027016989071018606 | validation: 0.0428122446192072]
	TIME [epoch: 5.7 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027901827952978586		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.027901827952978586 | validation: 0.04299110569771486]
	TIME [epoch: 5.72 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028243050681696153		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.028243050681696153 | validation: 0.05223703913668473]
	TIME [epoch: 5.7 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02964142206940875		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.02964142206940875 | validation: 0.04498216332212538]
	TIME [epoch: 5.7 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0311679886156179		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.0311679886156179 | validation: 0.05071263840640418]
	TIME [epoch: 5.7 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029267899354048818		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.029267899354048818 | validation: 0.041799081084392856]
	TIME [epoch: 5.7 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026752110523868673		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.026752110523868673 | validation: 0.04381291618266453]
	TIME [epoch: 5.7 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026180103713307673		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.026180103713307673 | validation: 0.04140872309122428]
	TIME [epoch: 5.7 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026535338050725388		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.026535338050725388 | validation: 0.0453007269704212]
	TIME [epoch: 5.7 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026476218088509276		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.026476218088509276 | validation: 0.04183159291564477]
	TIME [epoch: 5.7 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02405410858672985		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.02405410858672985 | validation: 0.04042217197357101]
	TIME [epoch: 5.7 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02657019197799624		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.02657019197799624 | validation: 0.04679741283492398]
	TIME [epoch: 5.71 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027416886976926763		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.027416886976926763 | validation: 0.04469029114922926]
	TIME [epoch: 5.7 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028607079234872056		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.028607079234872056 | validation: 0.04965714046652621]
	TIME [epoch: 5.7 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02833713176968136		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.02833713176968136 | validation: 0.04210123041665569]
	TIME [epoch: 5.7 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027652668767385098		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.027652668767385098 | validation: 0.04764102905293192]
	TIME [epoch: 5.69 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02835139132832474		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.02835139132832474 | validation: 0.040112386028733384]
	TIME [epoch: 5.7 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027191001373632187		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.027191001373632187 | validation: 0.04620348994561357]
	TIME [epoch: 5.69 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027819782893193753		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.027819782893193753 | validation: 0.0411433490600756]
	TIME [epoch: 5.7 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027370721174493733		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.027370721174493733 | validation: 0.04131051168938435]
	TIME [epoch: 5.7 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027415932655408676		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.027415932655408676 | validation: 0.04149097938401709]
	TIME [epoch: 5.69 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02669335108531957		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.02669335108531957 | validation: 0.04345178165974143]
	TIME [epoch: 5.69 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026249546813698718		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.026249546813698718 | validation: 0.041987599552571656]
	TIME [epoch: 5.7 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026468913272645993		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.026468913272645993 | validation: 0.044969813359575395]
	TIME [epoch: 5.7 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025941109778346138		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.025941109778346138 | validation: 0.04123512848672417]
	TIME [epoch: 5.69 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026496374015951583		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.026496374015951583 | validation: 0.04713529376857232]
	TIME [epoch: 5.69 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028686885969364972		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.028686885969364972 | validation: 0.04280487998934704]
	TIME [epoch: 5.69 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030934455458503964		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.030934455458503964 | validation: 0.045038246218484713]
	TIME [epoch: 5.7 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027449701459479954		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.027449701459479954 | validation: 0.038661702305411216]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1145.pth
	Model improved!!!
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026674862874671065		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.026674862874671065 | validation: 0.04249900446811028]
	TIME [epoch: 5.69 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026613563959072266		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.026613563959072266 | validation: 0.04180091519119328]
	TIME [epoch: 5.7 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02617897207515899		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.02617897207515899 | validation: 0.03949053857579782]
	TIME [epoch: 5.7 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02557926354880908		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.02557926354880908 | validation: 0.044115911712739965]
	TIME [epoch: 5.7 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028310559207394243		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.028310559207394243 | validation: 0.0393607786856226]
	TIME [epoch: 5.69 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025866932611886925		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.025866932611886925 | validation: 0.04580799736984294]
	TIME [epoch: 5.71 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026684461080781582		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.026684461080781582 | validation: 0.04279624703947116]
	TIME [epoch: 5.7 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027266649450038108		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.027266649450038108 | validation: 0.04117128013243352]
	TIME [epoch: 5.69 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025720316461762593		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.025720316461762593 | validation: 0.04131207339970754]
	TIME [epoch: 5.71 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025500580517680643		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.025500580517680643 | validation: 0.045171985962673404]
	TIME [epoch: 5.7 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027757324623282304		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.027757324623282304 | validation: 0.03803447562754141]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1156.pth
	Model improved!!!
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02727475601323683		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.02727475601323683 | validation: 0.046224518048253]
	TIME [epoch: 5.7 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025666071396477984		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.025666071396477984 | validation: 0.03687594757004904]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1158.pth
	Model improved!!!
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02622018446728694		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.02622018446728694 | validation: 0.04684753768153124]
	TIME [epoch: 5.71 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025380860058099995		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.025380860058099995 | validation: 0.04277667021948509]
	TIME [epoch: 5.69 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026333969950188395		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.026333969950188395 | validation: 0.04202013935800517]
	TIME [epoch: 5.7 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025695830122878204		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.025695830122878204 | validation: 0.037150058193837356]
	TIME [epoch: 5.69 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026896857596222436		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.026896857596222436 | validation: 0.05029001338088143]
	TIME [epoch: 5.69 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027720440328480415		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.027720440328480415 | validation: 0.039110088424906424]
	TIME [epoch: 5.71 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026427307834345198		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.026427307834345198 | validation: 0.046237017877318244]
	TIME [epoch: 5.71 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02573889551224971		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.02573889551224971 | validation: 0.04261528136311271]
	TIME [epoch: 5.7 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025306241503095746		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.025306241503095746 | validation: 0.04254437545683341]
	TIME [epoch: 5.69 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026954941106145436		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.026954941106145436 | validation: 0.040585151924325484]
	TIME [epoch: 5.71 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023727000362931478		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.023727000362931478 | validation: 0.04132211622745439]
	TIME [epoch: 5.7 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024850163761891544		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.024850163761891544 | validation: 0.03996629609628989]
	TIME [epoch: 5.71 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025280780041299664		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.025280780041299664 | validation: 0.04132326621595964]
	TIME [epoch: 5.69 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02640936234172373		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.02640936234172373 | validation: 0.03623438333925653]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1172.pth
	Model improved!!!
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02582277076107797		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.02582277076107797 | validation: 0.03583424515618835]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1173.pth
	Model improved!!!
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025637450121418732		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.025637450121418732 | validation: 0.04601643105074552]
	TIME [epoch: 5.72 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026406857930615125		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.026406857930615125 | validation: 0.04061354203255979]
	TIME [epoch: 5.73 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026352156733625925		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.026352156733625925 | validation: 0.04875654655911206]
	TIME [epoch: 5.71 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02887117995997688		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.02887117995997688 | validation: 0.040736217536282676]
	TIME [epoch: 5.73 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026995419190412803		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.026995419190412803 | validation: 0.04140739946470872]
	TIME [epoch: 5.71 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026588542303493962		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.026588542303493962 | validation: 0.04106946155496166]
	TIME [epoch: 5.72 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02627444031383172		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.02627444031383172 | validation: 0.039559188145795156]
	TIME [epoch: 5.72 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026113459379183183		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.026113459379183183 | validation: 0.04301709102639978]
	TIME [epoch: 5.72 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026064660419888624		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.026064660419888624 | validation: 0.03968239268464124]
	TIME [epoch: 5.71 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026329938082822683		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.026329938082822683 | validation: 0.04307261798977394]
	TIME [epoch: 5.73 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025784474993695135		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.025784474993695135 | validation: 0.03990686212602151]
	TIME [epoch: 5.71 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026626822991873994		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.026626822991873994 | validation: 0.043673666590652506]
	TIME [epoch: 5.73 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024959680836575023		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.024959680836575023 | validation: 0.04290899179256391]
	TIME [epoch: 5.71 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024579167022206656		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.024579167022206656 | validation: 0.0369745490726111]
	TIME [epoch: 5.72 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024602380497075783		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.024602380497075783 | validation: 0.03834881690603542]
	TIME [epoch: 5.72 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024478236825210848		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.024478236825210848 | validation: 0.04880579658705743]
	TIME [epoch: 5.72 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026210095532147513		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.026210095532147513 | validation: 0.042242629057795494]
	TIME [epoch: 5.73 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026273160149467897		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.026273160149467897 | validation: 0.042726907557518234]
	TIME [epoch: 5.72 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027759975949879747		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.027759975949879747 | validation: 0.03447681301673015]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1192.pth
	Model improved!!!
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025464985371307956		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.025464985371307956 | validation: 0.037368997624681814]
	TIME [epoch: 5.7 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024708670529067986		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.024708670529067986 | validation: 0.04071430430534038]
	TIME [epoch: 5.69 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025609525883474708		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.025609525883474708 | validation: 0.041035786512117804]
	TIME [epoch: 5.7 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02492815387995931		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.02492815387995931 | validation: 0.03574527772237457]
	TIME [epoch: 5.69 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025661721464865526		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.025661721464865526 | validation: 0.04524582941973857]
	TIME [epoch: 5.7 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02538523849751949		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.02538523849751949 | validation: 0.03908953420217753]
	TIME [epoch: 5.69 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026539986356983833		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.026539986356983833 | validation: 0.0444934359555178]
	TIME [epoch: 5.69 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02447211466399936		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.02447211466399936 | validation: 0.03860716722501452]
	TIME [epoch: 5.69 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024007920208573834		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.024007920208573834 | validation: 0.04076422646693471]
	TIME [epoch: 5.7 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023359874997278603		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.023359874997278603 | validation: 0.039268051736411104]
	TIME [epoch: 5.7 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025935994434509703		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.025935994434509703 | validation: 0.03713071007994922]
	TIME [epoch: 5.7 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02581372587454781		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.02581372587454781 | validation: 0.037222390022840356]
	TIME [epoch: 5.69 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023971974719840024		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.023971974719840024 | validation: 0.03763018274884165]
	TIME [epoch: 5.7 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024375096394145725		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.024375096394145725 | validation: 0.04140896055981394]
	TIME [epoch: 5.7 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027513067297448957		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.027513067297448957 | validation: 0.04745090967851267]
	TIME [epoch: 5.69 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02744403394037872		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.02744403394037872 | validation: 0.035638546319938604]
	TIME [epoch: 5.7 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02850400341127637		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.02850400341127637 | validation: 0.03937259204889607]
	TIME [epoch: 5.7 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025409421096576047		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.025409421096576047 | validation: 0.03894109141895702]
	TIME [epoch: 5.69 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02430427969407893		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.02430427969407893 | validation: 0.03497163630015674]
	TIME [epoch: 5.7 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02513839136688703		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.02513839136688703 | validation: 0.03893093460072122]
	TIME [epoch: 5.7 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025137600632391354		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.025137600632391354 | validation: 0.04154414323122553]
	TIME [epoch: 5.69 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023487352917357037		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.023487352917357037 | validation: 0.03786102634025804]
	TIME [epoch: 5.7 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023669531746138946		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.023669531746138946 | validation: 0.04218181746282425]
	TIME [epoch: 5.7 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025958770251839142		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.025958770251839142 | validation: 0.037252149140419376]
	TIME [epoch: 5.7 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02554371989751851		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.02554371989751851 | validation: 0.04222655246950207]
	TIME [epoch: 5.69 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025767879812401168		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.025767879812401168 | validation: 0.038252850600435054]
	TIME [epoch: 5.69 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025737158605267946		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.025737158605267946 | validation: 0.034415882345351055]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1219.pth
	Model improved!!!
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023126431552485224		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.023126431552485224 | validation: 0.03867658269343737]
	TIME [epoch: 5.7 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026185741667251503		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.026185741667251503 | validation: 0.04051530635753631]
	TIME [epoch: 5.7 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02443102009356209		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.02443102009356209 | validation: 0.03435840678527337]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1222.pth
	Model improved!!!
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023809964132286893		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.023809964132286893 | validation: 0.04280998308161465]
	TIME [epoch: 5.69 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026217532944207016		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.026217532944207016 | validation: 0.03684306807028022]
	TIME [epoch: 5.69 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023812554041980013		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.023812554041980013 | validation: 0.03805016101783809]
	TIME [epoch: 5.69 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025116660344020318		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.025116660344020318 | validation: 0.03949888378834067]
	TIME [epoch: 5.69 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02524513719863023		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.02524513719863023 | validation: 0.04401641070366219]
	TIME [epoch: 5.69 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027459424402162473		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.027459424402162473 | validation: 0.03940822620376488]
	TIME [epoch: 5.69 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02701299199345235		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.02701299199345235 | validation: 0.03350757235990356]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1229.pth
	Model improved!!!
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025736660930931604		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.025736660930931604 | validation: 0.04315261602430335]
	TIME [epoch: 5.69 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02327043883619641		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.02327043883619641 | validation: 0.040156325141661446]
	TIME [epoch: 5.69 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022685120635165236		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.022685120635165236 | validation: 0.03891167003471793]
	TIME [epoch: 5.7 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0246524124144211		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.0246524124144211 | validation: 0.037037171846442576]
	TIME [epoch: 5.69 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02438494449371049		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.02438494449371049 | validation: 0.04076917714051545]
	TIME [epoch: 5.69 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022762659275893746		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.022762659275893746 | validation: 0.03942109077369184]
	TIME [epoch: 5.69 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024548096451660433		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.024548096451660433 | validation: 0.0339598364886351]
	TIME [epoch: 5.69 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022787152065461		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.022787152065461 | validation: 0.039562748679993015]
	TIME [epoch: 5.7 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024207716226257527		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.024207716226257527 | validation: 0.03453256844291245]
	TIME [epoch: 5.69 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02583915900935003		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.02583915900935003 | validation: 0.03519553647462214]
	TIME [epoch: 5.69 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025174505753881863		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.025174505753881863 | validation: 0.03645570416849823]
	TIME [epoch: 5.69 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02467307282228383		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.02467307282228383 | validation: 0.043035711895843425]
	TIME [epoch: 5.69 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02560770497934132		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.02560770497934132 | validation: 0.03668242454898355]
	TIME [epoch: 5.7 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025132929727872143		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.025132929727872143 | validation: 0.03849307663481305]
	TIME [epoch: 5.69 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024918546207047518		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.024918546207047518 | validation: 0.03722301595085281]
	TIME [epoch: 5.7 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026228050830687317		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.026228050830687317 | validation: 0.033256280025329876]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1245.pth
	Model improved!!!
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024671324527135247		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.024671324527135247 | validation: 0.040863261446975746]
	TIME [epoch: 5.69 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023565834611432985		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.023565834611432985 | validation: 0.03721592993152896]
	TIME [epoch: 5.72 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024723498913023445		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.024723498913023445 | validation: 0.03945814657812562]
	TIME [epoch: 5.69 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024086141505977335		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.024086141505977335 | validation: 0.03792425164224327]
	TIME [epoch: 5.69 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023783141718017573		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.023783141718017573 | validation: 0.038554348170368014]
	TIME [epoch: 5.69 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02355663348362427		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.02355663348362427 | validation: 0.03906373294741397]
	TIME [epoch: 5.69 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024380225702503144		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.024380225702503144 | validation: 0.04643386291988382]
	TIME [epoch: 5.7 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02487122891452812		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.02487122891452812 | validation: 0.03632991648442503]
	TIME [epoch: 5.69 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025299658008274585		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.025299658008274585 | validation: 0.044576762889721656]
	TIME [epoch: 5.69 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02367997047010423		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.02367997047010423 | validation: 0.03654447509365242]
	TIME [epoch: 5.69 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024011021562257536		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.024011021562257536 | validation: 0.037368456134682336]
	TIME [epoch: 5.69 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023263288902247478		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.023263288902247478 | validation: 0.038878246423913444]
	TIME [epoch: 5.69 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023286241922552017		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.023286241922552017 | validation: 0.03512385853971337]
	TIME [epoch: 5.69 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022717280868497728		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.022717280868497728 | validation: 0.03675403903908905]
	TIME [epoch: 5.69 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024283353935298107		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.024283353935298107 | validation: 0.0425866955697759]
	TIME [epoch: 5.69 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02445925267751319		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.02445925267751319 | validation: 0.03543199382422774]
	TIME [epoch: 5.69 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02488271552592556		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.02488271552592556 | validation: 0.040693331329547194]
	TIME [epoch: 5.69 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024580899654066703		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.024580899654066703 | validation: 0.03614011470585319]
	TIME [epoch: 5.69 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02297909400361794		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.02297909400361794 | validation: 0.03841375074384828]
	TIME [epoch: 5.69 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02308924925489667		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.02308924925489667 | validation: 0.03680940394546419]
	TIME [epoch: 5.69 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024137994732005463		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.024137994732005463 | validation: 0.03506276633188471]
	TIME [epoch: 5.69 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0223185523907987		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.0223185523907987 | validation: 0.0373065063909647]
	TIME [epoch: 5.69 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024145871056814247		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.024145871056814247 | validation: 0.039370713678205216]
	TIME [epoch: 5.69 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024806037490717758		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.024806037490717758 | validation: 0.03320025437583862]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1269.pth
	Model improved!!!
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022837494368537267		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.022837494368537267 | validation: 0.037849541239609444]
	TIME [epoch: 5.69 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02369024590396446		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.02369024590396446 | validation: 0.036477646063797976]
	TIME [epoch: 5.69 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02303010423291892		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.02303010423291892 | validation: 0.03687466681610289]
	TIME [epoch: 5.69 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022087168666232247		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.022087168666232247 | validation: 0.03730606512714355]
	TIME [epoch: 5.7 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0234825282015099		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.0234825282015099 | validation: 0.03840526974435671]
	TIME [epoch: 5.69 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02356077198565528		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.02356077198565528 | validation: 0.034270039682909946]
	TIME [epoch: 5.69 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023781871233664216		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.023781871233664216 | validation: 0.036874189178530574]
	TIME [epoch: 5.69 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025243869562984265		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.025243869562984265 | validation: 0.038941939465742474]
	TIME [epoch: 5.69 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021956901119541482		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.021956901119541482 | validation: 0.03277077085899626]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1278.pth
	Model improved!!!
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02510845725767516		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.02510845725767516 | validation: 0.04592400302548908]
	TIME [epoch: 5.69 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027463806878690815		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.027463806878690815 | validation: 0.0350386781309183]
	TIME [epoch: 5.69 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02410794221041484		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.02410794221041484 | validation: 0.038640443594492306]
	TIME [epoch: 5.69 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022881152309639365		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.022881152309639365 | validation: 0.04173085655411229]
	TIME [epoch: 5.68 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02455135801583249		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.02455135801583249 | validation: 0.034020464687737506]
	TIME [epoch: 5.69 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023484467043987427		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.023484467043987427 | validation: 0.03171250695592596]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1284.pth
	Model improved!!!
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02239274740519112		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.02239274740519112 | validation: 0.034751001330421104]
	TIME [epoch: 5.69 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022803008033868092		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.022803008033868092 | validation: 0.03520902328738137]
	TIME [epoch: 5.69 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022252818370518394		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.022252818370518394 | validation: 0.03952224283039721]
	TIME [epoch: 5.69 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0242482657036899		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.0242482657036899 | validation: 0.03545646714767969]
	TIME [epoch: 5.68 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023545582410678653		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.023545582410678653 | validation: 0.033453989271033005]
	TIME [epoch: 5.69 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023449688116704514		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.023449688116704514 | validation: 0.04091224814761396]
	TIME [epoch: 5.68 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02316142391544643		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.02316142391544643 | validation: 0.03471797210953719]
	TIME [epoch: 5.69 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023265925078123022		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.023265925078123022 | validation: 0.04043041976190546]
	TIME [epoch: 5.69 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022001082487079274		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.022001082487079274 | validation: 0.03859564241861411]
	TIME [epoch: 5.69 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020821576980599516		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.020821576980599516 | validation: 0.037614450054663046]
	TIME [epoch: 5.69 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023568911512821124		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.023568911512821124 | validation: 0.03469831682222674]
	TIME [epoch: 5.69 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023574683424758262		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.023574683424758262 | validation: 0.03813399603841808]
	TIME [epoch: 5.69 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02541050224278937		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.02541050224278937 | validation: 0.0345042155602064]
	TIME [epoch: 5.69 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025689261914179237		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.025689261914179237 | validation: 0.037392254601975254]
	TIME [epoch: 5.69 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022749548705017695		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.022749548705017695 | validation: 0.03822005699215632]
	TIME [epoch: 5.69 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022154870237195992		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.022154870237195992 | validation: 0.03406295959078973]
	TIME [epoch: 5.69 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023278903559258		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.023278903559258 | validation: 0.035633597876130844]
	TIME [epoch: 5.72 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023747104575002797		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.023747104575002797 | validation: 0.03549613727907415]
	TIME [epoch: 5.72 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02316092032440911		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.02316092032440911 | validation: 0.03617202955300202]
	TIME [epoch: 5.73 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023993320016585225		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.023993320016585225 | validation: 0.035732693387220064]
	TIME [epoch: 5.73 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023606843783511897		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.023606843783511897 | validation: 0.03257886839218217]
	TIME [epoch: 5.72 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02263491578594521		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.02263491578594521 | validation: 0.03706735305309671]
	TIME [epoch: 5.72 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02401567520455531		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.02401567520455531 | validation: 0.03708331417378923]
	TIME [epoch: 5.72 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02249689790586303		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.02249689790586303 | validation: 0.034792473251973675]
	TIME [epoch: 5.72 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022867214297617514		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.022867214297617514 | validation: 0.03263284112649958]
	TIME [epoch: 5.71 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02293810242121796		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.02293810242121796 | validation: 0.03995488618967742]
	TIME [epoch: 5.72 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023843897654625107		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.023843897654625107 | validation: 0.03295044869598312]
	TIME [epoch: 5.72 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02176253030398377		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.02176253030398377 | validation: 0.036289438305424214]
	TIME [epoch: 5.72 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023242854099097262		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.023242854099097262 | validation: 0.035249512004471885]
	TIME [epoch: 5.72 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020633961486957585		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.020633961486957585 | validation: 0.03775399362084837]
	TIME [epoch: 5.72 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022746109732934464		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.022746109732934464 | validation: 0.03880737889577307]
	TIME [epoch: 5.72 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023017276712954865		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.023017276712954865 | validation: 0.03483532265526559]
	TIME [epoch: 5.72 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022469159394764574		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.022469159394764574 | validation: 0.04092643134520238]
	TIME [epoch: 5.72 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024938535897560034		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.024938535897560034 | validation: 0.03320415454794208]
	TIME [epoch: 5.72 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02333957596512513		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.02333957596512513 | validation: 0.036845557215709444]
	TIME [epoch: 5.72 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02273314128661494		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.02273314128661494 | validation: 0.034903760343682866]
	TIME [epoch: 5.72 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023864271849000477		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.023864271849000477 | validation: 0.037358423328236905]
	TIME [epoch: 5.72 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02493180636015527		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.02493180636015527 | validation: 0.03332789218009881]
	TIME [epoch: 5.72 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02342985283433866		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.02342985283433866 | validation: 0.03623755473210242]
	TIME [epoch: 5.72 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022143778008351617		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.022143778008351617 | validation: 0.03612294262128745]
	TIME [epoch: 5.72 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022983254860819563		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.022983254860819563 | validation: 0.03746433394733317]
	TIME [epoch: 5.73 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022876258549629443		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.022876258549629443 | validation: 0.032475391869992425]
	TIME [epoch: 5.72 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02396490218226105		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.02396490218226105 | validation: 0.035846734934449065]
	TIME [epoch: 5.72 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02313540695488385		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.02313540695488385 | validation: 0.03401732137140675]
	TIME [epoch: 5.72 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021751076346893914		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.021751076346893914 | validation: 0.028793636539223778]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1329.pth
	Model improved!!!
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02167428002468622		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.02167428002468622 | validation: 0.03464803935314866]
	TIME [epoch: 5.69 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02181634134937144		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.02181634134937144 | validation: 0.038687432278502035]
	TIME [epoch: 5.68 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024592064387503065		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.024592064387503065 | validation: 0.03369581147784541]
	TIME [epoch: 5.68 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021148551693154594		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.021148551693154594 | validation: 0.037484053259003]
	TIME [epoch: 5.69 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02386665469889425		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.02386665469889425 | validation: 0.03278052371053963]
	TIME [epoch: 5.68 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02306371282949468		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.02306371282949468 | validation: 0.038740756079252425]
	TIME [epoch: 5.68 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022983124030941385		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.022983124030941385 | validation: 0.03223185659187638]
	TIME [epoch: 5.68 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02345617809102442		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.02345617809102442 | validation: 0.03397989148239612]
	TIME [epoch: 5.71 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02314764598860505		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.02314764598860505 | validation: 0.03940833857989115]
	TIME [epoch: 5.72 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024163850015913948		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.024163850015913948 | validation: 0.032420026223165345]
	TIME [epoch: 5.72 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021815611125142825		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.021815611125142825 | validation: 0.034149977995083534]
	TIME [epoch: 5.72 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02297430448611489		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.02297430448611489 | validation: 0.037763752377695206]
	TIME [epoch: 5.73 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02315291918855996		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.02315291918855996 | validation: 0.03407876771502132]
	TIME [epoch: 5.71 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023493619361596548		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.023493619361596548 | validation: 0.03275729313724401]
	TIME [epoch: 5.73 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023411253158508796		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.023411253158508796 | validation: 0.03553376974753247]
	TIME [epoch: 5.72 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02171211240727077		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.02171211240727077 | validation: 0.032861369873747305]
	TIME [epoch: 5.72 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024432874884550648		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.024432874884550648 | validation: 0.03395305640226667]
	TIME [epoch: 5.72 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021722943140828576		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.021722943140828576 | validation: 0.03393567805869734]
	TIME [epoch: 5.72 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022199562110938955		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.022199562110938955 | validation: 0.03151905567041187]
	TIME [epoch: 5.72 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022840339421692387		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.022840339421692387 | validation: 0.03241247860865153]
	TIME [epoch: 5.72 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021078729788487946		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.021078729788487946 | validation: 0.0328519768015007]
	TIME [epoch: 5.72 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021841928859122405		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.021841928859122405 | validation: 0.0364138146660458]
	TIME [epoch: 5.73 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024167096860568567		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.024167096860568567 | validation: 0.035150180074031645]
	TIME [epoch: 5.7 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022322616968220577		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.022322616968220577 | validation: 0.03412174721773431]
	TIME [epoch: 5.72 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022996275764523944		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.022996275764523944 | validation: 0.03538835763743366]
	TIME [epoch: 5.72 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023642949726850493		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.023642949726850493 | validation: 0.03334266972255356]
	TIME [epoch: 5.72 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02147180329079136		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.02147180329079136 | validation: 0.03327352433464011]
	TIME [epoch: 5.73 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02054108245635563		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.02054108245635563 | validation: 0.036034955833105786]
	TIME [epoch: 5.74 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023404907028691524		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.023404907028691524 | validation: 0.03538373152529678]
	TIME [epoch: 5.73 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022411575143329393		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.022411575143329393 | validation: 0.037213337293334224]
	TIME [epoch: 5.73 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023225464949398075		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.023225464949398075 | validation: 0.03403077865610056]
	TIME [epoch: 5.72 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022372247331326492		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.022372247331326492 | validation: 0.035038379258105615]
	TIME [epoch: 5.73 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022504308802089458		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.022504308802089458 | validation: 0.03789583549150332]
	TIME [epoch: 5.72 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021368947752791235		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.021368947752791235 | validation: 0.03654973703746905]
	TIME [epoch: 5.72 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02228892695556988		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.02228892695556988 | validation: 0.03194897119256978]
	TIME [epoch: 5.72 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02190929818089133		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.02190929818089133 | validation: 0.03380300233478386]
	TIME [epoch: 5.72 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02067592325840395		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.02067592325840395 | validation: 0.039390462478844195]
	TIME [epoch: 5.72 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023333715228127837		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.023333715228127837 | validation: 0.03585673255051911]
	TIME [epoch: 5.72 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024455598654598242		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.024455598654598242 | validation: 0.0369290290896682]
	TIME [epoch: 5.69 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021978811089681666		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.021978811089681666 | validation: 0.03330860979736074]
	TIME [epoch: 5.69 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022369617039227956		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.022369617039227956 | validation: 0.03246865334335678]
	TIME [epoch: 5.68 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022088456279012264		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.022088456279012264 | validation: 0.036720473721456016]
	TIME [epoch: 5.69 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021411554190216738		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.021411554190216738 | validation: 0.03737535360751787]
	TIME [epoch: 5.71 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02132654970015243		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.02132654970015243 | validation: 0.03324240193218451]
	TIME [epoch: 5.71 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022554689128102396		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.022554689128102396 | validation: 0.034895385895807464]
	TIME [epoch: 5.71 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022451208473010933		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.022451208473010933 | validation: 0.03336270031809615]
	TIME [epoch: 5.72 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021889433632154218		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.021889433632154218 | validation: 0.03379837388346916]
	TIME [epoch: 5.71 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022259143461427176		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.022259143461427176 | validation: 0.03243044696237142]
	TIME [epoch: 5.72 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02120263915191201		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.02120263915191201 | validation: 0.03263600309012137]
	TIME [epoch: 5.72 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02297478351868982		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.02297478351868982 | validation: 0.029487279534961233]
	TIME [epoch: 5.71 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022217580631226294		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.022217580631226294 | validation: 0.03326827557565577]
	TIME [epoch: 5.73 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023597158852033822		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.023597158852033822 | validation: 0.03236039453512714]
	TIME [epoch: 5.72 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023319835138457697		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.023319835138457697 | validation: 0.033479417415431346]
	TIME [epoch: 5.72 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023010295416065724		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.023010295416065724 | validation: 0.03812471492148241]
	TIME [epoch: 5.71 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022264592304910624		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.022264592304910624 | validation: 0.02987043293888151]
	TIME [epoch: 5.71 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02201494090041803		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.02201494090041803 | validation: 0.03586862208688817]
	TIME [epoch: 5.71 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02198595578857091		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.02198595578857091 | validation: 0.033922411871789425]
	TIME [epoch: 5.71 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021610692036207836		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.021610692036207836 | validation: 0.03548602095374124]
	TIME [epoch: 5.71 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02215556596195792		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.02215556596195792 | validation: 0.03245119651595422]
	TIME [epoch: 5.72 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02205049097946016		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.02205049097946016 | validation: 0.03255885691942684]
	TIME [epoch: 5.71 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02188355958281881		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.02188355958281881 | validation: 0.03101702528615007]
	TIME [epoch: 5.71 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023457547961603806		[learning rate: 8.6788e-05]
	Learning Rate: 8.67884e-05
	LOSS [training: 0.023457547961603806 | validation: 0.03625002977268676]
	TIME [epoch: 5.71 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020684351848587492		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.020684351848587492 | validation: 0.03546692016908979]
	TIME [epoch: 5.71 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02166300539186543		[learning rate: 8.6176e-05]
	Learning Rate: 8.61757e-05
	LOSS [training: 0.02166300539186543 | validation: 0.031092339243624437]
	TIME [epoch: 5.72 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023479842214959384		[learning rate: 8.5871e-05]
	Learning Rate: 8.5871e-05
	LOSS [training: 0.023479842214959384 | validation: 0.03851553337778061]
	TIME [epoch: 5.71 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021172970708182567		[learning rate: 8.5567e-05]
	Learning Rate: 8.55673e-05
	LOSS [training: 0.021172970708182567 | validation: 0.03286728236482742]
	TIME [epoch: 5.71 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02267425054104418		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.02267425054104418 | validation: 0.029316547884145273]
	TIME [epoch: 5.71 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02254500394215486		[learning rate: 8.4963e-05]
	Learning Rate: 8.49632e-05
	LOSS [training: 0.02254500394215486 | validation: 0.0337202621922058]
	TIME [epoch: 5.71 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02250962919668482		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.02250962919668482 | validation: 0.03049862707467015]
	TIME [epoch: 5.71 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022405714737477697		[learning rate: 8.4363e-05]
	Learning Rate: 8.43634e-05
	LOSS [training: 0.022405714737477697 | validation: 0.031298805414952245]
	TIME [epoch: 5.71 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02106846672981228		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.02106846672981228 | validation: 0.03332231238762575]
	TIME [epoch: 5.71 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02100179467679553		[learning rate: 8.3768e-05]
	Learning Rate: 8.37678e-05
	LOSS [training: 0.02100179467679553 | validation: 0.03216030998949071]
	TIME [epoch: 5.69 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02118811363366433		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.02118811363366433 | validation: 0.031885592523799224]
	TIME [epoch: 5.69 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020290313461149623		[learning rate: 8.3176e-05]
	Learning Rate: 8.31764e-05
	LOSS [training: 0.020290313461149623 | validation: 0.03552909430420218]
	TIME [epoch: 5.7 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0216845741899414		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.0216845741899414 | validation: 0.03128110863671028]
	TIME [epoch: 5.69 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02224880423875874		[learning rate: 8.2589e-05]
	Learning Rate: 8.25892e-05
	LOSS [training: 0.02224880423875874 | validation: 0.03560826404107253]
	TIME [epoch: 5.69 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021743991429690235		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.021743991429690235 | validation: 0.0374952622568524]
	TIME [epoch: 5.69 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02100154707051267		[learning rate: 8.2006e-05]
	Learning Rate: 8.20061e-05
	LOSS [training: 0.02100154707051267 | validation: 0.03274998376406614]
	TIME [epoch: 5.69 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020835176117245977		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.020835176117245977 | validation: 0.031130063448536662]
	TIME [epoch: 5.69 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022549162050256318		[learning rate: 8.1427e-05]
	Learning Rate: 8.14272e-05
	LOSS [training: 0.022549162050256318 | validation: 0.028196423200902734]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1409.pth
	Model improved!!!
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02126518873965123		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.02126518873965123 | validation: 0.032961961070268865]
	TIME [epoch: 5.69 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0232050178526826		[learning rate: 8.0852e-05]
	Learning Rate: 8.08523e-05
	LOSS [training: 0.0232050178526826 | validation: 0.032888867930212194]
	TIME [epoch: 5.7 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021007529338800932		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.021007529338800932 | validation: 0.0299956781836511]
	TIME [epoch: 5.7 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02236041853486425		[learning rate: 8.0281e-05]
	Learning Rate: 8.02815e-05
	LOSS [training: 0.02236041853486425 | validation: 0.04032069183905488]
	TIME [epoch: 5.7 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02213737569842727		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.02213737569842727 | validation: 0.03386240094905212]
	TIME [epoch: 5.7 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020627189489046717		[learning rate: 7.9715e-05]
	Learning Rate: 7.97147e-05
	LOSS [training: 0.020627189489046717 | validation: 0.03299716798433572]
	TIME [epoch: 5.7 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02273962240676552		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.02273962240676552 | validation: 0.032692348054015476]
	TIME [epoch: 5.7 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021344852488590336		[learning rate: 7.9152e-05]
	Learning Rate: 7.9152e-05
	LOSS [training: 0.021344852488590336 | validation: 0.03354901707911001]
	TIME [epoch: 5.71 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021796975496997283		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.021796975496997283 | validation: 0.029509810524833438]
	TIME [epoch: 5.71 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02116143522451124		[learning rate: 7.8593e-05]
	Learning Rate: 7.85931e-05
	LOSS [training: 0.02116143522451124 | validation: 0.03085037522729034]
	TIME [epoch: 5.72 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02039436050075288		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.02039436050075288 | validation: 0.029481900755781457]
	TIME [epoch: 5.71 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020380844842193914		[learning rate: 7.8038e-05]
	Learning Rate: 7.80383e-05
	LOSS [training: 0.020380844842193914 | validation: 0.030243516121183135]
	TIME [epoch: 5.72 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02163030632940526		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.02163030632940526 | validation: 0.034878670928558224]
	TIME [epoch: 5.72 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020968613288050025		[learning rate: 7.7487e-05]
	Learning Rate: 7.74873e-05
	LOSS [training: 0.020968613288050025 | validation: 0.031203848287158886]
	TIME [epoch: 5.72 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01953652546112255		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.01953652546112255 | validation: 0.030434118594348747]
	TIME [epoch: 5.72 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02084377367168707		[learning rate: 7.694e-05]
	Learning Rate: 7.69403e-05
	LOSS [training: 0.02084377367168707 | validation: 0.03524837319996889]
	TIME [epoch: 5.72 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023088753569635214		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.023088753569635214 | validation: 0.02959017880370468]
	TIME [epoch: 5.71 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02094474406719442		[learning rate: 7.6397e-05]
	Learning Rate: 7.63971e-05
	LOSS [training: 0.02094474406719442 | validation: 0.03663027118492862]
	TIME [epoch: 5.72 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020840077041355737		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.020840077041355737 | validation: 0.030389618826901277]
	TIME [epoch: 5.72 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02114375301934114		[learning rate: 7.5858e-05]
	Learning Rate: 7.58578e-05
	LOSS [training: 0.02114375301934114 | validation: 0.03343855242881705]
	TIME [epoch: 5.74 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02131005866167115		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.02131005866167115 | validation: 0.03505313967715491]
	TIME [epoch: 5.72 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021816132347123044		[learning rate: 7.5322e-05]
	Learning Rate: 7.53222e-05
	LOSS [training: 0.021816132347123044 | validation: 0.03461968991636879]
	TIME [epoch: 5.73 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02127962258439645		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.02127962258439645 | validation: 0.028163311045950858]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1432.pth
	Model improved!!!
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02172936473344744		[learning rate: 7.479e-05]
	Learning Rate: 7.47905e-05
	LOSS [training: 0.02172936473344744 | validation: 0.030882900799925363]
	TIME [epoch: 5.7 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021119306299117718		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.021119306299117718 | validation: 0.02868749520338927]
	TIME [epoch: 5.7 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02086699957317407		[learning rate: 7.4262e-05]
	Learning Rate: 7.42625e-05
	LOSS [training: 0.02086699957317407 | validation: 0.032658911185054984]
	TIME [epoch: 5.69 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022643756228707287		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.022643756228707287 | validation: 0.03492323245321417]
	TIME [epoch: 5.69 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020913958738834025		[learning rate: 7.3738e-05]
	Learning Rate: 7.37382e-05
	LOSS [training: 0.020913958738834025 | validation: 0.03148580054442769]
	TIME [epoch: 5.7 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020458878832308246		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.020458878832308246 | validation: 0.02825191445420925]
	TIME [epoch: 5.69 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021266868904728512		[learning rate: 7.3218e-05]
	Learning Rate: 7.32176e-05
	LOSS [training: 0.021266868904728512 | validation: 0.03253282552029727]
	TIME [epoch: 5.7 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02108910949753367		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.02108910949753367 | validation: 0.030573648776075837]
	TIME [epoch: 5.7 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02274016100922598		[learning rate: 7.2701e-05]
	Learning Rate: 7.27007e-05
	LOSS [training: 0.02274016100922598 | validation: 0.03111709373596664]
	TIME [epoch: 5.69 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02204433247890039		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.02204433247890039 | validation: 0.03570186316081602]
	TIME [epoch: 5.7 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02148129314744519		[learning rate: 7.2187e-05]
	Learning Rate: 7.21874e-05
	LOSS [training: 0.02148129314744519 | validation: 0.030143877648253117]
	TIME [epoch: 5.69 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020863409024344783		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.020863409024344783 | validation: 0.03367391122954202]
	TIME [epoch: 5.69 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020784533703495294		[learning rate: 7.1678e-05]
	Learning Rate: 7.16778e-05
	LOSS [training: 0.020784533703495294 | validation: 0.033718127727089965]
	TIME [epoch: 5.7 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019987566260413394		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.019987566260413394 | validation: 0.03224508795553873]
	TIME [epoch: 5.69 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020905034977016887		[learning rate: 7.1172e-05]
	Learning Rate: 7.11718e-05
	LOSS [training: 0.020905034977016887 | validation: 0.03579961105341883]
	TIME [epoch: 5.69 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02178067778523512		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.02178067778523512 | validation: 0.031983926710198396]
	TIME [epoch: 5.7 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022272391966611918		[learning rate: 7.0669e-05]
	Learning Rate: 7.06693e-05
	LOSS [training: 0.022272391966611918 | validation: 0.029025380672058267]
	TIME [epoch: 5.69 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020602663426830564		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.020602663426830564 | validation: 0.030181577777281567]
	TIME [epoch: 5.7 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020337284164645815		[learning rate: 7.017e-05]
	Learning Rate: 7.01704e-05
	LOSS [training: 0.020337284164645815 | validation: 0.031072857676277578]
	TIME [epoch: 5.69 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02124342220187717		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.02124342220187717 | validation: 0.03367890885400069]
	TIME [epoch: 5.69 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021984868432452332		[learning rate: 6.9675e-05]
	Learning Rate: 6.9675e-05
	LOSS [training: 0.021984868432452332 | validation: 0.02878720587935113]
	TIME [epoch: 5.7 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02068806700066742		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.02068806700066742 | validation: 0.033637820798257334]
	TIME [epoch: 5.69 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02060372650135043		[learning rate: 6.9183e-05]
	Learning Rate: 6.91831e-05
	LOSS [training: 0.02060372650135043 | validation: 0.03143833344311408]
	TIME [epoch: 5.69 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021877405289462616		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.021877405289462616 | validation: 0.03285080968381695]
	TIME [epoch: 5.71 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02157920166427234		[learning rate: 6.8695e-05]
	Learning Rate: 6.86947e-05
	LOSS [training: 0.02157920166427234 | validation: 0.033321541172506484]
	TIME [epoch: 5.71 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022017544299254854		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.022017544299254854 | validation: 0.030275206009651147]
	TIME [epoch: 5.71 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02041504665333228		[learning rate: 6.821e-05]
	Learning Rate: 6.82097e-05
	LOSS [training: 0.02041504665333228 | validation: 0.03170011761336824]
	TIME [epoch: 5.71 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022638234237437313		[learning rate: 6.7969e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.022638234237437313 | validation: 0.0323109347142429]
	TIME [epoch: 5.72 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02168335622040059		[learning rate: 6.7728e-05]
	Learning Rate: 6.77282e-05
	LOSS [training: 0.02168335622040059 | validation: 0.03433103009525638]
	TIME [epoch: 5.71 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02153996001277066		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.02153996001277066 | validation: 0.029171704792354404]
	TIME [epoch: 5.71 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02058897338017412		[learning rate: 6.725e-05]
	Learning Rate: 6.725e-05
	LOSS [training: 0.02058897338017412 | validation: 0.03193543464565849]
	TIME [epoch: 5.72 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021079935993237795		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.021079935993237795 | validation: 0.03125986888223375]
	TIME [epoch: 5.7 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022643280969951284		[learning rate: 6.6775e-05]
	Learning Rate: 6.67752e-05
	LOSS [training: 0.022643280969951284 | validation: 0.030880794130099243]
	TIME [epoch: 5.69 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02229654778650768		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.02229654778650768 | validation: 0.0315828909813979]
	TIME [epoch: 5.69 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021169500783763572		[learning rate: 6.6304e-05]
	Learning Rate: 6.63038e-05
	LOSS [training: 0.021169500783763572 | validation: 0.033974304711719854]
	TIME [epoch: 5.68 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021673249670863547		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.021673249670863547 | validation: 0.02845428527274607]
	TIME [epoch: 5.71 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022246240641458446		[learning rate: 6.5836e-05]
	Learning Rate: 6.58357e-05
	LOSS [training: 0.022246240641458446 | validation: 0.030366860079503278]
	TIME [epoch: 5.72 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02050825430419181		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.02050825430419181 | validation: 0.03382297823208489]
	TIME [epoch: 5.71 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021891978102556945		[learning rate: 6.5371e-05]
	Learning Rate: 6.53709e-05
	LOSS [training: 0.021891978102556945 | validation: 0.030630643544348624]
	TIME [epoch: 5.72 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02081984468769855		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.02081984468769855 | validation: 0.03136349536406443]
	TIME [epoch: 5.72 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020663408141223312		[learning rate: 6.4909e-05]
	Learning Rate: 6.49094e-05
	LOSS [training: 0.020663408141223312 | validation: 0.03153230498964149]
	TIME [epoch: 5.71 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01933905276025871		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.01933905276025871 | validation: 0.03328764806947331]
	TIME [epoch: 5.72 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021150009997505617		[learning rate: 6.4451e-05]
	Learning Rate: 6.44512e-05
	LOSS [training: 0.021150009997505617 | validation: 0.03016051197293722]
	TIME [epoch: 5.72 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021604598309932842		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.021604598309932842 | validation: 0.03122198326237018]
	TIME [epoch: 5.72 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021501339381176533		[learning rate: 6.3996e-05]
	Learning Rate: 6.39962e-05
	LOSS [training: 0.021501339381176533 | validation: 0.030239045688587242]
	TIME [epoch: 5.71 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020460786339583908		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.020460786339583908 | validation: 0.03145465947869985]
	TIME [epoch: 5.71 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02152402225295613		[learning rate: 6.3544e-05]
	Learning Rate: 6.35444e-05
	LOSS [training: 0.02152402225295613 | validation: 0.030197999014779178]
	TIME [epoch: 5.73 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021615460065577102		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.021615460065577102 | validation: 0.030341728252360735]
	TIME [epoch: 5.71 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019303534423783183		[learning rate: 6.3096e-05]
	Learning Rate: 6.30958e-05
	LOSS [training: 0.019303534423783183 | validation: 0.02831774956731238]
	TIME [epoch: 5.72 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021844433407264944		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.021844433407264944 | validation: 0.03619336911167379]
	TIME [epoch: 5.71 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021380405480256794		[learning rate: 6.265e-05]
	Learning Rate: 6.26503e-05
	LOSS [training: 0.021380405480256794 | validation: 0.03133607382783917]
	TIME [epoch: 5.72 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02236685068319641		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.02236685068319641 | validation: 0.031127965736871977]
	TIME [epoch: 5.71 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019581832371881044		[learning rate: 6.2208e-05]
	Learning Rate: 6.2208e-05
	LOSS [training: 0.019581832371881044 | validation: 0.03470028999253113]
	TIME [epoch: 5.72 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02097436700084968		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.02097436700084968 | validation: 0.029699472452546782]
	TIME [epoch: 5.71 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021478950369755636		[learning rate: 6.1769e-05]
	Learning Rate: 6.17688e-05
	LOSS [training: 0.021478950369755636 | validation: 0.03330822068833668]
	TIME [epoch: 5.71 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022884929627184053		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.022884929627184053 | validation: 0.03620799976872757]
	TIME [epoch: 5.71 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020587475550263506		[learning rate: 6.1333e-05]
	Learning Rate: 6.13327e-05
	LOSS [training: 0.020587475550263506 | validation: 0.0327690714947915]
	TIME [epoch: 5.72 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02141408156724001		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.02141408156724001 | validation: 0.033249670055690815]
	TIME [epoch: 5.71 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02156153517086663		[learning rate: 6.09e-05]
	Learning Rate: 6.08998e-05
	LOSS [training: 0.02156153517086663 | validation: 0.03280669394158525]
	TIME [epoch: 5.71 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021336357677809063		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.021336357677809063 | validation: 0.031179910644377085]
	TIME [epoch: 5.71 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02040617267103082		[learning rate: 6.047e-05]
	Learning Rate: 6.04698e-05
	LOSS [training: 0.02040617267103082 | validation: 0.028702123101201404]
	TIME [epoch: 5.71 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022380706414192778		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.022380706414192778 | validation: 0.03418757075207253]
	TIME [epoch: 5.71 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021646966236182685		[learning rate: 6.0043e-05]
	Learning Rate: 6.00429e-05
	LOSS [training: 0.021646966236182685 | validation: 0.03217294818278802]
	TIME [epoch: 5.71 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02192207210971573		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.02192207210971573 | validation: 0.027705600814926158]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1496.pth
	Model improved!!!
EPOCH 1497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020887600633167854		[learning rate: 5.9619e-05]
	Learning Rate: 5.9619e-05
	LOSS [training: 0.020887600633167854 | validation: 0.03654640149517385]
	TIME [epoch: 5.71 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020151936018985142		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.020151936018985142 | validation: 0.03118512686821009]
	TIME [epoch: 5.7 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01908603462813351		[learning rate: 5.9198e-05]
	Learning Rate: 5.91981e-05
	LOSS [training: 0.01908603462813351 | validation: 0.027516030340293242]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1499.pth
	Model improved!!!
EPOCH 1500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021386693996024723		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.021386693996024723 | validation: 0.03172755070769299]
	TIME [epoch: 5.69 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019523925065547645		[learning rate: 5.878e-05]
	Learning Rate: 5.87802e-05
	LOSS [training: 0.019523925065547645 | validation: 0.031610119730853904]
	TIME [epoch: 5.7 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02002231688949531		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.02002231688949531 | validation: 0.029176443059169357]
	TIME [epoch: 5.7 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021009518816781172		[learning rate: 5.8365e-05]
	Learning Rate: 5.83652e-05
	LOSS [training: 0.021009518816781172 | validation: 0.0351624356833997]
	TIME [epoch: 5.7 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02032110663692745		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.02032110663692745 | validation: 0.034449568402197074]
	TIME [epoch: 5.7 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022199908164051846		[learning rate: 5.7953e-05]
	Learning Rate: 5.79531e-05
	LOSS [training: 0.022199908164051846 | validation: 0.030315552943321625]
	TIME [epoch: 5.7 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019987086550186547		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.019987086550186547 | validation: 0.031109601122490352]
	TIME [epoch: 5.7 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02128946182390515		[learning rate: 5.7544e-05]
	Learning Rate: 5.7544e-05
	LOSS [training: 0.02128946182390515 | validation: 0.031204610611282914]
	TIME [epoch: 5.72 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02265552643055675		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.02265552643055675 | validation: 0.02968241272332244]
	TIME [epoch: 5.7 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018944771949399985		[learning rate: 5.7138e-05]
	Learning Rate: 5.71378e-05
	LOSS [training: 0.018944771949399985 | validation: 0.028225035049196625]
	TIME [epoch: 5.72 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02175344676987811		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.02175344676987811 | validation: 0.03346441763671015]
	TIME [epoch: 5.69 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02134523508861298		[learning rate: 5.6734e-05]
	Learning Rate: 5.67344e-05
	LOSS [training: 0.02134523508861298 | validation: 0.03085566849587934]
	TIME [epoch: 5.72 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01990794745299953		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.01990794745299953 | validation: 0.031083499191458076]
	TIME [epoch: 5.69 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02094663700520746		[learning rate: 5.6334e-05]
	Learning Rate: 5.63338e-05
	LOSS [training: 0.02094663700520746 | validation: 0.03163818871132462]
	TIME [epoch: 5.72 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021269240481817		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.021269240481817 | validation: 0.03167708287387371]
	TIME [epoch: 5.68 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020281227835887813		[learning rate: 5.5936e-05]
	Learning Rate: 5.59361e-05
	LOSS [training: 0.020281227835887813 | validation: 0.03094783455973296]
	TIME [epoch: 5.72 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019795506602456516		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.019795506602456516 | validation: 0.03107901644968485]
	TIME [epoch: 5.68 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02114025519765336		[learning rate: 5.5541e-05]
	Learning Rate: 5.55412e-05
	LOSS [training: 0.02114025519765336 | validation: 0.03290689172712319]
	TIME [epoch: 5.72 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02013895100955774		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.02013895100955774 | validation: 0.033322146798863576]
	TIME [epoch: 5.69 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020314452920515903		[learning rate: 5.5149e-05]
	Learning Rate: 5.51491e-05
	LOSS [training: 0.020314452920515903 | validation: 0.031013316701326288]
	TIME [epoch: 5.71 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02046587824891869		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.02046587824891869 | validation: 0.02710415357191344]
	TIME [epoch: 5.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1520.pth
	Model improved!!!
EPOCH 1521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02120779278980903		[learning rate: 5.476e-05]
	Learning Rate: 5.47598e-05
	LOSS [training: 0.02120779278980903 | validation: 0.032251999160670376]
	TIME [epoch: 5.7 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022662581307844622		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.022662581307844622 | validation: 0.03036290579475648]
	TIME [epoch: 5.69 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02176723345713313		[learning rate: 5.4373e-05]
	Learning Rate: 5.43732e-05
	LOSS [training: 0.02176723345713313 | validation: 0.03037638867248347]
	TIME [epoch: 5.7 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021693283645781044		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.021693283645781044 | validation: 0.02969485929839424]
	TIME [epoch: 5.7 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020479099963060587		[learning rate: 5.3989e-05]
	Learning Rate: 5.39893e-05
	LOSS [training: 0.020479099963060587 | validation: 0.03071172442675827]
	TIME [epoch: 5.71 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021701276430933433		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.021701276430933433 | validation: 0.03256450595149107]
	TIME [epoch: 5.7 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020277624166948716		[learning rate: 5.3608e-05]
	Learning Rate: 5.36082e-05
	LOSS [training: 0.020277624166948716 | validation: 0.030173641056735326]
	TIME [epoch: 5.72 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021390247103317012		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.021390247103317012 | validation: 0.0310446683106143]
	TIME [epoch: 5.71 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020412291729245392		[learning rate: 5.323e-05]
	Learning Rate: 5.32297e-05
	LOSS [training: 0.020412291729245392 | validation: 0.03128646623879944]
	TIME [epoch: 5.71 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020255937074928277		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.020255937074928277 | validation: 0.0308442049087571]
	TIME [epoch: 5.71 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02024102028903866		[learning rate: 5.2854e-05]
	Learning Rate: 5.28539e-05
	LOSS [training: 0.02024102028903866 | validation: 0.03231605839120907]
	TIME [epoch: 5.71 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020421146154056897		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.020421146154056897 | validation: 0.027254008585493063]
	TIME [epoch: 5.71 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019456964623460013		[learning rate: 5.2481e-05]
	Learning Rate: 5.24807e-05
	LOSS [training: 0.019456964623460013 | validation: 0.03137007251556861]
	TIME [epoch: 5.72 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020046355250312155		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.020046355250312155 | validation: 0.028938136241066205]
	TIME [epoch: 5.71 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020448795124577507		[learning rate: 5.211e-05]
	Learning Rate: 5.21103e-05
	LOSS [training: 0.020448795124577507 | validation: 0.029817053944714525]
	TIME [epoch: 5.72 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01892607207020987		[learning rate: 5.1926e-05]
	Learning Rate: 5.1926e-05
	LOSS [training: 0.01892607207020987 | validation: 0.03585621669693292]
	TIME [epoch: 5.71 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020596230074416803		[learning rate: 5.1742e-05]
	Learning Rate: 5.17424e-05
	LOSS [training: 0.020596230074416803 | validation: 0.030600818339598646]
	TIME [epoch: 5.71 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020060335162105876		[learning rate: 5.1559e-05]
	Learning Rate: 5.15594e-05
	LOSS [training: 0.020060335162105876 | validation: 0.030814306902298583]
	TIME [epoch: 5.71 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020948586294521877		[learning rate: 5.1377e-05]
	Learning Rate: 5.13771e-05
	LOSS [training: 0.020948586294521877 | validation: 0.03055430952043389]
	TIME [epoch: 5.72 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020661781950902732		[learning rate: 5.1195e-05]
	Learning Rate: 5.11954e-05
	LOSS [training: 0.020661781950902732 | validation: 0.029716507996213094]
	TIME [epoch: 5.71 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020724001167389042		[learning rate: 5.1014e-05]
	Learning Rate: 5.10144e-05
	LOSS [training: 0.020724001167389042 | validation: 0.03179165320594277]
	TIME [epoch: 5.71 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022023786850389807		[learning rate: 5.0834e-05]
	Learning Rate: 5.0834e-05
	LOSS [training: 0.022023786850389807 | validation: 0.031190827611085548]
	TIME [epoch: 5.71 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0205742723996105		[learning rate: 5.0654e-05]
	Learning Rate: 5.06542e-05
	LOSS [training: 0.0205742723996105 | validation: 0.031514625263744614]
	TIME [epoch: 5.71 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019824045500247797		[learning rate: 5.0475e-05]
	Learning Rate: 5.04751e-05
	LOSS [training: 0.019824045500247797 | validation: 0.031681786466613696]
	TIME [epoch: 5.71 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020656315411146108		[learning rate: 5.0297e-05]
	Learning Rate: 5.02966e-05
	LOSS [training: 0.020656315411146108 | validation: 0.027938132733224152]
	TIME [epoch: 5.71 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020858679228378526		[learning rate: 5.0119e-05]
	Learning Rate: 5.01187e-05
	LOSS [training: 0.020858679228378526 | validation: 0.030183323778422866]
	TIME [epoch: 5.71 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020505555145551996		[learning rate: 4.9942e-05]
	Learning Rate: 4.99415e-05
	LOSS [training: 0.020505555145551996 | validation: 0.028564953200309707]
	TIME [epoch: 5.71 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0190186391567579		[learning rate: 4.9765e-05]
	Learning Rate: 4.97649e-05
	LOSS [training: 0.0190186391567579 | validation: 0.028003437694661792]
	TIME [epoch: 5.71 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020566688093480275		[learning rate: 4.9589e-05]
	Learning Rate: 4.95889e-05
	LOSS [training: 0.020566688093480275 | validation: 0.0315358198027214]
	TIME [epoch: 5.71 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0211470653164952		[learning rate: 4.9414e-05]
	Learning Rate: 4.94136e-05
	LOSS [training: 0.0211470653164952 | validation: 0.034562640804867685]
	TIME [epoch: 5.71 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018892669295379767		[learning rate: 4.9239e-05]
	Learning Rate: 4.92388e-05
	LOSS [training: 0.018892669295379767 | validation: 0.027731762280704342]
	TIME [epoch: 5.71 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01987893624837276		[learning rate: 4.9065e-05]
	Learning Rate: 4.90647e-05
	LOSS [training: 0.01987893624837276 | validation: 0.0353064822876245]
	TIME [epoch: 5.71 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021032477422259978		[learning rate: 4.8891e-05]
	Learning Rate: 4.88912e-05
	LOSS [training: 0.021032477422259978 | validation: 0.029782775760686022]
	TIME [epoch: 5.71 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019793734432133045		[learning rate: 4.8718e-05]
	Learning Rate: 4.87183e-05
	LOSS [training: 0.019793734432133045 | validation: 0.026817619622339795]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1554.pth
	Model improved!!!
EPOCH 1555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02017345639819777		[learning rate: 4.8546e-05]
	Learning Rate: 4.85461e-05
	LOSS [training: 0.02017345639819777 | validation: 0.030717367155429755]
	TIME [epoch: 5.7 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01983831448474679		[learning rate: 4.8374e-05]
	Learning Rate: 4.83744e-05
	LOSS [training: 0.01983831448474679 | validation: 0.029498803167824408]
	TIME [epoch: 5.7 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022501872220576865		[learning rate: 4.8203e-05]
	Learning Rate: 4.82033e-05
	LOSS [training: 0.022501872220576865 | validation: 0.03092371750152232]
	TIME [epoch: 5.7 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019212210200888223		[learning rate: 4.8033e-05]
	Learning Rate: 4.80329e-05
	LOSS [training: 0.019212210200888223 | validation: 0.028381022030388073]
	TIME [epoch: 5.7 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02022954771869696		[learning rate: 4.7863e-05]
	Learning Rate: 4.7863e-05
	LOSS [training: 0.02022954771869696 | validation: 0.029717336560815266]
	TIME [epoch: 5.71 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02166722220069455		[learning rate: 4.7694e-05]
	Learning Rate: 4.76938e-05
	LOSS [training: 0.02166722220069455 | validation: 0.035019415546437616]
	TIME [epoch: 5.7 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019457677920540168		[learning rate: 4.7525e-05]
	Learning Rate: 4.75251e-05
	LOSS [training: 0.019457677920540168 | validation: 0.033380931513914434]
	TIME [epoch: 5.7 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019843352141460206		[learning rate: 4.7357e-05]
	Learning Rate: 4.73571e-05
	LOSS [training: 0.019843352141460206 | validation: 0.03187412041551838]
	TIME [epoch: 5.7 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019501912794185318		[learning rate: 4.719e-05]
	Learning Rate: 4.71896e-05
	LOSS [training: 0.019501912794185318 | validation: 0.03209043989208778]
	TIME [epoch: 5.7 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02136603109652965		[learning rate: 4.7023e-05]
	Learning Rate: 4.70227e-05
	LOSS [training: 0.02136603109652965 | validation: 0.029179741158241845]
	TIME [epoch: 5.72 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020036358576169012		[learning rate: 4.6856e-05]
	Learning Rate: 4.68564e-05
	LOSS [training: 0.020036358576169012 | validation: 0.029747933640440996]
	TIME [epoch: 5.71 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020855374859207555		[learning rate: 4.6691e-05]
	Learning Rate: 4.66907e-05
	LOSS [training: 0.020855374859207555 | validation: 0.026464700433700628]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1566.pth
	Model improved!!!
EPOCH 1567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020814821194050117		[learning rate: 4.6526e-05]
	Learning Rate: 4.65257e-05
	LOSS [training: 0.020814821194050117 | validation: 0.030931268160340464]
	TIME [epoch: 5.7 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02190890935936313		[learning rate: 4.6361e-05]
	Learning Rate: 4.63611e-05
	LOSS [training: 0.02190890935936313 | validation: 0.031395334406067635]
	TIME [epoch: 5.7 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01999256764925938		[learning rate: 4.6197e-05]
	Learning Rate: 4.61972e-05
	LOSS [training: 0.01999256764925938 | validation: 0.02992969130298713]
	TIME [epoch: 5.7 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020074476276783076		[learning rate: 4.6034e-05]
	Learning Rate: 4.60338e-05
	LOSS [training: 0.020074476276783076 | validation: 0.029378909841437928]
	TIME [epoch: 5.71 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02116787364713768		[learning rate: 4.5871e-05]
	Learning Rate: 4.5871e-05
	LOSS [training: 0.02116787364713768 | validation: 0.03085359927917295]
	TIME [epoch: 5.71 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019217547350867788		[learning rate: 4.5709e-05]
	Learning Rate: 4.57088e-05
	LOSS [training: 0.019217547350867788 | validation: 0.032907014203964614]
	TIME [epoch: 5.71 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020403806213001065		[learning rate: 4.5547e-05]
	Learning Rate: 4.55472e-05
	LOSS [training: 0.020403806213001065 | validation: 0.02785512823536921]
	TIME [epoch: 5.71 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019729418451959947		[learning rate: 4.5386e-05]
	Learning Rate: 4.53861e-05
	LOSS [training: 0.019729418451959947 | validation: 0.03459010232551753]
	TIME [epoch: 5.72 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01970363064332266		[learning rate: 4.5226e-05]
	Learning Rate: 4.52256e-05
	LOSS [training: 0.01970363064332266 | validation: 0.0310451765889737]
	TIME [epoch: 5.71 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01950222859897067		[learning rate: 4.5066e-05]
	Learning Rate: 4.50657e-05
	LOSS [training: 0.01950222859897067 | validation: 0.02861613788227828]
	TIME [epoch: 5.71 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0198601634080768		[learning rate: 4.4906e-05]
	Learning Rate: 4.49064e-05
	LOSS [training: 0.0198601634080768 | validation: 0.029868971406492063]
	TIME [epoch: 5.71 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02030369928364249		[learning rate: 4.4748e-05]
	Learning Rate: 4.47476e-05
	LOSS [training: 0.02030369928364249 | validation: 0.031197791499567376]
	TIME [epoch: 5.71 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018829166937444887		[learning rate: 4.4589e-05]
	Learning Rate: 4.45893e-05
	LOSS [training: 0.018829166937444887 | validation: 0.029167635027094265]
	TIME [epoch: 5.71 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019805270350275638		[learning rate: 4.4432e-05]
	Learning Rate: 4.44316e-05
	LOSS [training: 0.019805270350275638 | validation: 0.029063252258652805]
	TIME [epoch: 5.72 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020940611410614957		[learning rate: 4.4275e-05]
	Learning Rate: 4.42745e-05
	LOSS [training: 0.020940611410614957 | validation: 0.029029954564085983]
	TIME [epoch: 5.71 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019531850078522674		[learning rate: 4.4118e-05]
	Learning Rate: 4.4118e-05
	LOSS [training: 0.019531850078522674 | validation: 0.026993871199182506]
	TIME [epoch: 5.71 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02007611365996079		[learning rate: 4.3962e-05]
	Learning Rate: 4.3962e-05
	LOSS [training: 0.02007611365996079 | validation: 0.030643477167135072]
	TIME [epoch: 5.71 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02020867081949854		[learning rate: 4.3807e-05]
	Learning Rate: 4.38065e-05
	LOSS [training: 0.02020867081949854 | validation: 0.02597279525714772]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1584.pth
	Model improved!!!
EPOCH 1585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022137345647181447		[learning rate: 4.3652e-05]
	Learning Rate: 4.36516e-05
	LOSS [training: 0.022137345647181447 | validation: 0.030239195207290882]
	TIME [epoch: 5.7 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019906069033134563		[learning rate: 4.3497e-05]
	Learning Rate: 4.34972e-05
	LOSS [training: 0.019906069033134563 | validation: 0.03368425072910859]
	TIME [epoch: 5.69 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018175306111190954		[learning rate: 4.3343e-05]
	Learning Rate: 4.33434e-05
	LOSS [training: 0.018175306111190954 | validation: 0.033057112400283695]
	TIME [epoch: 5.69 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021704303406094083		[learning rate: 4.319e-05]
	Learning Rate: 4.31902e-05
	LOSS [training: 0.021704303406094083 | validation: 0.028277576614455482]
	TIME [epoch: 5.69 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0202386297030018		[learning rate: 4.3037e-05]
	Learning Rate: 4.30374e-05
	LOSS [training: 0.0202386297030018 | validation: 0.030205259810076126]
	TIME [epoch: 5.69 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019841916316379195		[learning rate: 4.2885e-05]
	Learning Rate: 4.28852e-05
	LOSS [training: 0.019841916316379195 | validation: 0.027026912922294345]
	TIME [epoch: 5.7 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019616716183587953		[learning rate: 4.2734e-05]
	Learning Rate: 4.27336e-05
	LOSS [training: 0.019616716183587953 | validation: 0.03087298150629797]
	TIME [epoch: 5.69 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021457136315984997		[learning rate: 4.2582e-05]
	Learning Rate: 4.25825e-05
	LOSS [training: 0.021457136315984997 | validation: 0.027018261770429777]
	TIME [epoch: 5.69 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01925712080033665		[learning rate: 4.2432e-05]
	Learning Rate: 4.24319e-05
	LOSS [training: 0.01925712080033665 | validation: 0.028489736637193477]
	TIME [epoch: 5.69 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01848594550165948		[learning rate: 4.2282e-05]
	Learning Rate: 4.22819e-05
	LOSS [training: 0.01848594550165948 | validation: 0.031202860755085174]
	TIME [epoch: 5.69 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019976799005586143		[learning rate: 4.2132e-05]
	Learning Rate: 4.21323e-05
	LOSS [training: 0.019976799005586143 | validation: 0.02807290161265529]
	TIME [epoch: 5.7 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020031161571018204		[learning rate: 4.1983e-05]
	Learning Rate: 4.19833e-05
	LOSS [training: 0.020031161571018204 | validation: 0.029107363916203943]
	TIME [epoch: 5.69 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020874625932262924		[learning rate: 4.1835e-05]
	Learning Rate: 4.18349e-05
	LOSS [training: 0.020874625932262924 | validation: 0.02900656398778989]
	TIME [epoch: 5.69 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021558420703460642		[learning rate: 4.1687e-05]
	Learning Rate: 4.1687e-05
	LOSS [training: 0.021558420703460642 | validation: 0.02891708977593878]
	TIME [epoch: 5.7 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02090449268873192		[learning rate: 4.154e-05]
	Learning Rate: 4.15395e-05
	LOSS [training: 0.02090449268873192 | validation: 0.028409955299118918]
	TIME [epoch: 5.69 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019871292270238566		[learning rate: 4.1393e-05]
	Learning Rate: 4.13926e-05
	LOSS [training: 0.019871292270238566 | validation: 0.033005637677000446]
	TIME [epoch: 5.7 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020446338598690154		[learning rate: 4.1246e-05]
	Learning Rate: 4.12463e-05
	LOSS [training: 0.020446338598690154 | validation: 0.02916158486966075]
	TIME [epoch: 5.73 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0205543329107482		[learning rate: 4.11e-05]
	Learning Rate: 4.11004e-05
	LOSS [training: 0.0205543329107482 | validation: 0.0322884494450173]
	TIME [epoch: 5.71 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019892496614001238		[learning rate: 4.0955e-05]
	Learning Rate: 4.09551e-05
	LOSS [training: 0.019892496614001238 | validation: 0.02840772457437224]
	TIME [epoch: 5.72 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021210835346599118		[learning rate: 4.081e-05]
	Learning Rate: 4.08103e-05
	LOSS [training: 0.021210835346599118 | validation: 0.030053831606036564]
	TIME [epoch: 5.72 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020634566133308118		[learning rate: 4.0666e-05]
	Learning Rate: 4.06659e-05
	LOSS [training: 0.020634566133308118 | validation: 0.030918782735078934]
	TIME [epoch: 5.72 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019382373516575277		[learning rate: 4.0522e-05]
	Learning Rate: 4.05221e-05
	LOSS [training: 0.019382373516575277 | validation: 0.028812049547300846]
	TIME [epoch: 5.72 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020152707873636037		[learning rate: 4.0379e-05]
	Learning Rate: 4.03788e-05
	LOSS [training: 0.020152707873636037 | validation: 0.0289712467190125]
	TIME [epoch: 5.72 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019803493537041982		[learning rate: 4.0236e-05]
	Learning Rate: 4.02361e-05
	LOSS [training: 0.019803493537041982 | validation: 0.028990532555388793]
	TIME [epoch: 5.72 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019635484593216165		[learning rate: 4.0094e-05]
	Learning Rate: 4.00938e-05
	LOSS [training: 0.019635484593216165 | validation: 0.02834175460017159]
	TIME [epoch: 5.72 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01903021779363376		[learning rate: 3.9952e-05]
	Learning Rate: 3.9952e-05
	LOSS [training: 0.01903021779363376 | validation: 0.027801276048288737]
	TIME [epoch: 5.71 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018813059524590222		[learning rate: 3.9811e-05]
	Learning Rate: 3.98107e-05
	LOSS [training: 0.018813059524590222 | validation: 0.029961713739462426]
	TIME [epoch: 5.72 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01916447523261027		[learning rate: 3.967e-05]
	Learning Rate: 3.967e-05
	LOSS [training: 0.01916447523261027 | validation: 0.029973106530776263]
	TIME [epoch: 5.71 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019240890476500374		[learning rate: 3.953e-05]
	Learning Rate: 3.95297e-05
	LOSS [training: 0.019240890476500374 | validation: 0.029934267444064813]
	TIME [epoch: 5.72 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02075088362621727		[learning rate: 3.939e-05]
	Learning Rate: 3.93899e-05
	LOSS [training: 0.02075088362621727 | validation: 0.03063905865098503]
	TIME [epoch: 5.72 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020009005967472283		[learning rate: 3.9251e-05]
	Learning Rate: 3.92506e-05
	LOSS [training: 0.020009005967472283 | validation: 0.02540342505808082]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1615.pth
	Model improved!!!
EPOCH 1616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01851665024553545		[learning rate: 3.9112e-05]
	Learning Rate: 3.91118e-05
	LOSS [training: 0.01851665024553545 | validation: 0.029544865113503828]
	TIME [epoch: 5.69 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02047667780003829		[learning rate: 3.8973e-05]
	Learning Rate: 3.89735e-05
	LOSS [training: 0.02047667780003829 | validation: 0.03134072781398441]
	TIME [epoch: 5.68 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021537644219565198		[learning rate: 3.8836e-05]
	Learning Rate: 3.88357e-05
	LOSS [training: 0.021537644219565198 | validation: 0.029121337542317462]
	TIME [epoch: 5.69 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021399879214634137		[learning rate: 3.8698e-05]
	Learning Rate: 3.86983e-05
	LOSS [training: 0.021399879214634137 | validation: 0.027602922585242176]
	TIME [epoch: 5.68 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019075766866539383		[learning rate: 3.8561e-05]
	Learning Rate: 3.85615e-05
	LOSS [training: 0.019075766866539383 | validation: 0.030996872464512493]
	TIME [epoch: 5.69 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019912099094859418		[learning rate: 3.8425e-05]
	Learning Rate: 3.84251e-05
	LOSS [training: 0.019912099094859418 | validation: 0.0309256944221777]
	TIME [epoch: 5.69 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019977670777515107		[learning rate: 3.8289e-05]
	Learning Rate: 3.82893e-05
	LOSS [training: 0.019977670777515107 | validation: 0.030418646097489333]
	TIME [epoch: 5.69 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01985436816974762		[learning rate: 3.8154e-05]
	Learning Rate: 3.81539e-05
	LOSS [training: 0.01985436816974762 | validation: 0.0287356182851247]
	TIME [epoch: 5.68 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02065888046189147		[learning rate: 3.8019e-05]
	Learning Rate: 3.8019e-05
	LOSS [training: 0.02065888046189147 | validation: 0.03248109838497835]
	TIME [epoch: 5.69 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020769310991545904		[learning rate: 3.7885e-05]
	Learning Rate: 3.78845e-05
	LOSS [training: 0.020769310991545904 | validation: 0.0312599880089053]
	TIME [epoch: 5.69 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019225168034180676		[learning rate: 3.7751e-05]
	Learning Rate: 3.77505e-05
	LOSS [training: 0.019225168034180676 | validation: 0.028917828714062302]
	TIME [epoch: 5.7 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019321297629490083		[learning rate: 3.7617e-05]
	Learning Rate: 3.7617e-05
	LOSS [training: 0.019321297629490083 | validation: 0.034711282037348905]
	TIME [epoch: 5.69 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02148449613878789		[learning rate: 3.7484e-05]
	Learning Rate: 3.7484e-05
	LOSS [training: 0.02148449613878789 | validation: 0.03195444776323555]
	TIME [epoch: 5.69 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02025340268531571		[learning rate: 3.7351e-05]
	Learning Rate: 3.73515e-05
	LOSS [training: 0.02025340268531571 | validation: 0.03062545144747321]
	TIME [epoch: 5.69 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020489423457962657		[learning rate: 3.7219e-05]
	Learning Rate: 3.72194e-05
	LOSS [training: 0.020489423457962657 | validation: 0.027171686838649112]
	TIME [epoch: 5.69 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021033434044727323		[learning rate: 3.7088e-05]
	Learning Rate: 3.70878e-05
	LOSS [training: 0.021033434044727323 | validation: 0.028737562346735336]
	TIME [epoch: 5.69 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019727871478986463		[learning rate: 3.6957e-05]
	Learning Rate: 3.69566e-05
	LOSS [training: 0.019727871478986463 | validation: 0.025851506521607337]
	TIME [epoch: 5.69 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01890702310627288		[learning rate: 3.6826e-05]
	Learning Rate: 3.68259e-05
	LOSS [training: 0.01890702310627288 | validation: 0.029429433235022298]
	TIME [epoch: 5.68 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018661805213437254		[learning rate: 3.6696e-05]
	Learning Rate: 3.66957e-05
	LOSS [training: 0.018661805213437254 | validation: 0.029231446028143628]
	TIME [epoch: 5.69 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019615104927354025		[learning rate: 3.6566e-05]
	Learning Rate: 3.6566e-05
	LOSS [training: 0.019615104927354025 | validation: 0.029069232280608506]
	TIME [epoch: 5.68 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01954227196338369		[learning rate: 3.6437e-05]
	Learning Rate: 3.64367e-05
	LOSS [training: 0.01954227196338369 | validation: 0.03127907819330381]
	TIME [epoch: 5.69 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018700150085770754		[learning rate: 3.6308e-05]
	Learning Rate: 3.63078e-05
	LOSS [training: 0.018700150085770754 | validation: 0.0314308099001591]
	TIME [epoch: 5.69 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020806742238909683		[learning rate: 3.6179e-05]
	Learning Rate: 3.61794e-05
	LOSS [training: 0.020806742238909683 | validation: 0.0288351504423354]
	TIME [epoch: 5.69 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02001059808423542		[learning rate: 3.6051e-05]
	Learning Rate: 3.60515e-05
	LOSS [training: 0.02001059808423542 | validation: 0.029545551218484168]
	TIME [epoch: 5.69 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01921132224888585		[learning rate: 3.5924e-05]
	Learning Rate: 3.5924e-05
	LOSS [training: 0.01921132224888585 | validation: 0.031052131098263516]
	TIME [epoch: 5.68 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02017322931433686		[learning rate: 3.5797e-05]
	Learning Rate: 3.5797e-05
	LOSS [training: 0.02017322931433686 | validation: 0.026962321771526656]
	TIME [epoch: 5.69 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01870663941237003		[learning rate: 3.567e-05]
	Learning Rate: 3.56704e-05
	LOSS [training: 0.01870663941237003 | validation: 0.02858746033484201]
	TIME [epoch: 5.69 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019087340326492258		[learning rate: 3.5544e-05]
	Learning Rate: 3.55442e-05
	LOSS [training: 0.019087340326492258 | validation: 0.02840515527635683]
	TIME [epoch: 5.69 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01922098641983663		[learning rate: 3.5419e-05]
	Learning Rate: 3.54186e-05
	LOSS [training: 0.01922098641983663 | validation: 0.02859195649947334]
	TIME [epoch: 5.7 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019401820113290887		[learning rate: 3.5293e-05]
	Learning Rate: 3.52933e-05
	LOSS [training: 0.019401820113290887 | validation: 0.027889437431510934]
	TIME [epoch: 5.7 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019975821191355683		[learning rate: 3.5169e-05]
	Learning Rate: 3.51685e-05
	LOSS [training: 0.019975821191355683 | validation: 0.030211723852544038]
	TIME [epoch: 5.72 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01995927356633568		[learning rate: 3.5044e-05]
	Learning Rate: 3.50441e-05
	LOSS [training: 0.01995927356633568 | validation: 0.032406298985968564]
	TIME [epoch: 5.72 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01942719322856901		[learning rate: 3.492e-05]
	Learning Rate: 3.49202e-05
	LOSS [training: 0.01942719322856901 | validation: 0.031428189845976194]
	TIME [epoch: 5.69 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021559157517808982		[learning rate: 3.4797e-05]
	Learning Rate: 3.47967e-05
	LOSS [training: 0.021559157517808982 | validation: 0.029843562876362875]
	TIME [epoch: 5.69 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01952557227334521		[learning rate: 3.4674e-05]
	Learning Rate: 3.46737e-05
	LOSS [training: 0.01952557227334521 | validation: 0.028282581968388322]
	TIME [epoch: 5.69 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018923858766366982		[learning rate: 3.4551e-05]
	Learning Rate: 3.45511e-05
	LOSS [training: 0.018923858766366982 | validation: 0.03085615285653337]
	TIME [epoch: 5.69 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019498518771387318		[learning rate: 3.4429e-05]
	Learning Rate: 3.44289e-05
	LOSS [training: 0.019498518771387318 | validation: 0.028958989757070664]
	TIME [epoch: 5.69 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019680378824911776		[learning rate: 3.4307e-05]
	Learning Rate: 3.43072e-05
	LOSS [training: 0.019680378824911776 | validation: 0.02830849711200414]
	TIME [epoch: 5.68 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019094278032105196		[learning rate: 3.4186e-05]
	Learning Rate: 3.41858e-05
	LOSS [training: 0.019094278032105196 | validation: 0.03241624299413848]
	TIME [epoch: 5.68 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018820852567781272		[learning rate: 3.4065e-05]
	Learning Rate: 3.4065e-05
	LOSS [training: 0.018820852567781272 | validation: 0.02712839838769723]
	TIME [epoch: 5.69 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019664464682355158		[learning rate: 3.3944e-05]
	Learning Rate: 3.39445e-05
	LOSS [training: 0.019664464682355158 | validation: 0.02893728551784165]
	TIME [epoch: 5.69 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01824870146037519		[learning rate: 3.3824e-05]
	Learning Rate: 3.38245e-05
	LOSS [training: 0.01824870146037519 | validation: 0.032108495421936]
	TIME [epoch: 5.69 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018921643668101173		[learning rate: 3.3705e-05]
	Learning Rate: 3.37049e-05
	LOSS [training: 0.018921643668101173 | validation: 0.02945255992249245]
	TIME [epoch: 5.69 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019495664071427617		[learning rate: 3.3586e-05]
	Learning Rate: 3.35857e-05
	LOSS [training: 0.019495664071427617 | validation: 0.02821231312132059]
	TIME [epoch: 5.69 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02029996429058644		[learning rate: 3.3467e-05]
	Learning Rate: 3.34669e-05
	LOSS [training: 0.02029996429058644 | validation: 0.02888334802287894]
	TIME [epoch: 5.69 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0189927986726483		[learning rate: 3.3349e-05]
	Learning Rate: 3.33486e-05
	LOSS [training: 0.0189927986726483 | validation: 0.02987163956021288]
	TIME [epoch: 5.69 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02039734643391871		[learning rate: 3.3231e-05]
	Learning Rate: 3.32306e-05
	LOSS [training: 0.02039734643391871 | validation: 0.030453569731156385]
	TIME [epoch: 5.68 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021288582755116494		[learning rate: 3.3113e-05]
	Learning Rate: 3.31131e-05
	LOSS [training: 0.021288582755116494 | validation: 0.03015391566746657]
	TIME [epoch: 5.69 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01966515350943111		[learning rate: 3.2996e-05]
	Learning Rate: 3.2996e-05
	LOSS [training: 0.01966515350943111 | validation: 0.029929141977155196]
	TIME [epoch: 5.7 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020157160028105886		[learning rate: 3.2879e-05]
	Learning Rate: 3.28794e-05
	LOSS [training: 0.020157160028105886 | validation: 0.02981545538100218]
	TIME [epoch: 5.69 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019022911317429888		[learning rate: 3.2763e-05]
	Learning Rate: 3.27631e-05
	LOSS [training: 0.019022911317429888 | validation: 0.03100879192412457]
	TIME [epoch: 5.68 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0196614097986172		[learning rate: 3.2647e-05]
	Learning Rate: 3.26472e-05
	LOSS [training: 0.0196614097986172 | validation: 0.027270488153441963]
	TIME [epoch: 5.69 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018520033191098233		[learning rate: 3.2532e-05]
	Learning Rate: 3.25318e-05
	LOSS [training: 0.018520033191098233 | validation: 0.028652081564253362]
	TIME [epoch: 5.69 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0195596776679391		[learning rate: 3.2417e-05]
	Learning Rate: 3.24167e-05
	LOSS [training: 0.0195596776679391 | validation: 0.029116637566884308]
	TIME [epoch: 5.69 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01942522985169977		[learning rate: 3.2302e-05]
	Learning Rate: 3.23021e-05
	LOSS [training: 0.01942522985169977 | validation: 0.02936965491376028]
	TIME [epoch: 5.68 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019886263468007512		[learning rate: 3.2188e-05]
	Learning Rate: 3.21879e-05
	LOSS [training: 0.019886263468007512 | validation: 0.027535870591103684]
	TIME [epoch: 5.69 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01851929371730308		[learning rate: 3.2074e-05]
	Learning Rate: 3.20741e-05
	LOSS [training: 0.01851929371730308 | validation: 0.026893313887151927]
	TIME [epoch: 5.69 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01967233934776962		[learning rate: 3.1961e-05]
	Learning Rate: 3.19606e-05
	LOSS [training: 0.01967233934776962 | validation: 0.02975288858331171]
	TIME [epoch: 5.69 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019671577427021122		[learning rate: 3.1848e-05]
	Learning Rate: 3.18476e-05
	LOSS [training: 0.019671577427021122 | validation: 0.030659250794724202]
	TIME [epoch: 5.68 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019609297488036387		[learning rate: 3.1735e-05]
	Learning Rate: 3.1735e-05
	LOSS [training: 0.019609297488036387 | validation: 0.02944431528313939]
	TIME [epoch: 5.68 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019705539913608485		[learning rate: 3.1623e-05]
	Learning Rate: 3.16228e-05
	LOSS [training: 0.019705539913608485 | validation: 0.02990374260407397]
	TIME [epoch: 5.68 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020458442349137133		[learning rate: 3.1511e-05]
	Learning Rate: 3.1511e-05
	LOSS [training: 0.020458442349137133 | validation: 0.028850105259101325]
	TIME [epoch: 5.69 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01989534016339269		[learning rate: 3.14e-05]
	Learning Rate: 3.13995e-05
	LOSS [training: 0.01989534016339269 | validation: 0.028148159530586428]
	TIME [epoch: 5.68 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02062475617213104		[learning rate: 3.1288e-05]
	Learning Rate: 3.12885e-05
	LOSS [training: 0.02062475617213104 | validation: 0.030061159766868097]
	TIME [epoch: 5.68 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019760689952706648		[learning rate: 3.1178e-05]
	Learning Rate: 3.11779e-05
	LOSS [training: 0.019760689952706648 | validation: 0.028948172590026213]
	TIME [epoch: 5.69 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021304485864514878		[learning rate: 3.1068e-05]
	Learning Rate: 3.10676e-05
	LOSS [training: 0.021304485864514878 | validation: 0.02445662910967185]
	TIME [epoch: 5.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1681.pth
	Model improved!!!
EPOCH 1682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020604810896952267		[learning rate: 3.0958e-05]
	Learning Rate: 3.09577e-05
	LOSS [training: 0.020604810896952267 | validation: 0.029526866061971746]
	TIME [epoch: 5.69 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01955796409523963		[learning rate: 3.0848e-05]
	Learning Rate: 3.08483e-05
	LOSS [training: 0.01955796409523963 | validation: 0.03100178564222461]
	TIME [epoch: 5.68 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019509899494939767		[learning rate: 3.0739e-05]
	Learning Rate: 3.07392e-05
	LOSS [training: 0.019509899494939767 | validation: 0.026812686372731043]
	TIME [epoch: 5.68 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01999348813748647		[learning rate: 3.063e-05]
	Learning Rate: 3.06305e-05
	LOSS [training: 0.01999348813748647 | validation: 0.028490594105315282]
	TIME [epoch: 5.68 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01941041137084624		[learning rate: 3.0522e-05]
	Learning Rate: 3.05222e-05
	LOSS [training: 0.01941041137084624 | validation: 0.03124014280097034]
	TIME [epoch: 5.68 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020451077160385318		[learning rate: 3.0414e-05]
	Learning Rate: 3.04142e-05
	LOSS [training: 0.020451077160385318 | validation: 0.03158796589839933]
	TIME [epoch: 5.68 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020600847251519578		[learning rate: 3.0307e-05]
	Learning Rate: 3.03067e-05
	LOSS [training: 0.020600847251519578 | validation: 0.028943738860959335]
	TIME [epoch: 5.68 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01928484762347609		[learning rate: 3.02e-05]
	Learning Rate: 3.01995e-05
	LOSS [training: 0.01928484762347609 | validation: 0.02834731659762192]
	TIME [epoch: 5.69 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01996515893421564		[learning rate: 3.0093e-05]
	Learning Rate: 3.00927e-05
	LOSS [training: 0.01996515893421564 | validation: 0.02725221597661347]
	TIME [epoch: 5.68 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01948350387928705		[learning rate: 2.9986e-05]
	Learning Rate: 2.99863e-05
	LOSS [training: 0.01948350387928705 | validation: 0.03157099687894158]
	TIME [epoch: 5.69 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019928962244829772		[learning rate: 2.988e-05]
	Learning Rate: 2.98803e-05
	LOSS [training: 0.019928962244829772 | validation: 0.03011997809409278]
	TIME [epoch: 5.68 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018918394999851144		[learning rate: 2.9775e-05]
	Learning Rate: 2.97746e-05
	LOSS [training: 0.018918394999851144 | validation: 0.03027302628805122]
	TIME [epoch: 5.68 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0196057916892788		[learning rate: 2.9669e-05]
	Learning Rate: 2.96693e-05
	LOSS [training: 0.0196057916892788 | validation: 0.02396372029900247]
	TIME [epoch: 5.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1694.pth
	Model improved!!!
EPOCH 1695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020485119923154518		[learning rate: 2.9564e-05]
	Learning Rate: 2.95644e-05
	LOSS [training: 0.020485119923154518 | validation: 0.030146904983720482]
	TIME [epoch: 5.68 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019716529275067493		[learning rate: 2.946e-05]
	Learning Rate: 2.94599e-05
	LOSS [training: 0.019716529275067493 | validation: 0.02887525070437539]
	TIME [epoch: 5.68 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019549712021934704		[learning rate: 2.9356e-05]
	Learning Rate: 2.93557e-05
	LOSS [training: 0.019549712021934704 | validation: 0.0270733739960352]
	TIME [epoch: 5.68 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020236637798375128		[learning rate: 2.9252e-05]
	Learning Rate: 2.92519e-05
	LOSS [training: 0.020236637798375128 | validation: 0.029116190826978874]
	TIME [epoch: 5.68 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01983492914821695		[learning rate: 2.9148e-05]
	Learning Rate: 2.91485e-05
	LOSS [training: 0.01983492914821695 | validation: 0.027761016319313582]
	TIME [epoch: 5.68 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019980307885323963		[learning rate: 2.9045e-05]
	Learning Rate: 2.90454e-05
	LOSS [training: 0.019980307885323963 | validation: 0.028394580804753503]
	TIME [epoch: 5.68 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019214292730083734		[learning rate: 2.8943e-05]
	Learning Rate: 2.89427e-05
	LOSS [training: 0.019214292730083734 | validation: 0.028523823462844978]
	TIME [epoch: 5.68 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018179588363292913		[learning rate: 2.884e-05]
	Learning Rate: 2.88403e-05
	LOSS [training: 0.018179588363292913 | validation: 0.028415519391401703]
	TIME [epoch: 5.68 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018922392624873025		[learning rate: 2.8738e-05]
	Learning Rate: 2.87383e-05
	LOSS [training: 0.018922392624873025 | validation: 0.02883988572385171]
	TIME [epoch: 5.68 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018535238714093134		[learning rate: 2.8637e-05]
	Learning Rate: 2.86367e-05
	LOSS [training: 0.018535238714093134 | validation: 0.029033797826654918]
	TIME [epoch: 5.68 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020019378750705787		[learning rate: 2.8535e-05]
	Learning Rate: 2.85355e-05
	LOSS [training: 0.020019378750705787 | validation: 0.028018231222801716]
	TIME [epoch: 5.68 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0207038955902097		[learning rate: 2.8435e-05]
	Learning Rate: 2.84345e-05
	LOSS [training: 0.0207038955902097 | validation: 0.025333945637958457]
	TIME [epoch: 5.68 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018059100362199692		[learning rate: 2.8334e-05]
	Learning Rate: 2.8334e-05
	LOSS [training: 0.018059100362199692 | validation: 0.029393793509847468]
	TIME [epoch: 5.68 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0194978539982397		[learning rate: 2.8234e-05]
	Learning Rate: 2.82338e-05
	LOSS [training: 0.0194978539982397 | validation: 0.027534663979006637]
	TIME [epoch: 5.68 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018985686761357916		[learning rate: 2.8134e-05]
	Learning Rate: 2.8134e-05
	LOSS [training: 0.018985686761357916 | validation: 0.0277086156469893]
	TIME [epoch: 5.68 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018147112231566016		[learning rate: 2.8034e-05]
	Learning Rate: 2.80345e-05
	LOSS [training: 0.018147112231566016 | validation: 0.030590058149842093]
	TIME [epoch: 5.69 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019280068333020997		[learning rate: 2.7935e-05]
	Learning Rate: 2.79353e-05
	LOSS [training: 0.019280068333020997 | validation: 0.031656756702000244]
	TIME [epoch: 5.68 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018776820382981087		[learning rate: 2.7837e-05]
	Learning Rate: 2.78366e-05
	LOSS [training: 0.018776820382981087 | validation: 0.029650480752130438]
	TIME [epoch: 5.68 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019990687092657913		[learning rate: 2.7738e-05]
	Learning Rate: 2.77381e-05
	LOSS [training: 0.019990687092657913 | validation: 0.028532635130269393]
	TIME [epoch: 5.68 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019350883563187024		[learning rate: 2.764e-05]
	Learning Rate: 2.764e-05
	LOSS [training: 0.019350883563187024 | validation: 0.02832460980964374]
	TIME [epoch: 5.68 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019131711609898586		[learning rate: 2.7542e-05]
	Learning Rate: 2.75423e-05
	LOSS [training: 0.019131711609898586 | validation: 0.028133143243213057]
	TIME [epoch: 5.68 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020368273917522512		[learning rate: 2.7445e-05]
	Learning Rate: 2.74449e-05
	LOSS [training: 0.020368273917522512 | validation: 0.029539298766125977]
	TIME [epoch: 5.68 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01871221999675304		[learning rate: 2.7348e-05]
	Learning Rate: 2.73478e-05
	LOSS [training: 0.01871221999675304 | validation: 0.026770039798320724]
	TIME [epoch: 5.68 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018410085933131922		[learning rate: 2.7251e-05]
	Learning Rate: 2.72511e-05
	LOSS [training: 0.018410085933131922 | validation: 0.027089993082579367]
	TIME [epoch: 5.68 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019128242236742612		[learning rate: 2.7155e-05]
	Learning Rate: 2.71548e-05
	LOSS [training: 0.019128242236742612 | validation: 0.0273903993666221]
	TIME [epoch: 5.68 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019129687412122155		[learning rate: 2.7059e-05]
	Learning Rate: 2.70587e-05
	LOSS [training: 0.019129687412122155 | validation: 0.028978129168841984]
	TIME [epoch: 5.69 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01922554807951411		[learning rate: 2.6963e-05]
	Learning Rate: 2.69631e-05
	LOSS [training: 0.01922554807951411 | validation: 0.029187803354253264]
	TIME [epoch: 5.68 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01921771070869194		[learning rate: 2.6868e-05]
	Learning Rate: 2.68677e-05
	LOSS [training: 0.01921771070869194 | validation: 0.02894007358945443]
	TIME [epoch: 5.68 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019028456291873466		[learning rate: 2.6773e-05]
	Learning Rate: 2.67727e-05
	LOSS [training: 0.019028456291873466 | validation: 0.027655874838390396]
	TIME [epoch: 5.68 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01859827266619927		[learning rate: 2.6678e-05]
	Learning Rate: 2.6678e-05
	LOSS [training: 0.01859827266619927 | validation: 0.029497183988005216]
	TIME [epoch: 5.68 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01900388253386779		[learning rate: 2.6584e-05]
	Learning Rate: 2.65837e-05
	LOSS [training: 0.01900388253386779 | validation: 0.03160179165628305]
	TIME [epoch: 5.68 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019904478698662586		[learning rate: 2.649e-05]
	Learning Rate: 2.64897e-05
	LOSS [training: 0.019904478698662586 | validation: 0.027332323814654714]
	TIME [epoch: 5.68 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0191049036127698		[learning rate: 2.6396e-05]
	Learning Rate: 2.6396e-05
	LOSS [training: 0.0191049036127698 | validation: 0.027388085736838354]
	TIME [epoch: 5.68 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020137262903374156		[learning rate: 2.6303e-05]
	Learning Rate: 2.63027e-05
	LOSS [training: 0.020137262903374156 | validation: 0.02794767850044784]
	TIME [epoch: 5.68 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020081998124489617		[learning rate: 2.621e-05]
	Learning Rate: 2.62097e-05
	LOSS [training: 0.020081998124489617 | validation: 0.029604254765337335]
	TIME [epoch: 5.68 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020632854667539276		[learning rate: 2.6117e-05]
	Learning Rate: 2.6117e-05
	LOSS [training: 0.020632854667539276 | validation: 0.026984952685566933]
	TIME [epoch: 5.68 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0190940751630882		[learning rate: 2.6025e-05]
	Learning Rate: 2.60246e-05
	LOSS [training: 0.0190940751630882 | validation: 0.024162860797863863]
	TIME [epoch: 5.68 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01998744273029806		[learning rate: 2.5933e-05]
	Learning Rate: 2.59326e-05
	LOSS [training: 0.01998744273029806 | validation: 0.025686029147440493]
	TIME [epoch: 5.68 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020081067321365945		[learning rate: 2.5841e-05]
	Learning Rate: 2.58409e-05
	LOSS [training: 0.020081067321365945 | validation: 0.030531112978932864]
	TIME [epoch: 5.68 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01903415859104723		[learning rate: 2.575e-05]
	Learning Rate: 2.57495e-05
	LOSS [training: 0.01903415859104723 | validation: 0.026952472852824972]
	TIME [epoch: 5.68 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018899886496361845		[learning rate: 2.5658e-05]
	Learning Rate: 2.56585e-05
	LOSS [training: 0.018899886496361845 | validation: 0.029542729028410486]
	TIME [epoch: 5.68 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018268768030248125		[learning rate: 2.5568e-05]
	Learning Rate: 2.55677e-05
	LOSS [training: 0.018268768030248125 | validation: 0.027039346380874532]
	TIME [epoch: 5.68 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01814325233069421		[learning rate: 2.5477e-05]
	Learning Rate: 2.54773e-05
	LOSS [training: 0.01814325233069421 | validation: 0.026576127763902824]
	TIME [epoch: 5.68 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019428635001095717		[learning rate: 2.5387e-05]
	Learning Rate: 2.53872e-05
	LOSS [training: 0.019428635001095717 | validation: 0.027275685386132054]
	TIME [epoch: 5.68 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018447234721851245		[learning rate: 2.5297e-05]
	Learning Rate: 2.52975e-05
	LOSS [training: 0.018447234721851245 | validation: 0.0319801032180576]
	TIME [epoch: 5.68 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01931012954450834		[learning rate: 2.5208e-05]
	Learning Rate: 2.5208e-05
	LOSS [training: 0.01931012954450834 | validation: 0.028466838688899654]
	TIME [epoch: 5.68 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019471065510340963		[learning rate: 2.5119e-05]
	Learning Rate: 2.51189e-05
	LOSS [training: 0.019471065510340963 | validation: 0.02644443132406279]
	TIME [epoch: 5.69 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018855751084560928		[learning rate: 2.503e-05]
	Learning Rate: 2.503e-05
	LOSS [training: 0.018855751084560928 | validation: 0.02712028983802789]
	TIME [epoch: 5.69 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01951771386181334		[learning rate: 2.4942e-05]
	Learning Rate: 2.49415e-05
	LOSS [training: 0.01951771386181334 | validation: 0.02604112593809608]
	TIME [epoch: 5.68 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019846234043239812		[learning rate: 2.4853e-05]
	Learning Rate: 2.48533e-05
	LOSS [training: 0.019846234043239812 | validation: 0.026091114082026568]
	TIME [epoch: 5.68 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018898199036655124		[learning rate: 2.4765e-05]
	Learning Rate: 2.47655e-05
	LOSS [training: 0.018898199036655124 | validation: 0.024983019500097972]
	TIME [epoch: 5.68 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018679126291168947		[learning rate: 2.4678e-05]
	Learning Rate: 2.46779e-05
	LOSS [training: 0.018679126291168947 | validation: 0.030128025856023945]
	TIME [epoch: 5.68 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020352882429920602		[learning rate: 2.4591e-05]
	Learning Rate: 2.45906e-05
	LOSS [training: 0.020352882429920602 | validation: 0.028749634603838583]
	TIME [epoch: 5.68 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019946627079988417		[learning rate: 2.4504e-05]
	Learning Rate: 2.45037e-05
	LOSS [training: 0.019946627079988417 | validation: 0.02475567717521541]
	TIME [epoch: 5.68 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01887118843193593		[learning rate: 2.4417e-05]
	Learning Rate: 2.4417e-05
	LOSS [training: 0.01887118843193593 | validation: 0.030629835971352094]
	TIME [epoch: 5.68 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01942664735467911		[learning rate: 2.4331e-05]
	Learning Rate: 2.43307e-05
	LOSS [training: 0.01942664735467911 | validation: 0.027754718513454613]
	TIME [epoch: 5.68 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019567787809302146		[learning rate: 2.4245e-05]
	Learning Rate: 2.42446e-05
	LOSS [training: 0.019567787809302146 | validation: 0.02789872179892139]
	TIME [epoch: 5.68 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017910757134509778		[learning rate: 2.4159e-05]
	Learning Rate: 2.41589e-05
	LOSS [training: 0.017910757134509778 | validation: 0.026344610662230074]
	TIME [epoch: 5.69 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01846996142861294		[learning rate: 2.4073e-05]
	Learning Rate: 2.40735e-05
	LOSS [training: 0.01846996142861294 | validation: 0.029753002718337584]
	TIME [epoch: 5.68 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019973550803796293		[learning rate: 2.3988e-05]
	Learning Rate: 2.39883e-05
	LOSS [training: 0.019973550803796293 | validation: 0.028181441082266237]
	TIME [epoch: 5.68 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018109916707875373		[learning rate: 2.3904e-05]
	Learning Rate: 2.39035e-05
	LOSS [training: 0.018109916707875373 | validation: 0.024707702616734897]
	TIME [epoch: 5.68 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01856350191213031		[learning rate: 2.3819e-05]
	Learning Rate: 2.3819e-05
	LOSS [training: 0.01856350191213031 | validation: 0.027952571058721256]
	TIME [epoch: 5.68 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01925030784640453		[learning rate: 2.3735e-05]
	Learning Rate: 2.37347e-05
	LOSS [training: 0.01925030784640453 | validation: 0.02797877708849944]
	TIME [epoch: 5.69 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01965768210188664		[learning rate: 2.3651e-05]
	Learning Rate: 2.36508e-05
	LOSS [training: 0.01965768210188664 | validation: 0.026336550140776807]
	TIME [epoch: 5.68 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01868865773746187		[learning rate: 2.3567e-05]
	Learning Rate: 2.35672e-05
	LOSS [training: 0.01868865773746187 | validation: 0.025898872161979714]
	TIME [epoch: 5.68 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01917838072887656		[learning rate: 2.3484e-05]
	Learning Rate: 2.34838e-05
	LOSS [training: 0.01917838072887656 | validation: 0.028267314415421452]
	TIME [epoch: 5.68 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018055770660739847		[learning rate: 2.3401e-05]
	Learning Rate: 2.34008e-05
	LOSS [training: 0.018055770660739847 | validation: 0.024185880458912346]
	TIME [epoch: 5.68 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0172104828387868		[learning rate: 2.3318e-05]
	Learning Rate: 2.33181e-05
	LOSS [training: 0.0172104828387868 | validation: 0.027709034728348472]
	TIME [epoch: 5.68 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01841454794973988		[learning rate: 2.3236e-05]
	Learning Rate: 2.32356e-05
	LOSS [training: 0.01841454794973988 | validation: 0.030285992147258623]
	TIME [epoch: 5.68 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020323124226574275		[learning rate: 2.3153e-05]
	Learning Rate: 2.31534e-05
	LOSS [training: 0.020323124226574275 | validation: 0.027749554240234255]
	TIME [epoch: 5.68 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01998523114531479		[learning rate: 2.3072e-05]
	Learning Rate: 2.30716e-05
	LOSS [training: 0.01998523114531479 | validation: 0.02816061865421328]
	TIME [epoch: 5.69 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01851773517528127		[learning rate: 2.299e-05]
	Learning Rate: 2.299e-05
	LOSS [training: 0.01851773517528127 | validation: 0.029419465544521295]
	TIME [epoch: 5.68 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0179713785778346		[learning rate: 2.2909e-05]
	Learning Rate: 2.29087e-05
	LOSS [training: 0.0179713785778346 | validation: 0.024785858846684328]
	TIME [epoch: 5.69 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02027403102331352		[learning rate: 2.2828e-05]
	Learning Rate: 2.28277e-05
	LOSS [training: 0.02027403102331352 | validation: 0.025812454838458412]
	TIME [epoch: 5.68 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019728176562335765		[learning rate: 2.2747e-05]
	Learning Rate: 2.27469e-05
	LOSS [training: 0.019728176562335765 | validation: 0.027816641268019082]
	TIME [epoch: 5.68 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01965797555788206		[learning rate: 2.2667e-05]
	Learning Rate: 2.26665e-05
	LOSS [training: 0.01965797555788206 | validation: 0.02876489408982298]
	TIME [epoch: 5.68 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018742289518620187		[learning rate: 2.2586e-05]
	Learning Rate: 2.25864e-05
	LOSS [training: 0.018742289518620187 | validation: 0.02818645376147502]
	TIME [epoch: 5.68 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01928926317118826		[learning rate: 2.2506e-05]
	Learning Rate: 2.25065e-05
	LOSS [training: 0.01928926317118826 | validation: 0.029780491970784008]
	TIME [epoch: 5.69 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019432875028241866		[learning rate: 2.2427e-05]
	Learning Rate: 2.24269e-05
	LOSS [training: 0.019432875028241866 | validation: 0.027101346663211935]
	TIME [epoch: 5.68 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01867905695624529		[learning rate: 2.2348e-05]
	Learning Rate: 2.23476e-05
	LOSS [training: 0.01867905695624529 | validation: 0.030013499334713124]
	TIME [epoch: 5.68 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01883953980278424		[learning rate: 2.2269e-05]
	Learning Rate: 2.22686e-05
	LOSS [training: 0.01883953980278424 | validation: 0.02509804038607545]
	TIME [epoch: 5.67 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01993710273384601		[learning rate: 2.219e-05]
	Learning Rate: 2.21898e-05
	LOSS [training: 0.01993710273384601 | validation: 0.02786642980232137]
	TIME [epoch: 5.68 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020039558347268054		[learning rate: 2.2111e-05]
	Learning Rate: 2.21114e-05
	LOSS [training: 0.020039558347268054 | validation: 0.027492054337770935]
	TIME [epoch: 5.68 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019503684865008804		[learning rate: 2.2033e-05]
	Learning Rate: 2.20332e-05
	LOSS [training: 0.019503684865008804 | validation: 0.026614371786443537]
	TIME [epoch: 5.68 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019722144695689517		[learning rate: 2.1955e-05]
	Learning Rate: 2.19553e-05
	LOSS [training: 0.019722144695689517 | validation: 0.024404739379634124]
	TIME [epoch: 5.68 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02018178199854673		[learning rate: 2.1878e-05]
	Learning Rate: 2.18776e-05
	LOSS [training: 0.02018178199854673 | validation: 0.029207127079918106]
	TIME [epoch: 5.69 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01814608746428446		[learning rate: 2.18e-05]
	Learning Rate: 2.18003e-05
	LOSS [training: 0.01814608746428446 | validation: 0.02554641597087356]
	TIME [epoch: 5.68 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018750778044480207		[learning rate: 2.1723e-05]
	Learning Rate: 2.17232e-05
	LOSS [training: 0.018750778044480207 | validation: 0.030472944279671434]
	TIME [epoch: 5.68 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020063565636786553		[learning rate: 2.1646e-05]
	Learning Rate: 2.16464e-05
	LOSS [training: 0.020063565636786553 | validation: 0.02816187420121692]
	TIME [epoch: 5.68 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019344478909273634		[learning rate: 2.157e-05]
	Learning Rate: 2.15698e-05
	LOSS [training: 0.019344478909273634 | validation: 0.024283489466503695]
	TIME [epoch: 5.68 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018460811044203468		[learning rate: 2.1494e-05]
	Learning Rate: 2.14935e-05
	LOSS [training: 0.018460811044203468 | validation: 0.028420276668641598]
	TIME [epoch: 5.68 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019404759487599982		[learning rate: 2.1418e-05]
	Learning Rate: 2.14175e-05
	LOSS [training: 0.019404759487599982 | validation: 0.023918896570949456]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1786.pth
	Model improved!!!
EPOCH 1787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019707428795768145		[learning rate: 2.1342e-05]
	Learning Rate: 2.13418e-05
	LOSS [training: 0.019707428795768145 | validation: 0.02761596374365749]
	TIME [epoch: 5.69 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018856186840630264		[learning rate: 2.1266e-05]
	Learning Rate: 2.12663e-05
	LOSS [training: 0.018856186840630264 | validation: 0.02916676371921544]
	TIME [epoch: 5.69 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019505439223504697		[learning rate: 2.1191e-05]
	Learning Rate: 2.11911e-05
	LOSS [training: 0.019505439223504697 | validation: 0.032801915623066946]
	TIME [epoch: 5.68 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020660058916208843		[learning rate: 2.1116e-05]
	Learning Rate: 2.11162e-05
	LOSS [training: 0.020660058916208843 | validation: 0.0281675800756239]
	TIME [epoch: 5.68 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019081208900086383		[learning rate: 2.1042e-05]
	Learning Rate: 2.10415e-05
	LOSS [training: 0.019081208900086383 | validation: 0.032004406562018105]
	TIME [epoch: 5.68 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01977166142575394		[learning rate: 2.0967e-05]
	Learning Rate: 2.09671e-05
	LOSS [training: 0.01977166142575394 | validation: 0.028380672516434047]
	TIME [epoch: 5.68 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02013718064657782		[learning rate: 2.0893e-05]
	Learning Rate: 2.0893e-05
	LOSS [training: 0.02013718064657782 | validation: 0.030065895235762353]
	TIME [epoch: 5.69 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018212406068426883		[learning rate: 2.0819e-05]
	Learning Rate: 2.08191e-05
	LOSS [training: 0.018212406068426883 | validation: 0.026238447681958245]
	TIME [epoch: 5.68 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019212195361127036		[learning rate: 2.0745e-05]
	Learning Rate: 2.07455e-05
	LOSS [training: 0.019212195361127036 | validation: 0.029683418753983327]
	TIME [epoch: 5.68 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017631449200443343		[learning rate: 2.0672e-05]
	Learning Rate: 2.06721e-05
	LOSS [training: 0.017631449200443343 | validation: 0.025327618651406505]
	TIME [epoch: 5.68 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019641818663384356		[learning rate: 2.0599e-05]
	Learning Rate: 2.0599e-05
	LOSS [training: 0.019641818663384356 | validation: 0.0296138281829922]
	TIME [epoch: 5.68 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019440564264986568		[learning rate: 2.0526e-05]
	Learning Rate: 2.05262e-05
	LOSS [training: 0.019440564264986568 | validation: 0.027739527443095126]
	TIME [epoch: 5.68 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020395842569155137		[learning rate: 2.0454e-05]
	Learning Rate: 2.04536e-05
	LOSS [training: 0.020395842569155137 | validation: 0.03049294826841427]
	TIME [epoch: 5.69 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01953855131310827		[learning rate: 2.0381e-05]
	Learning Rate: 2.03812e-05
	LOSS [training: 0.01953855131310827 | validation: 0.028899273043657484]
	TIME [epoch: 5.69 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01995503778041526		[learning rate: 2.0309e-05]
	Learning Rate: 2.03092e-05
	LOSS [training: 0.01995503778041526 | validation: 0.029165974787264706]
	TIME [epoch: 5.69 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01936959627147718		[learning rate: 2.0237e-05]
	Learning Rate: 2.02374e-05
	LOSS [training: 0.01936959627147718 | validation: 0.028480046425665796]
	TIME [epoch: 5.69 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018607397039137497		[learning rate: 2.0166e-05]
	Learning Rate: 2.01658e-05
	LOSS [training: 0.018607397039137497 | validation: 0.028843559505704164]
	TIME [epoch: 5.69 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020466048161773644		[learning rate: 2.0094e-05]
	Learning Rate: 2.00945e-05
	LOSS [training: 0.020466048161773644 | validation: 0.02853091811680343]
	TIME [epoch: 5.69 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019597442519670136		[learning rate: 2.0023e-05]
	Learning Rate: 2.00234e-05
	LOSS [training: 0.019597442519670136 | validation: 0.027661344046360316]
	TIME [epoch: 5.68 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01996559313816982		[learning rate: 1.9953e-05]
	Learning Rate: 1.99526e-05
	LOSS [training: 0.01996559313816982 | validation: 0.02794025348943543]
	TIME [epoch: 5.68 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019740077478350795		[learning rate: 1.9882e-05]
	Learning Rate: 1.98821e-05
	LOSS [training: 0.019740077478350795 | validation: 0.024730764805865092]
	TIME [epoch: 5.68 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019655358035959542		[learning rate: 1.9812e-05]
	Learning Rate: 1.98118e-05
	LOSS [training: 0.019655358035959542 | validation: 0.025879658752063873]
	TIME [epoch: 5.68 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019253190605900968		[learning rate: 1.9742e-05]
	Learning Rate: 1.97417e-05
	LOSS [training: 0.019253190605900968 | validation: 0.02840628457997363]
	TIME [epoch: 5.68 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01796522713181628		[learning rate: 1.9672e-05]
	Learning Rate: 1.96719e-05
	LOSS [training: 0.01796522713181628 | validation: 0.028849201134869898]
	TIME [epoch: 5.68 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019459112927840038		[learning rate: 1.9602e-05]
	Learning Rate: 1.96023e-05
	LOSS [training: 0.019459112927840038 | validation: 0.025023328720326532]
	TIME [epoch: 5.69 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019244483142088382		[learning rate: 1.9533e-05]
	Learning Rate: 1.9533e-05
	LOSS [training: 0.019244483142088382 | validation: 0.02799379368748909]
	TIME [epoch: 5.68 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019127304340421616		[learning rate: 1.9464e-05]
	Learning Rate: 1.94639e-05
	LOSS [training: 0.019127304340421616 | validation: 0.032509986740938766]
	TIME [epoch: 5.68 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020221362990423656		[learning rate: 1.9395e-05]
	Learning Rate: 1.93951e-05
	LOSS [training: 0.020221362990423656 | validation: 0.030730124853309473]
	TIME [epoch: 5.68 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018786801931475878		[learning rate: 1.9327e-05]
	Learning Rate: 1.93265e-05
	LOSS [training: 0.018786801931475878 | validation: 0.028867449614805554]
	TIME [epoch: 5.69 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020221364120910724		[learning rate: 1.9258e-05]
	Learning Rate: 1.92582e-05
	LOSS [training: 0.020221364120910724 | validation: 0.026376104973875783]
	TIME [epoch: 5.68 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018486595101857732		[learning rate: 1.919e-05]
	Learning Rate: 1.91901e-05
	LOSS [training: 0.018486595101857732 | validation: 0.027631480983709766]
	TIME [epoch: 5.68 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019589462389295778		[learning rate: 1.9122e-05]
	Learning Rate: 1.91222e-05
	LOSS [training: 0.019589462389295778 | validation: 0.03152987852388406]
	TIME [epoch: 5.68 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019540221555685734		[learning rate: 1.9055e-05]
	Learning Rate: 1.90546e-05
	LOSS [training: 0.019540221555685734 | validation: 0.0251279599738784]
	TIME [epoch: 5.68 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018795569473370268		[learning rate: 1.8987e-05]
	Learning Rate: 1.89872e-05
	LOSS [training: 0.018795569473370268 | validation: 0.028491698773024876]
	TIME [epoch: 5.68 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018703797931439975		[learning rate: 1.892e-05]
	Learning Rate: 1.89201e-05
	LOSS [training: 0.018703797931439975 | validation: 0.02495366796102824]
	TIME [epoch: 5.68 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01987431373103091		[learning rate: 1.8853e-05]
	Learning Rate: 1.88532e-05
	LOSS [training: 0.01987431373103091 | validation: 0.027373630372181258]
	TIME [epoch: 5.68 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019728190983325372		[learning rate: 1.8787e-05]
	Learning Rate: 1.87865e-05
	LOSS [training: 0.019728190983325372 | validation: 0.02988735990499444]
	TIME [epoch: 5.68 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019036001122384585		[learning rate: 1.872e-05]
	Learning Rate: 1.87201e-05
	LOSS [training: 0.019036001122384585 | validation: 0.028714065806089054]
	TIME [epoch: 5.68 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01851675538096179		[learning rate: 1.8654e-05]
	Learning Rate: 1.86539e-05
	LOSS [training: 0.01851675538096179 | validation: 0.027651821171338378]
	TIME [epoch: 5.68 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018947620402223638		[learning rate: 1.8588e-05]
	Learning Rate: 1.85879e-05
	LOSS [training: 0.018947620402223638 | validation: 0.03264382220010015]
	TIME [epoch: 5.68 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01821684228576226		[learning rate: 1.8522e-05]
	Learning Rate: 1.85222e-05
	LOSS [training: 0.01821684228576226 | validation: 0.029577907100077818]
	TIME [epoch: 5.68 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019156409384986178		[learning rate: 1.8457e-05]
	Learning Rate: 1.84567e-05
	LOSS [training: 0.019156409384986178 | validation: 0.02563703478638606]
	TIME [epoch: 5.68 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019005026505522042		[learning rate: 1.8391e-05]
	Learning Rate: 1.83914e-05
	LOSS [training: 0.019005026505522042 | validation: 0.027522962240647975]
	TIME [epoch: 5.68 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01710464917809276		[learning rate: 1.8326e-05]
	Learning Rate: 1.83264e-05
	LOSS [training: 0.01710464917809276 | validation: 0.027782360156792543]
	TIME [epoch: 5.69 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021017282419393126		[learning rate: 1.8262e-05]
	Learning Rate: 1.82616e-05
	LOSS [training: 0.021017282419393126 | validation: 0.02559707296593146]
	TIME [epoch: 5.68 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018762373429663727		[learning rate: 1.8197e-05]
	Learning Rate: 1.8197e-05
	LOSS [training: 0.018762373429663727 | validation: 0.025945450326817134]
	TIME [epoch: 5.68 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0196776367036911		[learning rate: 1.8133e-05]
	Learning Rate: 1.81327e-05
	LOSS [training: 0.0196776367036911 | validation: 0.027732032533371465]
	TIME [epoch: 5.69 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019123754223782742		[learning rate: 1.8069e-05]
	Learning Rate: 1.80685e-05
	LOSS [training: 0.019123754223782742 | validation: 0.027676409603088203]
	TIME [epoch: 5.68 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018502572088029606		[learning rate: 1.8005e-05]
	Learning Rate: 1.80047e-05
	LOSS [training: 0.018502572088029606 | validation: 0.03015296688695355]
	TIME [epoch: 5.69 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021231134310154342		[learning rate: 1.7941e-05]
	Learning Rate: 1.7941e-05
	LOSS [training: 0.021231134310154342 | validation: 0.026597994908821723]
	TIME [epoch: 5.68 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018514771916647903		[learning rate: 1.7878e-05]
	Learning Rate: 1.78775e-05
	LOSS [training: 0.018514771916647903 | validation: 0.029026525393897187]
	TIME [epoch: 5.68 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01950232748475076		[learning rate: 1.7814e-05]
	Learning Rate: 1.78143e-05
	LOSS [training: 0.01950232748475076 | validation: 0.025738965127395777]
	TIME [epoch: 5.68 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01943631012944822		[learning rate: 1.7751e-05]
	Learning Rate: 1.77513e-05
	LOSS [training: 0.01943631012944822 | validation: 0.027135251166240083]
	TIME [epoch: 5.68 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020188661741461163		[learning rate: 1.7689e-05]
	Learning Rate: 1.76886e-05
	LOSS [training: 0.020188661741461163 | validation: 0.03136123890343744]
	TIME [epoch: 5.68 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01755890063719557		[learning rate: 1.7626e-05]
	Learning Rate: 1.7626e-05
	LOSS [training: 0.01755890063719557 | validation: 0.02966822486300398]
	TIME [epoch: 5.68 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01817176904255473		[learning rate: 1.7564e-05]
	Learning Rate: 1.75637e-05
	LOSS [training: 0.01817176904255473 | validation: 0.024795139282640113]
	TIME [epoch: 5.68 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019992421431793105		[learning rate: 1.7502e-05]
	Learning Rate: 1.75016e-05
	LOSS [training: 0.019992421431793105 | validation: 0.03012717921960656]
	TIME [epoch: 5.68 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018718381231418477		[learning rate: 1.744e-05]
	Learning Rate: 1.74397e-05
	LOSS [training: 0.018718381231418477 | validation: 0.029543540978392548]
	TIME [epoch: 5.68 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019768073251400533		[learning rate: 1.7378e-05]
	Learning Rate: 1.7378e-05
	LOSS [training: 0.019768073251400533 | validation: 0.029982203628489936]
	TIME [epoch: 5.68 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01920395417297156		[learning rate: 1.7317e-05]
	Learning Rate: 1.73166e-05
	LOSS [training: 0.01920395417297156 | validation: 0.03258378708142129]
	TIME [epoch: 5.68 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01789930247031191		[learning rate: 1.7255e-05]
	Learning Rate: 1.72553e-05
	LOSS [training: 0.01789930247031191 | validation: 0.02910842234564214]
	TIME [epoch: 5.68 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018873356651651355		[learning rate: 1.7194e-05]
	Learning Rate: 1.71943e-05
	LOSS [training: 0.018873356651651355 | validation: 0.030326961104261453]
	TIME [epoch: 5.68 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02005595842853019		[learning rate: 1.7134e-05]
	Learning Rate: 1.71335e-05
	LOSS [training: 0.02005595842853019 | validation: 0.02973036909740702]
	TIME [epoch: 5.68 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017672001404492148		[learning rate: 1.7073e-05]
	Learning Rate: 1.70729e-05
	LOSS [training: 0.017672001404492148 | validation: 0.025568452492431806]
	TIME [epoch: 5.68 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018737298110659297		[learning rate: 1.7013e-05]
	Learning Rate: 1.70125e-05
	LOSS [training: 0.018737298110659297 | validation: 0.02691323608404657]
	TIME [epoch: 5.69 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018250865112873434		[learning rate: 1.6952e-05]
	Learning Rate: 1.69524e-05
	LOSS [training: 0.018250865112873434 | validation: 0.027194097560492216]
	TIME [epoch: 5.68 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018826731508160003		[learning rate: 1.6892e-05]
	Learning Rate: 1.68924e-05
	LOSS [training: 0.018826731508160003 | validation: 0.029083840367554195]
	TIME [epoch: 5.68 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017676405334689112		[learning rate: 1.6833e-05]
	Learning Rate: 1.68327e-05
	LOSS [training: 0.017676405334689112 | validation: 0.02721900789713948]
	TIME [epoch: 5.68 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017776817972332208		[learning rate: 1.6773e-05]
	Learning Rate: 1.67732e-05
	LOSS [training: 0.017776817972332208 | validation: 0.028566794749435732]
	TIME [epoch: 5.68 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02050451770459345		[learning rate: 1.6714e-05]
	Learning Rate: 1.67139e-05
	LOSS [training: 0.02050451770459345 | validation: 0.031102165372627246]
	TIME [epoch: 5.69 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01934156072514206		[learning rate: 1.6655e-05]
	Learning Rate: 1.66548e-05
	LOSS [training: 0.01934156072514206 | validation: 0.026632201810091585]
	TIME [epoch: 5.68 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01873053134116395		[learning rate: 1.6596e-05]
	Learning Rate: 1.65959e-05
	LOSS [training: 0.01873053134116395 | validation: 0.02913210891055712]
	TIME [epoch: 5.68 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018929236316593264		[learning rate: 1.6537e-05]
	Learning Rate: 1.65372e-05
	LOSS [training: 0.018929236316593264 | validation: 0.025523166604811832]
	TIME [epoch: 5.69 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019591864821038737		[learning rate: 1.6479e-05]
	Learning Rate: 1.64787e-05
	LOSS [training: 0.019591864821038737 | validation: 0.025653489928868102]
	TIME [epoch: 5.68 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018947426707678586		[learning rate: 1.642e-05]
	Learning Rate: 1.64204e-05
	LOSS [training: 0.018947426707678586 | validation: 0.030183347243144122]
	TIME [epoch: 5.69 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019151905133568178		[learning rate: 1.6362e-05]
	Learning Rate: 1.63624e-05
	LOSS [training: 0.019151905133568178 | validation: 0.02757346461558674]
	TIME [epoch: 5.68 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017909794445934754		[learning rate: 1.6305e-05]
	Learning Rate: 1.63045e-05
	LOSS [training: 0.017909794445934754 | validation: 0.02771083021060592]
	TIME [epoch: 5.68 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018658202227696204		[learning rate: 1.6247e-05]
	Learning Rate: 1.62469e-05
	LOSS [training: 0.018658202227696204 | validation: 0.02716840943968245]
	TIME [epoch: 5.68 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01897496773211787		[learning rate: 1.6189e-05]
	Learning Rate: 1.61894e-05
	LOSS [training: 0.01897496773211787 | validation: 0.0292412651847502]
	TIME [epoch: 5.68 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018698125559961525		[learning rate: 1.6132e-05]
	Learning Rate: 1.61322e-05
	LOSS [training: 0.018698125559961525 | validation: 0.024855558925452704]
	TIME [epoch: 5.68 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01883422013746962		[learning rate: 1.6075e-05]
	Learning Rate: 1.60751e-05
	LOSS [training: 0.01883422013746962 | validation: 0.02786160527203604]
	TIME [epoch: 5.69 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019154568761260305		[learning rate: 1.6018e-05]
	Learning Rate: 1.60183e-05
	LOSS [training: 0.019154568761260305 | validation: 0.025930067448684382]
	TIME [epoch: 5.68 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019150436417404555		[learning rate: 1.5962e-05]
	Learning Rate: 1.59616e-05
	LOSS [training: 0.019150436417404555 | validation: 0.029186485778040394]
	TIME [epoch: 5.68 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019803693830056073		[learning rate: 1.5905e-05]
	Learning Rate: 1.59052e-05
	LOSS [training: 0.019803693830056073 | validation: 0.030646053455624725]
	TIME [epoch: 5.68 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01904749222220103		[learning rate: 1.5849e-05]
	Learning Rate: 1.58489e-05
	LOSS [training: 0.01904749222220103 | validation: 0.026280715343309547]
	TIME [epoch: 5.68 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01808430626830861		[learning rate: 1.5793e-05]
	Learning Rate: 1.57929e-05
	LOSS [training: 0.01808430626830861 | validation: 0.0276166605447187]
	TIME [epoch: 5.69 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018455359045296733		[learning rate: 1.5737e-05]
	Learning Rate: 1.5737e-05
	LOSS [training: 0.018455359045296733 | validation: 0.028359777199549952]
	TIME [epoch: 5.68 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019147726309228934		[learning rate: 1.5681e-05]
	Learning Rate: 1.56814e-05
	LOSS [training: 0.019147726309228934 | validation: 0.024219342716631887]
	TIME [epoch: 5.68 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019430626091273575		[learning rate: 1.5626e-05]
	Learning Rate: 1.56259e-05
	LOSS [training: 0.019430626091273575 | validation: 0.025678507918776128]
	TIME [epoch: 5.68 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017946516397934615		[learning rate: 1.5571e-05]
	Learning Rate: 1.55707e-05
	LOSS [training: 0.017946516397934615 | validation: 0.024875391148585436]
	TIME [epoch: 5.69 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01740904896842402		[learning rate: 1.5516e-05]
	Learning Rate: 1.55156e-05
	LOSS [training: 0.01740904896842402 | validation: 0.027605868891815058]
	TIME [epoch: 5.68 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019018042134379683		[learning rate: 1.5461e-05]
	Learning Rate: 1.54608e-05
	LOSS [training: 0.019018042134379683 | validation: 0.025976786280150422]
	TIME [epoch: 5.68 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016924170175852606		[learning rate: 1.5406e-05]
	Learning Rate: 1.54061e-05
	LOSS [training: 0.016924170175852606 | validation: 0.027207514332551377]
	TIME [epoch: 5.68 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0189396835629641		[learning rate: 1.5352e-05]
	Learning Rate: 1.53516e-05
	LOSS [training: 0.0189396835629641 | validation: 0.02734117478720166]
	TIME [epoch: 5.69 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019271412967625946		[learning rate: 1.5297e-05]
	Learning Rate: 1.52973e-05
	LOSS [training: 0.019271412967625946 | validation: 0.027271459975188442]
	TIME [epoch: 5.68 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018186753949473587		[learning rate: 1.5243e-05]
	Learning Rate: 1.52432e-05
	LOSS [training: 0.018186753949473587 | validation: 0.024636164783579495]
	TIME [epoch: 5.69 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019492457359399036		[learning rate: 1.5189e-05]
	Learning Rate: 1.51893e-05
	LOSS [training: 0.019492457359399036 | validation: 0.025635870007710004]
	TIME [epoch: 5.68 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017635144540983968		[learning rate: 1.5136e-05]
	Learning Rate: 1.51356e-05
	LOSS [training: 0.017635144540983968 | validation: 0.028707137019288387]
	TIME [epoch: 5.68 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018993226147339032		[learning rate: 1.5082e-05]
	Learning Rate: 1.50821e-05
	LOSS [training: 0.018993226147339032 | validation: 0.02555826624128481]
	TIME [epoch: 5.68 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01864840781512386		[learning rate: 1.5029e-05]
	Learning Rate: 1.50288e-05
	LOSS [training: 0.01864840781512386 | validation: 0.026451898178158597]
	TIME [epoch: 5.68 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017109399672297913		[learning rate: 1.4976e-05]
	Learning Rate: 1.49756e-05
	LOSS [training: 0.017109399672297913 | validation: 0.026015083451597754]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v1_6_v_mmd4_1887.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 7775.707 seconds.
