Args:
Namespace(name='model_phi1_4a_distortion_v2r_4_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2r_4/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2r_4/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.018265799, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2009801662

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.428857018104192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.428857018104192 | validation: 4.121938646375165]
	TIME [epoch: 166 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.468690021281442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.468690021281442 | validation: 5.517746420725257]
	TIME [epoch: 0.814 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.876453525249878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.876453525249878 | validation: 4.3580625661111085]
	TIME [epoch: 0.693 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.3591683539379105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3591683539379105 | validation: 2.956408530649525]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.313882004908421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.313882004908421 | validation: 3.16410954438836]
	TIME [epoch: 0.696 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.198416127204929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.198416127204929 | validation: 3.368679357307743]
	TIME [epoch: 0.694 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0021098552467014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0021098552467014 | validation: 2.6112021417409963]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7898856629199993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7898856629199993 | validation: 2.901600789444989]
	TIME [epoch: 0.693 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.643343407985045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.643343407985045 | validation: 2.7659162869169656]
	TIME [epoch: 0.696 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5363378280811713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5363378280811713 | validation: 3.1303374842218843]
	TIME [epoch: 0.691 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4930375589984575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4930375589984575 | validation: 2.5636980840927612]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8012292435488564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8012292435488564 | validation: 2.6198807863149267]
	TIME [epoch: 0.696 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2118954599644893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2118954599644893 | validation: 2.792371021352743]
	TIME [epoch: 0.696 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2259418409616467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2259418409616467 | validation: 2.3445520239989333]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.276046530750406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.276046530750406 | validation: 2.4180383179909417]
	TIME [epoch: 0.698 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.05924734132325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.05924734132325 | validation: 2.388584671689167]
	TIME [epoch: 0.696 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9880894221057994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9880894221057994 | validation: 2.1652369025910967]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9527595250276677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9527595250276677 | validation: 2.380934508341018]
	TIME [epoch: 0.695 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9377459014006615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9377459014006615 | validation: 2.035481405767564]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.032724421123744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.032724421123744 | validation: 2.592576931654823]
	TIME [epoch: 0.693 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0171608014479396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0171608014479396 | validation: 1.9664538337397823]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.062129459931801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.062129459931801 | validation: 1.8704558576643118]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7710501240694712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7710501240694712 | validation: 2.2806545062329335]
	TIME [epoch: 0.696 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8468366840462351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8468366840462351 | validation: 1.7731434626855063]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8124559641346907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8124559641346907 | validation: 1.6956536642954205]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6801941071502358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6801941071502358 | validation: 1.989928428697821]
	TIME [epoch: 0.694 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6769436183354756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6769436183354756 | validation: 1.6463549630304648]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6603763197484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6603763197484 | validation: 1.6095777369526694]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5820001075508578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5820001075508578 | validation: 1.7327985306541551]
	TIME [epoch: 0.696 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5528496679785215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5528496679785215 | validation: 1.4802087148602046]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5270692820922636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5270692820922636 | validation: 1.5330035808541638]
	TIME [epoch: 0.693 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4869372839377928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4869372839377928 | validation: 1.5301318102246482]
	TIME [epoch: 0.705 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4703015841773042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4703015841773042 | validation: 1.3739380749203611]
	TIME [epoch: 0.747 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4488912643970655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4488912643970655 | validation: 1.5587399342162513]
	TIME [epoch: 0.692 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4714804370006638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4714804370006638 | validation: 1.3247021224951216]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5174327131736214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5174327131736214 | validation: 1.6214932003270506]
	TIME [epoch: 0.694 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5465797095648055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5465797095648055 | validation: 1.3813247994122644]
	TIME [epoch: 0.968 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3785634989518014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3785634989518014 | validation: 1.1791545458046844]
	TIME [epoch: 0.766 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3907518433908044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3907518433908044 | validation: 1.5229724892420151]
	TIME [epoch: 0.694 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4009961773875808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4009961773875808 | validation: 1.153885103958774]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3660787980882523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3660787980882523 | validation: 1.3476321024017233]
	TIME [epoch: 0.694 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.349256207577692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.349256207577692 | validation: 1.2940128525848504]
	TIME [epoch: 0.691 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3881121790645623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3881121790645623 | validation: 1.2948596883840384]
	TIME [epoch: 0.691 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3514517146181992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3514517146181992 | validation: 1.210951656619494]
	TIME [epoch: 0.692 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3379574767564146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3379574767564146 | validation: 1.2785958963612956]
	TIME [epoch: 0.692 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2976347885945412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2976347885945412 | validation: 1.1058285883075656]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.295302808731568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.295302808731568 | validation: 1.321173854758546]
	TIME [epoch: 0.694 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2844552234695943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2844552234695943 | validation: 1.0993410918350583]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3226611553401446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3226611553401446 | validation: 1.288613374104543]
	TIME [epoch: 0.695 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2806803954478658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2806803954478658 | validation: 1.0708755876691514]
	TIME [epoch: 0.731 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2625775173269198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2625775173269198 | validation: 1.158947726007985]
	TIME [epoch: 0.697 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.238982487702479		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.238982487702479 | validation: 1.1784656394524906]
	TIME [epoch: 0.694 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2440750578982858		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.2440750578982858 | validation: 1.0775892637997608]
	TIME [epoch: 0.693 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2889785826491182		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.2889785826491182 | validation: 1.5073356142319498]
	TIME [epoch: 0.695 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4108542413104181		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.4108542413104181 | validation: 1.0635237712513756]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2269241491761729		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.2269241491761729 | validation: 1.1208812396217638]
	TIME [epoch: 0.693 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.208404076581918		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.208404076581918 | validation: 1.1407125828078706]
	TIME [epoch: 0.691 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.215744393160209		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.215744393160209 | validation: 1.1356918083959062]
	TIME [epoch: 0.69 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2144719814177405		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.2144719814177405 | validation: 1.0912382508627638]
	TIME [epoch: 0.729 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2422224397304527		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.2422224397304527 | validation: 1.2492210962602535]
	TIME [epoch: 0.705 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2581069291672822		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.2581069291672822 | validation: 1.052722796705041]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3637277203959988		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.3637277203959988 | validation: 1.2294337029099596]
	TIME [epoch: 0.693 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2153748911328333		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.2153748911328333 | validation: 1.0874355116386938]
	TIME [epoch: 0.694 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1990326599503982		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.1990326599503982 | validation: 1.1102333294704827]
	TIME [epoch: 0.691 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2244655175301962		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.2244655175301962 | validation: 1.113586121685797]
	TIME [epoch: 0.691 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2018920583158081		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.2018920583158081 | validation: 1.1114641303156951]
	TIME [epoch: 0.703 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.199546559929476		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.199546559929476 | validation: 1.0748348405598538]
	TIME [epoch: 0.693 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.187573774823838		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.187573774823838 | validation: 1.1360535624478736]
	TIME [epoch: 0.692 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.187407990916575		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.187407990916575 | validation: 1.0311220838246198]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1877798062853697		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.1877798062853697 | validation: 1.326173168535139]
	TIME [epoch: 0.695 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2623703204350158		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.2623703204350158 | validation: 1.0025233433302603]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3210563162454907		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.3210563162454907 | validation: 1.3665329778783446]
	TIME [epoch: 0.693 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2789765733337741		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.2789765733337741 | validation: 1.139392823683206]
	TIME [epoch: 0.692 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1771648236778152		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.1771648236778152 | validation: 1.0064250931427439]
	TIME [epoch: 0.691 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2157819448870253		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.2157819448870253 | validation: 1.1561193734277688]
	TIME [epoch: 0.69 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2027973512299588		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.2027973512299588 | validation: 1.174566499382172]
	TIME [epoch: 0.701 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2130772214125918		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.2130772214125918 | validation: 1.0793539005267492]
	TIME [epoch: 0.69 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2435256015541705		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.2435256015541705 | validation: 1.1097497657279185]
	TIME [epoch: 0.689 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1994158371332038		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.1994158371332038 | validation: 1.1317953814375052]
	TIME [epoch: 0.69 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1868244928885086		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.1868244928885086 | validation: 1.0681392587530945]
	TIME [epoch: 0.69 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1690034283478554		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.1690034283478554 | validation: 1.0675415566631565]
	TIME [epoch: 0.689 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.173848227029699		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.173848227029699 | validation: 1.120287088416875]
	TIME [epoch: 0.691 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1679918647841216		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.1679918647841216 | validation: 1.0461611794132264]
	TIME [epoch: 0.695 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1820313277667263		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.1820313277667263 | validation: 1.219828019552527]
	TIME [epoch: 0.702 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2197627541711584		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.2197627541711584 | validation: 1.0958840776174452]
	TIME [epoch: 0.69 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3693685304938055		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.3693685304938055 | validation: 1.1487553423656114]
	TIME [epoch: 0.69 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1711439804106756		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.1711439804106756 | validation: 1.0814339215800495]
	TIME [epoch: 0.69 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1945977399959424		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.1945977399959424 | validation: 1.1880930634277038]
	TIME [epoch: 0.691 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2335849995341446		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.2335849995341446 | validation: 1.0745597697587457]
	TIME [epoch: 0.691 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1759028270173404		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.1759028270173404 | validation: 1.125508892071138]
	TIME [epoch: 0.702 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1631198322790597		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.1631198322790597 | validation: 1.0461382777119301]
	TIME [epoch: 0.69 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1541083919320878		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.1541083919320878 | validation: 1.1432510142365577]
	TIME [epoch: 0.69 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1690890860042245		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.1690890860042245 | validation: 0.976216656745569]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2076658109756833		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.2076658109756833 | validation: 1.3876748580557763]
	TIME [epoch: 0.694 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2804620193982748		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.2804620193982748 | validation: 0.9785276173984472]
	TIME [epoch: 0.69 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2561306545316544		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.2561306545316544 | validation: 1.0829132474803573]
	TIME [epoch: 0.689 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1561145208980892		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.1561145208980892 | validation: 1.1700816273465224]
	TIME [epoch: 0.69 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1908167780948484		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.1908167780948484 | validation: 1.0666785146419888]
	TIME [epoch: 0.689 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.211579328381788		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.211579328381788 | validation: 1.0649723794976262]
	TIME [epoch: 0.689 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1972514391198348		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.1972514391198348 | validation: 1.2690238538800622]
	TIME [epoch: 0.692 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2343526678170418		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.2343526678170418 | validation: 1.0481348493177627]
	TIME [epoch: 0.694 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.16892083937355		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.16892083937355 | validation: 1.0465585774867836]
	TIME [epoch: 0.691 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1527420469198852		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.1527420469198852 | validation: 1.0805000291259657]
	TIME [epoch: 0.691 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1450880854125374		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.1450880854125374 | validation: 1.0620400763490672]
	TIME [epoch: 0.691 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1512701237897032		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.1512701237897032 | validation: 1.059015216968146]
	TIME [epoch: 0.69 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1448622146157956		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.1448622146157956 | validation: 1.1045010432987306]
	TIME [epoch: 0.69 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1535790436819082		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.1535790436819082 | validation: 1.0706505820957508]
	TIME [epoch: 0.691 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.219160614164364		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.219160614164364 | validation: 1.466125955663541]
	TIME [epoch: 0.692 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.343768573553569		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.343768573553569 | validation: 0.9789329553421677]
	TIME [epoch: 0.691 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1793144264060222		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.1793144264060222 | validation: 1.2778242961509552]
	TIME [epoch: 0.69 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2447171791027025		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.2447171791027025 | validation: 1.0926463605376835]
	TIME [epoch: 0.69 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2660202441195405		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.2660202441195405 | validation: 1.0512386306500126]
	TIME [epoch: 0.691 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1375582728161033		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.1375582728161033 | validation: 1.177969363895199]
	TIME [epoch: 0.787 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1941991902173974		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.1941991902173974 | validation: 1.0534882148847546]
	TIME [epoch: 0.69 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1935738812426449		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.1935738812426449 | validation: 1.075393220943696]
	TIME [epoch: 0.693 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1400690169426821		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.1400690169426821 | validation: 1.0769859155009127]
	TIME [epoch: 0.69 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.154755970633135		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.154755970633135 | validation: 1.0471070871391772]
	TIME [epoch: 0.69 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1528832375285574		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.1528832375285574 | validation: 1.0971585823636543]
	TIME [epoch: 0.69 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1425162640066935		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.1425162640066935 | validation: 1.033725134876758]
	TIME [epoch: 0.69 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1476061761607457		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.1476061761607457 | validation: 1.1273443558566543]
	TIME [epoch: 0.695 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1503423555454293		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.1503423555454293 | validation: 0.9843117012403287]
	TIME [epoch: 0.69 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.182160065358506		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.182160065358506 | validation: 1.440919089886872]
	TIME [epoch: 0.69 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.29670377668159		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.29670377668159 | validation: 0.9795628046116767]
	TIME [epoch: 0.694 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1881168350102518		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.1881168350102518 | validation: 1.0638996464804131]
	TIME [epoch: 0.69 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1345551637791422		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.1345551637791422 | validation: 1.1089364466761544]
	TIME [epoch: 0.691 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.154483199068786		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.154483199068786 | validation: 1.0111152610292187]
	TIME [epoch: 0.69 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.154126526002167		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.154126526002167 | validation: 1.084366283935266]
	TIME [epoch: 0.692 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1584801585161193		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.1584801585161193 | validation: 1.1629105452895656]
	TIME [epoch: 0.691 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.221177105228935		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.221177105228935 | validation: 1.114412961019512]
	TIME [epoch: 0.696 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.204159106687248		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.204159106687248 | validation: 1.1431038139355172]
	TIME [epoch: 0.701 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1757683701272357		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.1757683701272357 | validation: 1.0119480272035914]
	TIME [epoch: 0.692 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1425597966987844		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.1425597966987844 | validation: 1.0931625363072712]
	TIME [epoch: 0.691 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1349642170280398		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.1349642170280398 | validation: 1.0033076630267002]
	TIME [epoch: 0.691 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.150346037437593		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.150346037437593 | validation: 1.1857695422684824]
	TIME [epoch: 0.689 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1773029018081487		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.1773029018081487 | validation: 0.953877445068844]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2019009198889572		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.2019009198889572 | validation: 1.2066392569701963]
	TIME [epoch: 0.698 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1787354768449		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.1787354768449 | validation: 1.0096337279681578]
	TIME [epoch: 0.696 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1376266143642482		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.1376266143642482 | validation: 1.0590750122197061]
	TIME [epoch: 0.696 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1459512650876367		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.1459512650876367 | validation: 1.1096188887868657]
	TIME [epoch: 0.695 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1622837139362583		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.1622837139362583 | validation: 1.1031001230781161]
	TIME [epoch: 0.695 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.192533689671053		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.192533689671053 | validation: 1.0817189459217804]
	TIME [epoch: 0.695 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1472738707040488		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.1472738707040488 | validation: 1.0453139518467367]
	TIME [epoch: 0.695 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1411388334706205		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.1411388334706205 | validation: 1.0466395201823941]
	TIME [epoch: 0.695 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1284254901645017		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.1284254901645017 | validation: 1.0485176250663684]
	TIME [epoch: 0.694 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1146827706603808		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.1146827706603808 | validation: 1.029451358942985]
	TIME [epoch: 0.693 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1234722960607908		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.1234722960607908 | validation: 1.0843119576906188]
	TIME [epoch: 0.693 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1316293672148665		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.1316293672148665 | validation: 0.9758688880847975]
	TIME [epoch: 0.694 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1983418563998924		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.1983418563998924 | validation: 1.5629979565993974]
	TIME [epoch: 0.704 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3960990459571014		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.3960990459571014 | validation: 1.009962585531618]
	TIME [epoch: 0.703 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1662828461817214		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.1662828461817214 | validation: 1.037044244772076]
	TIME [epoch: 0.695 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2054014615505475		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.2054014615505475 | validation: 1.2812151623252173]
	TIME [epoch: 0.694 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2226278920650333		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.2226278920650333 | validation: 1.0898744812276349]
	TIME [epoch: 0.694 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1394483039868382		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.1394483039868382 | validation: 1.0171655704354354]
	TIME [epoch: 0.693 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1657751180597866		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.1657751180597866 | validation: 1.0848408588475102]
	TIME [epoch: 0.695 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.150997300715747		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.150997300715747 | validation: 1.0961116456019517]
	TIME [epoch: 0.695 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.141510115104056		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.141510115104056 | validation: 1.0172863463220208]
	TIME [epoch: 0.695 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1354967814439914		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.1354967814439914 | validation: 1.05667717182918]
	TIME [epoch: 0.695 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1342207416915309		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.1342207416915309 | validation: 1.0484548513655447]
	TIME [epoch: 0.694 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1290622009859628		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.1290622009859628 | validation: 1.0255396668122267]
	TIME [epoch: 0.693 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1247764383824446		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.1247764383824446 | validation: 1.0678674385186129]
	TIME [epoch: 0.698 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1389918363485012		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.1389918363485012 | validation: 1.0434537579883776]
	TIME [epoch: 0.744 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1474162993767758		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.1474162993767758 | validation: 1.168290214877855]
	TIME [epoch: 0.697 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.182567352227682		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.182567352227682 | validation: 1.051936129876128]
	TIME [epoch: 0.695 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2068280917968617		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.2068280917968617 | validation: 1.0933833239649384]
	TIME [epoch: 0.695 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1348243825799673		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.1348243825799673 | validation: 1.019943528309749]
	TIME [epoch: 0.695 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1257089604511696		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.1257089604511696 | validation: 1.092498082230227]
	TIME [epoch: 0.696 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1469755493748184		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.1469755493748184 | validation: 1.0217885062607193]
	TIME [epoch: 0.694 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1551126155995926		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.1551126155995926 | validation: 1.1957718117393934]
	TIME [epoch: 0.695 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1742125850339227		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.1742125850339227 | validation: 0.9802777414723693]
	TIME [epoch: 0.695 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1617076108850228		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.1617076108850228 | validation: 1.1690275236479664]
	TIME [epoch: 0.7 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1686257508088247		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.1686257508088247 | validation: 0.9844059609273433]
	TIME [epoch: 0.694 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1362968072812225		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.1362968072812225 | validation: 1.071230757421019]
	TIME [epoch: 0.694 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1324836687852144		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.1324836687852144 | validation: 1.0375222093760648]
	TIME [epoch: 0.696 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1249976915871471		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.1249976915871471 | validation: 1.0217572087761677]
	TIME [epoch: 0.697 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.115667321834018		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.115667321834018 | validation: 1.056529464968286]
	TIME [epoch: 0.695 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1269627584269915		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.1269627584269915 | validation: 1.0535840953859261]
	TIME [epoch: 0.696 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.151496556586877		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.151496556586877 | validation: 1.1884343977794345]
	TIME [epoch: 0.695 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1962401825984603		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.1962401825984603 | validation: 1.0299802415429895]
	TIME [epoch: 0.705 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1597463880932797		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.1597463880932797 | validation: 1.1341157305150873]
	TIME [epoch: 0.694 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1434226239614835		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.1434226239614835 | validation: 0.977872397331873]
	TIME [epoch: 0.695 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1596756213627155		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.1596756213627155 | validation: 1.242155377331859]
	TIME [epoch: 0.695 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1966953450311897		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.1966953450311897 | validation: 0.9786058373881507]
	TIME [epoch: 0.696 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1706894596245994		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.1706894596245994 | validation: 1.0650463048476133]
	TIME [epoch: 0.694 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1318168599404357		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.1318168599404357 | validation: 1.0860254633089463]
	TIME [epoch: 0.694 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1344130266307348		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.1344130266307348 | validation: 0.9832615587823992]
	TIME [epoch: 0.699 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1400056611861056		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.1400056611861056 | validation: 1.0880120179069315]
	TIME [epoch: 0.694 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1286670614316119		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.1286670614316119 | validation: 1.014897468823784]
	TIME [epoch: 0.694 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1238467974438013		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.1238467974438013 | validation: 1.0923267952160425]
	TIME [epoch: 0.694 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1310478796060301		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.1310478796060301 | validation: 1.0340746150647682]
	TIME [epoch: 0.696 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1378535113134716		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.1378535113134716 | validation: 1.0966403263556501]
	TIME [epoch: 0.926 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1613850797503742		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.1613850797503742 | validation: 1.0905932994680647]
	TIME [epoch: 0.695 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.175415100883549		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.175415100883549 | validation: 1.056969868965859]
	TIME [epoch: 0.705 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1293373073104382		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.1293373073104382 | validation: 1.0129405466728956]
	TIME [epoch: 0.697 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.115793211837651		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.115793211837651 | validation: 1.1085820247812723]
	TIME [epoch: 0.696 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1451442272044023		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.1451442272044023 | validation: 0.9981534907099427]
	TIME [epoch: 0.696 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1766002493661514		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.1766002493661514 | validation: 1.3183161832137964]
	TIME [epoch: 0.698 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.228145082370589		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.228145082370589 | validation: 0.988733067955128]
	TIME [epoch: 0.698 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1332390190380255		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.1332390190380255 | validation: 1.0271841019569377]
	TIME [epoch: 0.699 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1253843459456152		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.1253843459456152 | validation: 1.1014474536242242]
	TIME [epoch: 0.696 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1417106954932326		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.1417106954932326 | validation: 1.019465786363007]
	TIME [epoch: 0.697 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1296812584459042		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.1296812584459042 | validation: 1.0777741785395472]
	TIME [epoch: 175 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1225097148127972		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.1225097148127972 | validation: 1.0222565649844377]
	TIME [epoch: 1.38 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.125181411811832		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.125181411811832 | validation: 1.0536091577670559]
	TIME [epoch: 1.36 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1247288985482908		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.1247288985482908 | validation: 1.01742699984842]
	TIME [epoch: 1.36 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1167816771636327		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.1167816771636327 | validation: 1.0517637623083276]
	TIME [epoch: 1.36 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1317137896655987		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.1317137896655987 | validation: 1.0532164841305496]
	TIME [epoch: 1.36 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.141687368558485		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.141687368558485 | validation: 1.0613491351355622]
	TIME [epoch: 1.36 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1756752429293968		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.1756752429293968 | validation: 1.139080470088081]
	TIME [epoch: 1.36 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1640223319139755		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.1640223319139755 | validation: 0.9599985929117789]
	TIME [epoch: 1.36 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1822047848443495		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.1822047848443495 | validation: 1.16979300624605]
	TIME [epoch: 1.36 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1520096197678173		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.1520096197678173 | validation: 1.0202613086869308]
	TIME [epoch: 1.36 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1326956505782722		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.1326956505782722 | validation: 1.0233212116081682]
	TIME [epoch: 1.36 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1311904483571251		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 1.1311904483571251 | validation: 1.0783259833753962]
	TIME [epoch: 1.36 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1286719937852356		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.1286719937852356 | validation: 1.011613012025495]
	TIME [epoch: 1.36 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1265522421975775		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 1.1265522421975775 | validation: 1.0894638577131806]
	TIME [epoch: 1.35 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1276825360413736		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.1276825360413736 | validation: 1.0083895141062036]
	TIME [epoch: 1.36 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1226037429264524		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.1226037429264524 | validation: 1.0778124094099724]
	TIME [epoch: 1.37 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1207339476813802		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.1207339476813802 | validation: 1.0076020678038808]
	TIME [epoch: 1.36 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1126041087394354		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.1126041087394354 | validation: 1.1075448307279494]
	TIME [epoch: 1.36 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1324176627189595		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.1324176627189595 | validation: 1.0027126215989242]
	TIME [epoch: 1.36 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1534553122516509		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 1.1534553122516509 | validation: 1.2674387128185118]
	TIME [epoch: 1.36 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2077049404491078		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.2077049404491078 | validation: 1.003113001956733]
	TIME [epoch: 1.36 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1413879685115813		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 1.1413879685115813 | validation: 1.025043791835041]
	TIME [epoch: 1.36 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1303533268249857		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.1303533268249857 | validation: 1.0993415284256756]
	TIME [epoch: 1.36 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1312198884227005		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 1.1312198884227005 | validation: 1.0014562375508895]
	TIME [epoch: 1.36 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1372552365082167		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 1.1372552365082167 | validation: 1.08354647611703]
	TIME [epoch: 1.5 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1392192795155232		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.1392192795155232 | validation: 1.0283052580521546]
	TIME [epoch: 1.36 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.133480180601855		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.133480180601855 | validation: 1.03709505412026]
	TIME [epoch: 1.36 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1275563627361214		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 1.1275563627361214 | validation: 1.0466556382483247]
	TIME [epoch: 1.36 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.119355537078061		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.119355537078061 | validation: 1.0442620660362325]
	TIME [epoch: 1.36 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.121337728356367		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 1.121337728356367 | validation: 1.0101295945306668]
	TIME [epoch: 1.36 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1249049220604344		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.1249049220604344 | validation: 1.0861776015662574]
	TIME [epoch: 1.36 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1220508525792665		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 1.1220508525792665 | validation: 0.975099668419481]
	TIME [epoch: 1.36 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1503401246703993		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.1503401246703993 | validation: 1.1694984769872878]
	TIME [epoch: 1.36 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.156724746033506		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 1.156724746033506 | validation: 0.9846460807447994]
	TIME [epoch: 1.36 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1414418031889486		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.1414418031889486 | validation: 1.0868672276873788]
	TIME [epoch: 1.36 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1227798117012033		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 1.1227798117012033 | validation: 1.0485975087028692]
	TIME [epoch: 1.36 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1283365986669198		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 1.1283365986669198 | validation: 1.0396424080830275]
	TIME [epoch: 1.36 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1306406675878866		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 1.1306406675878866 | validation: 1.0958485633918507]
	TIME [epoch: 1.36 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.146523058986063		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 1.146523058986063 | validation: 1.073839230021057]
	TIME [epoch: 1.36 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1457817476584		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 1.1457817476584 | validation: 1.0257415396405667]
	TIME [epoch: 1.36 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1248074616466959		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 1.1248074616466959 | validation: 1.0903672470106363]
	TIME [epoch: 1.36 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1296208643306316		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 1.1296208643306316 | validation: 0.9517987210145894]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_243.pth
	Model improved!!!
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1324265256288897		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 1.1324265256288897 | validation: 1.1450461721255418]
	TIME [epoch: 1.36 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.147230623677409		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 1.147230623677409 | validation: 0.9888047450237032]
	TIME [epoch: 1.36 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1344832285568978		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.1344832285568978 | validation: 1.0834373259728582]
	TIME [epoch: 1.36 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1268734938432032		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 1.1268734938432032 | validation: 1.0080587997198112]
	TIME [epoch: 1.36 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1161469394272547		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 1.1161469394272547 | validation: 1.0474825265387513]
	TIME [epoch: 1.36 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1150246312194994		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 1.1150246312194994 | validation: 1.0977674155430592]
	TIME [epoch: 1.36 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.121627551444554		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 1.121627551444554 | validation: 1.004451610456596]
	TIME [epoch: 1.36 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1448358701556651		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 1.1448358701556651 | validation: 1.1722516949822503]
	TIME [epoch: 1.36 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.173182901567858		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 1.173182901567858 | validation: 1.0024897645962398]
	TIME [epoch: 1.36 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1337622254404853		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 1.1337622254404853 | validation: 1.069912482239145]
	TIME [epoch: 1.36 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1272591453835876		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 1.1272591453835876 | validation: 1.0252507435544944]
	TIME [epoch: 1.36 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.120264423505583		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 1.120264423505583 | validation: 1.0801574625950003]
	TIME [epoch: 1.36 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1172790660588539		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 1.1172790660588539 | validation: 1.0155974568697077]
	TIME [epoch: 1.36 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1245655774672607		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 1.1245655774672607 | validation: 1.0970740735611215]
	TIME [epoch: 1.36 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1418742553530856		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 1.1418742553530856 | validation: 0.993771342137894]
	TIME [epoch: 1.36 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1460115277779246		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 1.1460115277779246 | validation: 1.1166956007223128]
	TIME [epoch: 1.37 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1360428948667973		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 1.1360428948667973 | validation: 0.9809275762404837]
	TIME [epoch: 1.36 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1280460312373328		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 1.1280460312373328 | validation: 1.0419571290627936]
	TIME [epoch: 1.36 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1176455667790446		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 1.1176455667790446 | validation: 1.0512479190015178]
	TIME [epoch: 1.36 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1207467029726412		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 1.1207467029726412 | validation: 1.020629191618861]
	TIME [epoch: 1.36 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.128497444570236		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 1.128497444570236 | validation: 1.0738926696275737]
	TIME [epoch: 1.36 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1303730586219418		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 1.1303730586219418 | validation: 1.0195586195222979]
	TIME [epoch: 1.36 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.131200415104508		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 1.131200415104508 | validation: 1.0841159690197268]
	TIME [epoch: 1.36 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.13308569518061		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 1.13308569518061 | validation: 1.0181936330927557]
	TIME [epoch: 1.36 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1410605897966455		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 1.1410605897966455 | validation: 1.069882712603186]
	TIME [epoch: 1.36 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1260161208746515		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 1.1260161208746515 | validation: 1.0343907599146338]
	TIME [epoch: 1.36 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.123227931996617		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 1.123227931996617 | validation: 1.0623175609127153]
	TIME [epoch: 1.36 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1259109556575455		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 1.1259109556575455 | validation: 1.0027632684264487]
	TIME [epoch: 1.36 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1251495930110953		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 1.1251495930110953 | validation: 1.0717458614521498]
	TIME [epoch: 1.36 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1259843576879736		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 1.1259843576879736 | validation: 0.9789413482990826]
	TIME [epoch: 1.36 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1460711693223862		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 1.1460711693223862 | validation: 1.151810701012457]
	TIME [epoch: 1.36 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1490737875408972		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 1.1490737875408972 | validation: 0.9962770158492504]
	TIME [epoch: 1.36 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1349702610041343		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 1.1349702610041343 | validation: 1.0664324890892645]
	TIME [epoch: 1.36 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.130642409965551		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 1.130642409965551 | validation: 1.0693176974527294]
	TIME [epoch: 1.36 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1508451450497124		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 1.1508451450497124 | validation: 1.0783340932702252]
	TIME [epoch: 1.36 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1411536544870582		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 1.1411536544870582 | validation: 1.019314065164602]
	TIME [epoch: 1.36 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1144108509928703		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 1.1144108509928703 | validation: 1.029768203070244]
	TIME [epoch: 1.36 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.125068752518957		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 1.125068752518957 | validation: 1.0295831070360435]
	TIME [epoch: 1.36 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1111640222496997		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 1.1111640222496997 | validation: 1.0219341202611871]
	TIME [epoch: 1.36 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1199820339283775		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 1.1199820339283775 | validation: 1.048876054190711]
	TIME [epoch: 1.36 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1200675135586031		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 1.1200675135586031 | validation: 1.0096242869254695]
	TIME [epoch: 1.36 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1142139364095196		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 1.1142139364095196 | validation: 1.0982574558750782]
	TIME [epoch: 1.36 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.127830068091304		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 1.127830068091304 | validation: 0.9686826486921455]
	TIME [epoch: 1.36 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1356146775370972		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 1.1356146775370972 | validation: 1.1395093264719525]
	TIME [epoch: 1.36 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1467979760700453		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 1.1467979760700453 | validation: 1.0031861534250164]
	TIME [epoch: 1.36 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1204351463718918		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 1.1204351463718918 | validation: 1.044971795519658]
	TIME [epoch: 1.36 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1169352843647307		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 1.1169352843647307 | validation: 1.052615329049461]
	TIME [epoch: 1.36 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1158252265775677		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 1.1158252265775677 | validation: 1.0363683738809972]
	TIME [epoch: 1.36 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1201978449081036		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 1.1201978449081036 | validation: 1.0573028458273759]
	TIME [epoch: 1.36 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1275742448453903		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 1.1275742448453903 | validation: 1.0350337178461415]
	TIME [epoch: 1.36 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1287293554451399		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 1.1287293554451399 | validation: 1.0715430434629458]
	TIME [epoch: 1.36 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1317130199888985		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 1.1317130199888985 | validation: 1.042061890378265]
	TIME [epoch: 1.38 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1167094381964517		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 1.1167094381964517 | validation: 1.0109086258780147]
	TIME [epoch: 1.36 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1164299084685838		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 1.1164299084685838 | validation: 1.0591040410392054]
	TIME [epoch: 1.36 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1155774881611253		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 1.1155774881611253 | validation: 0.977610424818883]
	TIME [epoch: 1.36 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.134986408093611		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 1.134986408093611 | validation: 1.18929147651187]
	TIME [epoch: 1.36 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1644450279046807		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 1.1644450279046807 | validation: 0.9906758926030375]
	TIME [epoch: 1.36 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.127937420521203		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 1.127937420521203 | validation: 1.0234080230445042]
	TIME [epoch: 1.36 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1201382231209782		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 1.1201382231209782 | validation: 1.0713177140536343]
	TIME [epoch: 1.36 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1251614572808517		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 1.1251614572808517 | validation: 1.0085337227254378]
	TIME [epoch: 1.36 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1205594988582972		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 1.1205594988582972 | validation: 1.0574818528106098]
	TIME [epoch: 1.36 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1206476472026135		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 1.1206476472026135 | validation: 1.0272541478935453]
	TIME [epoch: 1.36 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1280918047995148		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 1.1280918047995148 | validation: 1.0498643513940589]
	TIME [epoch: 1.36 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1319371772666547		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 1.1319371772666547 | validation: 1.0441922524471756]
	TIME [epoch: 1.36 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.132861095745569		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 1.132861095745569 | validation: 1.0428074447053763]
	TIME [epoch: 1.36 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1253518531770177		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 1.1253518531770177 | validation: 1.0345565294146695]
	TIME [epoch: 1.36 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1212317932955602		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 1.1212317932955602 | validation: 1.0298869241476647]
	TIME [epoch: 1.36 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1139474541226366		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 1.1139474541226366 | validation: 1.0590144959422376]
	TIME [epoch: 1.36 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1254543692618737		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 1.1254543692618737 | validation: 1.0035230744702146]
	TIME [epoch: 1.36 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1291778548745728		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 1.1291778548745728 | validation: 1.1719804260026263]
	TIME [epoch: 1.36 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.154002060347968		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 1.154002060347968 | validation: 0.9868680426637699]
	TIME [epoch: 1.36 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1282349752145033		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 1.1282349752145033 | validation: 1.025714897533128]
	TIME [epoch: 1.36 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1175299039024358		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 1.1175299039024358 | validation: 1.0658300394082225]
	TIME [epoch: 1.36 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.120442766040977		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 1.120442766040977 | validation: 0.9975747665379102]
	TIME [epoch: 1.36 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1139247033573736		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 1.1139247033573736 | validation: 1.0676860197888034]
	TIME [epoch: 1.36 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1251287390337805		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 1.1251287390337805 | validation: 1.0139364800011204]
	TIME [epoch: 1.36 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1221842287306802		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 1.1221842287306802 | validation: 1.0554247907295367]
	TIME [epoch: 1.36 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1208882385163008		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 1.1208882385163008 | validation: 1.0429347398886148]
	TIME [epoch: 1.36 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1302853455129138		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 1.1302853455129138 | validation: 1.0327470274977102]
	TIME [epoch: 1.36 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1257550166164834		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 1.1257550166164834 | validation: 1.0679543049982934]
	TIME [epoch: 1.36 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1252247408482046		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 1.1252247408482046 | validation: 0.9891275077542073]
	TIME [epoch: 1.36 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.135185937596525		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 1.135185937596525 | validation: 1.1001325643510864]
	TIME [epoch: 1.36 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1324319974577197		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 1.1324319974577197 | validation: 0.9923204109796567]
	TIME [epoch: 1.36 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.124928066489817		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 1.124928066489817 | validation: 1.0944879338441313]
	TIME [epoch: 1.36 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1247225394111922		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 1.1247225394111922 | validation: 1.0284746674215652]
	TIME [epoch: 1.36 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1182391929018407		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 1.1182391929018407 | validation: 1.024345291357235]
	TIME [epoch: 1.36 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1170209942982472		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 1.1170209942982472 | validation: 1.0194205170299149]
	TIME [epoch: 1.36 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.121620144340215		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 1.121620144340215 | validation: 1.0434964472806663]
	TIME [epoch: 1.36 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1275611587500942		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 1.1275611587500942 | validation: 1.0417614455156912]
	TIME [epoch: 1.36 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1237509692015815		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 1.1237509692015815 | validation: 1.0884180884282804]
	TIME [epoch: 1.36 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1349657447969161		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 1.1349657447969161 | validation: 1.0167091387281504]
	TIME [epoch: 1.37 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1256859571831725		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 1.1256859571831725 | validation: 1.1013779414654778]
	TIME [epoch: 1.36 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1200539001890095		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 1.1200539001890095 | validation: 0.9950505735408717]
	TIME [epoch: 1.36 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1438467222215019		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 1.1438467222215019 | validation: 1.0998877532254645]
	TIME [epoch: 1.36 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1283190559324099		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 1.1283190559324099 | validation: 1.0020769970022383]
	TIME [epoch: 1.36 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.116109163717142		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 1.116109163717142 | validation: 1.0597258614974485]
	TIME [epoch: 1.36 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1172533463402659		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 1.1172533463402659 | validation: 1.0406742081547766]
	TIME [epoch: 1.36 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1175663922866317		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 1.1175663922866317 | validation: 1.0057660232143248]
	TIME [epoch: 1.36 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1171750616848557		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 1.1171750616848557 | validation: 1.073810095415373]
	TIME [epoch: 1.36 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1193006474340796		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 1.1193006474340796 | validation: 1.0092123178235448]
	TIME [epoch: 1.36 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.121541197911715		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 1.121541197911715 | validation: 1.0294674904017687]
	TIME [epoch: 1.36 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1202960900998549		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 1.1202960900998549 | validation: 1.0189614532983893]
	TIME [epoch: 1.36 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1221261020253044		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 1.1221261020253044 | validation: 1.047765026098708]
	TIME [epoch: 1.36 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.12910762100979		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 1.12910762100979 | validation: 1.0676421824363285]
	TIME [epoch: 1.36 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.137524587035109		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 1.137524587035109 | validation: 1.0133785791173189]
	TIME [epoch: 1.36 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1224664732723508		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 1.1224664732723508 | validation: 1.0822181995929174]
	TIME [epoch: 1.36 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1266884688735435		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 1.1266884688735435 | validation: 1.0052790546181416]
	TIME [epoch: 1.36 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1276277580325333		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 1.1276277580325333 | validation: 1.1023569232599337]
	TIME [epoch: 1.36 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.129773501571422		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 1.129773501571422 | validation: 0.9846372140151147]
	TIME [epoch: 1.37 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1193360061315882		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 1.1193360061315882 | validation: 1.0352179316575514]
	TIME [epoch: 1.36 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1173810572585914		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 1.1173810572585914 | validation: 1.0314644486146214]
	TIME [epoch: 1.36 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1173470228944014		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 1.1173470228944014 | validation: 1.0142019369529025]
	TIME [epoch: 1.36 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1215134307795354		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 1.1215134307795354 | validation: 1.0585022791915273]
	TIME [epoch: 1.36 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1195592851675424		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 1.1195592851675424 | validation: 1.0382224597362355]
	TIME [epoch: 1.36 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1250278708000765		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 1.1250278708000765 | validation: 1.0280390618684863]
	TIME [epoch: 1.36 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.113905550023563		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 1.113905550023563 | validation: 1.0770168654526313]
	TIME [epoch: 1.36 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1230034608124908		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 1.1230034608124908 | validation: 1.0094770380184168]
	TIME [epoch: 1.36 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.120881421883718		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 1.120881421883718 | validation: 1.1084034874472204]
	TIME [epoch: 1.36 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1337227268448293		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 1.1337227268448293 | validation: 0.9936166731384724]
	TIME [epoch: 1.36 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1159186696972003		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 1.1159186696972003 | validation: 1.0552602824414963]
	TIME [epoch: 1.36 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.117288685432262		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 1.117288685432262 | validation: 1.0247922542650227]
	TIME [epoch: 1.36 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1234958280623542		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 1.1234958280623542 | validation: 1.033025668936262]
	TIME [epoch: 1.36 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.113238644084597		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 1.113238644084597 | validation: 1.0525162354330069]
	TIME [epoch: 1.36 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1151930855019363		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 1.1151930855019363 | validation: 0.9986207554988913]
	TIME [epoch: 1.36 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1183536491259838		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 1.1183536491259838 | validation: 1.0911031403905118]
	TIME [epoch: 1.37 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.124483929639618		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 1.124483929639618 | validation: 1.0018079184897328]
	TIME [epoch: 1.36 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1312544586086082		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 1.1312544586086082 | validation: 1.0572460955380198]
	TIME [epoch: 1.36 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1269210690358817		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 1.1269210690358817 | validation: 1.043488983455019]
	TIME [epoch: 1.36 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.127968395735666		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 1.127968395735666 | validation: 1.0081551643973197]
	TIME [epoch: 1.36 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1119381546948348		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 1.1119381546948348 | validation: 1.0450242968528756]
	TIME [epoch: 1.36 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1156505711761104		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 1.1156505711761104 | validation: 1.0304060320720827]
	TIME [epoch: 1.36 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1155344966061669		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 1.1155344966061669 | validation: 1.051809993567073]
	TIME [epoch: 1.36 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1062556943209543		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 1.1062556943209543 | validation: 1.018354358654016]
	TIME [epoch: 1.36 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1089258096269108		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 1.1089258096269108 | validation: 1.062819044052638]
	TIME [epoch: 1.36 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.118699908337392		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 1.118699908337392 | validation: 0.9928203863615569]
	TIME [epoch: 1.36 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1266752758782435		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 1.1266752758782435 | validation: 1.0755085918534977]
	TIME [epoch: 1.53 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.120884140731192		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 1.120884140731192 | validation: 0.9949409993008363]
	TIME [epoch: 1.36 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1272899016933489		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 1.1272899016933489 | validation: 1.0627800633615638]
	TIME [epoch: 1.36 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1218246616894625		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 1.1218246616894625 | validation: 1.0381232956676738]
	TIME [epoch: 1.36 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1216010936128666		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 1.1216010936128666 | validation: 1.0439711518668264]
	TIME [epoch: 1.36 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1340961111243126		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 1.1340961111243126 | validation: 1.0248092761304253]
	TIME [epoch: 1.36 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1222497886035376		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 1.1222497886035376 | validation: 1.0299022042848367]
	TIME [epoch: 1.36 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1173451029500012		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 1.1173451029500012 | validation: 1.02608183264914]
	TIME [epoch: 1.36 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.115779324418371		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 1.115779324418371 | validation: 1.0283367664617806]
	TIME [epoch: 1.36 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1100059560508657		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 1.1100059560508657 | validation: 1.0433088294793311]
	TIME [epoch: 1.36 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1203025296063513		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 1.1203025296063513 | validation: 1.0005476619646263]
	TIME [epoch: 1.36 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1256098956764637		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 1.1256098956764637 | validation: 1.1000444697879572]
	TIME [epoch: 1.36 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.124257121732765		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 1.124257121732765 | validation: 1.0020495553581212]
	TIME [epoch: 1.36 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1283536399772813		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 1.1283536399772813 | validation: 1.0434538357570713]
	TIME [epoch: 1.36 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.113983041516994		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 1.113983041516994 | validation: 1.043108697191781]
	TIME [epoch: 1.36 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1078393952949244		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 1.1078393952949244 | validation: 1.0107500101711988]
	TIME [epoch: 1.36 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1080707083101449		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 1.1080707083101449 | validation: 1.045861966469462]
	TIME [epoch: 1.36 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1169277795050776		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 1.1169277795050776 | validation: 0.9958426111844869]
	TIME [epoch: 1.36 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1216979882878924		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 1.1216979882878924 | validation: 1.0603185434375626]
	TIME [epoch: 1.36 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1223183466657412		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 1.1223183466657412 | validation: 1.0157706167904585]
	TIME [epoch: 1.36 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1140271762394205		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 1.1140271762394205 | validation: 1.0238165595780488]
	TIME [epoch: 1.36 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1135454726230265		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 1.1135454726230265 | validation: 1.0250841212793305]
	TIME [epoch: 1.36 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1173733551104335		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 1.1173733551104335 | validation: 1.0122578826533544]
	TIME [epoch: 1.36 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1135757210448713		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 1.1135757210448713 | validation: 1.0232521214492427]
	TIME [epoch: 1.36 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1102372916993883		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 1.1102372916993883 | validation: 1.0067827253508046]
	TIME [epoch: 1.36 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1180133723538894		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 1.1180133723538894 | validation: 1.1147646521936914]
	TIME [epoch: 1.36 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.139249969567157		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 1.139249969567157 | validation: 0.9951328730379738]
	TIME [epoch: 1.37 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1255248147818566		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 1.1255248147818566 | validation: 1.0353015486625066]
	TIME [epoch: 1.36 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1088591063800106		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 1.1088591063800106 | validation: 1.0527373539492988]
	TIME [epoch: 1.36 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1188012987314773		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 1.1188012987314773 | validation: 0.9924317059951054]
	TIME [epoch: 1.36 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1185257535748958		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 1.1185257535748958 | validation: 1.0661455439521148]
	TIME [epoch: 1.36 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1223015617500662		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 1.1223015617500662 | validation: 1.0267365606597747]
	TIME [epoch: 1.36 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1185838394233423		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 1.1185838394233423 | validation: 1.0217413172857681]
	TIME [epoch: 1.36 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.121674886299307		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 1.121674886299307 | validation: 1.0330214338059782]
	TIME [epoch: 1.36 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1184286695241117		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 1.1184286695241117 | validation: 1.013975375406125]
	TIME [epoch: 1.36 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1160549813235998		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 1.1160549813235998 | validation: 1.0361445737644652]
	TIME [epoch: 1.37 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1143842221116813		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 1.1143842221116813 | validation: 1.0278166792366974]
	TIME [epoch: 1.36 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.108711784296438		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 1.108711784296438 | validation: 1.019876919083462]
	TIME [epoch: 1.36 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1162324151388336		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 1.1162324151388336 | validation: 1.0562244672886771]
	TIME [epoch: 1.36 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1148494666880289		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 1.1148494666880289 | validation: 1.0068172873645265]
	TIME [epoch: 1.36 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1208126548353476		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 1.1208126548353476 | validation: 1.0585121122968912]
	TIME [epoch: 1.36 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.117739230808228		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 1.117739230808228 | validation: 0.9797848047557928]
	TIME [epoch: 1.36 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1269578810854382		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 1.1269578810854382 | validation: 1.0845432010581477]
	TIME [epoch: 1.36 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.120060263010405		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 1.120060263010405 | validation: 1.0175103245675543]
	TIME [epoch: 1.36 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1138566641854792		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 1.1138566641854792 | validation: 1.0016144975602206]
	TIME [epoch: 1.36 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1132885680254652		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 1.1132885680254652 | validation: 1.0515460098092262]
	TIME [epoch: 1.36 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1226583989986005		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 1.1226583989986005 | validation: 0.9907283654835974]
	TIME [epoch: 1.36 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.120068261402597		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 1.120068261402597 | validation: 1.0853081517995051]
	TIME [epoch: 1.36 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1169608576807102		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 1.1169608576807102 | validation: 1.0452691376831045]
	TIME [epoch: 1.36 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1318884124961497		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 1.1318884124961497 | validation: 1.0255389855068004]
	TIME [epoch: 1.36 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.12778214168592		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 1.12778214168592 | validation: 1.0429806421854786]
	TIME [epoch: 1.36 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.110369143571853		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 1.110369143571853 | validation: 1.0181074817450046]
	TIME [epoch: 1.36 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1116641976492532		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 1.1116641976492532 | validation: 1.0177822311690452]
	TIME [epoch: 1.36 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1189181931380028		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 1.1189181931380028 | validation: 1.0614863635812313]
	TIME [epoch: 1.36 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1197494800678924		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 1.1197494800678924 | validation: 1.0329978959661699]
	TIME [epoch: 1.36 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.117489990066427		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 1.117489990066427 | validation: 1.0216722760672248]
	TIME [epoch: 1.36 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1149217692322642		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 1.1149217692322642 | validation: 1.0527206206781525]
	TIME [epoch: 1.36 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1142317862712443		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 1.1142317862712443 | validation: 0.9921827068566131]
	TIME [epoch: 1.36 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.119450341014237		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 1.119450341014237 | validation: 1.0967066402650107]
	TIME [epoch: 1.36 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.126863606521363		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 1.126863606521363 | validation: 0.9871998990561845]
	TIME [epoch: 1.36 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1183460666589353		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 1.1183460666589353 | validation: 1.05380340319388]
	TIME [epoch: 1.37 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1139758315040933		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 1.1139758315040933 | validation: 1.036402346703813]
	TIME [epoch: 1.36 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1171650405982232		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 1.1171650405982232 | validation: 0.9810809297519978]
	TIME [epoch: 1.36 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1176694301940528		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 1.1176694301940528 | validation: 1.0711592625332784]
	TIME [epoch: 1.36 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1123442714849956		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 1.1123442714849956 | validation: 1.029194532681872]
	TIME [epoch: 1.36 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1120671057284808		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 1.1120671057284808 | validation: 1.0468939590607274]
	TIME [epoch: 1.36 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.114515798886229		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 1.114515798886229 | validation: 1.0438132215091631]
	TIME [epoch: 1.36 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1200135392989574		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 1.1200135392989574 | validation: 1.0270576374621185]
	TIME [epoch: 1.36 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.120734636167446		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 1.120734636167446 | validation: 1.0296442363228717]
	TIME [epoch: 1.36 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.118948963693938		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 1.118948963693938 | validation: 1.0403296770418746]
	TIME [epoch: 1.36 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1163689365692755		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 1.1163689365692755 | validation: 1.0588876686575506]
	TIME [epoch: 1.36 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1130205497428884		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 1.1130205497428884 | validation: 1.0200664509495398]
	TIME [epoch: 1.36 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1196816859100343		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 1.1196816859100343 | validation: 1.0885815323683856]
	TIME [epoch: 1.36 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.122061198254694		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 1.122061198254694 | validation: 0.9957251017151283]
	TIME [epoch: 1.36 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1182128471143817		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 1.1182128471143817 | validation: 1.0489065901970072]
	TIME [epoch: 1.36 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.126082109012589		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 1.126082109012589 | validation: 1.042922122802606]
	TIME [epoch: 1.36 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.121396221843176		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 1.121396221843176 | validation: 1.002011462930456]
	TIME [epoch: 1.36 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.121205793622503		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 1.121205793622503 | validation: 1.0658884214115723]
	TIME [epoch: 1.36 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1171738999931458		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 1.1171738999931458 | validation: 1.0039991602282172]
	TIME [epoch: 1.37 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1168209011677375		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 1.1168209011677375 | validation: 1.0322258616491664]
	TIME [epoch: 1.36 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1127215968508832		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 1.1127215968508832 | validation: 1.004153541949183]
	TIME [epoch: 1.36 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1140291760382055		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 1.1140291760382055 | validation: 1.0396374954119956]
	TIME [epoch: 1.36 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1156363643261857		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 1.1156363643261857 | validation: 1.046668157183634]
	TIME [epoch: 1.37 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1256438141252845		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 1.1256438141252845 | validation: 1.0127876561424232]
	TIME [epoch: 1.36 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1134646022281325		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 1.1134646022281325 | validation: 1.0455123590099402]
	TIME [epoch: 1.36 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1151204851413739		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 1.1151204851413739 | validation: 0.9892370614008286]
	TIME [epoch: 1.36 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.122661595836712		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 1.122661595836712 | validation: 1.0929909194236926]
	TIME [epoch: 1.36 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1165212369258348		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 1.1165212369258348 | validation: 0.9996368783249481]
	TIME [epoch: 1.36 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1153987994803545		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 1.1153987994803545 | validation: 1.03073930509802]
	TIME [epoch: 1.36 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1149319261967727		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 1.1149319261967727 | validation: 1.065032641671607]
	TIME [epoch: 1.36 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1129653039818093		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 1.1129653039818093 | validation: 1.0276783223008361]
	TIME [epoch: 1.36 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1100780170533702		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 1.1100780170533702 | validation: 1.0551650771138668]
	TIME [epoch: 1.35 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1235855328949393		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 1.1235855328949393 | validation: 1.0019711108317597]
	TIME [epoch: 1.35 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1146844478374258		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 1.1146844478374258 | validation: 1.0653642607419123]
	TIME [epoch: 1.36 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1160304332860396		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 1.1160304332860396 | validation: 1.010886547022095]
	TIME [epoch: 1.36 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1118483102762806		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 1.1118483102762806 | validation: 1.0613046219926332]
	TIME [epoch: 1.36 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.122309261430743		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 1.122309261430743 | validation: 1.021235081644235]
	TIME [epoch: 1.37 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1150289288397368		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 1.1150289288397368 | validation: 1.0564602902712845]
	TIME [epoch: 1.36 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1112539081811124		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 1.1112539081811124 | validation: 1.0164869979256375]
	TIME [epoch: 1.36 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1127828010755711		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 1.1127828010755711 | validation: 1.0810277047444363]
	TIME [epoch: 1.36 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1190129350461664		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 1.1190129350461664 | validation: 1.0000749106272018]
	TIME [epoch: 1.36 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.110732889707599		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 1.110732889707599 | validation: 1.0449986790271724]
	TIME [epoch: 1.36 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.106829550468852		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 1.106829550468852 | validation: 1.021346425549917]
	TIME [epoch: 1.36 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.112634194785304		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 1.112634194785304 | validation: 1.0327117414790323]
	TIME [epoch: 1.36 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1093026445022185		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 1.1093026445022185 | validation: 1.0324609585480478]
	TIME [epoch: 1.36 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1035645632202031		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 1.1035645632202031 | validation: 1.0357084589069272]
	TIME [epoch: 1.36 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1145602984982514		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 1.1145602984982514 | validation: 1.0414449259378464]
	TIME [epoch: 1.36 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1137833484202941		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 1.1137833484202941 | validation: 1.0007091265065344]
	TIME [epoch: 1.37 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.11223951933818		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 1.11223951933818 | validation: 1.063299432469941]
	TIME [epoch: 1.36 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1160075644492073		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 1.1160075644492073 | validation: 1.0026483209088022]
	TIME [epoch: 1.36 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1194761063085759		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 1.1194761063085759 | validation: 1.0441428711080105]
	TIME [epoch: 1.36 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1181823751330129		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 1.1181823751330129 | validation: 1.0250553413369268]
	TIME [epoch: 1.36 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1169969011176055		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 1.1169969011176055 | validation: 0.9990174918979646]
	TIME [epoch: 1.36 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1152920412752674		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 1.1152920412752674 | validation: 1.039078888401501]
	TIME [epoch: 1.36 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.117827976703852		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 1.117827976703852 | validation: 1.0423028866094357]
	TIME [epoch: 1.36 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.115300863928729		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 1.115300863928729 | validation: 1.010439969918879]
	TIME [epoch: 1.36 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1183819056723632		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 1.1183819056723632 | validation: 1.0492858224216095]
	TIME [epoch: 1.36 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.114318848930255		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 1.114318848930255 | validation: 1.0086849654132581]
	TIME [epoch: 1.36 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1160733209826459		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 1.1160733209826459 | validation: 1.0389642247788626]
	TIME [epoch: 1.36 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.114501073275645		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 1.114501073275645 | validation: 1.0134846055512197]
	TIME [epoch: 1.36 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.11186324935141		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 1.11186324935141 | validation: 1.051196538263946]
	TIME [epoch: 1.36 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1169144314098898		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 1.1169144314098898 | validation: 0.9840328284111793]
	TIME [epoch: 1.36 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1095542081574907		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 1.1095542081574907 | validation: 1.0590683444311029]
	TIME [epoch: 180 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.119945403147968		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 1.119945403147968 | validation: 1.0187430973993459]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_4_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_4_v_mmd4_502.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 1120.807 seconds.
