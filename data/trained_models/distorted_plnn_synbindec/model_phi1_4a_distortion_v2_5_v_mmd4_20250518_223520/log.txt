Args:
Namespace(name='model_phi1_4a_distortion_v2_5_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_5/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_5/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.02510099858045578, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3876967875

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.1597410541632724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.1597410541632724 | validation: 6.629817335956284]
	TIME [epoch: 165 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.201374517315267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.201374517315267 | validation: 6.620266686325836]
	TIME [epoch: 0.791 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.908089443487979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.908089443487979 | validation: 6.760054541354389]
	TIME [epoch: 0.695 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.714947554424793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.714947554424793 | validation: 6.674723955204891]
	TIME [epoch: 0.695 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.9775699426191675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.9775699426191675 | validation: 7.13518487079692]
	TIME [epoch: 0.693 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.631335124362067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.631335124362067 | validation: 6.263340522855367]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.24705516970452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.24705516970452 | validation: 6.513896932873028]
	TIME [epoch: 0.696 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.080445950716124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.080445950716124 | validation: 6.586645499471536]
	TIME [epoch: 0.693 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.047467957474087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.047467957474087 | validation: 6.484720201478872]
	TIME [epoch: 0.693 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.8061292180976105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.8061292180976105 | validation: 6.206006507357627]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.392676498336929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.392676498336929 | validation: 5.942432609464554]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.203907352035732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.203907352035732 | validation: 5.844141185483885]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.144438788632274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.144438788632274 | validation: 6.125823709771736]
	TIME [epoch: 0.698 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.28158966824375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.28158966824375 | validation: 5.991764987841671]
	TIME [epoch: 0.697 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.1389214121114515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1389214121114515 | validation: 5.8574612583585886]
	TIME [epoch: 0.697 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.972964096513357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.972964096513357 | validation: 5.858560486714407]
	TIME [epoch: 0.697 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.06001295312545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.06001295312545 | validation: 5.747097133364304]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9795886422548667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9795886422548667 | validation: 5.803499147143555]
	TIME [epoch: 0.693 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8806795025537504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8806795025537504 | validation: 5.739661147124096]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.823230976728666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.823230976728666 | validation: 5.638524763517636]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.816878752480718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.816878752480718 | validation: 5.7021444911544945]
	TIME [epoch: 0.696 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7862900154749655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7862900154749655 | validation: 5.6487083511613445]
	TIME [epoch: 0.697 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7487989066681067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7487989066681067 | validation: 5.593597774029978]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7171210994648476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7171210994648476 | validation: 5.585278454800166]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.694076442736981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.694076442736981 | validation: 5.575234589801664]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6917546821425016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6917546821425016 | validation: 5.565947486389487]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.676600664694248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.676600664694248 | validation: 5.525775902083535]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6620483567935445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6620483567935445 | validation: 5.509614225602731]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6408430552710285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6408430552710285 | validation: 5.4858727496828585]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.624461622642189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.624461622642189 | validation: 5.461699195651051]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6007966631763817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6007966631763817 | validation: 5.461584137934821]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5889058962358535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5889058962358535 | validation: 5.421556921884745]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.580838986964916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.580838986964916 | validation: 5.424631923347478]
	TIME [epoch: 0.695 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5725361733520646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5725361733520646 | validation: 5.378493114996813]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.570866864277148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.570866864277148 | validation: 5.417404565329669]
	TIME [epoch: 0.695 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.599491262144315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.599491262144315 | validation: 5.3362834784908415]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.552579588361598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.552579588361598 | validation: 5.350911341860097]
	TIME [epoch: 0.694 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5385909516270466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5385909516270466 | validation: 5.320311778675759]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5124979902359144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5124979902359144 | validation: 5.292509052906903]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.498437862985734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.498437862985734 | validation: 5.2821535397102615]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4861637022442724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4861637022442724 | validation: 5.258233629084668]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4720262327697338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4720262327697338 | validation: 5.237357732807888]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.459871068879619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.459871068879619 | validation: 5.208977264550834]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4524534767139974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4524534767139974 | validation: 5.191640592309085]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.436763999860095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.436763999860095 | validation: 5.186970206806149]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.435443060265286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.435443060265286 | validation: 5.151395995380276]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.454804153860369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.454804153860369 | validation: 5.143880355333192]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.497974296334444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.497974296334444 | validation: 5.1061738412708335]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.397266339497771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.397266339497771 | validation: 5.094240414842596]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4263246747192713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4263246747192713 | validation: 5.052088924772919]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3777032839327594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3777032839327594 | validation: 5.026511476355163]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3627363373969987		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.3627363373969987 | validation: 5.004565753931475]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3396636251704015		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 3.3396636251704015 | validation: 4.965750791768805]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3209687399712777		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 3.3209687399712777 | validation: 4.925958041777929]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.307510836846004		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 3.307510836846004 | validation: 4.879432606802393]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.282179285270113		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 3.282179285270113 | validation: 4.843589628560761]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2551336721158406		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 3.2551336721158406 | validation: 4.7403931222329545]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2080042250238376		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 3.2080042250238376 | validation: 4.50121038236656]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.074288431510552		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 3.074288431510552 | validation: 4.255014098346663]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8815802607997147		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 2.8815802607997147 | validation: 4.088579581554672]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6909829212723615		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 2.6909829212723615 | validation: 3.982397760373075]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6326806815116788		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 2.6326806815116788 | validation: 3.9162371846631348]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5986711776167306		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 2.5986711776167306 | validation: 3.7936578176909204]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5063719500198		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 2.5063719500198 | validation: 3.691161351007576]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4314939264441082		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 2.4314939264441082 | validation: 3.545523703286193]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.364529256682285		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 2.364529256682285 | validation: 3.3788704364428543]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.292983772414512		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 2.292983772414512 | validation: 3.2615259912851258]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.224671867149381		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 2.224671867149381 | validation: 2.8302581012288055]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2123837648634663		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 2.2123837648634663 | validation: 3.693436917257685]
	TIME [epoch: 0.698 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6443387261438		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 2.6443387261438 | validation: 2.8332398176475504]
	TIME [epoch: 0.696 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1002260720698978		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 2.1002260720698978 | validation: 2.618569184372874]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1684627687061853		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 2.1684627687061853 | validation: 2.836198648668331]
	TIME [epoch: 0.696 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0995071646648378		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 2.0995071646648378 | validation: 2.5315332979404186]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0312067659550523		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 2.0312067659550523 | validation: 2.4122131966999945]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9988286854477957		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.9988286854477957 | validation: 2.5081784254496493]
	TIME [epoch: 0.699 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9997846647096342		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.9997846647096342 | validation: 2.254991907942256]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2196080716869706		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 2.2196080716869706 | validation: 2.954632609682979]
	TIME [epoch: 0.699 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.238850936033901		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 2.238850936033901 | validation: 2.377993921107112]
	TIME [epoch: 0.697 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9545336973304839		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.9545336973304839 | validation: 2.1627145360165856]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9962082579113378		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.9962082579113378 | validation: 2.158248476267102]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8996215844472004		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.8996215844472004 | validation: 2.1149729455033546]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8661502264916312		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.8661502264916312 | validation: 1.8655092027687319]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8623886489673496		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.8623886489673496 | validation: 2.27906900499138]
	TIME [epoch: 0.693 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.931997074207498		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.931997074207498 | validation: 1.88010943294601]
	TIME [epoch: 0.692 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0349237521574848		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 2.0349237521574848 | validation: 2.4936189603962386]
	TIME [epoch: 0.695 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.039053750155341		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 2.039053750155341 | validation: 1.9168263190471775]
	TIME [epoch: 0.692 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.784475869860318		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.784475869860318 | validation: 1.7780819893590534]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8399614833004543		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.8399614833004543 | validation: 1.9244132277301724]
	TIME [epoch: 0.694 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7796313279820215		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.7796313279820215 | validation: 1.5320635781650211]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6849011158950322		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.6849011158950322 | validation: 1.4942376703215503]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6277599607592919		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.6277599607592919 | validation: 1.3484242990695587]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5982393214734187		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.5982393214734187 | validation: 1.7192656603086343]
	TIME [epoch: 0.694 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6964571431557334		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.6964571431557334 | validation: 2.092641171388473]
	TIME [epoch: 0.693 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.35174480185176		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 2.35174480185176 | validation: 1.9933888332827092]
	TIME [epoch: 0.692 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7878888300517384		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.7878888300517384 | validation: 1.6045549677292739]
	TIME [epoch: 0.692 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.605090725613273		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.605090725613273 | validation: 1.4037653376919415]
	TIME [epoch: 0.692 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6469556807130112		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.6469556807130112 | validation: 1.5682731743546803]
	TIME [epoch: 0.692 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.614470231728585		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.614470231728585 | validation: 1.1484327083052]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5925019469584731		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.5925019469584731 | validation: 1.1942790813495758]
	TIME [epoch: 0.706 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.454110034583476		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.454110034583476 | validation: 1.1871116596244506]
	TIME [epoch: 0.697 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4581582477236033		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.4581582477236033 | validation: 1.1155905571031512]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4255654678121192		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.4255654678121192 | validation: 1.0887657137797018]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3976372855089785		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.3976372855089785 | validation: 1.1061234084370282]
	TIME [epoch: 0.695 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3900433159921335		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.3900433159921335 | validation: 1.038343903927842]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3852582326047491		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.3852582326047491 | validation: 1.1802322382951878]
	TIME [epoch: 0.693 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3967986434796773		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.3967986434796773 | validation: 1.400423124021978]
	TIME [epoch: 0.693 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8073053042643112		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.8073053042643112 | validation: 2.3890123631997375]
	TIME [epoch: 0.692 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.996216107994693		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.996216107994693 | validation: 1.751509752884587]
	TIME [epoch: 0.691 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6543031418836176		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.6543031418836176 | validation: 1.3831871757303251]
	TIME [epoch: 0.692 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6610652715659233		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.6610652715659233 | validation: 1.2963618845446108]
	TIME [epoch: 0.692 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.459432238526769		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.459432238526769 | validation: 1.1241035952634417]
	TIME [epoch: 0.692 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4197323645949187		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.4197323645949187 | validation: 0.9870802833437615]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.357036844831498		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.357036844831498 | validation: 1.0660561417231689]
	TIME [epoch: 0.698 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3598076173604772		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.3598076173604772 | validation: 0.9833241557653207]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.339880115235295		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.339880115235295 | validation: 1.0845280650925688]
	TIME [epoch: 0.695 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3221372573431203		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.3221372573431203 | validation: 0.961758951394089]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3330408722048026		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.3330408722048026 | validation: 1.2668513791485325]
	TIME [epoch: 0.697 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3754605947197536		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.3754605947197536 | validation: 1.0226256783464658]
	TIME [epoch: 0.695 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4341814886894546		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.4341814886894546 | validation: 1.5774891791725691]
	TIME [epoch: 0.697 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5271502536820247		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.5271502536820247 | validation: 0.9202626311664948]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3022051671774977		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.3022051671774977 | validation: 0.9191400559205332]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2624158931108012		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.2624158931108012 | validation: 1.0756887795372285]
	TIME [epoch: 0.693 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2790444855785628		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.2790444855785628 | validation: 0.9605911936262601]
	TIME [epoch: 0.691 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3340305666649932		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.3340305666649932 | validation: 1.6770124772868842]
	TIME [epoch: 0.692 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5390363569479524		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.5390363569479524 | validation: 0.8944830970160265]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2989714642549035		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.2989714642549035 | validation: 0.9658577778947914]
	TIME [epoch: 0.693 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2774962372834329		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.2774962372834329 | validation: 0.8631948051344662]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.245992151194704		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.245992151194704 | validation: 0.9928716038511664]
	TIME [epoch: 0.693 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2278376990978759		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.2278376990978759 | validation: 0.923104613906784]
	TIME [epoch: 0.691 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3072255415752891		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.3072255415752891 | validation: 1.896615296501575]
	TIME [epoch: 0.691 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6408690023512973		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.6408690023512973 | validation: 0.8736775854310679]
	TIME [epoch: 0.692 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.196165007372803		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.196165007372803 | validation: 1.0425110836442435]
	TIME [epoch: 0.7 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.423079730732178		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.423079730732178 | validation: 1.8697176075805388]
	TIME [epoch: 0.693 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6179488604485837		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.6179488604485837 | validation: 1.1532982320830334]
	TIME [epoch: 0.692 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3905154047711585		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.3905154047711585 | validation: 1.0266299519531235]
	TIME [epoch: 0.692 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3967751313906525		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.3967751313906525 | validation: 1.311240602161553]
	TIME [epoch: 0.692 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3816799058267273		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.3816799058267273 | validation: 0.9123380904169368]
	TIME [epoch: 0.692 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1896763963012287		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.1896763963012287 | validation: 0.7953276773824387]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2052141748119913		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.2052141748119913 | validation: 1.0627757539569813]
	TIME [epoch: 0.693 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2219744207515133		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.2219744207515133 | validation: 0.8757542502669416]
	TIME [epoch: 0.692 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1999274468875039		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.1999274468875039 | validation: 1.0951058460164738]
	TIME [epoch: 0.693 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2073665289057869		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.2073665289057869 | validation: 0.8176949234089652]
	TIME [epoch: 0.692 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2203689473751147		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.2203689473751147 | validation: 1.269604691596638]
	TIME [epoch: 0.693 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.26136017170074		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.26136017170074 | validation: 0.8391383907334465]
	TIME [epoch: 0.693 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1925681587989905		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.1925681587989905 | validation: 1.1302921637613572]
	TIME [epoch: 0.694 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.216455264216046		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.216455264216046 | validation: 0.844321980981447]
	TIME [epoch: 0.693 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1885660464412338		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.1885660464412338 | validation: 1.261936999195413]
	TIME [epoch: 0.692 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2445128286277334		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.2445128286277334 | validation: 0.7923208333674238]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1575179068745531		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.1575179068745531 | validation: 1.1219751775245788]
	TIME [epoch: 0.697 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.20273305176949		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.20273305176949 | validation: 0.7993856349575862]
	TIME [epoch: 0.696 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1923226190249412		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.1923226190249412 | validation: 1.1690691852820716]
	TIME [epoch: 0.698 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2002409578163589		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.2002409578163589 | validation: 0.7935068990466125]
	TIME [epoch: 0.696 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1442145545981972		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.1442145545981972 | validation: 1.173547974836296]
	TIME [epoch: 0.696 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1855745385959562		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.1855745385959562 | validation: 0.9075068699769008]
	TIME [epoch: 0.695 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.18612336217366		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.18612336217366 | validation: 1.101390803482978]
	TIME [epoch: 0.697 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1564378778743394		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.1564378778743394 | validation: 0.7262967101095624]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0934347511001739		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.0934347511001739 | validation: 1.10227830898145]
	TIME [epoch: 0.698 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1287469969599926		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.1287469969599926 | validation: 0.8422274303704995]
	TIME [epoch: 0.696 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1614593486952842		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.1614593486952842 | validation: 1.3145318298572748]
	TIME [epoch: 0.696 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2425380665431585		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.2425380665431585 | validation: 0.7233164346130286]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1044991149807961		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.1044991149807961 | validation: 0.9743655735362803]
	TIME [epoch: 0.694 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0510267522977563		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.0510267522977563 | validation: 0.7186716828175441]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0493597282239353		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.0493597282239353 | validation: 1.0586993898244328]
	TIME [epoch: 0.696 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0751213375969504		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.0751213375969504 | validation: 0.9792365748152909]
	TIME [epoch: 0.694 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2332338487831516		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.2332338487831516 | validation: 1.4318654617934303]
	TIME [epoch: 0.694 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2531517597640713		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.2531517597640713 | validation: 0.7494311666936793]
	TIME [epoch: 0.694 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0038701745196128		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.0038701745196128 | validation: 0.8291504266609702]
	TIME [epoch: 0.693 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.00487788130041		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.00487788130041 | validation: 0.8793498859888417]
	TIME [epoch: 0.693 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0631884600670407		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.0631884600670407 | validation: 0.8697763464840897]
	TIME [epoch: 0.694 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0889026060886833		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.0889026060886833 | validation: 1.1240761914992103]
	TIME [epoch: 0.693 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1303549299995101		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.1303549299995101 | validation: 0.7936840347716037]
	TIME [epoch: 0.702 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2275428507470938		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.2275428507470938 | validation: 1.5158677455256697]
	TIME [epoch: 0.693 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2604415359146208		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.2604415359146208 | validation: 0.8045193632197982]
	TIME [epoch: 0.693 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9764798844508207		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.9764798844508207 | validation: 0.716737933420554]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.977193906763026		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.977193906763026 | validation: 1.0396495446036067]
	TIME [epoch: 0.696 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0370370079568094		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.0370370079568094 | validation: 0.9108976352084563]
	TIME [epoch: 0.695 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1345270730599626		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.1345270730599626 | validation: 1.371968894377082]
	TIME [epoch: 0.697 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1941838469586978		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.1941838469586978 | validation: 0.7619951622204155]
	TIME [epoch: 0.695 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9814203015826632		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.9814203015826632 | validation: 0.8142540807307825]
	TIME [epoch: 0.695 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.934200433362434		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.934200433362434 | validation: 0.7635304841452045]
	TIME [epoch: 0.696 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9367934910460067		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.9367934910460067 | validation: 0.958213708467742]
	TIME [epoch: 0.697 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9756229102830475		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.9756229102830475 | validation: 0.916732074428392]
	TIME [epoch: 0.694 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1105471536680602		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.1105471536680602 | validation: 1.5756435368984993]
	TIME [epoch: 0.695 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.268734733898869		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.268734733898869 | validation: 0.6930989352586184]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_184.pth
	Model improved!!!
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.977947096976962		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.977947096976962 | validation: 0.862989254200498]
	TIME [epoch: 0.695 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9488293810438152		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.9488293810438152 | validation: 0.8147122335120468]
	TIME [epoch: 0.692 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.946948512717093		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.946948512717093 | validation: 0.849172805986266]
	TIME [epoch: 0.693 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0126343018228294		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.0126343018228294 | validation: 1.0692199386380807]
	TIME [epoch: 0.693 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0825881200502423		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.0825881200502423 | validation: 0.761158720417804]
	TIME [epoch: 0.692 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0426522766100041		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.0426522766100041 | validation: 1.3303870429775198]
	TIME [epoch: 0.692 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.122276241863805		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.122276241863805 | validation: 0.8009486401324168]
	TIME [epoch: 0.694 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0149931597648836		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.0149931597648836 | validation: 0.97559533479208]
	TIME [epoch: 0.693 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9673402004953805		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.9673402004953805 | validation: 0.7533751937699013]
	TIME [epoch: 0.692 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9976030611052766		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.9976030611052766 | validation: 1.2047604373223404]
	TIME [epoch: 0.692 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0755320076524015		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.0755320076524015 | validation: 0.7706234408579634]
	TIME [epoch: 0.692 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9542321671599362		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.9542321671599362 | validation: 1.0352224999133366]
	TIME [epoch: 0.693 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9735631172910978		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.9735631172910978 | validation: 0.7653897310949057]
	TIME [epoch: 0.692 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0078743609853185		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.0078743609853185 | validation: 1.1972095082069756]
	TIME [epoch: 0.692 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0329238401940404		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.0329238401940404 | validation: 0.6841385294064595]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_199.pth
	Model improved!!!
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9981726462583449		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.9981726462583449 | validation: 1.09800510484208]
	TIME [epoch: 0.693 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9846001578491504		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.9846001578491504 | validation: 0.7466825902819297]
	TIME [epoch: 175 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9682853863692427		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.9682853863692427 | validation: 1.0396561854942834]
	TIME [epoch: 1.37 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0298146962006889		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.0298146962006889 | validation: 0.856662800908071]
	TIME [epoch: 1.35 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9699078137399708		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.9699078137399708 | validation: 0.9554233177306046]
	TIME [epoch: 1.35 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9264631479384372		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.9264631479384372 | validation: 0.7380278782244618]
	TIME [epoch: 1.35 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9494856111762081		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.9494856111762081 | validation: 1.1465781918975118]
	TIME [epoch: 1.35 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9998202963162456		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.9998202963162456 | validation: 0.7410193125325648]
	TIME [epoch: 1.35 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.96086338348313		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.96086338348313 | validation: 0.9924774235258564]
	TIME [epoch: 1.35 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9370284929980177		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.9370284929980177 | validation: 0.7635568627354732]
	TIME [epoch: 1.36 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9412522687089974		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.9412522687089974 | validation: 1.1119061443274318]
	TIME [epoch: 1.36 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9993917979229987		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.9993917979229987 | validation: 0.7757602028600907]
	TIME [epoch: 1.35 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9925624603876244		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.9925624603876244 | validation: 1.109953389406001]
	TIME [epoch: 1.35 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9839087622544481		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.9839087622544481 | validation: 0.6681779330147759]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9968546023106305		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.9968546023106305 | validation: 0.9839741937689853]
	TIME [epoch: 1.35 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9138801432907139		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.9138801432907139 | validation: 0.7174539346980264]
	TIME [epoch: 1.35 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8925771057421404		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.8925771057421404 | validation: 1.0748623827512287]
	TIME [epoch: 1.35 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9653497556342291		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.9653497556342291 | validation: 0.8545902743087517]
	TIME [epoch: 1.35 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.004768460480449		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.004768460480449 | validation: 1.0349722771929561]
	TIME [epoch: 1.35 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9408721263233423		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.9408721263233423 | validation: 0.7043056639397806]
	TIME [epoch: 1.35 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9185734093182214		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.9185734093182214 | validation: 1.1016345188838745]
	TIME [epoch: 1.35 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9551388088330832		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.9551388088330832 | validation: 0.732356644590876]
	TIME [epoch: 1.35 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9331151894614863		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.9331151894614863 | validation: 1.0893102858052441]
	TIME [epoch: 1.35 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9420658385432106		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.9420658385432106 | validation: 0.697000869920429]
	TIME [epoch: 1.35 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9193157352095903		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.9193157352095903 | validation: 1.0872151813106885]
	TIME [epoch: 1.35 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9345529614197338		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.9345529614197338 | validation: 0.7017610002242067]
	TIME [epoch: 1.35 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9399939566722675		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.9399939566722675 | validation: 1.0760681768072828]
	TIME [epoch: 1.35 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.970699411645405		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.970699411645405 | validation: 0.842844079128862]
	TIME [epoch: 1.35 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9457594289698069		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.9457594289698069 | validation: 0.8412398175752505]
	TIME [epoch: 1.35 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9788993842475137		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.9788993842475137 | validation: 0.8808697441511232]
	TIME [epoch: 1.36 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9033199140485897		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.9033199140485897 | validation: 0.7178691697893342]
	TIME [epoch: 1.35 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.863568130994887		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.863568130994887 | validation: 0.7976619700708565]
	TIME [epoch: 1.36 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8631660514516191		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.8631660514516191 | validation: 0.8172432885844337]
	TIME [epoch: 1.35 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9211258554648039		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.9211258554648039 | validation: 1.0480639959770108]
	TIME [epoch: 1.35 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9355622049674901		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.9355622049674901 | validation: 0.7193467594854899]
	TIME [epoch: 1.35 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9678154754689294		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.9678154754689294 | validation: 1.3218883806284172]
	TIME [epoch: 1.35 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0417206833222177		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.0417206833222177 | validation: 0.6611546669208561]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9037786580085947		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.9037786580085947 | validation: 0.9209110055447012]
	TIME [epoch: 1.36 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8587020286587357		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.8587020286587357 | validation: 0.7039125850289638]
	TIME [epoch: 1.35 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8725266479635565		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.8725266479635565 | validation: 0.992744755736874]
	TIME [epoch: 1.35 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9170246282352327		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.9170246282352327 | validation: 0.824665433190412]
	TIME [epoch: 1.35 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9534227536922893		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.9534227536922893 | validation: 1.0108610924110328]
	TIME [epoch: 1.35 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9166285621171131		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.9166285621171131 | validation: 0.7724739433546226]
	TIME [epoch: 1.35 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8945795745279556		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.8945795745279556 | validation: 1.0502682776823165]
	TIME [epoch: 1.35 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9301348030042145		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.9301348030042145 | validation: 0.7266843486763896]
	TIME [epoch: 1.35 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9056948093978704		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.9056948093978704 | validation: 0.9332860722865957]
	TIME [epoch: 1.35 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8676711971214386		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.8676711971214386 | validation: 0.6656419767541069]
	TIME [epoch: 1.35 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8517616293113507		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.8517616293113507 | validation: 1.02388116756655]
	TIME [epoch: 1.35 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9014645858478084		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.9014645858478084 | validation: 0.7065803497332429]
	TIME [epoch: 1.35 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9921017322285102		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.9921017322285102 | validation: 1.22525513303145]
	TIME [epoch: 1.35 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.995840977201105		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.995840977201105 | validation: 0.7340653348306895]
	TIME [epoch: 1.35 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8756069601488179		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.8756069601488179 | validation: 0.8442222249077947]
	TIME [epoch: 1.35 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8945018888576404		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.8945018888576404 | validation: 0.8741677606585964]
	TIME [epoch: 1.35 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8964946990202968		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.8964946990202968 | validation: 0.8042822955594341]
	TIME [epoch: 1.36 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8655431879995126		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.8655431879995126 | validation: 0.737896753730964]
	TIME [epoch: 1.35 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8621676349826995		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.8621676349826995 | validation: 0.9030766211224294]
	TIME [epoch: 1.35 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8746782912351699		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.8746782912351699 | validation: 0.7039460988316923]
	TIME [epoch: 1.35 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8749907665196807		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.8749907665196807 | validation: 0.9336066999485046]
	TIME [epoch: 1.35 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8781666032767025		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.8781666032767025 | validation: 0.7328266746018497]
	TIME [epoch: 1.35 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9137082653669409		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.9137082653669409 | validation: 1.2452180963897095]
	TIME [epoch: 1.35 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9937558633817076		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.9937558633817076 | validation: 0.7415339185820211]
	TIME [epoch: 1.35 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8971297519294091		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.8971297519294091 | validation: 0.9419168671266042]
	TIME [epoch: 1.35 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8514523320894927		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.8514523320894927 | validation: 0.7116113235666636]
	TIME [epoch: 1.35 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8626726507319147		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.8626726507319147 | validation: 1.077930056459984]
	TIME [epoch: 1.35 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9141199438115993		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.9141199438115993 | validation: 0.740037037505066]
	TIME [epoch: 1.35 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8944506779804136		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.8944506779804136 | validation: 1.0246650678270321]
	TIME [epoch: 1.35 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8868648607862301		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.8868648607862301 | validation: 0.7579540579032515]
	TIME [epoch: 1.35 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8517225358260231		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.8517225358260231 | validation: 0.9102253345599798]
	TIME [epoch: 1.35 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8609125537882133		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.8609125537882133 | validation: 0.7517979432604851]
	TIME [epoch: 1.35 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8651864691388755		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.8651864691388755 | validation: 0.9408922131174633]
	TIME [epoch: 1.35 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8596797188603487		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.8596797188603487 | validation: 0.6912226696599589]
	TIME [epoch: 1.35 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8487886061697065		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.8487886061697065 | validation: 1.0982030122718955]
	TIME [epoch: 1.35 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9010571002130662		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.9010571002130662 | validation: 0.6765651972916491]
	TIME [epoch: 1.35 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.953224404324308		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.953224404324308 | validation: 1.087891845606421]
	TIME [epoch: 1.35 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9134516241888448		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.9134516241888448 | validation: 0.7108420153533983]
	TIME [epoch: 1.35 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8444057960096174		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.8444057960096174 | validation: 0.9070261905895092]
	TIME [epoch: 1.36 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8815506770311092		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.8815506770311092 | validation: 0.9280879166875856]
	TIME [epoch: 1.35 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9422565005634894		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.9422565005634894 | validation: 0.8710341526598313]
	TIME [epoch: 1.35 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8805887772534362		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.8805887772534362 | validation: 0.7286529591926412]
	TIME [epoch: 1.35 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8329297877702274		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.8329297877702274 | validation: 0.8264844864167591]
	TIME [epoch: 1.35 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8137437114119268		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.8137437114119268 | validation: 0.7662259028734433]
	TIME [epoch: 1.35 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.808209996478935		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.808209996478935 | validation: 0.7306035035294409]
	TIME [epoch: 1.35 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8177150104706317		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.8177150104706317 | validation: 0.9793189815255356]
	TIME [epoch: 1.35 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8546564042035887		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.8546564042035887 | validation: 0.7178255663757439]
	TIME [epoch: 1.35 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9107789415386957		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.9107789415386957 | validation: 1.220703916841377]
	TIME [epoch: 1.35 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9745802254458701		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.9745802254458701 | validation: 0.7573160721257094]
	TIME [epoch: 1.35 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8960302009540675		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.8960302009540675 | validation: 0.9146286670351839]
	TIME [epoch: 1.36 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.879865901849432		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.879865901849432 | validation: 0.8224826806317499]
	TIME [epoch: 1.35 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8305762141787887		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.8305762141787887 | validation: 0.8178159971463836]
	TIME [epoch: 1.35 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8063690036135903		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.8063690036135903 | validation: 0.7330745602688151]
	TIME [epoch: 1.35 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8133723905105618		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.8133723905105618 | validation: 0.9083364687772634]
	TIME [epoch: 1.35 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8464108153351398		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.8464108153351398 | validation: 0.7538333310663274]
	TIME [epoch: 1.35 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8806243504175449		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.8806243504175449 | validation: 1.0524923709565024]
	TIME [epoch: 1.35 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9093267081038826		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.9093267081038826 | validation: 0.6883944211982537]
	TIME [epoch: 1.35 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8661438251516222		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.8661438251516222 | validation: 1.0574378132289686]
	TIME [epoch: 1.35 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.875194107480335		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.875194107480335 | validation: 0.7400672433642811]
	TIME [epoch: 1.35 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8917590893272801		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.8917590893272801 | validation: 0.9662570301668855]
	TIME [epoch: 1.35 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8561563921828435		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.8561563921828435 | validation: 0.7557893244258631]
	TIME [epoch: 1.36 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8248958888697291		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.8248958888697291 | validation: 0.8227024361862054]
	TIME [epoch: 1.35 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8112132986663858		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.8112132986663858 | validation: 0.7756776376560406]
	TIME [epoch: 1.35 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8274966351295407		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.8274966351295407 | validation: 0.9767416526581253]
	TIME [epoch: 1.35 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.867121487863644		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.867121487863644 | validation: 0.8055133590452558]
	TIME [epoch: 1.36 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9273208917144701		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.9273208917144701 | validation: 1.0090700097545262]
	TIME [epoch: 1.36 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8748267900112259		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.8748267900112259 | validation: 0.7435193317308273]
	TIME [epoch: 1.36 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8132786108444716		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.8132786108444716 | validation: 0.8888675258089327]
	TIME [epoch: 1.36 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8132470804136172		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.8132470804136172 | validation: 0.7333135619907861]
	TIME [epoch: 1.36 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8326690414832503		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.8326690414832503 | validation: 0.8756238912862087]
	TIME [epoch: 1.36 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8384062650933365		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.8384062650933365 | validation: 0.7763304450720746]
	TIME [epoch: 1.36 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8318935119870617		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.8318935119870617 | validation: 0.8222962829889767]
	TIME [epoch: 1.36 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8488104348115008		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.8488104348115008 | validation: 0.865353252255747]
	TIME [epoch: 1.36 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8527149713640684		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.8527149713640684 | validation: 0.750649963742576]
	TIME [epoch: 1.35 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.846386807515699		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.846386807515699 | validation: 0.8734610486559644]
	TIME [epoch: 1.35 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8109163300473218		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.8109163300473218 | validation: 0.6942785046840685]
	TIME [epoch: 1.35 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8151631261272136		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.8151631261272136 | validation: 1.0630539597186979]
	TIME [epoch: 1.35 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8785469039402329		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.8785469039402329 | validation: 0.7323846605366245]
	TIME [epoch: 1.35 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8935694317251389		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.8935694317251389 | validation: 1.0825570784591576]
	TIME [epoch: 1.35 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9003010142393312		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.9003010142393312 | validation: 0.7489414829367761]
	TIME [epoch: 1.35 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8557630803152926		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.8557630803152926 | validation: 0.8589625254870349]
	TIME [epoch: 1.35 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8033356326790576		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.8033356326790576 | validation: 0.7769689235558531]
	TIME [epoch: 1.35 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7940169529135267		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.7940169529135267 | validation: 0.7494401350457319]
	TIME [epoch: 1.35 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.80290524290413		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.80290524290413 | validation: 0.8662730844042517]
	TIME [epoch: 1.35 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8067271407838069		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.8067271407838069 | validation: 0.703558761512404]
	TIME [epoch: 1.35 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.809066342386033		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.809066342386033 | validation: 0.9505758605656853]
	TIME [epoch: 1.35 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8215262333520988		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.8215262333520988 | validation: 0.7216456949425594]
	TIME [epoch: 1.35 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8517173610771842		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.8517173610771842 | validation: 1.0962922213234112]
	TIME [epoch: 1.35 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9077647552766993		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.9077647552766993 | validation: 0.7780045408250518]
	TIME [epoch: 1.35 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8628284519782718		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.8628284519782718 | validation: 0.8844579676620739]
	TIME [epoch: 1.35 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8481341735684892		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.8481341735684892 | validation: 0.809760230526495]
	TIME [epoch: 1.35 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8082382360248981		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.8082382360248981 | validation: 0.7746244910932367]
	TIME [epoch: 1.35 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7945966749579775		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.7945966749579775 | validation: 0.7582212156045041]
	TIME [epoch: 1.35 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8014981964536405		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.8014981964536405 | validation: 0.910546261606386]
	TIME [epoch: 1.35 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8197032934438647		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.8197032934438647 | validation: 0.7295306794635843]
	TIME [epoch: 1.35 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8250543542170067		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.8250543542170067 | validation: 0.9578296328228222]
	TIME [epoch: 1.35 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8167828865698072		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.8167828865698072 | validation: 0.67847655756084]
	TIME [epoch: 1.35 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8390540807262559		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.8390540807262559 | validation: 1.1285043154306267]
	TIME [epoch: 1.35 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9044644350569729		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.9044644350569729 | validation: 0.743903856061086]
	TIME [epoch: 1.35 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8777690477433427		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.8777690477433427 | validation: 0.8708393585558252]
	TIME [epoch: 1.35 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8355366987658082		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.8355366987658082 | validation: 0.8250889659570038]
	TIME [epoch: 1.35 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8059626460684317		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.8059626460684317 | validation: 0.737156052147582]
	TIME [epoch: 1.35 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8155112276260823		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.8155112276260823 | validation: 0.8470675854970988]
	TIME [epoch: 1.35 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7895608217463987		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.7895608217463987 | validation: 0.7092705137227309]
	TIME [epoch: 1.35 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8046597802506569		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.8046597802506569 | validation: 0.8589224547075738]
	TIME [epoch: 1.36 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.790790301944571		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.790790301944571 | validation: 0.735683278728889]
	TIME [epoch: 1.35 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.782916427906362		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.782916427906362 | validation: 0.850860672311449]
	TIME [epoch: 1.35 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7839681400414924		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.7839681400414924 | validation: 0.7123871562996105]
	TIME [epoch: 1.35 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.79313071799375		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.79313071799375 | validation: 1.0480823016862286]
	TIME [epoch: 1.35 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8827574316244423		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.8827574316244423 | validation: 0.8174204292259486]
	TIME [epoch: 1.35 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9579019417104166		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.9579019417104166 | validation: 0.9552092987107997]
	TIME [epoch: 1.35 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8411616080909866		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.8411616080909866 | validation: 0.7440422994206496]
	TIME [epoch: 1.35 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7737558742835501		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.7737558742835501 | validation: 0.8200385105002796]
	TIME [epoch: 1.35 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7617736364295706		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.7617736364295706 | validation: 0.7194216888233611]
	TIME [epoch: 1.35 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7769385800187586		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.7769385800187586 | validation: 0.9014266184303846]
	TIME [epoch: 1.35 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8587026619263526		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.8587026619263526 | validation: 0.7694617439638787]
	TIME [epoch: 1.35 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8601580671956258		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.8601580671956258 | validation: 0.8748359192026888]
	TIME [epoch: 1.35 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8062339583491306		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.8062339583491306 | validation: 0.7564785340782999]
	TIME [epoch: 1.35 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8077904311358844		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.8077904311358844 | validation: 0.8833094563251193]
	TIME [epoch: 1.35 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7999081694344516		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.7999081694344516 | validation: 0.7525298177137124]
	TIME [epoch: 1.36 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8165912963065766		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.8165912963065766 | validation: 0.9680497486735453]
	TIME [epoch: 1.35 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8230186955458885		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.8230186955458885 | validation: 0.6790348705979242]
	TIME [epoch: 1.35 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8246363928813734		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.8246363928813734 | validation: 1.007873545913034]
	TIME [epoch: 1.35 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.845841262736323		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.845841262736323 | validation: 0.7569772369668916]
	TIME [epoch: 1.35 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8145504855324585		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.8145504855324585 | validation: 0.7676488714894136]
	TIME [epoch: 1.35 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7764350249015428		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.7764350249015428 | validation: 0.831818528139308]
	TIME [epoch: 1.36 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7873930260456597		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.7873930260456597 | validation: 0.7179858241160728]
	TIME [epoch: 1.36 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8145491946180955		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.8145491946180955 | validation: 1.041849098922295]
	TIME [epoch: 1.35 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8483042219549582		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.8483042219549582 | validation: 0.7098097770085899]
	TIME [epoch: 1.35 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8161792580331861		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.8161792580331861 | validation: 0.9053514163308899]
	TIME [epoch: 1.35 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7884169206577445		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.7884169206577445 | validation: 0.700138951439242]
	TIME [epoch: 1.35 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7781846937536457		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.7781846937536457 | validation: 0.8843331806257793]
	TIME [epoch: 1.35 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8026629275345394		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.8026629275345394 | validation: 0.7880165810347101]
	TIME [epoch: 1.35 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8016110690622834		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.8016110690622834 | validation: 0.7548693065785015]
	TIME [epoch: 1.35 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7692379960269327		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.7692379960269327 | validation: 0.7821917148801041]
	TIME [epoch: 1.35 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7559411067342618		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.7559411067342618 | validation: 0.7558456609988536]
	TIME [epoch: 1.35 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7663913195792824		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.7663913195792824 | validation: 0.8147385054088586]
	TIME [epoch: 1.35 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7923473440258927		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.7923473440258927 | validation: 0.7680505405134143]
	TIME [epoch: 1.35 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7968191986071151		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.7968191986071151 | validation: 0.8893412052937704]
	TIME [epoch: 1.35 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8182375893923529		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.8182375893923529 | validation: 0.9156895680626023]
	TIME [epoch: 1.35 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9561129714379593		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.9561129714379593 | validation: 0.9011392592107534]
	TIME [epoch: 1.35 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8316389203734511		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.8316389203734511 | validation: 0.7091176716370136]
	TIME [epoch: 1.35 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.78811456221763		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.78811456221763 | validation: 0.9562690608671958]
	TIME [epoch: 1.35 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8026374809646944		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.8026374809646944 | validation: 0.6803599789434238]
	TIME [epoch: 1.35 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8162516959773839		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.8162516959773839 | validation: 0.9047134073714063]
	TIME [epoch: 1.35 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7900071470690675		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.7900071470690675 | validation: 0.7055068388409917]
	TIME [epoch: 1.36 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7615228311092566		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.7615228311092566 | validation: 0.8612924452172349]
	TIME [epoch: 1.36 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.766338342382407		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.766338342382407 | validation: 0.7422170828569343]
	TIME [epoch: 1.36 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7717708953914187		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.7717708953914187 | validation: 0.9189421242001441]
	TIME [epoch: 1.35 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.80218560019785		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.80218560019785 | validation: 0.7854148971275433]
	TIME [epoch: 1.35 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8759605770001853		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.8759605770001853 | validation: 0.9534469761007487]
	TIME [epoch: 1.35 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8172074542798001		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.8172074542798001 | validation: 0.7641084262787413]
	TIME [epoch: 1.35 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7741155909139914		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.7741155909139914 | validation: 0.8180554016535656]
	TIME [epoch: 1.35 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7797172965345149		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.7797172965345149 | validation: 0.8180516848417438]
	TIME [epoch: 1.35 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7814360381664335		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.7814360381664335 | validation: 0.7690537140766871]
	TIME [epoch: 1.35 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7702519205429919		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.7702519205429919 | validation: 0.793328994134268]
	TIME [epoch: 1.35 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7660129838908878		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.7660129838908878 | validation: 0.7505564560556899]
	TIME [epoch: 1.36 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7643890826710065		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.7643890826710065 | validation: 0.8454239472753464]
	TIME [epoch: 1.36 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7749733928922322		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.7749733928922322 | validation: 0.74405055984075]
	TIME [epoch: 1.35 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7992648452250134		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.7992648452250134 | validation: 0.8206047356461078]
	TIME [epoch: 1.35 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7932348530523704		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.7932348530523704 | validation: 0.8033344366918533]
	TIME [epoch: 1.35 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7859154473888359		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.7859154473888359 | validation: 0.7574604249606751]
	TIME [epoch: 1.36 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8060365089939961		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.8060365089939961 | validation: 1.0952427649741718]
	TIME [epoch: 1.36 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8741092694922352		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.8741092694922352 | validation: 0.6986350717964998]
	TIME [epoch: 1.35 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8233977866264272		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.8233977866264272 | validation: 0.895692062836125]
	TIME [epoch: 1.36 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7691357307571575		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.7691357307571575 | validation: 0.7553527662467996]
	TIME [epoch: 1.36 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7658851315977704		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.7658851315977704 | validation: 0.7997471904955091]
	TIME [epoch: 1.35 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7775646193556384		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.7775646193556384 | validation: 0.8088017912735179]
	TIME [epoch: 1.36 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7715072002435087		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.7715072002435087 | validation: 0.7580712019568838]
	TIME [epoch: 1.36 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7526429752541733		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.7526429752541733 | validation: 0.8224972725132633]
	TIME [epoch: 1.36 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7607105907033338		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.7607105907033338 | validation: 0.7557742409240346]
	TIME [epoch: 1.36 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7964485690592565		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.7964485690592565 | validation: 1.0070242498545652]
	TIME [epoch: 1.35 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8479588629824187		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.8479588629824187 | validation: 0.7497717856009369]
	TIME [epoch: 1.35 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8035643337423602		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.8035643337423602 | validation: 0.8193094118051443]
	TIME [epoch: 1.36 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.759379209160772		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.759379209160772 | validation: 0.7594129842180004]
	TIME [epoch: 1.36 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7503077154251321		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.7503077154251321 | validation: 0.7608762022247807]
	TIME [epoch: 1.36 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7546488572374359		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.7546488572374359 | validation: 0.8107148502217885]
	TIME [epoch: 1.35 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7897875252990966		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.7897875252990966 | validation: 0.8084866805186742]
	TIME [epoch: 1.36 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8194259698005428		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.8194259698005428 | validation: 0.7931792308507595]
	TIME [epoch: 1.35 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7743677994693282		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.7743677994693282 | validation: 0.7937965909204282]
	TIME [epoch: 1.36 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7513887200053756		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.7513887200053756 | validation: 0.7308195570692334]
	TIME [epoch: 1.36 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7732722445945492		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.7732722445945492 | validation: 0.9196530333183749]
	TIME [epoch: 1.38 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8002406241643668		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.8002406241643668 | validation: 0.724226833022184]
	TIME [epoch: 1.35 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8092762085539482		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.8092762085539482 | validation: 0.9879381452866559]
	TIME [epoch: 1.36 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7943199909242126		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.7943199909242126 | validation: 0.7242171616612605]
	TIME [epoch: 1.35 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7862208520192263		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.7862208520192263 | validation: 0.8764761834749216]
	TIME [epoch: 1.35 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7717986438590329		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.7717986438590329 | validation: 0.7359843712426103]
	TIME [epoch: 1.35 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7619800824432557		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.7619800824432557 | validation: 0.8564398236955881]
	TIME [epoch: 1.35 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.761609365016864		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.761609365016864 | validation: 0.7377186385799006]
	TIME [epoch: 1.35 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7597396388227735		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.7597396388227735 | validation: 0.8433897837709301]
	TIME [epoch: 1.36 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7637908090635278		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.7637908090635278 | validation: 0.8040729149645817]
	TIME [epoch: 1.35 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7949297870899704		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.7949297870899704 | validation: 0.8106435465024339]
	TIME [epoch: 1.36 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8074832085834999		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.8074832085834999 | validation: 0.8524633617016312]
	TIME [epoch: 1.35 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7930094503493405		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.7930094503493405 | validation: 0.7486326145337557]
	TIME [epoch: 1.35 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.760487379704135		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.760487379704135 | validation: 0.7858456870686301]
	TIME [epoch: 1.36 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7463659706398929		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.7463659706398929 | validation: 0.7385469350939557]
	TIME [epoch: 1.35 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.750945321741042		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.750945321741042 | validation: 0.9166430227767672]
	TIME [epoch: 1.35 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7713055291218208		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.7713055291218208 | validation: 0.7294174715415056]
	TIME [epoch: 1.35 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7793247692819173		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.7793247692819173 | validation: 0.9202195892863255]
	TIME [epoch: 1.35 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7753974582906489		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.7753974582906489 | validation: 0.7003198158598873]
	TIME [epoch: 1.35 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.775053792245129		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.775053792245129 | validation: 0.9093661903771744]
	TIME [epoch: 1.35 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7700445088388436		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.7700445088388436 | validation: 0.7264822737683545]
	TIME [epoch: 1.35 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7637490143036243		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.7637490143036243 | validation: 0.8460016324522434]
	TIME [epoch: 1.35 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7654224882103102		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.7654224882103102 | validation: 0.7979562592261257]
	TIME [epoch: 1.35 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.770358269643531		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.770358269643531 | validation: 0.8239155376268045]
	TIME [epoch: 1.36 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7826611774911784		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.7826611774911784 | validation: 0.8036893016673267]
	TIME [epoch: 1.35 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7884434150551692		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.7884434150551692 | validation: 0.743072603699964]
	TIME [epoch: 1.36 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7592290140277507		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.7592290140277507 | validation: 0.8311184235825867]
	TIME [epoch: 1.36 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7588846810869169		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.7588846810869169 | validation: 0.7536977792462256]
	TIME [epoch: 1.35 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7521863589613039		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.7521863589613039 | validation: 0.8083473790065313]
	TIME [epoch: 1.36 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7530049540735689		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.7530049540735689 | validation: 0.7667243897544127]
	TIME [epoch: 1.35 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7427856369776078		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.7427856369776078 | validation: 0.7510307149298011]
	TIME [epoch: 1.36 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.758403305243383		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.758403305243383 | validation: 0.8891697062465541]
	TIME [epoch: 1.36 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7578661333097075		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.7578661333097075 | validation: 0.7326033136216522]
	TIME [epoch: 1.36 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7768271235543383		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.7768271235543383 | validation: 0.9963271438001855]
	TIME [epoch: 1.36 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8118980087901375		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.8118980087901375 | validation: 0.7235592519212783]
	TIME [epoch: 1.36 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7841988795843741		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.7841988795843741 | validation: 0.8451851048915346]
	TIME [epoch: 1.35 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7487679018516065		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.7487679018516065 | validation: 0.7612823749826184]
	TIME [epoch: 1.36 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.749997232534299		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.749997232534299 | validation: 0.7769206937951356]
	TIME [epoch: 1.35 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7520121168039102		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.7520121168039102 | validation: 0.7784674429634753]
	TIME [epoch: 1.36 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7575381664088476		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.7575381664088476 | validation: 0.8040948123586351]
	TIME [epoch: 1.35 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7564790573451966		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.7564790573451966 | validation: 0.7764689923945841]
	TIME [epoch: 1.35 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7548414975034949		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.7548414975034949 | validation: 0.8234558450082439]
	TIME [epoch: 1.36 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7691858693787179		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.7691858693787179 | validation: 0.7453572859998752]
	TIME [epoch: 1.35 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7525348150305817		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.7525348150305817 | validation: 0.8231396747515678]
	TIME [epoch: 1.35 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7373375178324717		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.7373375178324717 | validation: 0.7127884233158533]
	TIME [epoch: 1.35 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7351399182356524		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.7351399182356524 | validation: 0.9225727207834368]
	TIME [epoch: 1.36 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7609768059955101		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.7609768059955101 | validation: 0.7429205630115341]
	TIME [epoch: 1.35 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8085723856372714		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.8085723856372714 | validation: 0.9284494675962478]
	TIME [epoch: 1.35 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.761096326063076		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.761096326063076 | validation: 0.7377792266352607]
	TIME [epoch: 1.35 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7348253065257856		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.7348253065257856 | validation: 0.7920666693820116]
	TIME [epoch: 1.35 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7329620560856828		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.7329620560856828 | validation: 0.7943188736725273]
	TIME [epoch: 1.36 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7507972110184633		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.7507972110184633 | validation: 0.7611882250373081]
	TIME [epoch: 1.36 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7543874248698859		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.7543874248698859 | validation: 0.8268875275309275]
	TIME [epoch: 1.35 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.773942075270459		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.773942075270459 | validation: 0.8097815801944711]
	TIME [epoch: 1.36 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7854555426556611		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.7854555426556611 | validation: 0.8202296263567856]
	TIME [epoch: 1.36 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7671159196293019		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.7671159196293019 | validation: 0.8108429712559982]
	TIME [epoch: 1.35 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7435273578955703		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.7435273578955703 | validation: 0.7405973196225508]
	TIME [epoch: 1.35 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7281136289223272		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.7281136289223272 | validation: 0.7845377476606333]
	TIME [epoch: 1.35 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7357798106135099		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.7357798106135099 | validation: 0.8187484271910116]
	TIME [epoch: 1.35 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7541271799704551		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.7541271799704551 | validation: 0.796392982809085]
	TIME [epoch: 1.36 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7730228365933836		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.7730228365933836 | validation: 0.8335514683542633]
	TIME [epoch: 1.37 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7471378363740749		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.7471378363740749 | validation: 0.752013434884649]
	TIME [epoch: 1.35 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7332772833729462		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.7332772833729462 | validation: 0.8872572093173746]
	TIME [epoch: 1.36 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7468656618341923		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.7468656618341923 | validation: 0.7126472317907537]
	TIME [epoch: 1.35 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.780651888008437		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.780651888008437 | validation: 0.97356346789597]
	TIME [epoch: 1.35 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7671706443388189		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.7671706443388189 | validation: 0.7188441318188209]
	TIME [epoch: 1.36 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7344627152855467		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.7344627152855467 | validation: 0.8066348306595249]
	TIME [epoch: 1.35 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7314673867044386		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.7314673867044386 | validation: 0.7936103715274587]
	TIME [epoch: 1.36 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7459981969263138		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.7459981969263138 | validation: 0.8012870493507315]
	TIME [epoch: 1.36 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7589825866320721		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.7589825866320721 | validation: 0.8245251437482358]
	TIME [epoch: 1.35 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7572094853650146		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.7572094853650146 | validation: 0.7760563561320674]
	TIME [epoch: 1.35 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7709548972780542		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.7709548972780542 | validation: 0.7913305318648717]
	TIME [epoch: 1.36 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7335863452393211		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.7335863452393211 | validation: 0.7479250087308928]
	TIME [epoch: 1.36 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7236662932859746		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.7236662932859746 | validation: 0.7555720069047788]
	TIME [epoch: 1.35 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7154759227963351		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.7154759227963351 | validation: 0.7998347227703386]
	TIME [epoch: 1.36 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7218736427542031		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.7218736427542031 | validation: 0.7217747616043412]
	TIME [epoch: 1.36 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7376928560421294		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.7376928560421294 | validation: 0.9082657141028595]
	TIME [epoch: 1.35 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7517547116066959		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.7517547116066959 | validation: 0.7231594322416328]
	TIME [epoch: 1.36 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.745240436688648		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.745240436688648 | validation: 0.8379374103379625]
	TIME [epoch: 1.36 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7366106378169823		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.7366106378169823 | validation: 0.739715499241119]
	TIME [epoch: 1.36 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7439102506188899		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.7439102506188899 | validation: 0.8237331269667094]
	TIME [epoch: 1.35 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7417526211719131		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.7417526211719131 | validation: 0.8569220374985884]
	TIME [epoch: 1.36 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7592341549020416		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.7592341549020416 | validation: 0.7470455297215868]
	TIME [epoch: 1.35 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7669016607065998		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.7669016607065998 | validation: 0.8924273787867414]
	TIME [epoch: 177 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7338368675817438		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.7338368675817438 | validation: 0.7289076774306474]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_5_v_mmd4_502.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 1126.392 seconds.
