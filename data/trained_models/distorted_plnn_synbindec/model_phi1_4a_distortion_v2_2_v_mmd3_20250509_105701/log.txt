Args:
Namespace(name='model_phi1_4a_distortion_v2_2_v_mmd3', outdir='out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_2/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_2/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.020249879, 0.1, 1.0], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1124044447

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0875047653389833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0875047653389833 | validation: 3.416876505237842]
	TIME [epoch: 125 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9707424248118035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9707424248118035 | validation: 3.264934950838147]
	TIME [epoch: 0.488 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.876006307060277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.876006307060277 | validation: 3.3769773992331142]
	TIME [epoch: 0.483 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2380354028396248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2380354028396248 | validation: 3.0987099363700406]
	TIME [epoch: 0.481 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7775207963342066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7775207963342066 | validation: 3.0007797436331636]
	TIME [epoch: 0.479 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7501133788775722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7501133788775722 | validation: 2.8989826991724814]
	TIME [epoch: 0.48 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6728929875324696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6728929875324696 | validation: 2.847801397053832]
	TIME [epoch: 0.48 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5752897836054265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5752897836054265 | validation: 2.8228512663807983]
	TIME [epoch: 0.478 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4975159141172902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4975159141172902 | validation: 2.5403996646502156]
	TIME [epoch: 0.481 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.380506793883751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.380506793883751 | validation: 2.6659498227214162]
	TIME [epoch: 0.48 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.338723789330649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.338723789330649 | validation: 2.98099578717917]
	TIME [epoch: 0.479 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.815155084765462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.815155084765462 | validation: 2.656956796797566]
	TIME [epoch: 0.479 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4256764601796554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4256764601796554 | validation: 2.4109970157178537]
	TIME [epoch: 0.481 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2980621493042084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2980621493042084 | validation: 2.219868921261351]
	TIME [epoch: 0.478 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.203553787817236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.203553787817236 | validation: 2.3346242854494155]
	TIME [epoch: 0.476 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1369527285745153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1369527285745153 | validation: 2.42511643495952]
	TIME [epoch: 0.478 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0801396127802936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0801396127802936 | validation: 2.2092631522518453]
	TIME [epoch: 0.478 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0436299289174085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0436299289174085 | validation: 2.174622018618747]
	TIME [epoch: 0.479 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1506654228461213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1506654228461213 | validation: 2.3723616130411123]
	TIME [epoch: 0.481 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.194332640097044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.194332640097044 | validation: 2.297095113713683]
	TIME [epoch: 0.478 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1044074831660877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1044074831660877 | validation: 2.0478104017226686]
	TIME [epoch: 0.478 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9809639008034332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9809639008034332 | validation: 1.902759208972351]
	TIME [epoch: 0.48 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9179023945774702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9179023945774702 | validation: 2.158059678024514]
	TIME [epoch: 0.48 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8932172937303438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8932172937303438 | validation: 2.243803400737137]
	TIME [epoch: 0.478 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0569547249914333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0569547249914333 | validation: 2.0761630162085183]
	TIME [epoch: 0.479 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9923104538643983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9923104538643983 | validation: 2.028015971292835]
	TIME [epoch: 0.479 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9073964959798928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9073964959798928 | validation: 1.8643401295128759]
	TIME [epoch: 0.479 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8767386331353266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8767386331353266 | validation: 1.8459388651390132]
	TIME [epoch: 0.478 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7929329786433692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7929329786433692 | validation: 1.9314716663172442]
	TIME [epoch: 0.484 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7296654615473102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7296654615473102 | validation: 1.9925256302329026]
	TIME [epoch: 0.48 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7402626163200363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7402626163200363 | validation: 1.8881926980063364]
	TIME [epoch: 0.479 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7682258183848416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7682258183848416 | validation: 1.7402500219443837]
	TIME [epoch: 0.479 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7430045857260712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7430045857260712 | validation: 1.9633191844223163]
	TIME [epoch: 0.479 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.744281849050438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.744281849050438 | validation: 1.800776513170848]
	TIME [epoch: 0.477 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6988727364329674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6988727364329674 | validation: 1.7979589351453897]
	TIME [epoch: 0.476 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6964432374501954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6964432374501954 | validation: 1.7840389473975258]
	TIME [epoch: 0.476 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6536267335535644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6536267335535644 | validation: 1.812882946088661]
	TIME [epoch: 0.476 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6470241445512503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6470241445512503 | validation: 1.7594858333359704]
	TIME [epoch: 0.476 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6374186281510175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6374186281510175 | validation: 1.8000617482509775]
	TIME [epoch: 0.477 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6381183984722392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6381183984722392 | validation: 1.7443404449416144]
	TIME [epoch: 0.478 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6363494149487883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6363494149487883 | validation: 1.8272452034439977]
	TIME [epoch: 0.479 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6371901585781705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6371901585781705 | validation: 1.7022350852659924]
	TIME [epoch: 0.479 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.637804606413672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.637804606413672 | validation: 1.7388405859215341]
	TIME [epoch: 0.479 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6473843452219337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6473843452219337 | validation: 1.7019304457427866]
	TIME [epoch: 0.478 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5900318234141957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5900318234141957 | validation: 1.72429943965885]
	TIME [epoch: 0.477 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5824779889617286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5824779889617286 | validation: 1.6312950417093077]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.584116313918902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.584116313918902 | validation: 1.816141969031383]
	TIME [epoch: 0.477 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5890651669829197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5890651669829197 | validation: 1.7271660350228506]
	TIME [epoch: 0.475 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6031453498127326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6031453498127326 | validation: 1.7464588135196637]
	TIME [epoch: 0.476 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5953497968461903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5953497968461903 | validation: 1.7399439468489555]
	TIME [epoch: 0.476 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5791540416475927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5791540416475927 | validation: 1.7174126502585332]
	TIME [epoch: 0.475 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5706164830557203		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.5706164830557203 | validation: 1.6471314759516176]
	TIME [epoch: 0.474 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5589533384725365		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.5589533384725365 | validation: 1.6180674539549356]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5344835927606988		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.5344835927606988 | validation: 1.8271050077668023]
	TIME [epoch: 0.477 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5576867171390405		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.5576867171390405 | validation: 1.6946687288413576]
	TIME [epoch: 0.476 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5436488204235317		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.5436488204235317 | validation: 1.789325343630222]
	TIME [epoch: 0.475 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.575367296459508		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.575367296459508 | validation: 1.6268905707507821]
	TIME [epoch: 0.475 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5243028660288753		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.5243028660288753 | validation: 1.6310307685357541]
	TIME [epoch: 0.475 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.508559554269777		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.508559554269777 | validation: 1.754526364943699]
	TIME [epoch: 0.476 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5186058409018515		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.5186058409018515 | validation: 1.6844182197461963]
	TIME [epoch: 0.475 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.525860460898359		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.525860460898359 | validation: 1.7122481798165161]
	TIME [epoch: 0.475 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5050061829750558		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.5050061829750558 | validation: 1.652902791706559]
	TIME [epoch: 0.474 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.492467820919137		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.492467820919137 | validation: 1.6593475572289624]
	TIME [epoch: 0.476 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4830734504759706		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.4830734504759706 | validation: 1.566588800486361]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.462059062675662		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.462059062675662 | validation: 1.5670511342053015]
	TIME [epoch: 0.477 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4530643732260995		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.4530643732260995 | validation: 1.5420517456614868]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4373801977558143		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.4373801977558143 | validation: 1.5911875251211696]
	TIME [epoch: 0.479 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.437127422630842		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.437127422630842 | validation: 1.9343181333774142]
	TIME [epoch: 0.476 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6226285837354641		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.6226285837354641 | validation: 1.762148223253638]
	TIME [epoch: 0.476 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5492063168702572		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.5492063168702572 | validation: 1.6438232912928283]
	TIME [epoch: 0.475 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.43350901697523		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.43350901697523 | validation: 1.5791490173104759]
	TIME [epoch: 0.476 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4109698389658842		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.4109698389658842 | validation: 1.5202457320264704]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3948192450554888		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.3948192450554888 | validation: 1.5057294133121668]
	TIME [epoch: 0.477 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.372631816247213		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.372631816247213 | validation: 1.484141899501263]
	TIME [epoch: 0.479 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.353068690136841		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.353068690136841 | validation: 1.5182275007458388]
	TIME [epoch: 0.476 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.383574588498889		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.383574588498889 | validation: 1.9921524475201817]
	TIME [epoch: 0.475 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6089411638868458		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.6089411638868458 | validation: 1.5446621866305787]
	TIME [epoch: 0.476 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3385155940711888		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.3385155940711888 | validation: 2.0010843891828594]
	TIME [epoch: 0.475 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7327674096415633		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.7327674096415633 | validation: 1.7833147626549612]
	TIME [epoch: 0.475 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.506200242120938		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.506200242120938 | validation: 1.6754355175911677]
	TIME [epoch: 0.475 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.428929817744809		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.428929817744809 | validation: 1.5021915301884996]
	TIME [epoch: 0.473 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.325105021764919		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.325105021764919 | validation: 1.4616382867839697]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.346083423254683		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.346083423254683 | validation: 1.3661031968175816]
	TIME [epoch: 0.477 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2758274608036306		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.2758274608036306 | validation: 1.3420965252601196]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2559484783610493		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.2559484783610493 | validation: 1.4081115040321455]
	TIME [epoch: 0.477 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2567830093310322		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.2567830093310322 | validation: 1.5228011705977826]
	TIME [epoch: 0.475 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.359843720757781		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.359843720757781 | validation: 1.3995052344973808]
	TIME [epoch: 0.474 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2326787613692582		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.2326787613692582 | validation: 1.250412204014089]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.160483264869453		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.160483264869453 | validation: 1.3132733535436465]
	TIME [epoch: 0.476 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1889284489926788		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.1889284489926788 | validation: 1.5275878204687698]
	TIME [epoch: 0.474 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.37461530707152		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.37461530707152 | validation: 1.4560931568627145]
	TIME [epoch: 0.474 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2557383974478633		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.2557383974478633 | validation: 1.2681841191098429]
	TIME [epoch: 0.473 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1929852808891308		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.1929852808891308 | validation: 1.4855962424582305]
	TIME [epoch: 0.473 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2932310580987598		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.2932310580987598 | validation: 1.2330583576149874]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1618964912533916		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.1618964912533916 | validation: 1.3559835902656305]
	TIME [epoch: 0.476 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1773540039963006		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.1773540039963006 | validation: 1.4077864427183973]
	TIME [epoch: 0.475 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2841790633325383		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.2841790633325383 | validation: 1.2817908738810222]
	TIME [epoch: 0.475 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1304625877634435		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.1304625877634435 | validation: 1.2150685899046627]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.155261260039635		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.155261260039635 | validation: 1.4391839347842703]
	TIME [epoch: 0.476 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.22116289172378		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.22116289172378 | validation: 1.2035996361843937]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1504292913285423		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.1504292913285423 | validation: 1.3569079809125741]
	TIME [epoch: 0.477 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1519933147706198		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.1519933147706198 | validation: 1.2525096057311413]
	TIME [epoch: 0.475 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.179872281415633		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.179872281415633 | validation: 1.3217327327746267]
	TIME [epoch: 0.475 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1297603855386988		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.1297603855386988 | validation: 1.2024791683908715]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.162951343375935		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.162951343375935 | validation: 1.339134462541674]
	TIME [epoch: 0.478 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1325423714587621		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.1325423714587621 | validation: 1.156238962813349]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1023292576026962		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.1023292576026962 | validation: 1.3310547779262092]
	TIME [epoch: 0.477 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1249886423180993		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.1249886423180993 | validation: 1.2067594199423357]
	TIME [epoch: 0.478 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1724051569322338		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.1724051569322338 | validation: 1.2230007971343375]
	TIME [epoch: 0.479 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0546054729286904		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.0546054729286904 | validation: 1.0861940475881275]
	TIME [epoch: 0.477 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.03139848846327		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.03139848846327 | validation: 1.4046339617184476]
	TIME [epoch: 0.477 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1512293455260931		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.1512293455260931 | validation: 1.23280065006068]
	TIME [epoch: 0.476 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2169106403565695		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.2169106403565695 | validation: 1.1243039699392645]
	TIME [epoch: 0.475 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0772945450626625		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.0772945450626625 | validation: 1.024217916999849]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9745780754096831		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.9745780754096831 | validation: 1.2991625721617726]
	TIME [epoch: 0.477 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0884871799682225		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.0884871799682225 | validation: 1.3486357725939606]
	TIME [epoch: 0.477 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.257417069706768		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.257417069706768 | validation: 0.9988932583982923]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0085122475312989		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.0085122475312989 | validation: 1.1783120115266519]
	TIME [epoch: 0.475 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0075188460988693		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.0075188460988693 | validation: 1.3131574857335904]
	TIME [epoch: 0.475 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1807147767326673		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.1807147767326673 | validation: 1.1527977721242877]
	TIME [epoch: 0.475 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0030077633899719		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.0030077633899719 | validation: 0.9856720632525175]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9943861390135967		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.9943861390135967 | validation: 1.0904305616126402]
	TIME [epoch: 0.476 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.030295431462045		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.030295431462045 | validation: 1.088792066616023]
	TIME [epoch: 0.476 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0038623359812888		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.0038623359812888 | validation: 1.6672067858758906]
	TIME [epoch: 0.476 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2930986870667887		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.2930986870667887 | validation: 1.040550819568841]
	TIME [epoch: 0.474 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0029320251773914		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.0029320251773914 | validation: 1.1648070574153713]
	TIME [epoch: 0.474 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9769762699866064		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.9769762699866064 | validation: 1.0086633365256354]
	TIME [epoch: 0.475 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9757103440734923		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.9757103440734923 | validation: 1.3428853168017685]
	TIME [epoch: 0.476 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0733735812215022		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.0733735812215022 | validation: 1.065683289835098]
	TIME [epoch: 0.474 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0653187795022212		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.0653187795022212 | validation: 1.0467043344812261]
	TIME [epoch: 0.474 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9552150443824132		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.9552150443824132 | validation: 0.9497707666603381]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8834366276445907		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.8834366276445907 | validation: 1.2877589390799893]
	TIME [epoch: 0.477 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0596061808532924		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.0596061808532924 | validation: 1.28292160565587]
	TIME [epoch: 0.475 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1727285093588053		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.1727285093588053 | validation: 0.9136101428558927]
	TIME [epoch: 0.48 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9374614434861178		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.9374614434861178 | validation: 1.0231119210336392]
	TIME [epoch: 0.476 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9032583328931446		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.9032583328931446 | validation: 1.0897254703791188]
	TIME [epoch: 0.476 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.000063973932681		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.000063973932681 | validation: 1.524096106206603]
	TIME [epoch: 0.474 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1767720102011858		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.1767720102011858 | validation: 0.9978920979303304]
	TIME [epoch: 0.475 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.960564402597424		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.960564402597424 | validation: 1.176417747613194]
	TIME [epoch: 0.476 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.973884019215157		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.973884019215157 | validation: 0.8903048626609955]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9216315110013644		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.9216315110013644 | validation: 1.0050781765116015]
	TIME [epoch: 0.478 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8726282354374759		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.8726282354374759 | validation: 0.860634902713885]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.843956399059143		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.843956399059143 | validation: 1.2923666070184912]
	TIME [epoch: 0.477 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0091764177846527		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.0091764177846527 | validation: 1.2746327695991149]
	TIME [epoch: 0.476 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1903904274832697		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.1903904274832697 | validation: 0.8442797039549839]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9424415355253376		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.9424415355253376 | validation: 1.1159325458991334]
	TIME [epoch: 0.668 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9240597108631817		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.9240597108631817 | validation: 0.9392039913024534]
	TIME [epoch: 0.476 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8586678930589631		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.8586678930589631 | validation: 1.1941274146171323]
	TIME [epoch: 0.475 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9694115844680677		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.9694115844680677 | validation: 1.194715773500478]
	TIME [epoch: 0.475 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1167941238232142		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.1167941238232142 | validation: 0.8516170583305848]
	TIME [epoch: 0.475 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8869142272443764		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.8869142272443764 | validation: 1.0175867301212136]
	TIME [epoch: 0.476 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8585566767364153		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.8585566767364153 | validation: 1.0189235188372165]
	TIME [epoch: 0.475 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9481625625748658		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.9481625625748658 | validation: 1.2693774777487183]
	TIME [epoch: 0.475 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9887531026870465		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.9887531026870465 | validation: 0.99527878914351]
	TIME [epoch: 0.474 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9857227483327132		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.9857227483327132 | validation: 1.000415035806912]
	TIME [epoch: 0.476 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8789754533497274		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.8789754533497274 | validation: 0.8485785791751526]
	TIME [epoch: 0.48 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7913531895810684		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.7913531895810684 | validation: 0.8823400614696087]
	TIME [epoch: 0.478 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8249756243024132		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.8249756243024132 | validation: 1.0831412232625648]
	TIME [epoch: 0.476 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8930087287461802		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.8930087287461802 | validation: 1.2006981868159674]
	TIME [epoch: 0.475 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1493727629342592		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.1493727629342592 | validation: 0.8138088445126282]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8694983551963889		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.8694983551963889 | validation: 0.9246008178626781]
	TIME [epoch: 0.476 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.785300700981226		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.785300700981226 | validation: 0.9129691587066966]
	TIME [epoch: 0.476 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8334232792159684		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.8334232792159684 | validation: 1.256010533551725]
	TIME [epoch: 0.475 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9790759914372146		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.9790759914372146 | validation: 1.0721152042053586]
	TIME [epoch: 0.475 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0125687048597698		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.0125687048597698 | validation: 0.8052210090180257]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7414071059499283		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.7414071059499283 | validation: 1.1132401022278324]
	TIME [epoch: 0.477 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8855267192817968		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.8855267192817968 | validation: 1.0622643679387582]
	TIME [epoch: 0.477 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0277951733740691		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.0277951733740691 | validation: 0.9701995665422422]
	TIME [epoch: 0.476 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8623210329607478		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.8623210329607478 | validation: 0.9284056800333662]
	TIME [epoch: 0.475 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9506656296998196		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.9506656296998196 | validation: 0.8764211199700572]
	TIME [epoch: 0.475 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.850469640164898		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.850469640164898 | validation: 1.152033453095682]
	TIME [epoch: 0.475 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8863456396453849		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.8863456396453849 | validation: 0.8626256804996886]
	TIME [epoch: 0.476 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8522741205455495		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.8522741205455495 | validation: 0.8975608520734277]
	TIME [epoch: 0.476 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7765935599426996		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.7765935599426996 | validation: 0.8320563431274597]
	TIME [epoch: 0.475 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7667983962418514		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.7667983962418514 | validation: 1.1289389217088548]
	TIME [epoch: 0.475 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8554951567974948		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.8554951567974948 | validation: 1.0034319223331298]
	TIME [epoch: 0.475 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9578940662851468		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.9578940662851468 | validation: 0.8434414575838883]
	TIME [epoch: 0.475 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7827031571815966		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.7827031571815966 | validation: 0.8708119713893837]
	TIME [epoch: 0.475 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7830720131045362		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.7830720131045362 | validation: 0.925963969527323]
	TIME [epoch: 0.476 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9568354387267233		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.9568354387267233 | validation: 0.8976665975480688]
	TIME [epoch: 0.475 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8128754436630155		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.8128754436630155 | validation: 0.7500772297946134]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7668108242478436		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.7668108242478436 | validation: 1.0715558637513163]
	TIME [epoch: 0.477 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8339885387647161		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.8339885387647161 | validation: 0.8781236321002468]
	TIME [epoch: 0.476 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8764359480519517		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.8764359480519517 | validation: 1.0742492272508792]
	TIME [epoch: 0.476 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8411631589319004		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.8411631589319004 | validation: 0.8234395859913568]
	TIME [epoch: 0.475 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8211733867551165		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.8211733867551165 | validation: 1.030856222801803]
	TIME [epoch: 0.475 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8061476709215221		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.8061476709215221 | validation: 0.7707260751611125]
	TIME [epoch: 0.475 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.806060889963316		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.806060889963316 | validation: 1.0534420266947087]
	TIME [epoch: 0.475 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8492599347253074		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.8492599347253074 | validation: 0.8314249126865543]
	TIME [epoch: 0.481 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8581607799170163		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.8581607799170163 | validation: 1.0492441355138855]
	TIME [epoch: 0.475 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8173916361383582		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.8173916361383582 | validation: 0.7651583405109128]
	TIME [epoch: 0.475 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7801784162931401		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.7801784162931401 | validation: 0.9508888258226019]
	TIME [epoch: 0.476 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7811559987870836		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.7811559987870836 | validation: 0.8064624305051847]
	TIME [epoch: 0.475 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7807301869345156		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.7807301869345156 | validation: 1.0612257985414846]
	TIME [epoch: 0.474 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.813157672607741		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.813157672607741 | validation: 0.9969779429466968]
	TIME [epoch: 0.475 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9442005657126109		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.9442005657126109 | validation: 0.7997590932091956]
	TIME [epoch: 0.475 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8023237357749181		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.8023237357749181 | validation: 0.9231936154198768]
	TIME [epoch: 0.475 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8736683531560602		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.8736683531560602 | validation: 0.8353398713344243]
	TIME [epoch: 0.474 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7949325180199462		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.7949325180199462 | validation: 0.7603650989522045]
	TIME [epoch: 0.474 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7485843576720845		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.7485843576720845 | validation: 0.9536771413518341]
	TIME [epoch: 0.475 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7740033263010809		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.7740033263010809 | validation: 0.8388018072407886]
	TIME [epoch: 132 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8289181458888968		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.8289181458888968 | validation: 0.899824498524173]
	TIME [epoch: 0.942 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7506624902586992		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.7506624902586992 | validation: 0.7832958241399788]
	TIME [epoch: 0.934 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7383953533962734		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.7383953533962734 | validation: 0.9529635463357929]
	TIME [epoch: 0.934 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7926383086316955		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.7926383086316955 | validation: 0.8730324465804902]
	TIME [epoch: 0.933 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8985526407403159		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.8985526407403159 | validation: 0.8557832729009052]
	TIME [epoch: 0.932 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7898646671120072		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.7898646671120072 | validation: 0.7817873145767191]
	TIME [epoch: 0.935 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6844319357363571		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.6844319357363571 | validation: 0.8279205663088209]
	TIME [epoch: 0.935 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7037752151717169		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.7037752151717169 | validation: 0.7550972395946168]
	TIME [epoch: 0.935 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.752089360713035		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.752089360713035 | validation: 0.8012952888161522]
	TIME [epoch: 0.938 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.880707947006863		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.880707947006863 | validation: 0.8023766993718833]
	TIME [epoch: 0.933 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7323981614940871		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.7323981614940871 | validation: 0.7434516724300911]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.704246195173685		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.704246195173685 | validation: 0.7833764797409661]
	TIME [epoch: 0.936 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7106732748814448		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.7106732748814448 | validation: 0.8389870674125941]
	TIME [epoch: 0.942 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.784506731417373		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.784506731417373 | validation: 0.9024284323864802]
	TIME [epoch: 0.935 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9040671801868632		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.9040671801868632 | validation: 0.9317810985013417]
	TIME [epoch: 0.935 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.767654569193083		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.767654569193083 | validation: 0.7010632874291653]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_217.pth
	Model improved!!!
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7099152246917919		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.7099152246917919 | validation: 1.1211562273161606]
	TIME [epoch: 0.935 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9128074173466806		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.9128074173466806 | validation: 0.9064213538009116]
	TIME [epoch: 0.936 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9630819125190156		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.9630819125190156 | validation: 0.8818608929595027]
	TIME [epoch: 0.936 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7513675776638141		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.7513675776638141 | validation: 0.709888508132277]
	TIME [epoch: 0.936 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7062737101022813		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.7062737101022813 | validation: 0.7393420098576495]
	TIME [epoch: 0.936 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7199892793438738		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.7199892793438738 | validation: 0.7414872453000163]
	TIME [epoch: 0.935 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7081910119371733		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.7081910119371733 | validation: 0.7165458895387428]
	TIME [epoch: 0.934 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7173655635395767		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.7173655635395767 | validation: 0.9862867151991837]
	TIME [epoch: 0.934 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7964271852481172		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.7964271852481172 | validation: 0.8100262236759761]
	TIME [epoch: 0.934 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8254400581288637		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.8254400581288637 | validation: 0.8740057336303231]
	TIME [epoch: 0.934 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7408570586242695		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.7408570586242695 | validation: 0.7066557225414946]
	TIME [epoch: 0.935 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7128790700133959		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.7128790700133959 | validation: 1.0475082169463803]
	TIME [epoch: 0.935 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8353609008732028		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.8353609008732028 | validation: 0.8925797111907259]
	TIME [epoch: 0.935 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9198527352403254		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.9198527352403254 | validation: 0.83713659814496]
	TIME [epoch: 0.933 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6954496833652177		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.6954496833652177 | validation: 0.6825094587363628]
	TIME [epoch: 0.931 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_232.pth
	Model improved!!!
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6569175218888256		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.6569175218888256 | validation: 0.7563469453819951]
	TIME [epoch: 0.932 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6643967552738016		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.6643967552738016 | validation: 0.6779070901826825]
	TIME [epoch: 0.931 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_234.pth
	Model improved!!!
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7513017471424498		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.7513017471424498 | validation: 1.1137450605511734]
	TIME [epoch: 0.932 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8799175065055388		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.8799175065055388 | validation: 0.6497955869951644]
	TIME [epoch: 0.931 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7307034876431019		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.7307034876431019 | validation: 0.8215111702125073]
	TIME [epoch: 0.933 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7018680356911139		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.7018680356911139 | validation: 0.6992887168271354]
	TIME [epoch: 0.934 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7329989750691996		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.7329989750691996 | validation: 0.9826464354500642]
	TIME [epoch: 0.932 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7856880236799638		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.7856880236799638 | validation: 0.721450593293938]
	TIME [epoch: 0.932 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7797635826459594		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.7797635826459594 | validation: 0.9136113916437716]
	TIME [epoch: 0.932 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7301164224066302		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.7301164224066302 | validation: 0.6580286450521418]
	TIME [epoch: 0.932 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6849921662631322		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.6849921662631322 | validation: 1.0533800635893742]
	TIME [epoch: 0.932 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8209409169409781		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.8209409169409781 | validation: 0.942058809823664]
	TIME [epoch: 0.937 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9745780006239767		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.9745780006239767 | validation: 0.6889024743056418]
	TIME [epoch: 0.93 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7173692063972853		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.7173692063972853 | validation: 0.7938689473633235]
	TIME [epoch: 0.93 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7039809908942969		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.7039809908942969 | validation: 0.7204791271563457]
	TIME [epoch: 0.931 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7597083590462892		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.7597083590462892 | validation: 0.9708191646635354]
	TIME [epoch: 0.931 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7740700998659064		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.7740700998659064 | validation: 0.6853355131550434]
	TIME [epoch: 0.93 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7323806926938784		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.7323806926938784 | validation: 0.8196458113970118]
	TIME [epoch: 0.93 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7002226538312688		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.7002226538312688 | validation: 0.6652103542679092]
	TIME [epoch: 0.931 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6936013232544196		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.6936013232544196 | validation: 0.8170426688253631]
	TIME [epoch: 0.93 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7196568187555559		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.7196568187555559 | validation: 0.6567992430600287]
	TIME [epoch: 0.931 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7301710090312601		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.7301710090312601 | validation: 0.8662551185902624]
	TIME [epoch: 0.931 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7368491298256601		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.7368491298256601 | validation: 0.6359712457064763]
	TIME [epoch: 0.933 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_255.pth
	Model improved!!!
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7212704001603873		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.7212704001603873 | validation: 0.8293755521181577]
	TIME [epoch: 0.934 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7105433650939841		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.7105433650939841 | validation: 0.6289018683115695]
	TIME [epoch: 0.931 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_257.pth
	Model improved!!!
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7173950895212593		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.7173950895212593 | validation: 0.8655454640881382]
	TIME [epoch: 0.933 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7424414014039611		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.7424414014039611 | validation: 0.6840822459322091]
	TIME [epoch: 0.931 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7341618115416902		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.7341618115416902 | validation: 0.7320063646255641]
	TIME [epoch: 0.931 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7260060140086568		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.7260060140086568 | validation: 0.7833094312078854]
	TIME [epoch: 0.93 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7382898964860354		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.7382898964860354 | validation: 0.7949380633009846]
	TIME [epoch: 0.931 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.789161757813865		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.789161757813865 | validation: 0.871875432784897]
	TIME [epoch: 0.929 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7111987103643975		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.7111987103643975 | validation: 0.6547958214212852]
	TIME [epoch: 0.931 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6933842105208785		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.6933842105208785 | validation: 0.7679341235496268]
	TIME [epoch: 0.93 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6792856749491034		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.6792856749491034 | validation: 0.6564469588529764]
	TIME [epoch: 0.93 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6611411790717726		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.6611411790717726 | validation: 0.8110046543413798]
	TIME [epoch: 0.93 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6752055322689541		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.6752055322689541 | validation: 0.6859893853655321]
	TIME [epoch: 0.931 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7329936489983148		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.7329936489983148 | validation: 0.863992720793757]
	TIME [epoch: 0.929 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7095070598304518		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.7095070598304518 | validation: 0.6233808798676224]
	TIME [epoch: 0.931 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.711632964430849		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.711632964430849 | validation: 0.8853421977555265]
	TIME [epoch: 0.933 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7932843413670085		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.7932843413670085 | validation: 0.703954902024415]
	TIME [epoch: 0.932 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7965609775302721		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.7965609775302721 | validation: 0.8260401202926598]
	TIME [epoch: 0.93 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7018479826965071		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.7018479826965071 | validation: 0.607917323321371]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_274.pth
	Model improved!!!
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.651398304693669		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.651398304693669 | validation: 0.7072911449063831]
	TIME [epoch: 0.934 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6563603256056396		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.6563603256056396 | validation: 0.6153670993917495]
	TIME [epoch: 0.933 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6700153707456875		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.6700153707456875 | validation: 0.7226092164521613]
	TIME [epoch: 0.931 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6822383326356666		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.6822383326356666 | validation: 0.6144093597421021]
	TIME [epoch: 0.932 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7021158258895543		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.7021158258895543 | validation: 0.8460137843700801]
	TIME [epoch: 0.931 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7279533124107905		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.7279533124107905 | validation: 0.623497890018772]
	TIME [epoch: 0.931 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6999249112929457		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.6999249112929457 | validation: 0.8194653408327212]
	TIME [epoch: 0.932 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6854300305080908		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.6854300305080908 | validation: 0.6914499772030086]
	TIME [epoch: 0.932 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7459955187367043		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.7459955187367043 | validation: 0.8954317732968949]
	TIME [epoch: 0.931 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7368200254311017		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.7368200254311017 | validation: 0.6899724163862296]
	TIME [epoch: 0.932 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.712023952926634		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.712023952926634 | validation: 0.8397153016697838]
	TIME [epoch: 0.93 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7287126147646592		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.7287126147646592 | validation: 0.6823939588745613]
	TIME [epoch: 0.933 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7421320582281831		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.7421320582281831 | validation: 0.7068458510072205]
	TIME [epoch: 0.93 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.650992113759811		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.650992113759811 | validation: 0.5977441751412883]
	TIME [epoch: 0.93 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_288.pth
	Model improved!!!
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6148958765932376		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.6148958765932376 | validation: 0.6940878719593853]
	TIME [epoch: 0.932 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6252684517994357		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.6252684517994357 | validation: 0.6091314959717231]
	TIME [epoch: 0.931 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.64290809396662		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.64290809396662 | validation: 0.9839208572343967]
	TIME [epoch: 0.93 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7403803806191386		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.7403803806191386 | validation: 0.6942835758780888]
	TIME [epoch: 0.933 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7847716395015023		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.7847716395015023 | validation: 0.7561660598053033]
	TIME [epoch: 0.932 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6970521815384598		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.6970521815384598 | validation: 0.686702494764316]
	TIME [epoch: 0.93 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7724702713955429		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.7724702713955429 | validation: 0.7315689004175016]
	TIME [epoch: 0.93 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7072918758202624		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.7072918758202624 | validation: 0.7238306180829087]
	TIME [epoch: 0.93 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.666579201214259		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.666579201214259 | validation: 0.6016717392163766]
	TIME [epoch: 0.933 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6396390002659239		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.6396390002659239 | validation: 0.7422217316602245]
	TIME [epoch: 0.934 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6565541719080008		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.6565541719080008 | validation: 0.6333678426162428]
	TIME [epoch: 0.934 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.655899021213065		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.655899021213065 | validation: 0.708185156120718]
	TIME [epoch: 0.931 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6648031693613501		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.6648031693613501 | validation: 0.6445271804243]
	TIME [epoch: 0.932 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6834710499851286		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.6834710499851286 | validation: 0.6895228771958459]
	TIME [epoch: 0.93 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6765457645350295		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.6765457645350295 | validation: 0.6483987348026082]
	TIME [epoch: 0.929 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6603250929792795		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.6603250929792795 | validation: 0.6085745910416405]
	TIME [epoch: 0.93 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6436952864408769		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.6436952864408769 | validation: 0.6817527387037512]
	TIME [epoch: 0.935 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6607410291565239		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.6607410291565239 | validation: 0.6191588749568069]
	TIME [epoch: 0.93 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7178365439733507		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.7178365439733507 | validation: 1.1120318729175878]
	TIME [epoch: 0.93 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8169666189669098		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.8169666189669098 | validation: 0.5768384638276335]
	TIME [epoch: 0.93 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6777221505723384		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.6777221505723384 | validation: 0.6726768051473334]
	TIME [epoch: 0.931 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6412568595355973		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.6412568595355973 | validation: 0.5809265755152012]
	TIME [epoch: 0.932 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6158110047217774		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.6158110047217774 | validation: 0.8440395497132697]
	TIME [epoch: 0.93 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6559552191448068		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.6559552191448068 | validation: 0.6501047989266989]
	TIME [epoch: 0.93 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7064002424925795		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.7064002424925795 | validation: 0.7492674692624511]
	TIME [epoch: 0.93 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6445830013518973		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.6445830013518973 | validation: 0.5470642217547591]
	TIME [epoch: 0.93 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_314.pth
	Model improved!!!
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.637251577257675		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.637251577257675 | validation: 0.7547903134049333]
	TIME [epoch: 0.93 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7096245559730554		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.7096245559730554 | validation: 0.6549648848626983]
	TIME [epoch: 0.929 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7569604531689257		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.7569604531689257 | validation: 0.7486097501711427]
	TIME [epoch: 0.928 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6770262612404734		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.6770262612404734 | validation: 0.6160482547844114]
	TIME [epoch: 0.929 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6325719669518819		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.6325719669518819 | validation: 0.6071718508912114]
	TIME [epoch: 0.932 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6550941639699533		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.6550941639699533 | validation: 0.7958440331515384]
	TIME [epoch: 0.931 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7001258203577007		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.7001258203577007 | validation: 0.6443471632847344]
	TIME [epoch: 0.931 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6643650155763982		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.6643650155763982 | validation: 0.6811044371350324]
	TIME [epoch: 0.93 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6295627690867426		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.6295627690867426 | validation: 0.5820416224179745]
	TIME [epoch: 0.929 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6073508022258411		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.6073508022258411 | validation: 0.6556202265877911]
	TIME [epoch: 0.929 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6268774322552664		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.6268774322552664 | validation: 0.5849270246682469]
	TIME [epoch: 0.93 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6586473191207552		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.6586473191207552 | validation: 0.6722911077425512]
	TIME [epoch: 0.929 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6614546262666933		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.6614546262666933 | validation: 0.6171573025254503]
	TIME [epoch: 0.929 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6425498846058139		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.6425498846058139 | validation: 0.6185631372699335]
	TIME [epoch: 0.929 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6515528811576005		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.6515528811576005 | validation: 0.7269592064318553]
	TIME [epoch: 0.931 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6613369135363614		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.6613369135363614 | validation: 0.6293464626010989]
	TIME [epoch: 0.931 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7233290228862798		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.7233290228862798 | validation: 0.8075589623628154]
	TIME [epoch: 0.929 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7144938636294438		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.7144938636294438 | validation: 0.5150071675201762]
	TIME [epoch: 0.93 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_332.pth
	Model improved!!!
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6091740554198597		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.6091740554198597 | validation: 0.5861483811666881]
	TIME [epoch: 0.932 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5776001876108489		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.5776001876108489 | validation: 0.6744874968619137]
	TIME [epoch: 0.931 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6045840417455479		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.6045840417455479 | validation: 0.647030410672334]
	TIME [epoch: 0.935 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6579346779787537		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.6579346779787537 | validation: 0.7268433772476177]
	TIME [epoch: 0.935 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6949457851856147		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.6949457851856147 | validation: 0.5934952259291689]
	TIME [epoch: 0.93 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.646079760852915		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.646079760852915 | validation: 0.7215107644771912]
	TIME [epoch: 0.93 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6136299744790064		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.6136299744790064 | validation: 0.5586778063342299]
	TIME [epoch: 0.929 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6073868653058868		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.6073868653058868 | validation: 0.8377305935511551]
	TIME [epoch: 0.929 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6336232535149168		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.6336232535149168 | validation: 0.5718979870604843]
	TIME [epoch: 0.929 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6624470025921219		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.6624470025921219 | validation: 0.7150433452947266]
	TIME [epoch: 0.93 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6553678984952228		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.6553678984952228 | validation: 0.5158206843335379]
	TIME [epoch: 0.93 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6562885038639357		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.6562885038639357 | validation: 0.7115775632208104]
	TIME [epoch: 0.929 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6740889649264518		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.6740889649264518 | validation: 0.6785451838726435]
	TIME [epoch: 0.93 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7461120655505933		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.7461120655505933 | validation: 0.7442559929459249]
	TIME [epoch: 0.928 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6435650788483639		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.6435650788483639 | validation: 0.5290605722339699]
	TIME [epoch: 0.929 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5942158352289628		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.5942158352289628 | validation: 0.6620354089328745]
	TIME [epoch: 0.93 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.594772788483051		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.594772788483051 | validation: 0.5473724568705939]
	TIME [epoch: 0.93 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6327289857246224		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.6327289857246224 | validation: 0.7557548946228789]
	TIME [epoch: 0.93 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.625674780341802		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.625674780341802 | validation: 0.5352733103246411]
	TIME [epoch: 0.929 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5917913028808014		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.5917913028808014 | validation: 0.6608626671676148]
	TIME [epoch: 0.93 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5799043872260965		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.5799043872260965 | validation: 0.5278400646311837]
	TIME [epoch: 0.929 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5997284277515648		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.5997284277515648 | validation: 0.7251505395028479]
	TIME [epoch: 0.929 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6303292290002671		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.6303292290002671 | validation: 0.5247227251851746]
	TIME [epoch: 0.929 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.64082315556157		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.64082315556157 | validation: 0.817263937895077]
	TIME [epoch: 0.932 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7222495580037928		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.7222495580037928 | validation: 0.6809799447419675]
	TIME [epoch: 0.93 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7142012561659928		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.7142012561659928 | validation: 0.5846324336932733]
	TIME [epoch: 0.931 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6143470226316228		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.6143470226316228 | validation: 0.6490195586830034]
	TIME [epoch: 0.93 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6097540099060424		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.6097540099060424 | validation: 0.5324098098908132]
	TIME [epoch: 0.93 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.563649307583373		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.563649307583373 | validation: 0.7271979840640697]
	TIME [epoch: 0.929 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6057807606305997		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.6057807606305997 | validation: 0.6030289704893811]
	TIME [epoch: 0.93 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6901899151882595		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.6901899151882595 | validation: 0.6596198213551093]
	TIME [epoch: 0.929 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6254969537849958		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.6254969537849958 | validation: 0.5388913256219358]
	TIME [epoch: 0.928 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5595420113783208		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.5595420113783208 | validation: 0.514670907002423]
	TIME [epoch: 0.929 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_365.pth
	Model improved!!!
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5535162068843481		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.5535162068843481 | validation: 0.5966445868667497]
	TIME [epoch: 0.932 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6074278829613815		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.6074278829613815 | validation: 0.5814078871639484]
	TIME [epoch: 0.935 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.681494202313829		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.681494202313829 | validation: 0.7244306774273914]
	TIME [epoch: 0.93 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6458167704826056		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.6458167704826056 | validation: 0.5203228703974405]
	TIME [epoch: 0.929 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6227379333858698		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.6227379333858698 | validation: 0.7379480215789396]
	TIME [epoch: 0.928 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6156537533457926		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.6156537533457926 | validation: 0.511758806140488]
	TIME [epoch: 0.928 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_371.pth
	Model improved!!!
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5996694323867479		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.5996694323867479 | validation: 0.7952611112530583]
	TIME [epoch: 0.931 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6087914958742953		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.6087914958742953 | validation: 0.5360906301314338]
	TIME [epoch: 0.93 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6292348174925558		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.6292348174925558 | validation: 0.6318426419426494]
	TIME [epoch: 0.929 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5816700951864857		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.5816700951864857 | validation: 0.4759720667005922]
	TIME [epoch: 0.93 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_375.pth
	Model improved!!!
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5620060822139539		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.5620060822139539 | validation: 0.657356519948285]
	TIME [epoch: 0.932 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6122161731729489		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.6122161731729489 | validation: 0.6320479345272582]
	TIME [epoch: 0.931 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7267969980377055		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.7267969980377055 | validation: 0.5968999816410218]
	TIME [epoch: 0.931 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6145760790350229		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.6145760790350229 | validation: 0.5511051104236413]
	TIME [epoch: 0.93 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5613759011912931		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.5613759011912931 | validation: 0.5256277334623641]
	TIME [epoch: 0.93 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5763455152123538		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.5763455152123538 | validation: 0.7292078356104632]
	TIME [epoch: 0.931 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6288709644037005		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.6288709644037005 | validation: 0.5727971407537265]
	TIME [epoch: 0.93 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6754844888391967		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.6754844888391967 | validation: 0.7126920794997513]
	TIME [epoch: 0.93 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6059777654463029		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.6059777654463029 | validation: 0.47057978983383536]
	TIME [epoch: 0.93 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_384.pth
	Model improved!!!
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5687783514786781		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.5687783514786781 | validation: 0.6058834362369503]
	TIME [epoch: 0.93 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5624877837856527		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.5624877837856527 | validation: 0.48380891661359143]
	TIME [epoch: 0.929 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5915245397154425		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.5915245397154425 | validation: 0.6216348781310368]
	TIME [epoch: 0.93 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.583792591614522		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.583792591614522 | validation: 0.4574105870444367]
	TIME [epoch: 0.929 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_388.pth
	Model improved!!!
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5578452770383239		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.5578452770383239 | validation: 0.6554000219195995]
	TIME [epoch: 0.931 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5663103547782767		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.5663103547782767 | validation: 0.5225580185607441]
	TIME [epoch: 0.93 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.588115764758016		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.588115764758016 | validation: 0.6754116791816226]
	TIME [epoch: 0.929 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5754116948981671		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.5754116948981671 | validation: 0.48026972769905546]
	TIME [epoch: 0.929 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5713170493328498		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.5713170493328498 | validation: 0.7000556437634837]
	TIME [epoch: 0.93 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5693252221590016		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.5693252221590016 | validation: 0.48005122506037773]
	TIME [epoch: 0.928 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.570094918991125		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.570094918991125 | validation: 0.7994493030198919]
	TIME [epoch: 0.929 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6709713619287444		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.6709713619287444 | validation: 0.7193717824071755]
	TIME [epoch: 0.928 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7873208007527219		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.7873208007527219 | validation: 0.5497675160333524]
	TIME [epoch: 0.934 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5582758824281886		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.5582758824281886 | validation: 0.5635831224119346]
	TIME [epoch: 0.928 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5428460761997496		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.5428460761997496 | validation: 0.5065458046579939]
	TIME [epoch: 0.928 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.613646095434197		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.613646095434197 | validation: 0.5830899065777683]
	TIME [epoch: 0.928 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.653138556604626		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.653138556604626 | validation: 0.5436667137371646]
	TIME [epoch: 0.931 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5829382581036511		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.5829382581036511 | validation: 0.5597575559695503]
	TIME [epoch: 0.929 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5625283516104531		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.5625283516104531 | validation: 0.5237302499602688]
	TIME [epoch: 0.929 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5702223294002867		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.5702223294002867 | validation: 0.4669548451207723]
	TIME [epoch: 0.929 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5708687706142774		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.5708687706142774 | validation: 0.5782523335404021]
	TIME [epoch: 0.931 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5625600500763775		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.5625600500763775 | validation: 0.5235445186766942]
	TIME [epoch: 0.93 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5843380661309184		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.5843380661309184 | validation: 0.6930654767357786]
	TIME [epoch: 0.931 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5923438776207967		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.5923438776207967 | validation: 0.49093099543965873]
	TIME [epoch: 0.929 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6047447122771498		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.6047447122771498 | validation: 0.6795116546625959]
	TIME [epoch: 0.929 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5887791402827885		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.5887791402827885 | validation: 0.4516534797385681]
	TIME [epoch: 0.929 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_410.pth
	Model improved!!!
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5452494410586819		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.5452494410586819 | validation: 0.6197308578588061]
	TIME [epoch: 0.931 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5442804858996909		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.5442804858996909 | validation: 0.4771843860531593]
	TIME [epoch: 0.929 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5580279274723744		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.5580279274723744 | validation: 0.7555588227289749]
	TIME [epoch: 0.929 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5685154627299503		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.5685154627299503 | validation: 0.4882147704699998]
	TIME [epoch: 0.929 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5860618543004361		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.5860618543004361 | validation: 0.6038315476985698]
	TIME [epoch: 0.929 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5802483262998736		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.5802483262998736 | validation: 0.5706402325603482]
	TIME [epoch: 0.929 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.633620795637016		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.633620795637016 | validation: 0.6115171028750472]
	TIME [epoch: 0.929 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6431435007521467		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.6431435007521467 | validation: 0.5108249481721719]
	TIME [epoch: 0.93 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5507272250338698		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.5507272250338698 | validation: 0.45899589845129596]
	TIME [epoch: 0.93 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5241514485126281		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.5241514485126281 | validation: 0.5478810224294877]
	TIME [epoch: 0.929 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5399196538109604		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.5399196538109604 | validation: 0.49854355549860724]
	TIME [epoch: 0.928 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5825042140288507		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.5825042140288507 | validation: 0.7505734605233236]
	TIME [epoch: 0.928 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6069102411263549		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.6069102411263549 | validation: 0.4724990096067222]
	TIME [epoch: 0.928 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5689119740343722		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.5689119740343722 | validation: 0.7207580734316135]
	TIME [epoch: 0.928 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5604209580978713		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.5604209580978713 | validation: 0.45316284870308776]
	TIME [epoch: 0.928 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5625037394569042		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.5625037394569042 | validation: 0.6365558604877055]
	TIME [epoch: 0.929 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5508686380816092		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.5508686380816092 | validation: 0.438638363550845]
	TIME [epoch: 0.93 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_427.pth
	Model improved!!!
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5409692262558691		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.5409692262558691 | validation: 0.5823622257581853]
	TIME [epoch: 0.937 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5531450143680308		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.5531450143680308 | validation: 0.5019875990068051]
	TIME [epoch: 0.932 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6002950880140852		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.6002950880140852 | validation: 0.5759798817692273]
	TIME [epoch: 0.931 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5825962876716168		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.5825962876716168 | validation: 0.6131098386409327]
	TIME [epoch: 0.929 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5774869158932224		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.5774869158932224 | validation: 0.48883712590475864]
	TIME [epoch: 0.931 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5776341952667007		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.5776341952667007 | validation: 0.6773511058011198]
	TIME [epoch: 0.93 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5669273161795215		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.5669273161795215 | validation: 0.44336852123248094]
	TIME [epoch: 0.929 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5447036171623333		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.5447036171623333 | validation: 0.5783299571565896]
	TIME [epoch: 0.93 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5341450233116731		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.5341450233116731 | validation: 0.43677814094498013]
	TIME [epoch: 0.929 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_436.pth
	Model improved!!!
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5306047308868017		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.5306047308868017 | validation: 0.5343035693741602]
	TIME [epoch: 0.934 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5317316194493528		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.5317316194493528 | validation: 0.4750485043260877]
	TIME [epoch: 0.931 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5569618440131777		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.5569618440131777 | validation: 0.5047019759767708]
	TIME [epoch: 0.932 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5694027070049424		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.5694027070049424 | validation: 0.5409390335363456]
	TIME [epoch: 0.932 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5629602675230363		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.5629602675230363 | validation: 0.49877278032473904]
	TIME [epoch: 0.931 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.570679082587573		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.570679082587573 | validation: 0.8111963840557856]
	TIME [epoch: 0.932 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6043924644505773		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.6043924644505773 | validation: 0.4658321138902192]
	TIME [epoch: 0.93 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6054301969500141		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.6054301969500141 | validation: 0.5335530894320706]
	TIME [epoch: 0.932 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5444462429600401		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.5444462429600401 | validation: 0.417831813845418]
	TIME [epoch: 0.929 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_445.pth
	Model improved!!!
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.502426843475644		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.502426843475644 | validation: 0.5838778786270155]
	TIME [epoch: 0.932 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5137075105893445		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.5137075105893445 | validation: 0.49242594883888535]
	TIME [epoch: 0.932 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5524121352671331		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.5524121352671331 | validation: 0.6222821019852147]
	TIME [epoch: 0.931 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5984112403456995		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.5984112403456995 | validation: 0.4403537814784622]
	TIME [epoch: 0.93 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5544518178315516		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.5544518178315516 | validation: 0.4552393275065454]
	TIME [epoch: 0.93 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5198342037029658		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.5198342037029658 | validation: 0.5227044052313854]
	TIME [epoch: 0.93 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5226402830092656		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.5226402830092656 | validation: 0.49344883932750294]
	TIME [epoch: 0.929 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5682873295698583		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.5682873295698583 | validation: 0.5970116391496355]
	TIME [epoch: 0.932 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.589685946309895		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.589685946309895 | validation: 0.43069696301392146]
	TIME [epoch: 0.933 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5562083137622671		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.5562083137622671 | validation: 0.5662466156570285]
	TIME [epoch: 0.932 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5465982664971357		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.5465982664971357 | validation: 0.3980034900175684]
	TIME [epoch: 0.931 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_456.pth
	Model improved!!!
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5175688493960585		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.5175688493960585 | validation: 0.5125404051350522]
	TIME [epoch: 0.933 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5153735193508716		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.5153735193508716 | validation: 0.5605265328982342]
	TIME [epoch: 0.935 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5496883675344121		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.5496883675344121 | validation: 0.5488413266792863]
	TIME [epoch: 0.93 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5493949682568888		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.5493949682568888 | validation: 0.5181616624026937]
	TIME [epoch: 0.929 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5477230495302083		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.5477230495302083 | validation: 0.45920577425597386]
	TIME [epoch: 0.93 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5504938140010865		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.5504938140010865 | validation: 0.6330413510061136]
	TIME [epoch: 0.93 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5507500792375873		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.5507500792375873 | validation: 0.44586593529187024]
	TIME [epoch: 0.929 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5516662094499344		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.5516662094499344 | validation: 0.622767845007817]
	TIME [epoch: 0.93 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5179584116122716		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.5179584116122716 | validation: 0.40128933997559707]
	TIME [epoch: 0.93 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5224472877828441		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.5224472877828441 | validation: 0.5988776655610903]
	TIME [epoch: 0.929 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5187410019159442		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.5187410019159442 | validation: 0.41662511561386845]
	TIME [epoch: 0.928 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5252370804224034		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.5252370804224034 | validation: 0.7039828621462791]
	TIME [epoch: 0.93 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5488949508579909		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.5488949508579909 | validation: 0.4677593977524474]
	TIME [epoch: 0.928 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5963196814098934		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.5963196814098934 | validation: 0.554260471659858]
	TIME [epoch: 0.929 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5716813819652319		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.5716813819652319 | validation: 0.4221372620132365]
	TIME [epoch: 0.929 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.530449608059846		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.530449608059846 | validation: 0.4625021460079428]
	TIME [epoch: 0.928 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5030030851754415		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.5030030851754415 | validation: 0.5234460074208265]
	TIME [epoch: 0.928 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.513981840415776		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.513981840415776 | validation: 0.4598533271119779]
	TIME [epoch: 0.928 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.533604628022235		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.533604628022235 | validation: 0.6182349610759051]
	TIME [epoch: 0.929 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5412540199300023		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.5412540199300023 | validation: 0.4425109420138494]
	TIME [epoch: 0.929 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5310260878217996		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.5310260878217996 | validation: 0.5119523075224448]
	TIME [epoch: 0.929 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5082055894727108		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.5082055894727108 | validation: 0.40875244342232464]
	TIME [epoch: 0.93 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4989169285174755		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.4989169285174755 | validation: 0.5298451782296046]
	TIME [epoch: 0.929 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4929522594116642		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.4929522594116642 | validation: 0.39445746694737804]
	TIME [epoch: 0.929 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_480.pth
	Model improved!!!
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5453164193817522		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.5453164193817522 | validation: 0.44418106511561795]
	TIME [epoch: 0.931 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5274708771177802		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.5274708771177802 | validation: 0.46580524823815317]
	TIME [epoch: 0.929 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5220560739600306		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.5220560739600306 | validation: 0.44502009776049356]
	TIME [epoch: 0.931 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5772383595543561		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.5772383595543561 | validation: 0.4925586229726097]
	TIME [epoch: 0.928 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5553640664863586		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.5553640664863586 | validation: 0.37247135150201516]
	TIME [epoch: 0.928 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_485.pth
	Model improved!!!
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5282939688872688		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.5282939688872688 | validation: 0.6590851524862318]
	TIME [epoch: 0.929 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5469313045892105		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.5469313045892105 | validation: 0.6090838943366625]
	TIME [epoch: 0.929 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5595258439108263		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.5595258439108263 | validation: 0.5947460631494981]
	TIME [epoch: 0.929 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5150959460191729		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.5150959460191729 | validation: 0.44192993511162887]
	TIME [epoch: 0.929 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4830639822666072		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.4830639822666072 | validation: 0.4502632379447848]
	TIME [epoch: 0.933 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49119731773831765		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.49119731773831765 | validation: 0.4760880740867155]
	TIME [epoch: 0.928 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5148605916741495		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.5148605916741495 | validation: 0.47844417180045284]
	TIME [epoch: 0.929 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5239687848322085		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.5239687848322085 | validation: 0.5927774045397184]
	TIME [epoch: 0.928 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5244711248418282		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.5244711248418282 | validation: 0.4119907474685448]
	TIME [epoch: 0.932 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5255678244003753		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.5255678244003753 | validation: 0.592433574038754]
	TIME [epoch: 0.928 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5254895023702587		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.5254895023702587 | validation: 0.39963543354280606]
	TIME [epoch: 0.928 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.498364952513159		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.498364952513159 | validation: 0.6202728591086697]
	TIME [epoch: 0.928 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49943727042085984		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.49943727042085984 | validation: 0.3961698612581728]
	TIME [epoch: 0.928 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5206403709763461		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.5206403709763461 | validation: 0.619233859078023]
	TIME [epoch: 0.927 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5195149933618369		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.5195149933618369 | validation: 0.4349645817909027]
	TIME [epoch: 0.929 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5493427999718274		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.5493427999718274 | validation: 0.5782726590838196]
	TIME [epoch: 135 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5644736066060336		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.5644736066060336 | validation: 0.45747307906173007]
	TIME [epoch: 1.85 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5117310400459171		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.5117310400459171 | validation: 0.44684212559846126]
	TIME [epoch: 1.84 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47997548363730874		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.47997548363730874 | validation: 0.5270099443107558]
	TIME [epoch: 1.85 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49046872494028065		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.49046872494028065 | validation: 0.41840375746653446]
	TIME [epoch: 1.84 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49770015930214484		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.49770015930214484 | validation: 0.5715970315668226]
	TIME [epoch: 1.84 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.514515547870494		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.514515547870494 | validation: 0.4020961108851173]
	TIME [epoch: 1.84 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.533572647072635		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.533572647072635 | validation: 0.481138223792648]
	TIME [epoch: 1.84 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4966516250397087		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.4966516250397087 | validation: 0.4071582135877301]
	TIME [epoch: 1.84 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48413450158857674		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.48413450158857674 | validation: 0.4123297198823817]
	TIME [epoch: 1.84 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6113937961608433		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.6113937961608433 | validation: 0.6302518264456353]
	TIME [epoch: 1.85 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7288793554579535		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.7288793554579535 | validation: 0.4275788547031752]
	TIME [epoch: 1.84 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.675483024262096		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.675483024262096 | validation: 0.4755211200984281]
	TIME [epoch: 1.84 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7448797991142775		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.7448797991142775 | validation: 0.4850216964766011]
	TIME [epoch: 1.84 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7498527204214801		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.7498527204214801 | validation: 0.5160079869255905]
	TIME [epoch: 1.85 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7540137488013968		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.7540137488013968 | validation: 0.4674077654394269]
	TIME [epoch: 1.84 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7081652212355312		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.7081652212355312 | validation: 0.465434948569437]
	TIME [epoch: 1.84 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.673454280768579		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.673454280768579 | validation: 0.45221624866886856]
	TIME [epoch: 1.84 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6475867338168223		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.6475867338168223 | validation: 0.43366220088573515]
	TIME [epoch: 1.84 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6148573313145389		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.6148573313145389 | validation: 0.461062644017922]
	TIME [epoch: 1.84 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6064760448657788		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.6064760448657788 | validation: 0.43706441570671706]
	TIME [epoch: 1.84 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5384481850219928		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.5384481850219928 | validation: 0.4432928399822289]
	TIME [epoch: 1.84 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5056304047050548		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.5056304047050548 | validation: 0.49465549845559686]
	TIME [epoch: 1.84 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5069317226947196		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.5069317226947196 | validation: 0.4795487807114895]
	TIME [epoch: 1.84 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5096004061218623		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.5096004061218623 | validation: 0.5012206312947324]
	TIME [epoch: 1.84 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5144242476596425		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.5144242476596425 | validation: 0.4269592451660549]
	TIME [epoch: 1.84 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5081945999648835		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.5081945999648835 | validation: 0.501218774435605]
	TIME [epoch: 1.84 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4998556760853168		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.4998556760853168 | validation: 0.40570799889061765]
	TIME [epoch: 1.84 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5040681882754028		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.5040681882754028 | validation: 0.696156818296329]
	TIME [epoch: 1.84 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5188107418718222		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.5188107418718222 | validation: 0.40185457834466914]
	TIME [epoch: 1.84 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5242735062283638		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.5242735062283638 | validation: 0.5489340544878971]
	TIME [epoch: 1.84 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49731230009625527		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.49731230009625527 | validation: 0.3913800638088589]
	TIME [epoch: 1.84 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4753764956345424		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.4753764956345424 | validation: 0.40023633137938736]
	TIME [epoch: 1.84 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5408716133385613		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.5408716133385613 | validation: 0.485280219707224]
	TIME [epoch: 1.84 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5449363029893491		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.5449363029893491 | validation: 0.4258136055143984]
	TIME [epoch: 1.84 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5244126776927275		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.5244126776927275 | validation: 0.48683902513617494]
	TIME [epoch: 1.85 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5824823980442901		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.5824823980442901 | validation: 0.5388887278633674]
	TIME [epoch: 1.84 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5441984107478937		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.5441984107478937 | validation: 0.7290444066021924]
	TIME [epoch: 1.84 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5511662986911983		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.5511662986911983 | validation: 0.40764322363954153]
	TIME [epoch: 1.84 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5267869920352507		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.5267869920352507 | validation: 0.4828460300880297]
	TIME [epoch: 1.84 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4963889623025598		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.4963889623025598 | validation: 0.40209759929541344]
	TIME [epoch: 1.84 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47882434059783097		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.47882434059783097 | validation: 0.40820032117464566]
	TIME [epoch: 1.84 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4545101709378507		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.4545101709378507 | validation: 0.3818904184983163]
	TIME [epoch: 1.84 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46381602220673374		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.46381602220673374 | validation: 0.45051450601419474]
	TIME [epoch: 1.84 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46047567780626963		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.46047567780626963 | validation: 0.4264861016115095]
	TIME [epoch: 1.84 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4613036167101587		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.4613036167101587 | validation: 0.564060061209905]
	TIME [epoch: 1.84 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45936910159191935		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.45936910159191935 | validation: 0.4256233244521444]
	TIME [epoch: 1.84 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48805713603790024		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.48805713603790024 | validation: 0.5950820383265789]
	TIME [epoch: 1.84 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6069174726373153		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.6069174726373153 | validation: 0.4177990854043987]
	TIME [epoch: 1.84 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5586244022001762		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.5586244022001762 | validation: 0.42375553034799557]
	TIME [epoch: 1.84 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4952282220877935		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.4952282220877935 | validation: 0.45792615779526574]
	TIME [epoch: 1.84 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.487708552491262		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.487708552491262 | validation: 0.44143167121011634]
	TIME [epoch: 1.84 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5049135070392042		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.5049135070392042 | validation: 0.7016268483357534]
	TIME [epoch: 1.84 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5221095319428296		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.5221095319428296 | validation: 0.3863498298596592]
	TIME [epoch: 1.84 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4938375643259454		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.4938375643259454 | validation: 0.40015521718584685]
	TIME [epoch: 1.84 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4900085447000851		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.4900085447000851 | validation: 0.3805654688002017]
	TIME [epoch: 1.84 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47948275303475085		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.47948275303475085 | validation: 0.37557435874592887]
	TIME [epoch: 1.84 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46636510401835246		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.46636510401835246 | validation: 0.44122021219776625]
	TIME [epoch: 1.84 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5061443358578852		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.5061443358578852 | validation: 0.4534689592504564]
	TIME [epoch: 1.84 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5328837042458416		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.5328837042458416 | validation: 0.6309473625527832]
	TIME [epoch: 1.84 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5105343479181231		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.5105343479181231 | validation: 0.4228497382221746]
	TIME [epoch: 1.84 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48312884616360663		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.48312884616360663 | validation: 0.5934130701137752]
	TIME [epoch: 1.84 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.481444795493983		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.481444795493983 | validation: 0.37239275256793936]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_563.pth
	Model improved!!!
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48231490753766565		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.48231490753766565 | validation: 0.5496464440309946]
	TIME [epoch: 1.84 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4798658860723126		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.4798658860723126 | validation: 0.3748543140610945]
	TIME [epoch: 1.84 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46483603528582984		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.46483603528582984 | validation: 0.5925276560418178]
	TIME [epoch: 1.84 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4673220576772464		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.4673220576772464 | validation: 0.3906599682462308]
	TIME [epoch: 1.85 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4656826759635058		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.4656826759635058 | validation: 0.5474919171440348]
	TIME [epoch: 1.84 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47220793194592375		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.47220793194592375 | validation: 0.397806031455237]
	TIME [epoch: 1.84 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49405447798894037		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.49405447798894037 | validation: 0.5305497040971495]
	TIME [epoch: 1.84 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5280457756855802		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.5280457756855802 | validation: 0.5288375392116594]
	TIME [epoch: 1.84 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5368309512531662		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.5368309512531662 | validation: 0.4175147055754733]
	TIME [epoch: 1.84 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5011316900105162		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.5011316900105162 | validation: 0.47262065165561346]
	TIME [epoch: 1.84 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47019525491756675		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.47019525491756675 | validation: 0.43233732442877354]
	TIME [epoch: 1.84 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4478180870304096		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.4478180870304096 | validation: 0.42491390258748923]
	TIME [epoch: 1.84 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45092611813049155		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.45092611813049155 | validation: 0.38364862306623126]
	TIME [epoch: 1.84 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47493599494771344		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.47493599494771344 | validation: 0.40317798296866986]
	TIME [epoch: 1.84 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4894622778927669		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.4894622778927669 | validation: 0.42924042980281674]
	TIME [epoch: 1.84 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48599200869148124		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.48599200869148124 | validation: 0.444490694062812]
	TIME [epoch: 1.84 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48272879685802944		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.48272879685802944 | validation: 0.43618576093525485]
	TIME [epoch: 1.84 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4719563580317239		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.4719563580317239 | validation: 0.39596638935636685]
	TIME [epoch: 1.84 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4645520209665432		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.4645520209665432 | validation: 0.3786168743337317]
	TIME [epoch: 1.84 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4666429186540709		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.4666429186540709 | validation: 0.39819383382287526]
	TIME [epoch: 1.85 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46683191593940904		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.46683191593940904 | validation: 0.458297479963632]
	TIME [epoch: 1.84 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4705609717238135		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.4705609717238135 | validation: 0.4625675606075138]
	TIME [epoch: 1.87 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47298914550423754		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.47298914550423754 | validation: 0.42419455754472213]
	TIME [epoch: 1.84 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4698859686231158		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.4698859686231158 | validation: 0.75594802583036]
	TIME [epoch: 1.84 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.560243824423863		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.560243824423863 | validation: 0.395045987801785]
	TIME [epoch: 1.84 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5625349655733808		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.5625349655733808 | validation: 0.42864688688809427]
	TIME [epoch: 1.84 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48830899002021033		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.48830899002021033 | validation: 0.3638461471311733]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_590.pth
	Model improved!!!
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4617680251830732		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.4617680251830732 | validation: 0.40749158185363765]
	TIME [epoch: 1.84 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46409810980246674		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.46409810980246674 | validation: 0.4948403075889678]
	TIME [epoch: 1.84 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4706122656101179		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.4706122656101179 | validation: 1.4354104839635824]
	TIME [epoch: 1.84 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9647389901311988		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.9647389901311988 | validation: 1.3537861508020805]
	TIME [epoch: 1.84 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9485749244349796		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.9485749244349796 | validation: 0.9941461268083892]
	TIME [epoch: 1.84 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7280394934081674		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.7280394934081674 | validation: 0.686298318525393]
	TIME [epoch: 1.84 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6982106460962288		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.6982106460962288 | validation: 0.35856233308786156]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_597.pth
	Model improved!!!
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.496750267376611		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.496750267376611 | validation: 0.5414261116001764]
	TIME [epoch: 1.85 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5193395123160054		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.5193395123160054 | validation: 0.45509899594522774]
	TIME [epoch: 1.84 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48238621791841857		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.48238621791841857 | validation: 0.3265809745247168]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_600.pth
	Model improved!!!
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4536829161018032		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.4536829161018032 | validation: 0.3634114666629842]
	TIME [epoch: 1.84 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4335580100379902		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.4335580100379902 | validation: 0.5791134553047341]
	TIME [epoch: 1.84 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4580791530685428		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.4580791530685428 | validation: 0.37448104361023027]
	TIME [epoch: 1.84 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4619519661814914		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.4619519661814914 | validation: 0.4498653376468553]
	TIME [epoch: 1.84 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49539883826690045		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.49539883826690045 | validation: 0.4428327853793128]
	TIME [epoch: 1.84 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4940952116146065		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.4940952116146065 | validation: 0.4190215217241433]
	TIME [epoch: 1.84 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4549739511722687		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.4549739511722687 | validation: 0.42071884421493416]
	TIME [epoch: 1.84 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43703730824214476		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.43703730824214476 | validation: 0.3860861684404673]
	TIME [epoch: 1.84 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43645808929781754		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.43645808929781754 | validation: 0.41802555529443064]
	TIME [epoch: 1.84 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44126099839846983		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.44126099839846983 | validation: 0.4082964719682705]
	TIME [epoch: 1.84 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46073899736389545		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.46073899736389545 | validation: 0.4003702859105369]
	TIME [epoch: 1.84 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4696658679204834		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.4696658679204834 | validation: 0.4653711833245182]
	TIME [epoch: 1.84 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46315746249640627		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.46315746249640627 | validation: 0.34400171332641954]
	TIME [epoch: 1.84 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46798764493776335		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.46798764493776335 | validation: 0.4447198125869119]
	TIME [epoch: 1.84 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4595665518495467		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.4595665518495467 | validation: 0.35127498812418567]
	TIME [epoch: 1.84 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44660198251895566		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.44660198251895566 | validation: 0.42537277843510357]
	TIME [epoch: 1.84 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4404483413501268		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.4404483413501268 | validation: 0.39078844803537227]
	TIME [epoch: 1.84 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44806176857306185		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.44806176857306185 | validation: 0.4598209035673261]
	TIME [epoch: 1.84 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45839653404278025		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.45839653404278025 | validation: 0.4384087792673572]
	TIME [epoch: 1.84 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46231323874715785		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.46231323874715785 | validation: 0.38860025356186156]
	TIME [epoch: 1.84 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4552869411272023		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.4552869411272023 | validation: 0.5425009619035918]
	TIME [epoch: 1.84 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4682056946753482		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.4682056946753482 | validation: 0.3705297381346762]
	TIME [epoch: 1.84 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46854783356542046		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.46854783356542046 | validation: 0.4726616576193207]
	TIME [epoch: 1.84 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44635131030699415		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.44635131030699415 | validation: 0.3425071206312682]
	TIME [epoch: 1.84 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44316463191683525		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.44316463191683525 | validation: 0.471109310664981]
	TIME [epoch: 1.84 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43727888552313404		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.43727888552313404 | validation: 0.35020682833824623]
	TIME [epoch: 1.84 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4508943280726165		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.4508943280726165 | validation: 0.526127602261666]
	TIME [epoch: 1.84 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45891681744999197		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.45891681744999197 | validation: 0.4109006067119529]
	TIME [epoch: 1.84 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47156117270075326		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.47156117270075326 | validation: 0.5161008475142567]
	TIME [epoch: 1.84 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46693548361267206		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.46693548361267206 | validation: 0.5271291030788462]
	TIME [epoch: 1.84 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4637465346298748		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.4637465346298748 | validation: 0.37094645305436835]
	TIME [epoch: 1.84 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4501828139069479		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.4501828139069479 | validation: 0.42247974993220927]
	TIME [epoch: 1.84 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4454242138421606		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.4454242138421606 | validation: 0.3434165286350821]
	TIME [epoch: 1.84 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4343296190003087		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.4343296190003087 | validation: 0.410510445602934]
	TIME [epoch: 1.84 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43356509899939727		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.43356509899939727 | validation: 0.40800636652002203]
	TIME [epoch: 1.84 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4399390596439147		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.4399390596439147 | validation: 0.7855625547044637]
	TIME [epoch: 1.84 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5716052032896923		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.5716052032896923 | validation: 0.35681587205850895]
	TIME [epoch: 1.84 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48994311356651293		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.48994311356651293 | validation: 0.37761787876821046]
	TIME [epoch: 1.84 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4515460178534179		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.4515460178534179 | validation: 0.3648999486775151]
	TIME [epoch: 1.84 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.453079567570245		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.453079567570245 | validation: 0.32945506980552824]
	TIME [epoch: 1.84 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47327017301224716		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.47327017301224716 | validation: 0.4898994420537761]
	TIME [epoch: 1.84 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45430794025444005		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.45430794025444005 | validation: 0.7095072225167585]
	TIME [epoch: 1.84 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4805768142577172		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.4805768142577172 | validation: 0.37936215166988646]
	TIME [epoch: 1.84 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4745183484907256		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.4745183484907256 | validation: 0.43268996318184016]
	TIME [epoch: 1.84 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44020337660868636		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.44020337660868636 | validation: 0.8003632869667087]
	TIME [epoch: 1.85 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5839670756065775		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.5839670756065775 | validation: 0.3974020483400409]
	TIME [epoch: 1.84 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48865618011775824		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.48865618011775824 | validation: 0.367930099486298]
	TIME [epoch: 1.84 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43391046331206545		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.43391046331206545 | validation: 0.4055455921793071]
	TIME [epoch: 1.84 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4358962613976388		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.4358962613976388 | validation: 0.33052790972998375]
	TIME [epoch: 1.84 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4294572511298341		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.4294572511298341 | validation: 0.374288184020481]
	TIME [epoch: 1.84 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41985189085716057		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.41985189085716057 | validation: 0.4099304825532488]
	TIME [epoch: 1.84 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42174143755778715		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.42174143755778715 | validation: 0.5176575655533155]
	TIME [epoch: 1.84 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45454594978815976		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.45454594978815976 | validation: 0.4502422416035994]
	TIME [epoch: 1.84 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5058938610283742		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.5058938610283742 | validation: 0.41129366672336665]
	TIME [epoch: 1.84 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45975730448813346		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.45975730448813346 | validation: 0.36169845728824107]
	TIME [epoch: 1.84 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43061711209832354		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.43061711209832354 | validation: 0.3765281346811779]
	TIME [epoch: 1.84 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4186628925377726		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.4186628925377726 | validation: 0.418219974792132]
	TIME [epoch: 1.84 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42035798686239517		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.42035798686239517 | validation: 0.3922760071834619]
	TIME [epoch: 1.84 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42949465834519884		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.42949465834519884 | validation: 0.5596798499942409]
	TIME [epoch: 1.84 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.458013625211004		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.458013625211004 | validation: 0.3975085793569475]
	TIME [epoch: 1.84 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.448853348504724		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.448853348504724 | validation: 0.4235992822476278]
	TIME [epoch: 1.84 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43390936605383784		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.43390936605383784 | validation: 0.3738918862799153]
	TIME [epoch: 1.84 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42498658417437507		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.42498658417437507 | validation: 0.36799477780820383]
	TIME [epoch: 1.84 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4332774207234657		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.4332774207234657 | validation: 0.49123083507714144]
	TIME [epoch: 1.84 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4616553252046173		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.4616553252046173 | validation: 0.367114893929877]
	TIME [epoch: 1.84 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.480188980111966		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.480188980111966 | validation: 0.5046930963409106]
	TIME [epoch: 1.84 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4471838189372525		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.4471838189372525 | validation: 0.3453180991911]
	TIME [epoch: 1.84 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4184799481427803		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.4184799481427803 | validation: 0.3239761422086339]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_668.pth
	Model improved!!!
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.424865373176045		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.424865373176045 | validation: 0.34793698914854904]
	TIME [epoch: 1.84 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42388512795337563		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.42388512795337563 | validation: 0.3020296291643031]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_670.pth
	Model improved!!!
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45015922745971126		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.45015922745971126 | validation: 0.4156723099330675]
	TIME [epoch: 1.84 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46343749423938085		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.46343749423938085 | validation: 0.42439912694054593]
	TIME [epoch: 1.84 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44916887189627686		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.44916887189627686 | validation: 0.447660128000405]
	TIME [epoch: 1.84 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4379701239104863		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.4379701239104863 | validation: 0.39688247928017745]
	TIME [epoch: 1.84 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4300543045358327		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.4300543045358327 | validation: 0.5070832046127668]
	TIME [epoch: 1.84 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4363850322904787		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.4363850322904787 | validation: 0.3087765392136815]
	TIME [epoch: 1.84 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.439489000405261		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.439489000405261 | validation: 0.4089739395786188]
	TIME [epoch: 1.84 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4459452044279027		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.4459452044279027 | validation: 0.3239037181680063]
	TIME [epoch: 1.84 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4322021982194482		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.4322021982194482 | validation: 0.35023749337945687]
	TIME [epoch: 1.84 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.417905355154606		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.417905355154606 | validation: 0.36462128406903327]
	TIME [epoch: 1.84 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40958658656938113		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.40958658656938113 | validation: 0.38615847932829733]
	TIME [epoch: 1.84 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41328868169714483		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.41328868169714483 | validation: 0.45995141782630944]
	TIME [epoch: 1.84 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4285859427249076		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.4285859427249076 | validation: 0.38262726600996233]
	TIME [epoch: 1.84 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4669115005750036		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.4669115005750036 | validation: 0.428095539760307]
	TIME [epoch: 1.84 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46465946966203264		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.46465946966203264 | validation: 0.42158966535225195]
	TIME [epoch: 1.84 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42968753062292775		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.42968753062292775 | validation: 0.3548526945169207]
	TIME [epoch: 1.84 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4335498673812577		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.4335498673812577 | validation: 0.6869683774966104]
	TIME [epoch: 1.84 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4745008424179518		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.4745008424179518 | validation: 0.3190826975247785]
	TIME [epoch: 1.84 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44581443689459205		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.44581443689459205 | validation: 0.32761421018666814]
	TIME [epoch: 1.84 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.427388440407517		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.427388440407517 | validation: 0.32091266774077004]
	TIME [epoch: 1.84 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43391045851604887		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.43391045851604887 | validation: 0.3526333010437873]
	TIME [epoch: 1.84 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4197800898411751		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.4197800898411751 | validation: 0.5808425274099632]
	TIME [epoch: 1.84 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44162483581531675		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.44162483581531675 | validation: 0.3266964334316426]
	TIME [epoch: 1.84 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4503008220582946		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.4503008220582946 | validation: 0.38666602447793147]
	TIME [epoch: 1.84 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4233208015656271		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.4233208015656271 | validation: 0.3344043483623277]
	TIME [epoch: 1.84 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41735673709507054		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.41735673709507054 | validation: 0.35592135697306776]
	TIME [epoch: 1.84 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4093219675788064		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.4093219675788064 | validation: 0.39979517086999306]
	TIME [epoch: 1.84 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4187396610367534		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.4187396610367534 | validation: 0.39162547145874804]
	TIME [epoch: 1.84 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43873605130208604		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.43873605130208604 | validation: 0.5321612657717248]
	TIME [epoch: 1.84 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4493274074930058		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.4493274074930058 | validation: 0.3459263969401955]
	TIME [epoch: 1.84 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43710634544690635		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.43710634544690635 | validation: 0.4127202260871477]
	TIME [epoch: 1.84 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4234864206356287		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.4234864206356287 | validation: 0.32959174671021324]
	TIME [epoch: 1.84 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4191777666629593		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.4191777666629593 | validation: 0.4554905147756902]
	TIME [epoch: 1.84 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41686156259727536		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.41686156259727536 | validation: 0.35394808056419946]
	TIME [epoch: 1.84 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4194636493583792		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.4194636493583792 | validation: 0.49487508907479816]
	TIME [epoch: 1.84 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4251107327599386		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.4251107327599386 | validation: 0.33392764558100296]
	TIME [epoch: 1.85 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4288818995712065		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.4288818995712065 | validation: 0.42264859471224087]
	TIME [epoch: 1.84 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42096826338056403		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.42096826338056403 | validation: 0.35615868650932514]
	TIME [epoch: 1.84 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4195409578745809		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.4195409578745809 | validation: 0.41608602098241615]
	TIME [epoch: 1.84 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42916003401307223		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.42916003401307223 | validation: 0.44133238764528393]
	TIME [epoch: 1.84 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44498321092968435		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.44498321092968435 | validation: 0.3380207874848797]
	TIME [epoch: 1.84 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4406694962297019		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.4406694962297019 | validation: 0.3293849601971752]
	TIME [epoch: 1.84 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43183912439542854		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.43183912439542854 | validation: 0.3572417107009351]
	TIME [epoch: 1.84 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41299760980089745		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.41299760980089745 | validation: 0.28562598786343196]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_714.pth
	Model improved!!!
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4673841174100037		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.4673841174100037 | validation: 0.34954879482677215]
	TIME [epoch: 1.84 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44307406965557833		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.44307406965557833 | validation: 0.3519573742518473]
	TIME [epoch: 1.84 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4194366561014606		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.4194366561014606 | validation: 0.48411884528399185]
	TIME [epoch: 1.84 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4240879668301071		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.4240879668301071 | validation: 0.35020206201998083]
	TIME [epoch: 1.84 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44468191301515475		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.44468191301515475 | validation: 0.4050515318029912]
	TIME [epoch: 1.84 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42703391694420234		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.42703391694420234 | validation: 0.4061414005650281]
	TIME [epoch: 1.84 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4136348194690096		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.4136348194690096 | validation: 0.33344004640056707]
	TIME [epoch: 1.84 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4213728295078968		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.4213728295078968 | validation: 0.5162774067182266]
	TIME [epoch: 1.84 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4261778980772512		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.4261778980772512 | validation: 0.3405995233274225]
	TIME [epoch: 1.84 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41758913469783643		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.41758913469783643 | validation: 0.4489170074067703]
	TIME [epoch: 1.84 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41216770149489834		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.41216770149489834 | validation: 0.3514272349612614]
	TIME [epoch: 1.84 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41046089673029057		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.41046089673029057 | validation: 0.41246726402332323]
	TIME [epoch: 1.84 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4156784272159911		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.4156784272159911 | validation: 0.39261086370985476]
	TIME [epoch: 1.84 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42573762659165704		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.42573762659165704 | validation: 0.3723726430742469]
	TIME [epoch: 1.84 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43282653836248797		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.43282653836248797 | validation: 0.444917984032595]
	TIME [epoch: 1.84 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.425144480819553		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.425144480819553 | validation: 0.3813864247647521]
	TIME [epoch: 1.84 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41178819404690914		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.41178819404690914 | validation: 0.508438325621915]
	TIME [epoch: 1.84 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4125741465602508		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.4125741465602508 | validation: 0.30236351197496864]
	TIME [epoch: 1.84 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42437743423585694		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.42437743423585694 | validation: 0.40223187179324144]
	TIME [epoch: 1.84 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41642984529719307		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.41642984529719307 | validation: 0.3445752941233109]
	TIME [epoch: 1.84 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40833896872474157		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.40833896872474157 | validation: 0.37291266446453897]
	TIME [epoch: 1.84 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40416199922736495		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.40416199922736495 | validation: 0.3681994449399966]
	TIME [epoch: 1.84 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40686049598290547		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.40686049598290547 | validation: 0.49625891384296744]
	TIME [epoch: 1.84 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42853913480792266		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.42853913480792266 | validation: 0.3676540805791806]
	TIME [epoch: 1.84 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43671607379245325		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.43671607379245325 | validation: 0.48612634709381636]
	TIME [epoch: 1.84 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42502443557428876		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.42502443557428876 | validation: 0.3423957102950281]
	TIME [epoch: 1.84 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.408155226897677		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.408155226897677 | validation: 0.3995061071528854]
	TIME [epoch: 1.84 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40087043629480135		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.40087043629480135 | validation: 0.2998767179555308]
	TIME [epoch: 1.84 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40892305325373446		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.40892305325373446 | validation: 0.4215314345717235]
	TIME [epoch: 1.84 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40802478345678533		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.40802478345678533 | validation: 0.2903662034989579]
	TIME [epoch: 1.84 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42267188866232647		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.42267188866232647 | validation: 0.39345043781967803]
	TIME [epoch: 1.84 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4072400557631767		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.4072400557631767 | validation: 0.36638406491170744]
	TIME [epoch: 1.84 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40533009482766064		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.40533009482766064 | validation: 0.5272128274561743]
	TIME [epoch: 1.84 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42233529600814174		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.42233529600814174 | validation: 0.32503231905115854]
	TIME [epoch: 1.93 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4448977129896541		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.4448977129896541 | validation: 0.36075864357933674]
	TIME [epoch: 1.84 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42658012672461554		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.42658012672461554 | validation: 0.46335225425848886]
	TIME [epoch: 1.84 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42083321820162073		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.42083321820162073 | validation: 0.3658227145222876]
	TIME [epoch: 1.84 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41963843592954236		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.41963843592954236 | validation: 0.391832675629743]
	TIME [epoch: 1.84 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40634364226936415		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.40634364226936415 | validation: 0.33895833363676775]
	TIME [epoch: 1.84 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40144981602860813		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.40144981602860813 | validation: 0.3957847272486794]
	TIME [epoch: 1.84 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4009602774722076		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.4009602774722076 | validation: 0.380374597854802]
	TIME [epoch: 1.84 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40884631565341245		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.40884631565341245 | validation: 0.41799685393959085]
	TIME [epoch: 1.84 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4118638455101521		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.4118638455101521 | validation: 0.3888943385413554]
	TIME [epoch: 1.84 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4127369553611734		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.4127369553611734 | validation: 0.39267529563739734]
	TIME [epoch: 1.84 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4080179748802749		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.4080179748802749 | validation: 0.3684135559428805]
	TIME [epoch: 1.84 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4114156576371578		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.4114156576371578 | validation: 0.3962681050129047]
	TIME [epoch: 1.84 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4088644622480333		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.4088644622480333 | validation: 0.7344891601967871]
	TIME [epoch: 1.84 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5191823256879556		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.5191823256879556 | validation: 0.35330405078925553]
	TIME [epoch: 1.84 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4713346455460239		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.4713346455460239 | validation: 0.3419428184342099]
	TIME [epoch: 1.84 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.413043614367354		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.413043614367354 | validation: 0.42163334113497525]
	TIME [epoch: 1.84 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41336547478060065		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.41336547478060065 | validation: 0.32917536037200873]
	TIME [epoch: 1.84 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40746003923649227		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.40746003923649227 | validation: 0.3972530032862405]
	TIME [epoch: 1.84 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4003133336181346		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.4003133336181346 | validation: 0.39787845236703284]
	TIME [epoch: 1.84 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39690929889040893		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.39690929889040893 | validation: 0.3789198215199358]
	TIME [epoch: 1.84 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39795322134437267		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.39795322134437267 | validation: 0.3646974505800448]
	TIME [epoch: 1.84 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39949982490926644		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.39949982490926644 | validation: 0.299212242496235]
	TIME [epoch: 1.84 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4238388827526394		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.4238388827526394 | validation: 0.41371597444947694]
	TIME [epoch: 1.84 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42714126509399253		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.42714126509399253 | validation: 0.3484346130169953]
	TIME [epoch: 1.84 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4159245565889128		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.4159245565889128 | validation: 0.31116184779863376]
	TIME [epoch: 1.84 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41224132866318997		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.41224132866318997 | validation: 0.4247563262748657]
	TIME [epoch: 1.84 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40786081445552524		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.40786081445552524 | validation: 0.3231725943334889]
	TIME [epoch: 1.84 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4067941686812682		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.4067941686812682 | validation: 0.5130841042984641]
	TIME [epoch: 1.84 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4140880513541421		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.4140880513541421 | validation: 0.3049505645800508]
	TIME [epoch: 1.84 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40759000753427677		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.40759000753427677 | validation: 0.37731479297690235]
	TIME [epoch: 1.84 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3974284284499208		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.3974284284499208 | validation: 0.2830190355606268]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_779.pth
	Model improved!!!
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4099462486802326		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.4099462486802326 | validation: 0.32036133537581224]
	TIME [epoch: 1.84 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4015285456255323		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.4015285456255323 | validation: 0.37805687124196896]
	TIME [epoch: 1.84 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40382817804283394		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.40382817804283394 | validation: 0.4479527456027708]
	TIME [epoch: 1.84 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4224160920043327		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.4224160920043327 | validation: 0.39492588109927373]
	TIME [epoch: 1.84 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42940337245909527		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.42940337245909527 | validation: 0.4500301354847703]
	TIME [epoch: 1.84 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4136962794768316		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.4136962794768316 | validation: 0.31417675563936065]
	TIME [epoch: 1.84 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4029630275477416		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.4029630275477416 | validation: 0.4353013298626655]
	TIME [epoch: 1.84 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3959876048092986		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.3959876048092986 | validation: 0.28509330721593207]
	TIME [epoch: 1.84 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4059861181438971		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.4059861181438971 | validation: 0.33324014838724225]
	TIME [epoch: 1.84 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4023387357936874		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.4023387357936874 | validation: 0.3643783372848894]
	TIME [epoch: 1.84 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3952535559226661		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.3952535559226661 | validation: 0.3979394281492839]
	TIME [epoch: 1.84 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4004346212301222		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.4004346212301222 | validation: 0.297897361257051]
	TIME [epoch: 1.84 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42084782160610873		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.42084782160610873 | validation: 0.37646241578592193]
	TIME [epoch: 1.84 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42201861843679966		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.42201861843679966 | validation: 0.33799826641742137]
	TIME [epoch: 1.84 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4102600593616267		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.4102600593616267 | validation: 0.36956478777946433]
	TIME [epoch: 1.84 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4051578771263135		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.4051578771263135 | validation: 0.4349623031827009]
	TIME [epoch: 1.84 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40516304053068447		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.40516304053068447 | validation: 0.3421257314214192]
	TIME [epoch: 1.84 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40779549019214123		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.40779549019214123 | validation: 0.3805192097842399]
	TIME [epoch: 1.84 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40037335087777615		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.40037335087777615 | validation: 0.2883604787182568]
	TIME [epoch: 1.84 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41423951621777627		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.41423951621777627 | validation: 0.33084812331907304]
	TIME [epoch: 1.84 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4019879640410333		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.4019879640410333 | validation: 0.33015002189599046]
	TIME [epoch: 1.84 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3936383102413549		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.3936383102413549 | validation: 0.38551534700070583]
	TIME [epoch: 1.84 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39190330370793264		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.39190330370793264 | validation: 0.3778128780699228]
	TIME [epoch: 1.84 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39185895138111604		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.39185895138111604 | validation: 0.28628450708941605]
	TIME [epoch: 1.84 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4088796021567657		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.4088796021567657 | validation: 0.3475036608976054]
	TIME [epoch: 1.84 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41539643224128		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.41539643224128 | validation: 0.36712576550777376]
	TIME [epoch: 1.84 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4109046380549382		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.4109046380549382 | validation: 0.4186153517544208]
	TIME [epoch: 1.84 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4239241010698932		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.4239241010698932 | validation: 0.4826637850162513]
	TIME [epoch: 1.84 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4185550508556035		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.4185550508556035 | validation: 0.2898149513721826]
	TIME [epoch: 1.84 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43154865633254347		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.43154865633254347 | validation: 0.3668689175330204]
	TIME [epoch: 1.84 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40701924856864447		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.40701924856864447 | validation: 0.3435181960768367]
	TIME [epoch: 1.84 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3950284854959817		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.3950284854959817 | validation: 0.36435477906852304]
	TIME [epoch: 1.84 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39140197898173057		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.39140197898173057 | validation: 0.4221909162959174]
	TIME [epoch: 1.84 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39610057111458824		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.39610057111458824 | validation: 0.34862634063698866]
	TIME [epoch: 1.84 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39597637057155055		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.39597637057155055 | validation: 0.43621438810679347]
	TIME [epoch: 1.84 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4068775470068217		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.4068775470068217 | validation: 0.3241484555278801]
	TIME [epoch: 1.84 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4164367406401367		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.4164367406401367 | validation: 0.4496296542831768]
	TIME [epoch: 1.84 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4057163074031625		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.4057163074031625 | validation: 0.4231489846376317]
	TIME [epoch: 1.84 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3974439285284266		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.3974439285284266 | validation: 0.3462436144002851]
	TIME [epoch: 1.84 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3977145536835633		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.3977145536835633 | validation: 0.5403043581123369]
	TIME [epoch: 1.84 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4078635328241198		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.4078635328241198 | validation: 0.2820975062216709]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_820.pth
	Model improved!!!
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43058652012717075		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.43058652012717075 | validation: 0.303786346588703]
	TIME [epoch: 1.84 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41030933334697056		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.41030933334697056 | validation: 0.362745708399914]
	TIME [epoch: 1.84 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.397551446190677		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.397551446190677 | validation: 0.3843846459929591]
	TIME [epoch: 1.84 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38968387917358066		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.38968387917358066 | validation: 0.2898964477855427]
	TIME [epoch: 1.84 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4143286755299903		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.4143286755299903 | validation: 0.3425739742856312]
	TIME [epoch: 1.84 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40539565388327964		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.40539565388327964 | validation: 0.35050087543785]
	TIME [epoch: 1.84 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39705205405046157		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.39705205405046157 | validation: 0.3727478005856837]
	TIME [epoch: 1.84 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39327271385952345		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.39327271385952345 | validation: 0.4534650097123013]
	TIME [epoch: 1.84 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3956031539327137		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.3956031539327137 | validation: 0.43582405517824185]
	TIME [epoch: 1.84 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4045054105001583		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.4045054105001583 | validation: 0.39406217141075806]
	TIME [epoch: 1.85 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.420809492869178		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.420809492869178 | validation: 0.4081546929426769]
	TIME [epoch: 1.84 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41200181273223024		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.41200181273223024 | validation: 0.38728911217583956]
	TIME [epoch: 1.84 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3944819901830084		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.3944819901830084 | validation: 0.32028403448151765]
	TIME [epoch: 1.84 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3950671769778325		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.3950671769778325 | validation: 0.4772002936996275]
	TIME [epoch: 1.89 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4017544616847513		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.4017544616847513 | validation: 0.31775013330414503]
	TIME [epoch: 1.84 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4010863253854147		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.4010863253854147 | validation: 0.355121139866827]
	TIME [epoch: 1.84 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3921775497155751		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.3921775497155751 | validation: 0.348894387356862]
	TIME [epoch: 1.84 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3874199658046535		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.3874199658046535 | validation: 0.37017866763514246]
	TIME [epoch: 1.85 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3901576314090541		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.3901576314090541 | validation: 0.7443900294668963]
	TIME [epoch: 1.84 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5001572291064027		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.5001572291064027 | validation: 0.35416763757859054]
	TIME [epoch: 1.84 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4382262197953068		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.4382262197953068 | validation: 0.3493287025956371]
	TIME [epoch: 1.84 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39164080233466336		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.39164080233466336 | validation: 0.42728847898912703]
	TIME [epoch: 1.84 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40338670577575314		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.40338670577575314 | validation: 0.3033403417114894]
	TIME [epoch: 1.84 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4027956853705459		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.4027956853705459 | validation: 0.32165505327093913]
	TIME [epoch: 1.84 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39750962284796815		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.39750962284796815 | validation: 0.3987010441474582]
	TIME [epoch: 1.84 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4048437358466424		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.4048437358466424 | validation: 0.2897890004662242]
	TIME [epoch: 1.85 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42206562774431206		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.42206562774431206 | validation: 0.3023723934131901]
	TIME [epoch: 1.84 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40933430831105455		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.40933430831105455 | validation: 0.3512703874226332]
	TIME [epoch: 1.84 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3961668042480055		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.3961668042480055 | validation: 0.34807210794787147]
	TIME [epoch: 1.84 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3896316391356483		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.3896316391356483 | validation: 0.5644021390176847]
	TIME [epoch: 1.84 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40853296402981143		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.40853296402981143 | validation: 0.3550564955730622]
	TIME [epoch: 1.84 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39787560815526074		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.39787560815526074 | validation: 0.3206339974434734]
	TIME [epoch: 1.84 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3965034240351079		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.3965034240351079 | validation: 0.38476243642889574]
	TIME [epoch: 1.84 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40439354559837354		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.40439354559837354 | validation: 0.3239769370515819]
	TIME [epoch: 1.84 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4070052119003937		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.4070052119003937 | validation: 0.33393566558407795]
	TIME [epoch: 1.84 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3991881676258062		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.3991881676258062 | validation: 0.4088660263267134]
	TIME [epoch: 1.84 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3942104009603827		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.3942104009603827 | validation: 0.338136583844457]
	TIME [epoch: 1.84 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40137128351127405		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.40137128351127405 | validation: 0.416181834377058]
	TIME [epoch: 1.84 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39417132419988393		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.39417132419988393 | validation: 0.34725139156121987]
	TIME [epoch: 1.84 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39144760660374983		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.39144760660374983 | validation: 0.5758901732653137]
	TIME [epoch: 1.84 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40663233196301807		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.40663233196301807 | validation: 0.3193962738918261]
	TIME [epoch: 1.84 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4026984273083521		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.4026984273083521 | validation: 0.3999632730724249]
	TIME [epoch: 1.84 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3906203084466521		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.3906203084466521 | validation: 0.3799332709822082]
	TIME [epoch: 1.84 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38826506235968056		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.38826506235968056 | validation: 0.3429065230602098]
	TIME [epoch: 1.84 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.386677299167978		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.386677299167978 | validation: 0.33653654653085263]
	TIME [epoch: 1.84 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3906128847200917		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.3906128847200917 | validation: 0.37048123930119914]
	TIME [epoch: 1.84 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39726804613490563		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.39726804613490563 | validation: 0.38351761363469317]
	TIME [epoch: 1.84 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40814501677752546		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.40814501677752546 | validation: 0.36827364969353305]
	TIME [epoch: 1.84 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39944378368227645		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.39944378368227645 | validation: 0.3279981532499723]
	TIME [epoch: 1.84 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3910299309382853		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.3910299309382853 | validation: 0.3777143898335676]
	TIME [epoch: 1.84 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38558280844720294		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.38558280844720294 | validation: 0.3688733487995224]
	TIME [epoch: 1.84 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3850335054138138		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.3850335054138138 | validation: 0.3625375471901037]
	TIME [epoch: 1.84 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38918811238484596		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.38918811238484596 | validation: 0.360100037997894]
	TIME [epoch: 1.84 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38698146903701186		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.38698146903701186 | validation: 0.36882145237687847]
	TIME [epoch: 1.84 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38570493488960245		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.38570493488960245 | validation: 0.3340050909364619]
	TIME [epoch: 1.84 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3887035407152661		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.3887035407152661 | validation: 0.8208696793413275]
	TIME [epoch: 1.84 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6058877160256887		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.6058877160256887 | validation: 0.5929922220455426]
	TIME [epoch: 1.84 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43015326364664896		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.43015326364664896 | validation: 0.358237724522517]
	TIME [epoch: 1.85 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4175195109365635		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.4175195109365635 | validation: 0.37371367668272715]
	TIME [epoch: 1.84 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39217103506054585		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.39217103506054585 | validation: 0.3861399576348603]
	TIME [epoch: 1.84 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3924452014967891		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.3924452014967891 | validation: 0.3269424055519754]
	TIME [epoch: 1.84 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3927957120942266		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.3927957120942266 | validation: 0.2742609990419812]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_882.pth
	Model improved!!!
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4103201535695795		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.4103201535695795 | validation: 0.30061758140485617]
	TIME [epoch: 1.84 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40768316358876894		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.40768316358876894 | validation: 0.32882100288081334]
	TIME [epoch: 1.84 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39951284349032534		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.39951284349032534 | validation: 0.31804463274686473]
	TIME [epoch: 1.84 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3961000530956467		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.3961000530956467 | validation: 0.3454881233710454]
	TIME [epoch: 1.84 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39011074377413596		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.39011074377413596 | validation: 0.37472380766666336]
	TIME [epoch: 1.84 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38740885183997475		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.38740885183997475 | validation: 0.3169667195858802]
	TIME [epoch: 1.84 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3871597678298727		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.3871597678298727 | validation: 0.30153130958690144]
	TIME [epoch: 1.84 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3956254285982476		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.3956254285982476 | validation: 0.2846347719529906]
	TIME [epoch: 1.84 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4154677403472708		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.4154677403472708 | validation: 0.35158306912340004]
	TIME [epoch: 1.84 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3976470589204062		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.3976470589204062 | validation: 0.7796357557800158]
	TIME [epoch: 1.83 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5439597634459199		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.5439597634459199 | validation: 0.5669337763317108]
	TIME [epoch: 1.84 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4347123520347551		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.4347123520347551 | validation: 0.37088820348182777]
	TIME [epoch: 1.84 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42364656697386155		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.42364656697386155 | validation: 0.3919863637178298]
	TIME [epoch: 1.84 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40373412632103395		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.40373412632103395 | validation: 0.4400930041082317]
	TIME [epoch: 1.84 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3922813230467044		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.3922813230467044 | validation: 0.33674486944828913]
	TIME [epoch: 1.84 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38941246583360556		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.38941246583360556 | validation: 0.3067028222784455]
	TIME [epoch: 1.84 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3882812782050513		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.3882812782050513 | validation: 0.3610953305589595]
	TIME [epoch: 1.84 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39031090471601515		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.39031090471601515 | validation: 0.36586370054944056]
	TIME [epoch: 1.85 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3914230296320848		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.3914230296320848 | validation: 0.32030665745290476]
	TIME [epoch: 1.84 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3987731117812055		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.3987731117812055 | validation: 0.33134467849320176]
	TIME [epoch: 1.84 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39041096870675285		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.39041096870675285 | validation: 0.35636806530125054]
	TIME [epoch: 1.84 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3875397120223545		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.3875397120223545 | validation: 0.3360604547199857]
	TIME [epoch: 1.84 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38736891651089045		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.38736891651089045 | validation: 0.35363417270994385]
	TIME [epoch: 1.83 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3840406447605464		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.3840406447605464 | validation: 0.2801491288235233]
	TIME [epoch: 1.83 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39741440788162397		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.39741440788162397 | validation: 0.3174340452511563]
	TIME [epoch: 1.84 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39310695635107834		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.39310695635107834 | validation: 0.3563948149601632]
	TIME [epoch: 1.84 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39073291632011015		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.39073291632011015 | validation: 0.33804338612376333]
	TIME [epoch: 1.85 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38450117445190424		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.38450117445190424 | validation: 0.3548720013423564]
	TIME [epoch: 1.84 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3833828019820271		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.3833828019820271 | validation: 0.33364442380307074]
	TIME [epoch: 1.84 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3862447593600953		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.3862447593600953 | validation: 0.3724903223222881]
	TIME [epoch: 1.84 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3886616849723079		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.3886616849723079 | validation: 0.3239563020482985]
	TIME [epoch: 1.84 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4080248066329451		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.4080248066329451 | validation: 0.37487649495176834]
	TIME [epoch: 1.84 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40391313858978206		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.40391313858978206 | validation: 0.37448216708936727]
	TIME [epoch: 1.84 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3884818277214843		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.3884818277214843 | validation: 0.29443512455078696]
	TIME [epoch: 1.84 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39178099838780794		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.39178099838780794 | validation: 0.3027276011872142]
	TIME [epoch: 1.84 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3964352955292314		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.3964352955292314 | validation: 0.3292386514727878]
	TIME [epoch: 1.84 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3925531649669772		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.3925531649669772 | validation: 0.32281429566993003]
	TIME [epoch: 1.84 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3929505157498877		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.3929505157498877 | validation: 0.37933589359259323]
	TIME [epoch: 1.84 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3917096099677297		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.3917096099677297 | validation: 0.3364669838640033]
	TIME [epoch: 1.84 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3892046762408877		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.3892046762408877 | validation: 0.40843465330542233]
	TIME [epoch: 1.84 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3860936745317909		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.3860936745317909 | validation: 0.35568058610397424]
	TIME [epoch: 1.84 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3832816719350322		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.3832816719350322 | validation: 0.41036562766511386]
	TIME [epoch: 1.84 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3850484950142976		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.3850484950142976 | validation: 0.34132790345909386]
	TIME [epoch: 1.84 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38303783092305155		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.38303783092305155 | validation: 0.2958247099734607]
	TIME [epoch: 1.84 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3989195577835089		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.3989195577835089 | validation: 0.3250817946295028]
	TIME [epoch: 1.84 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4013431425783574		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.4013431425783574 | validation: 0.34117270460180726]
	TIME [epoch: 1.84 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39227421629258497		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.39227421629258497 | validation: 0.3198915174418054]
	TIME [epoch: 1.84 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38894931689288137		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.38894931689288137 | validation: 0.46578456027279486]
	TIME [epoch: 1.84 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39571034517140197		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.39571034517140197 | validation: 0.27557049202334055]
	TIME [epoch: 1.84 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4112329714432659		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.4112329714432659 | validation: 0.30300438052991413]
	TIME [epoch: 1.84 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4010724760720119		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.4010724760720119 | validation: 0.3250506491416762]
	TIME [epoch: 1.84 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3946944793650887		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.3946944793650887 | validation: 0.31698287091770966]
	TIME [epoch: 1.84 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3878858160002596		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.3878858160002596 | validation: 0.37348871930154237]
	TIME [epoch: 1.84 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38350390846456733		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.38350390846456733 | validation: 0.39849636224560653]
	TIME [epoch: 1.84 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38606203723760735		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.38606203723760735 | validation: 0.3562500629621116]
	TIME [epoch: 1.84 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.386804580372923		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.386804580372923 | validation: 0.438702627122758]
	TIME [epoch: 1.84 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3963413744410761		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.3963413744410761 | validation: 0.36278655181963093]
	TIME [epoch: 1.84 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3881229655796627		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.3881229655796627 | validation: 0.33962446186978973]
	TIME [epoch: 1.84 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38758480182085153		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.38758480182085153 | validation: 0.3926173472283776]
	TIME [epoch: 1.85 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3889999668428132		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.3889999668428132 | validation: 0.3378419198927589]
	TIME [epoch: 1.84 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3849461989391971		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.3849461989391971 | validation: 0.6008012472506042]
	TIME [epoch: 1.84 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4113806301381594		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.4113806301381594 | validation: 0.3421271778004321]
	TIME [epoch: 1.84 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39388930769902497		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.39388930769902497 | validation: 0.37271297637641254]
	TIME [epoch: 1.84 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.385073283741238		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.385073283741238 | validation: 0.3583846460303835]
	TIME [epoch: 1.84 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38707224138058494		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.38707224138058494 | validation: 0.7537680501440924]
	TIME [epoch: 1.84 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5164961024679807		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.5164961024679807 | validation: 0.6079078689252405]
	TIME [epoch: 1.84 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43087368984488933		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.43087368984488933 | validation: 0.3718999301257511]
	TIME [epoch: 1.84 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41888716943961046		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.41888716943961046 | validation: 0.33065910073507715]
	TIME [epoch: 1.84 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39958170467640713		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.39958170467640713 | validation: 0.37144956494541637]
	TIME [epoch: 1.84 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38419199076185656		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.38419199076185656 | validation: 0.40778142066072]
	TIME [epoch: 1.84 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3883854451103096		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.3883854451103096 | validation: 0.3467952395274967]
	TIME [epoch: 1.84 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38211969296490994		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.38211969296490994 | validation: 0.5568101460805043]
	TIME [epoch: 1.84 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4078981451256521		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.4078981451256521 | validation: 0.4333808282424196]
	TIME [epoch: 1.84 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39318739877734843		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.39318739877734843 | validation: 0.3346654900172249]
	TIME [epoch: 1.84 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3935200021268447		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.3935200021268447 | validation: 0.3586873565534931]
	TIME [epoch: 1.85 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3839253082727085		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.3839253082727085 | validation: 0.3624404664468641]
	TIME [epoch: 1.84 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38461457066246846		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.38461457066246846 | validation: 0.34183040296641876]
	TIME [epoch: 1.84 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38203441907096347		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.38203441907096347 | validation: 0.3363239545374139]
	TIME [epoch: 1.84 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38482871513169253		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.38482871513169253 | validation: 0.34395853968912077]
	TIME [epoch: 1.84 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3897863330308433		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.3897863330308433 | validation: 0.3763919193246459]
	TIME [epoch: 1.84 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3888127361343669		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.3888127361343669 | validation: 0.33105774175570024]
	TIME [epoch: 1.84 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3842464683246149		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.3842464683246149 | validation: 0.3641164606939866]
	TIME [epoch: 1.84 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3835554625416476		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.3835554625416476 | validation: 0.307513299938918]
	TIME [epoch: 1.84 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3856335268175418		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.3856335268175418 | validation: 0.3391420404742606]
	TIME [epoch: 1.84 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38338975904653466		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.38338975904653466 | validation: 0.327165712776746]
	TIME [epoch: 1.84 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3836917835101676		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.3836917835101676 | validation: 0.32288866771056013]
	TIME [epoch: 1.84 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38209252643203023		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.38209252643203023 | validation: 0.36096295020591956]
	TIME [epoch: 1.84 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38276483769714004		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.38276483769714004 | validation: 0.3449680797723709]
	TIME [epoch: 1.84 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38058095334131287		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.38058095334131287 | validation: 0.36664905232491024]
	TIME [epoch: 1.84 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38298508302372714		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.38298508302372714 | validation: 0.37069202666781775]
	TIME [epoch: 1.84 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38602217710013575		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.38602217710013575 | validation: 0.33722539943853214]
	TIME [epoch: 1.84 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3915110466211291		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.3915110466211291 | validation: 0.36590234744883987]
	TIME [epoch: 1.84 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3883590860284025		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.3883590860284025 | validation: 0.34250857337183266]
	TIME [epoch: 1.84 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38264761965637323		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.38264761965637323 | validation: 0.3323641456968056]
	TIME [epoch: 1.84 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3811173719812331		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.3811173719812331 | validation: 0.3519462956543471]
	TIME [epoch: 1.84 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38007523317035274		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.38007523317035274 | validation: 0.3718670679913326]
	TIME [epoch: 1.84 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3831984194311026		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.3831984194311026 | validation: 0.3424479658550412]
	TIME [epoch: 1.84 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3877781396012018		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.3877781396012018 | validation: 0.30067142661397955]
	TIME [epoch: 1.84 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38824971608890885		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.38824971608890885 | validation: 0.4088066702633869]
	TIME [epoch: 1.84 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39079785172503934		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.39079785172503934 | validation: 0.3255051627931005]
	TIME [epoch: 1.84 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38401994577589676		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.38401994577589676 | validation: 0.2984745164712012]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_2_v_mmd3_983.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 1726.637 seconds.
