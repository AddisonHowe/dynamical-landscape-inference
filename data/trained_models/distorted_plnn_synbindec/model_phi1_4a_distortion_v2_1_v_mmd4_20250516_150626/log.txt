Args:
Namespace(name='model_phi1_4a_distortion_v2_1_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_1/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_1/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.021322314, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3091464896

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.257342626225601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.257342626225601 | validation: 5.717631111136415]
	TIME [epoch: 157 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.740796168811165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.740796168811165 | validation: 4.422912788236353]
	TIME [epoch: 0.803 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.227198074522867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.227198074522867 | validation: 6.221563487842182]
	TIME [epoch: 0.691 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.874603099508315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.874603099508315 | validation: 5.511216267083696]
	TIME [epoch: 0.689 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.131681062405129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.131681062405129 | validation: 4.99402829142608]
	TIME [epoch: 0.689 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.1916220301332645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1916220301332645 | validation: 4.440551585450513]
	TIME [epoch: 0.688 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.647237081661607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.647237081661607 | validation: 4.1659654737538085]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5041786193786097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5041786193786097 | validation: 4.273758129232303]
	TIME [epoch: 0.691 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.342474996721567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.342474996721567 | validation: 3.6597413636658582]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.009198082420456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.009198082420456 | validation: 3.138728304851032]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0245812146429034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0245812146429034 | validation: 3.3844010348175946]
	TIME [epoch: 0.69 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.961080319622383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.961080319622383 | validation: 3.0017474704507046]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8273848894280214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8273848894280214 | validation: 2.6706168496463714]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.680499079895921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.680499079895921 | validation: 3.140935832420143]
	TIME [epoch: 0.689 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.634545660299551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.634545660299551 | validation: 2.653981591561333]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6623332787283576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6623332787283576 | validation: 2.7034722424631528]
	TIME [epoch: 0.693 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4210820375802404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4210820375802404 | validation: 2.5169166068668503]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3825714881723736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3825714881723736 | validation: 2.1706133966543724]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.389396408819197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.389396408819197 | validation: 2.7204245899360378]
	TIME [epoch: 0.69 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.407640038351668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.407640038351668 | validation: 2.268668769299891]
	TIME [epoch: 0.687 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4033757329978416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4033757329978416 | validation: 1.9997838323791184]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.244647122427628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.244647122427628 | validation: 3.015439349523922]
	TIME [epoch: 0.691 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.489700675237426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.489700675237426 | validation: 2.136026973162788]
	TIME [epoch: 0.691 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3083778540111197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3083778540111197 | validation: 2.046250864141171]
	TIME [epoch: 0.69 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.262776222096607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.262776222096607 | validation: 1.9402880326813028]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1047016061773887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1047016061773887 | validation: 1.8584981233297655]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0737100671476676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0737100671476676 | validation: 1.7286186307304519]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0815051958603408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0815051958603408 | validation: 1.899418470910251]
	TIME [epoch: 0.693 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.040097394856034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.040097394856034 | validation: 1.8740315290399714]
	TIME [epoch: 0.691 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.069611734791836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.069611734791836 | validation: 1.5496174273677406]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9567958994510122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9567958994510122 | validation: 2.172921636833897]
	TIME [epoch: 0.693 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.119080563906228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.119080563906228 | validation: 2.332686742592977]
	TIME [epoch: 0.689 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.195065007526696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.195065007526696 | validation: 1.6676757050465634]
	TIME [epoch: 0.69 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8930288714477213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8930288714477213 | validation: 1.8460117399799303]
	TIME [epoch: 0.691 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9304172183552677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9304172183552677 | validation: 1.8143990801487833]
	TIME [epoch: 0.69 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.886775133455318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.886775133455318 | validation: 1.614625865684262]
	TIME [epoch: 0.688 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.793348968230871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.793348968230871 | validation: 1.4122244310386454]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.730741345501055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.730741345501055 | validation: 1.325662939560128]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6680385359535403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6680385359535403 | validation: 1.2332161950034344]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6098695304305917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6098695304305917 | validation: 1.1929483529113483]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.562206312783397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.562206312783397 | validation: 1.3377672055392367]
	TIME [epoch: 0.694 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5540319027954206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5540319027954206 | validation: 2.253794979766586]
	TIME [epoch: 0.692 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0062001424607443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0062001424607443 | validation: 1.5926647108876715]
	TIME [epoch: 0.69 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5636899551421146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5636899551421146 | validation: 1.2124628376146713]
	TIME [epoch: 0.69 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4024702671514866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4024702671514866 | validation: 1.0593919708306896]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3686063477190569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3686063477190569 | validation: 1.5294536288353315]
	TIME [epoch: 0.694 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5443435139674984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5443435139674984 | validation: 2.0933971342725033]
	TIME [epoch: 0.692 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.914835324591678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.914835324591678 | validation: 1.4587941835471083]
	TIME [epoch: 0.69 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4598072265302509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4598072265302509 | validation: 1.380754048701564]
	TIME [epoch: 0.69 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4298655526857391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4298655526857391 | validation: 1.428983267806617]
	TIME [epoch: 0.69 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4773671997362658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4773671997362658 | validation: 1.1003789282743743]
	TIME [epoch: 0.691 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2825644522313382		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.2825644522313382 | validation: 1.106976676934664]
	TIME [epoch: 0.69 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2747605817944698		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.2747605817944698 | validation: 1.2509398635313511]
	TIME [epoch: 0.691 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3052838175651365		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.3052838175651365 | validation: 1.3438589107036438]
	TIME [epoch: 0.69 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.388007963233078		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.388007963233078 | validation: 1.315524291105936]
	TIME [epoch: 0.691 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.333863020404842		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.333863020404842 | validation: 1.150401486782817]
	TIME [epoch: 0.69 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.211920466927448		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.211920466927448 | validation: 1.1076711474532495]
	TIME [epoch: 0.689 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1570840676950662		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.1570840676950662 | validation: 1.1274369363911443]
	TIME [epoch: 0.691 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2228072434293795		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.2228072434293795 | validation: 1.5285008307183905]
	TIME [epoch: 0.691 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.404404248959826		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.404404248959826 | validation: 1.1380599072014375]
	TIME [epoch: 0.69 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2408562848489342		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.2408562848489342 | validation: 1.018141666001781]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0993405779626906		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.0993405779626906 | validation: 0.9557616727793383]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1280692197553328		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.1280692197553328 | validation: 1.3361149966862744]
	TIME [epoch: 0.693 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2441295215748418		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.2441295215748418 | validation: 1.1862213039476885]
	TIME [epoch: 0.692 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3251596082641366		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.3251596082641366 | validation: 1.0674062849478323]
	TIME [epoch: 0.691 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0902405890164486		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.0902405890164486 | validation: 0.9486524858611939]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0748032786350608		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.0748032786350608 | validation: 1.2205205022098613]
	TIME [epoch: 0.692 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1601463938860876		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.1601463938860876 | validation: 1.153758478385308]
	TIME [epoch: 0.69 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3478049390550542		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.3478049390550542 | validation: 1.031648681620094]
	TIME [epoch: 0.688 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0350749366531		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.0350749366531 | validation: 0.9584057843031181]
	TIME [epoch: 0.689 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0251192392960484		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.0251192392960484 | validation: 0.8988654818605419]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.041149787418111		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.041149787418111 | validation: 1.0907473055093118]
	TIME [epoch: 0.691 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0675314562424263		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.0675314562424263 | validation: 0.9743246921188681]
	TIME [epoch: 0.69 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2455997782476227		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.2455997782476227 | validation: 1.3397145750517845]
	TIME [epoch: 0.691 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.199236927632542		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.199236927632542 | validation: 0.9838269367488952]
	TIME [epoch: 0.689 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0215027227167208		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.0215027227167208 | validation: 1.2097523917607385]
	TIME [epoch: 0.689 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3800840548451878		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.3800840548451878 | validation: 1.1280865137068916]
	TIME [epoch: 0.688 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0548830772545355		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.0548830772545355 | validation: 1.1309821882066764]
	TIME [epoch: 0.695 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0689714294458115		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.0689714294458115 | validation: 0.9595797821804045]
	TIME [epoch: 0.689 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2756336084489008		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.2756336084489008 | validation: 1.330651779985384]
	TIME [epoch: 0.688 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.293939658096789		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.293939658096789 | validation: 1.176421851142035]
	TIME [epoch: 0.688 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0801194753232946		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.0801194753232946 | validation: 1.2491672388460178]
	TIME [epoch: 0.69 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2468846011557646		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.2468846011557646 | validation: 1.1101206068351293]
	TIME [epoch: 0.693 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0214364557122455		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.0214364557122455 | validation: 1.0256785147348884]
	TIME [epoch: 0.689 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0946523105240449		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.0946523105240449 | validation: 0.8500972024694268]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9581507779307288		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 0.9581507779307288 | validation: 1.265411422477651]
	TIME [epoch: 0.693 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0981318077695514		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.0981318077695514 | validation: 1.129242944501837]
	TIME [epoch: 0.69 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.335807502202092		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.335807502202092 | validation: 0.9580495151706341]
	TIME [epoch: 0.689 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.109986133440181		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.109986133440181 | validation: 0.9265631713899047]
	TIME [epoch: 0.693 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9460208051683106		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.9460208051683106 | validation: 0.9613512045042244]
	TIME [epoch: 0.693 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9849502031304468		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 0.9849502031304468 | validation: 0.9907592728670518]
	TIME [epoch: 0.69 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9597419134162487		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.9597419134162487 | validation: 0.9010511381991269]
	TIME [epoch: 0.689 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9941955728707212		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.9941955728707212 | validation: 1.5169042191808757]
	TIME [epoch: 0.692 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2163430489693414		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.2163430489693414 | validation: 1.0370251345539505]
	TIME [epoch: 0.69 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2588349611615295		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.2588349611615295 | validation: 0.8677178421557921]
	TIME [epoch: 0.688 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0795834177308217		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.0795834177308217 | validation: 0.9458290306786147]
	TIME [epoch: 0.689 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9364118714812909		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.9364118714812909 | validation: 0.9408910702615341]
	TIME [epoch: 0.69 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9584078033754727		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.9584078033754727 | validation: 0.9882001726380175]
	TIME [epoch: 0.689 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9445873811522006		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.9445873811522006 | validation: 0.9132350682355045]
	TIME [epoch: 0.688 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9881769809035555		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.9881769809035555 | validation: 1.3974084241728102]
	TIME [epoch: 0.688 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1374926618885344		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.1374926618885344 | validation: 1.038241432412015]
	TIME [epoch: 0.693 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2461944638132694		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.2461944638132694 | validation: 0.8556448139258066]
	TIME [epoch: 0.69 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9872031904652522		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.9872031904652522 | validation: 1.1048786846963883]
	TIME [epoch: 0.69 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.019969815260561		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.019969815260561 | validation: 1.0030373641812422]
	TIME [epoch: 0.689 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0524177626016566		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.0524177626016566 | validation: 1.0088198488873497]
	TIME [epoch: 0.69 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9476478895881014		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 0.9476478895881014 | validation: 0.8972412389964268]
	TIME [epoch: 0.689 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9248681716831386		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.9248681716831386 | validation: 0.9519189045156726]
	TIME [epoch: 0.688 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9210795810486186		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.9210795810486186 | validation: 0.9043649079536241]
	TIME [epoch: 0.688 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9732874374454863		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.9732874374454863 | validation: 1.200024464654885]
	TIME [epoch: 0.688 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0143121954357677		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.0143121954357677 | validation: 0.8885714697085572]
	TIME [epoch: 0.691 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0446523168119026		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.0446523168119026 | validation: 1.106163293609156]
	TIME [epoch: 0.692 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9548675381558949		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.9548675381558949 | validation: 0.8986647800296581]
	TIME [epoch: 0.69 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9921904336084524		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.9921904336084524 | validation: 1.0475430488802948]
	TIME [epoch: 0.689 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9391998167626164		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.9391998167626164 | validation: 0.8692806108220662]
	TIME [epoch: 0.689 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9502428844092452		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.9502428844092452 | validation: 1.1182487109337913]
	TIME [epoch: 0.688 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9693165557242517		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.9693165557242517 | validation: 0.898250460337701]
	TIME [epoch: 0.689 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0931607684774232		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.0931607684774232 | validation: 0.9569310414333828]
	TIME [epoch: 0.689 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9099721117993553		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.9099721117993553 | validation: 1.1667013208295576]
	TIME [epoch: 0.688 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0303144115686202		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.0303144115686202 | validation: 0.8163691070145452]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.036003180424315		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.036003180424315 | validation: 1.1329485492866904]
	TIME [epoch: 0.694 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0183474148800453		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.0183474148800453 | validation: 0.9927793172518563]
	TIME [epoch: 0.693 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9452104237816962		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.9452104237816962 | validation: 0.8863345624969601]
	TIME [epoch: 0.689 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9852961215628002		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.9852961215628002 | validation: 1.1248392478558784]
	TIME [epoch: 0.692 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9614754937062644		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.9614754937062644 | validation: 0.8290532787696219]
	TIME [epoch: 0.692 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8700052418220221		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.8700052418220221 | validation: 0.884187551024143]
	TIME [epoch: 0.69 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8689360232456323		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.8689360232456323 | validation: 0.7953893343448374]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.911983588884124		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.911983588884124 | validation: 1.072946142018066]
	TIME [epoch: 0.692 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.926020883729189		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.926020883729189 | validation: 0.9742729419864745]
	TIME [epoch: 0.69 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0499580427099173		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.0499580427099173 | validation: 1.0092435305481542]
	TIME [epoch: 0.689 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8878208813674416		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.8878208813674416 | validation: 0.8218412957425472]
	TIME [epoch: 0.693 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.868313533930584		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.868313533930584 | validation: 1.0237015565098397]
	TIME [epoch: 0.69 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9087056970982945		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.9087056970982945 | validation: 0.938456773487345]
	TIME [epoch: 0.693 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0628869914461976		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.0628869914461976 | validation: 0.9937694875384093]
	TIME [epoch: 0.689 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8797469884548326		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.8797469884548326 | validation: 0.8500318800007828]
	TIME [epoch: 0.689 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8576309862403909		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.8576309862403909 | validation: 0.9389274165411168]
	TIME [epoch: 0.688 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8636359533941943		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.8636359533941943 | validation: 0.8833875415405796]
	TIME [epoch: 0.692 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9026030017864273		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.9026030017864273 | validation: 1.277346647787935]
	TIME [epoch: 0.689 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0047745869106997		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.0047745869106997 | validation: 0.8935597779917638]
	TIME [epoch: 0.691 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.20958024561512		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.20958024561512 | validation: 1.0800873266343445]
	TIME [epoch: 0.688 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2056304340319388		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.2056304340319388 | validation: 0.894061262265887]
	TIME [epoch: 0.688 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9779372428308878		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.9779372428308878 | validation: 1.146904732822829]
	TIME [epoch: 0.687 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9171559585785201		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.9171559585785201 | validation: 0.8488706364919683]
	TIME [epoch: 0.693 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9035546383414232		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.9035546383414232 | validation: 0.9832402279670959]
	TIME [epoch: 0.696 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8948349799128172		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.8948349799128172 | validation: 0.8789750056018729]
	TIME [epoch: 0.691 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8812111248060615		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.8812111248060615 | validation: 0.9858976158806553]
	TIME [epoch: 0.688 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8591356646354745		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.8591356646354745 | validation: 0.8668423476309143]
	TIME [epoch: 0.694 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8747274066897802		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.8747274066897802 | validation: 1.0529410274968447]
	TIME [epoch: 0.689 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9064139775783993		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.9064139775783993 | validation: 0.8723186337851102]
	TIME [epoch: 0.688 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.042610333241226		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.042610333241226 | validation: 0.9277669489701948]
	TIME [epoch: 0.692 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8283456502124591		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.8283456502124591 | validation: 0.8545234764052361]
	TIME [epoch: 0.692 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.815420363038798		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.815420363038798 | validation: 0.8100394347504765]
	TIME [epoch: 0.692 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8150894327684128		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.8150894327684128 | validation: 0.8663042565351251]
	TIME [epoch: 0.689 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8332020505016547		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.8332020505016547 | validation: 0.8561366242023413]
	TIME [epoch: 0.688 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9084724088632927		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.9084724088632927 | validation: 0.9299785983761029]
	TIME [epoch: 0.689 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8573692872644647		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.8573692872644647 | validation: 0.9472669648263538]
	TIME [epoch: 0.688 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.829963546596498		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.829963546596498 | validation: 0.8523632945895513]
	TIME [epoch: 0.688 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8993217657707914		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.8993217657707914 | validation: 1.3558008359739426]
	TIME [epoch: 0.688 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0326415371520523		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.0326415371520523 | validation: 0.8407507325283092]
	TIME [epoch: 0.692 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0271301486852753		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.0271301486852753 | validation: 0.9081011578461955]
	TIME [epoch: 0.688 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9224537671837248		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.9224537671837248 | validation: 0.9959723639697003]
	TIME [epoch: 0.692 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8666849559358383		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.8666849559358383 | validation: 0.8785731176400016]
	TIME [epoch: 0.696 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8320864135055724		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.8320864135055724 | validation: 0.8158020093741408]
	TIME [epoch: 0.689 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8040740090268641		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.8040740090268641 | validation: 0.8683858389847791]
	TIME [epoch: 0.689 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.805774291087571		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.805774291087571 | validation: 0.8420643796937303]
	TIME [epoch: 0.689 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8153745288801838		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.8153745288801838 | validation: 0.9879108152093461]
	TIME [epoch: 0.688 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.848299326201269		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.848299326201269 | validation: 0.8849059534770376]
	TIME [epoch: 0.688 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.919894175403366		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.919894175403366 | validation: 1.0985912587549524]
	TIME [epoch: 0.692 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8992170580540284		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.8992170580540284 | validation: 0.8346403819388274]
	TIME [epoch: 0.689 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9277517458034297		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.9277517458034297 | validation: 0.9903285489442617]
	TIME [epoch: 0.692 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8414450427519394		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.8414450427519394 | validation: 0.8162841811256936]
	TIME [epoch: 0.692 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8450232591430133		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.8450232591430133 | validation: 0.9825426304778646]
	TIME [epoch: 0.692 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8337328573208018		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.8337328573208018 | validation: 0.8209360692867801]
	TIME [epoch: 0.693 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8040733965825837		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.8040733965825837 | validation: 0.9134830601801522]
	TIME [epoch: 0.689 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8061918099981066		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.8061918099981066 | validation: 0.8098686898225984]
	TIME [epoch: 0.689 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8134265306755504		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.8134265306755504 | validation: 1.0116981927265314]
	TIME [epoch: 0.689 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8429813675474983		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.8429813675474983 | validation: 0.8510691607504761]
	TIME [epoch: 0.692 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7963854720758821		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.7963854720758821 | validation: 0.8073902437785286]
	TIME [epoch: 0.688 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8638194712372613		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.8638194712372613 | validation: 0.9291353415331596]
	TIME [epoch: 0.688 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8211124480718252		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.8211124480718252 | validation: 0.8225803747514295]
	TIME [epoch: 0.692 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9215852499764491		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.9215852499764491 | validation: 1.1341278499700687]
	TIME [epoch: 0.692 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8816463791646777		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.8816463791646777 | validation: 0.9872343921040516]
	TIME [epoch: 0.692 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8050879354333473		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.8050879354333473 | validation: 0.8477597198321464]
	TIME [epoch: 0.691 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9159176146154079		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.9159176146154079 | validation: 1.0794484935231947]
	TIME [epoch: 0.69 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8681613254777039		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.8681613254777039 | validation: 0.8936221032881048]
	TIME [epoch: 0.688 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7885991000032866		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.7885991000032866 | validation: 0.8433191420719531]
	TIME [epoch: 0.69 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8103574502249289		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.8103574502249289 | validation: 1.023649937919346]
	TIME [epoch: 0.689 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8177015432257561		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.8177015432257561 | validation: 0.7922530196294653]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_187.pth
	Model improved!!!
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7767937919822052		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.7767937919822052 | validation: 0.9430629411786905]
	TIME [epoch: 0.693 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8113693291796331		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.8113693291796331 | validation: 0.8539392456702141]
	TIME [epoch: 0.696 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.916018324244113		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.916018324244113 | validation: 1.0484310497182945]
	TIME [epoch: 0.69 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8308639411248504		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.8308639411248504 | validation: 0.835525104008735]
	TIME [epoch: 0.69 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7696657125734533		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.7696657125734533 | validation: 0.8566708978602892]
	TIME [epoch: 0.692 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7623036465034281		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.7623036465034281 | validation: 0.7955228695382464]
	TIME [epoch: 0.689 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7811878508235656		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.7811878508235656 | validation: 1.0404224024438171]
	TIME [epoch: 0.689 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8301401317202135		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.8301401317202135 | validation: 0.8001480080567513]
	TIME [epoch: 0.688 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8106192462531321		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.8106192462531321 | validation: 0.9314461094341685]
	TIME [epoch: 0.687 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7949332905378068		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.7949332905378068 | validation: 0.7865669104291966]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_197.pth
	Model improved!!!
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8644068146756092		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.8644068146756092 | validation: 1.0083175546398973]
	TIME [epoch: 0.691 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8152369888859707		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.8152369888859707 | validation: 0.8616027416294915]
	TIME [epoch: 0.688 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7481442024540849		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.7481442024540849 | validation: 0.7989123784423872]
	TIME [epoch: 0.689 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.764383409255172		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.764383409255172 | validation: 0.8858890764130465]
	TIME [epoch: 165 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7730477280273506		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.7730477280273506 | validation: 0.8532613907381162]
	TIME [epoch: 1.37 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7694788421970248		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.7694788421970248 | validation: 0.8376012146243849]
	TIME [epoch: 1.35 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7525454899143357		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.7525454899143357 | validation: 0.9037233453568061]
	TIME [epoch: 1.35 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7673372459653507		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.7673372459653507 | validation: 0.8113209036128706]
	TIME [epoch: 1.35 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.786635344206611		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.786635344206611 | validation: 1.0059348357296376]
	TIME [epoch: 1.35 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7816050220289258		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.7816050220289258 | validation: 0.8460355264595685]
	TIME [epoch: 1.35 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7570680844769511		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.7570680844769511 | validation: 0.8987212435596432]
	TIME [epoch: 1.36 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7709234110046288		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.7709234110046288 | validation: 0.8564512127624746]
	TIME [epoch: 1.35 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8122918878610839		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.8122918878610839 | validation: 1.142896301328834]
	TIME [epoch: 1.36 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8946515762408633		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.8946515762408633 | validation: 0.873358491831246]
	TIME [epoch: 1.35 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9335690872815784		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.9335690872815784 | validation: 0.8564963251242033]
	TIME [epoch: 1.35 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.75248414896572		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.75248414896572 | validation: 0.9399313705756438]
	TIME [epoch: 1.36 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7587334806746282		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.7587334806746282 | validation: 0.8509980649408049]
	TIME [epoch: 1.35 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7581624212834018		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.7581624212834018 | validation: 0.8630656634400697]
	TIME [epoch: 1.35 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7346985550181802		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.7346985550181802 | validation: 0.7875412080868531]
	TIME [epoch: 1.35 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7323491269582635		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.7323491269582635 | validation: 0.9075169743964111]
	TIME [epoch: 1.35 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7524505320963527		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.7524505320963527 | validation: 0.8737883982208651]
	TIME [epoch: 1.35 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7598441855293427		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.7598441855293427 | validation: 0.8801863480272495]
	TIME [epoch: 1.35 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7392192829545098		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.7392192829545098 | validation: 0.7762128952386304]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.731352870241249		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.731352870241249 | validation: 0.9219587721117102]
	TIME [epoch: 1.36 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7466368008480169		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.7466368008480169 | validation: 0.878910523593963]
	TIME [epoch: 1.35 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8786648513971101		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.8786648513971101 | validation: 0.9924142442112299]
	TIME [epoch: 1.35 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8536580800011909		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.8536580800011909 | validation: 0.8199834235927762]
	TIME [epoch: 1.35 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7262272576605687		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.7262272576605687 | validation: 0.922563838117263]
	TIME [epoch: 1.36 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7558653638270403		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.7558653638270403 | validation: 0.7861051286235572]
	TIME [epoch: 1.35 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7327110525897222		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.7327110525897222 | validation: 0.8803721851044184]
	TIME [epoch: 1.35 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7433283694184957		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.7433283694184957 | validation: 0.8251957704724746]
	TIME [epoch: 1.35 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7973372668307473		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.7973372668307473 | validation: 1.0552048167555614]
	TIME [epoch: 1.35 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8058260050305569		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.8058260050305569 | validation: 0.7856868404664683]
	TIME [epoch: 1.35 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7633527531196029		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.7633527531196029 | validation: 0.8550831064841735]
	TIME [epoch: 1.36 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7256835451009879		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.7256835451009879 | validation: 0.7987971226902992]
	TIME [epoch: 1.35 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7290938537401366		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.7290938537401366 | validation: 0.8794136950154642]
	TIME [epoch: 1.36 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7498937627356793		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.7498937627356793 | validation: 0.7870018223225984]
	TIME [epoch: 1.35 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.762156673144561		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.762156673144561 | validation: 0.9562528362492977]
	TIME [epoch: 1.38 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7589097508934921		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.7589097508934921 | validation: 0.7825773249636131]
	TIME [epoch: 1.35 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.73785638119792		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.73785638119792 | validation: 0.8688373681824084]
	TIME [epoch: 1.35 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.733302209775207		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.733302209775207 | validation: 0.7680952614771953]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_238.pth
	Model improved!!!
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7505826867319886		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.7505826867319886 | validation: 0.911528059440959]
	TIME [epoch: 1.35 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7207578089624681		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.7207578089624681 | validation: 0.7908104360043501]
	TIME [epoch: 1.36 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.735687129987272		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.735687129987272 | validation: 0.8686237910031184]
	TIME [epoch: 1.35 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7494721575250539		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.7494721575250539 | validation: 0.8365246940505]
	TIME [epoch: 1.35 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7459913518198854		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.7459913518198854 | validation: 0.9015213083559758]
	TIME [epoch: 1.35 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7246174694553709		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.7246174694553709 | validation: 0.7873429062412699]
	TIME [epoch: 1.36 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.718276425857523		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.718276425857523 | validation: 0.896627168315183]
	TIME [epoch: 1.36 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7284633517622043		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.7284633517622043 | validation: 0.7940717423103364]
	TIME [epoch: 1.35 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7193878504750782		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.7193878504750782 | validation: 0.9118541162033988]
	TIME [epoch: 1.35 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7182835788851112		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.7182835788851112 | validation: 0.7717549983474523]
	TIME [epoch: 1.35 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7220122274408795		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.7220122274408795 | validation: 0.8905522517693165]
	TIME [epoch: 1.35 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7544370062711019		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.7544370062711019 | validation: 0.8012531781117375]
	TIME [epoch: 1.35 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7197581763585978		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.7197581763585978 | validation: 0.809476657693579]
	TIME [epoch: 1.35 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7001230150359132		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.7001230150359132 | validation: 0.8871550415768668]
	TIME [epoch: 1.35 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7266400201813883		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.7266400201813883 | validation: 0.8459361649918233]
	TIME [epoch: 1.36 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7800246919654987		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.7800246919654987 | validation: 1.0020919783567357]
	TIME [epoch: 1.36 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7890936049979507		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.7890936049979507 | validation: 0.7928987833727097]
	TIME [epoch: 1.35 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8238145324371717		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.8238145324371717 | validation: 0.8686396018732524]
	TIME [epoch: 1.36 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7209539815263853		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.7209539815263853 | validation: 0.8985269214176301]
	TIME [epoch: 1.35 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6971711214501023		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.6971711214501023 | validation: 0.7731706687121372]
	TIME [epoch: 1.35 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6799052055096447		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.6799052055096447 | validation: 0.8301642313970782]
	TIME [epoch: 1.35 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6879368824447328		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.6879368824447328 | validation: 0.8188025314707401]
	TIME [epoch: 1.35 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7298034126459407		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.7298034126459407 | validation: 0.9146012695310985]
	TIME [epoch: 1.35 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7558542844824586		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.7558542844824586 | validation: 0.7960030480590078]
	TIME [epoch: 1.35 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7438893108302915		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.7438893108302915 | validation: 0.9005445093740813]
	TIME [epoch: 1.36 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6873031304192031		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.6873031304192031 | validation: 0.8343092043341166]
	TIME [epoch: 1.35 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6985012369096069		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.6985012369096069 | validation: 0.8061759970679081]
	TIME [epoch: 1.36 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7025967405724776		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.7025967405724776 | validation: 0.792555530599952]
	TIME [epoch: 1.35 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7287122627737844		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.7287122627737844 | validation: 0.8694397599913141]
	TIME [epoch: 1.35 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7259803860672848		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.7259803860672848 | validation: 0.8104508245579851]
	TIME [epoch: 1.36 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7283726744248676		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.7283726744248676 | validation: 0.8888527867693452]
	TIME [epoch: 1.36 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7106191788158885		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.7106191788158885 | validation: 0.7722643899817196]
	TIME [epoch: 1.36 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6825862234070109		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.6825862234070109 | validation: 0.8535719168319134]
	TIME [epoch: 1.36 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6888211027025978		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.6888211027025978 | validation: 0.7483571837424228]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_272.pth
	Model improved!!!
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7249358625035802		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.7249358625035802 | validation: 0.9395589688817012]
	TIME [epoch: 1.36 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7086824355666106		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.7086824355666106 | validation: 0.8537987939267865]
	TIME [epoch: 1.35 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7648437637941907		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.7648437637941907 | validation: 0.864764191049543]
	TIME [epoch: 1.35 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7336448859367508		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.7336448859367508 | validation: 0.7680701045505179]
	TIME [epoch: 1.35 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7092861645985994		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.7092861645985994 | validation: 0.875143600080444]
	TIME [epoch: 1.35 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6864094771466457		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.6864094771466457 | validation: 0.7956667007472067]
	TIME [epoch: 1.36 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6810589071518607		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.6810589071518607 | validation: 0.8634493794774145]
	TIME [epoch: 1.35 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6982883955968895		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.6982883955968895 | validation: 0.7778396838126328]
	TIME [epoch: 1.35 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7289411489901897		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.7289411489901897 | validation: 0.8908733526508457]
	TIME [epoch: 1.35 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7114843611238656		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.7114843611238656 | validation: 0.7787468861652428]
	TIME [epoch: 1.35 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6906449602040644		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.6906449602040644 | validation: 0.8342755779281741]
	TIME [epoch: 1.35 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6899476637660006		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.6899476637660006 | validation: 0.7496485272302471]
	TIME [epoch: 1.36 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7068761914459805		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.7068761914459805 | validation: 0.8729939291071347]
	TIME [epoch: 1.36 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6836177779685706		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.6836177779685706 | validation: 0.7748405112273519]
	TIME [epoch: 1.35 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6699086842980251		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.6699086842980251 | validation: 0.7991948919691415]
	TIME [epoch: 1.35 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6667689969150493		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.6667689969150493 | validation: 0.7678397718072087]
	TIME [epoch: 1.35 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6837459648258787		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.6837459648258787 | validation: 0.8574765736824701]
	TIME [epoch: 1.35 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7151113813528178		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.7151113813528178 | validation: 0.7950277395254094]
	TIME [epoch: 1.35 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7276786396804537		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.7276786396804537 | validation: 0.8869436163874274]
	TIME [epoch: 1.35 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6961848188567638		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.6961848188567638 | validation: 0.7720400586460968]
	TIME [epoch: 1.35 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7024697430743075		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.7024697430743075 | validation: 0.7842219597305914]
	TIME [epoch: 1.35 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.731928215128691		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.731928215128691 | validation: 0.827490023441759]
	TIME [epoch: 1.35 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6624965773588568		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.6624965773588568 | validation: 0.7509230840102923]
	TIME [epoch: 1.35 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6595765981417935		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.6595765981417935 | validation: 0.8571555278411079]
	TIME [epoch: 1.35 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6738587832467217		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.6738587832467217 | validation: 0.7733863989727036]
	TIME [epoch: 1.35 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6877405345052979		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.6877405345052979 | validation: 0.8423925544750759]
	TIME [epoch: 1.35 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7181026209844902		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.7181026209844902 | validation: 0.7757958345784223]
	TIME [epoch: 1.35 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.685200341379968		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.685200341379968 | validation: 0.8458651535185588]
	TIME [epoch: 1.36 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6769638064520374		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.6769638064520374 | validation: 0.7769450562167296]
	TIME [epoch: 1.35 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.708476414605677		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.708476414605677 | validation: 0.8520266504399555]
	TIME [epoch: 1.35 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.710702090765792		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.710702090765792 | validation: 0.7616925934153103]
	TIME [epoch: 1.35 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6796100725491124		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.6796100725491124 | validation: 0.8020693462485791]
	TIME [epoch: 1.36 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6525766872656266		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.6525766872656266 | validation: 0.743233840946759]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6571414748237036		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.6571414748237036 | validation: 0.8381800549160519]
	TIME [epoch: 1.35 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6645719049974759		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.6645719049974759 | validation: 0.753261016551673]
	TIME [epoch: 1.35 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6867754975413028		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.6867754975413028 | validation: 0.8404512885422938]
	TIME [epoch: 1.35 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6869148825132613		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.6869148825132613 | validation: 0.760062537964997]
	TIME [epoch: 1.35 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6738631181089403		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.6738631181089403 | validation: 0.7625934063978614]
	TIME [epoch: 1.35 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6652760697764313		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.6652760697764313 | validation: 0.7707678600592631]
	TIME [epoch: 1.35 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6683843039115428		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.6683843039115428 | validation: 0.7713242110188245]
	TIME [epoch: 1.35 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6499949805249413		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.6499949805249413 | validation: 0.7350766723552173]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_313.pth
	Model improved!!!
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6570301819043388		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.6570301819043388 | validation: 0.8321919256516987]
	TIME [epoch: 1.35 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6569155769140781		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.6569155769140781 | validation: 0.7649602490427198]
	TIME [epoch: 1.35 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6708468044190815		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.6708468044190815 | validation: 0.8151710754165269]
	TIME [epoch: 1.35 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6754240622384304		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.6754240622384304 | validation: 0.7679915589610197]
	TIME [epoch: 1.35 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7203323693775702		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.7203323693775702 | validation: 0.8434922842945419]
	TIME [epoch: 1.35 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6832143029658114		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.6832143029658114 | validation: 0.7425502800121473]
	TIME [epoch: 1.35 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6689296339940225		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.6689296339940225 | validation: 0.7793747848734331]
	TIME [epoch: 1.35 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.653169167284442		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.653169167284442 | validation: 0.7584163077644008]
	TIME [epoch: 1.36 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6411915341964207		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.6411915341964207 | validation: 0.7722154080556235]
	TIME [epoch: 1.35 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6491621969376239		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.6491621969376239 | validation: 0.7339034363692852]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_323.pth
	Model improved!!!
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6501278257211601		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.6501278257211601 | validation: 0.8331215017387589]
	TIME [epoch: 1.36 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6843281256342459		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.6843281256342459 | validation: 0.7419087241217283]
	TIME [epoch: 1.36 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.672889158298626		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.672889158298626 | validation: 0.8073067450307545]
	TIME [epoch: 1.35 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6647885499202579		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.6647885499202579 | validation: 0.7417930292787487]
	TIME [epoch: 1.35 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6649808349506147		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.6649808349506147 | validation: 0.8284684931146068]
	TIME [epoch: 1.35 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6378721729784178		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.6378721729784178 | validation: 0.730442145583219]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6332284319904363		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.6332284319904363 | validation: 0.7524542628246412]
	TIME [epoch: 1.35 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6345889928185232		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.6345889928185232 | validation: 0.765640469990692]
	TIME [epoch: 1.35 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6578681650695435		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.6578681650695435 | validation: 0.7574228659002249]
	TIME [epoch: 1.35 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.646343654343237		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.646343654343237 | validation: 0.7336027195830729]
	TIME [epoch: 1.35 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6344445345750778		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.6344445345750778 | validation: 0.800132798095514]
	TIME [epoch: 1.36 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.649907647653529		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.649907647653529 | validation: 0.738089845526479]
	TIME [epoch: 1.35 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6665175623972773		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.6665175623972773 | validation: 0.8225663082794412]
	TIME [epoch: 1.36 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6714523899516521		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.6714523899516521 | validation: 0.728557533920985]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_337.pth
	Model improved!!!
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6701338954319962		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.6701338954319962 | validation: 0.8121319669482349]
	TIME [epoch: 1.35 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6443683357865073		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.6443683357865073 | validation: 0.7279187792251184]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_339.pth
	Model improved!!!
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6365774472939812		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.6365774472939812 | validation: 0.7598495271411099]
	TIME [epoch: 1.35 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6286231073715165		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.6286231073715165 | validation: 0.7539185915416178]
	TIME [epoch: 1.35 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6294610320762262		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.6294610320762262 | validation: 0.7691894173705863]
	TIME [epoch: 1.35 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.636382046201928		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.636382046201928 | validation: 0.7482758517324597]
	TIME [epoch: 1.35 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6483044016255955		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.6483044016255955 | validation: 0.8477032324390955]
	TIME [epoch: 1.35 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6591721056811516		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.6591721056811516 | validation: 0.7166678414856942]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_345.pth
	Model improved!!!
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6429389754918896		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.6429389754918896 | validation: 0.7953819536695971]
	TIME [epoch: 1.35 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6427347960911024		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.6427347960911024 | validation: 0.7234110983806494]
	TIME [epoch: 1.35 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6262158836961782		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.6262158836961782 | validation: 0.7690728607972197]
	TIME [epoch: 1.35 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6232842823154418		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.6232842823154418 | validation: 0.7184177474622047]
	TIME [epoch: 1.35 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6239585695468925		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.6239585695468925 | validation: 0.7555941692476814]
	TIME [epoch: 1.35 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6375406041238479		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.6375406041238479 | validation: 0.7256703319760789]
	TIME [epoch: 1.35 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.669486732545353		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.669486732545353 | validation: 0.807918045855041]
	TIME [epoch: 1.35 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6576225235722187		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.6576225235722187 | validation: 0.7062320351456173]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_353.pth
	Model improved!!!
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6530494352211375		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.6530494352211375 | validation: 0.7948345638294406]
	TIME [epoch: 1.35 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6226303558026374		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.6226303558026374 | validation: 0.7107530693973662]
	TIME [epoch: 1.36 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6179716939731258		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.6179716939731258 | validation: 0.7222870780021002]
	TIME [epoch: 1.36 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6155946964063648		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.6155946964063648 | validation: 0.72808252751156]
	TIME [epoch: 1.36 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6092758066400797		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.6092758066400797 | validation: 0.7130442812906326]
	TIME [epoch: 1.35 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6053876306106697		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.6053876306106697 | validation: 0.7012808113198976]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_359.pth
	Model improved!!!
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6064401879673134		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.6064401879673134 | validation: 0.7750473559924084]
	TIME [epoch: 1.35 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6404214561557642		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.6404214561557642 | validation: 0.7186903230479155]
	TIME [epoch: 1.36 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6943191321035481		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.6943191321035481 | validation: 0.7862649116909336]
	TIME [epoch: 1.35 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6273341409768278		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.6273341409768278 | validation: 0.7027989083357385]
	TIME [epoch: 1.36 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6060868556611466		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.6060868556611466 | validation: 0.7105625795449021]
	TIME [epoch: 1.35 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6007421321915571		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.6007421321915571 | validation: 0.7123241833857552]
	TIME [epoch: 1.35 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6073973324867987		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.6073973324867987 | validation: 0.7093115843316434]
	TIME [epoch: 1.35 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6077717978460336		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.6077717978460336 | validation: 0.7059680655236442]
	TIME [epoch: 1.35 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6143130932609664		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.6143130932609664 | validation: 0.73993060544132]
	TIME [epoch: 1.4 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.612590724036311		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.612590724036311 | validation: 0.6755124830663215]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_369.pth
	Model improved!!!
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6105843711560198		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.6105843711560198 | validation: 0.8008715138153895]
	TIME [epoch: 1.35 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6249281592453959		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.6249281592453959 | validation: 0.7107478097378288]
	TIME [epoch: 1.35 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6391691162601417		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.6391691162601417 | validation: 0.7584132269873769]
	TIME [epoch: 1.35 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6350257974533888		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.6350257974533888 | validation: 0.6890439258145367]
	TIME [epoch: 1.35 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6127424761615998		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.6127424761615998 | validation: 0.7201103362417557]
	TIME [epoch: 1.35 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5889662890212634		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.5889662890212634 | validation: 0.6624675601199653]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_375.pth
	Model improved!!!
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5832668611117231		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.5832668611117231 | validation: 0.7125034064718878]
	TIME [epoch: 1.36 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5745712474162836		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.5745712474162836 | validation: 0.6736466647242525]
	TIME [epoch: 1.36 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.580854129427607		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.580854129427607 | validation: 0.7144993763991009]
	TIME [epoch: 1.35 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.591845544024581		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.591845544024581 | validation: 0.6962449905808132]
	TIME [epoch: 1.35 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6379245913397353		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.6379245913397353 | validation: 0.7996046228522261]
	TIME [epoch: 1.35 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6522846270879112		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.6522846270879112 | validation: 0.6821675779198356]
	TIME [epoch: 1.36 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6482908219899832		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.6482908219899832 | validation: 0.7556106021426916]
	TIME [epoch: 1.36 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5967952942380593		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.5967952942380593 | validation: 0.6747345111819668]
	TIME [epoch: 1.35 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5708351333567937		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.5708351333567937 | validation: 0.6810900803606691]
	TIME [epoch: 1.35 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5866578046992994		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.5866578046992994 | validation: 0.6862250142752787]
	TIME [epoch: 1.36 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5935890635070569		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.5935890635070569 | validation: 0.6950096146485899]
	TIME [epoch: 1.35 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5901476928248152		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.5901476928248152 | validation: 0.6699914635678945]
	TIME [epoch: 1.35 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5931873013718076		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.5931873013718076 | validation: 0.751713947914538]
	TIME [epoch: 1.35 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6018540290358034		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.6018540290358034 | validation: 0.6584649879647968]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_389.pth
	Model improved!!!
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6323342921880469		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.6323342921880469 | validation: 0.7615669545453886]
	TIME [epoch: 1.35 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5943002224496005		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.5943002224496005 | validation: 0.664952198223181]
	TIME [epoch: 1.35 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.569819282135649		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.569819282135649 | validation: 0.6779545604733701]
	TIME [epoch: 1.34 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5626074025843195		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.5626074025843195 | validation: 0.6586698115653793]
	TIME [epoch: 1.35 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5585317460632477		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.5585317460632477 | validation: 0.6690948023314736]
	TIME [epoch: 1.35 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5698799393663335		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.5698799393663335 | validation: 0.6617914213506673]
	TIME [epoch: 1.35 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5966417395004492		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.5966417395004492 | validation: 0.7155196595164661]
	TIME [epoch: 1.35 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6102734534924298		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.6102734534924298 | validation: 0.7424402562732985]
	TIME [epoch: 1.35 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6297823284883064		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.6297823284883064 | validation: 0.6380177626994159]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_398.pth
	Model improved!!!
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5725764425263324		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.5725764425263324 | validation: 0.8141126986069417]
	TIME [epoch: 1.36 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6081226967770758		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.6081226967770758 | validation: 0.6244831227703864]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_400.pth
	Model improved!!!
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6040133387873544		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.6040133387873544 | validation: 0.7046551420726329]
	TIME [epoch: 1.36 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5771748478704082		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.5771748478704082 | validation: 0.6653729936021353]
	TIME [epoch: 1.36 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5855170707941119		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.5855170707941119 | validation: 0.6959985080706217]
	TIME [epoch: 1.36 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.559260516097924		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.559260516097924 | validation: 0.6359417800877591]
	TIME [epoch: 1.36 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5556669433530655		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.5556669433530655 | validation: 0.6787743953768834]
	TIME [epoch: 1.35 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5478885496629192		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.5478885496629192 | validation: 0.629379104624774]
	TIME [epoch: 1.35 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5390455522840761		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.5390455522840761 | validation: 0.6427665760842818]
	TIME [epoch: 1.35 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5504228367621518		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.5504228367621518 | validation: 0.6450843319824183]
	TIME [epoch: 1.35 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5797593275982229		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.5797593275982229 | validation: 0.7236003616120196]
	TIME [epoch: 1.35 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5935760198422882		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.5935760198422882 | validation: 0.6103386044511694]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_410.pth
	Model improved!!!
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6161036985532602		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.6161036985532602 | validation: 0.8139938426763588]
	TIME [epoch: 1.36 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5953073841708652		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.5953073841708652 | validation: 0.5938196759069057]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_412.pth
	Model improved!!!
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5417933291525595		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.5417933291525595 | validation: 0.6272691883283095]
	TIME [epoch: 1.35 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.524494415361307		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.524494415361307 | validation: 0.6597120355981181]
	TIME [epoch: 1.35 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5232518262684671		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.5232518262684671 | validation: 0.6111890743378883]
	TIME [epoch: 1.35 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5193014570649256		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.5193014570649256 | validation: 0.6604402518595268]
	TIME [epoch: 1.35 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5460901276503035		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.5460901276503035 | validation: 0.6210505985147159]
	TIME [epoch: 1.35 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5874052866290611		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.5874052866290611 | validation: 0.734165423998447]
	TIME [epoch: 1.35 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5917206255358152		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.5917206255358152 | validation: 0.5859995143387248]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_419.pth
	Model improved!!!
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5763299699442535		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.5763299699442535 | validation: 0.7399804275692399]
	TIME [epoch: 1.35 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5643860665482292		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.5643860665482292 | validation: 0.568320981016336]
	TIME [epoch: 1.96 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_421.pth
	Model improved!!!
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5512640930934525		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.5512640930934525 | validation: 0.6693067027894302]
	TIME [epoch: 1.36 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5258958709316434		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.5258958709316434 | validation: 0.5989321881329839]
	TIME [epoch: 1.36 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5182977587313287		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.5182977587313287 | validation: 0.6354070332025505]
	TIME [epoch: 1.36 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5140030953607351		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.5140030953607351 | validation: 0.6154233543472476]
	TIME [epoch: 1.36 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5293048961593858		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.5293048961593858 | validation: 0.6539437849221019]
	TIME [epoch: 1.35 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5468989104638041		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.5468989104638041 | validation: 0.6505410652779924]
	TIME [epoch: 1.36 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5759264883219839		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.5759264883219839 | validation: 0.6212896535365348]
	TIME [epoch: 1.36 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5516062473714329		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.5516062473714329 | validation: 0.6134666546035743]
	TIME [epoch: 1.36 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5246365493604076		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.5246365493604076 | validation: 0.5886213003080611]
	TIME [epoch: 1.36 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5066397638065352		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.5066397638065352 | validation: 0.5967515000379902]
	TIME [epoch: 1.36 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.510381472129952		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.510381472129952 | validation: 0.7174157250712486]
	TIME [epoch: 1.36 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5453239962531669		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.5453239962531669 | validation: 0.567480870722722]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_433.pth
	Model improved!!!
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.595790908736307		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.595790908736307 | validation: 0.7863217296099492]
	TIME [epoch: 1.35 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5873129849737606		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.5873129849737606 | validation: 0.6028079744881328]
	TIME [epoch: 1.36 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5713720920868132		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.5713720920868132 | validation: 0.6497996458804062]
	TIME [epoch: 1.36 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5191240420980031		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.5191240420980031 | validation: 0.5937498386788372]
	TIME [epoch: 1.36 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4959785680127083		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.4959785680127083 | validation: 0.5841047844764481]
	TIME [epoch: 1.36 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4977281699323531		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.4977281699323531 | validation: 0.6092936310933095]
	TIME [epoch: 1.36 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.510247719994854		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.510247719994854 | validation: 0.5799734919430776]
	TIME [epoch: 1.36 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5125135772082292		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.5125135772082292 | validation: 0.5718729330418211]
	TIME [epoch: 1.36 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.507774013595298		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.507774013595298 | validation: 0.6199305484308704]
	TIME [epoch: 1.36 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5084810219259768		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.5084810219259768 | validation: 0.5609868647843479]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_443.pth
	Model improved!!!
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5371217808085855		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.5371217808085855 | validation: 0.7810382333374465]
	TIME [epoch: 1.36 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5771612542621765		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.5771612542621765 | validation: 0.5497585991122732]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_445.pth
	Model improved!!!
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.557455807014018		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.557455807014018 | validation: 0.6687194310087992]
	TIME [epoch: 1.36 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5237203294663866		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.5237203294663866 | validation: 0.6015178832166874]
	TIME [epoch: 1.36 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5010622070985056		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.5010622070985056 | validation: 0.5571414591906677]
	TIME [epoch: 1.36 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4993142825157036		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.4993142825157036 | validation: 0.6390862329241079]
	TIME [epoch: 1.36 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5060529060686705		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.5060529060686705 | validation: 0.5534551451318104]
	TIME [epoch: 1.36 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5008074674969201		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.5008074674969201 | validation: 0.602892402313141]
	TIME [epoch: 1.36 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49759318005845066		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.49759318005845066 | validation: 0.5648882693100871]
	TIME [epoch: 1.36 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49151681537057007		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.49151681537057007 | validation: 0.598376335512442]
	TIME [epoch: 1.36 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5019917151922116		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.5019917151922116 | validation: 0.6363593831806692]
	TIME [epoch: 1.35 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5369219841548882		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.5369219841548882 | validation: 0.6085852963239766]
	TIME [epoch: 1.36 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6270503460324048		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.6270503460324048 | validation: 0.7212534999743627]
	TIME [epoch: 1.36 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5183870840750651		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.5183870840750651 | validation: 0.6542356534038296]
	TIME [epoch: 1.36 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5114678959773452		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.5114678959773452 | validation: 0.5885367404984018]
	TIME [epoch: 1.35 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48866290450048566		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.48866290450048566 | validation: 0.5613295290407482]
	TIME [epoch: 1.35 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47736270112601065		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.47736270112601065 | validation: 0.594260823075483]
	TIME [epoch: 1.35 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.486885188326728		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.486885188326728 | validation: 0.5959555680514999]
	TIME [epoch: 1.36 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5065270316216797		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.5065270316216797 | validation: 0.5829845836678461]
	TIME [epoch: 1.36 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5208968056271711		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.5208968056271711 | validation: 0.5641193982688562]
	TIME [epoch: 1.36 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5172018740433405		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.5172018740433405 | validation: 0.6669833040948516]
	TIME [epoch: 1.35 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5110590916176574		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.5110590916176574 | validation: 0.5302370958997836]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_465.pth
	Model improved!!!
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5260623195866962		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.5260623195866962 | validation: 0.700018147222682]
	TIME [epoch: 1.36 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5099887133815931		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.5099887133815931 | validation: 0.5626553617013473]
	TIME [epoch: 1.35 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4803827678778061		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.4803827678778061 | validation: 0.5481937637484451]
	TIME [epoch: 1.35 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4763425388692005		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.4763425388692005 | validation: 0.5470727932708855]
	TIME [epoch: 1.36 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4708311767283436		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.4708311767283436 | validation: 0.5656281077214924]
	TIME [epoch: 1.35 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4790421134958708		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.4790421134958708 | validation: 0.5909386896232204]
	TIME [epoch: 1.36 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5013765495545861		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.5013765495545861 | validation: 0.527867702500334]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_472.pth
	Model improved!!!
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4824573656626135		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.4824573656626135 | validation: 0.5311748701815048]
	TIME [epoch: 1.36 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.471104582314733		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.471104582314733 | validation: 0.6634772898499224]
	TIME [epoch: 1.36 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4914135405427456		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.4914135405427456 | validation: 0.5321240260552821]
	TIME [epoch: 1.36 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5921432288282881		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.5921432288282881 | validation: 0.7744668074416388]
	TIME [epoch: 1.35 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5726294436437916		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.5726294436437916 | validation: 0.6479401606480808]
	TIME [epoch: 1.35 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5082280834255941		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.5082280834255941 | validation: 0.48802921965518886]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_478.pth
	Model improved!!!
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.518010776961931		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.518010776961931 | validation: 0.6165243385033065]
	TIME [epoch: 1.36 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48219129860265897		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.48219129860265897 | validation: 0.5833879513574098]
	TIME [epoch: 1.36 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4603322084591094		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.4603322084591094 | validation: 0.5633408874373668]
	TIME [epoch: 1.36 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47204476214490515		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.47204476214490515 | validation: 0.5815694460366022]
	TIME [epoch: 1.35 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4848394207344458		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.4848394207344458 | validation: 0.6774079311763981]
	TIME [epoch: 1.35 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5556062145444095		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.5556062145444095 | validation: 0.5915066535843135]
	TIME [epoch: 1.35 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5574961917302406		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.5574961917302406 | validation: 0.5953709393922807]
	TIME [epoch: 1.36 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4722140634893354		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.4722140634893354 | validation: 0.5753481891435509]
	TIME [epoch: 1.36 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4667918299529717		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.4667918299529717 | validation: 0.5700411757369012]
	TIME [epoch: 1.36 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4720669551374256		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.4720669551374256 | validation: 0.5648064117848169]
	TIME [epoch: 1.35 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45819209277934386		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.45819209277934386 | validation: 0.5677892321150285]
	TIME [epoch: 1.35 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.446910853136438		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.446910853136438 | validation: 0.5397870608436095]
	TIME [epoch: 1.36 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44263818319938		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.44263818319938 | validation: 0.5541330537716016]
	TIME [epoch: 1.36 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4484262790913854		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.4484262790913854 | validation: 0.5372638497310862]
	TIME [epoch: 1.35 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4495403396564299		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.4495403396564299 | validation: 0.5384036958945239]
	TIME [epoch: 1.35 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4768724384227499		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.4768724384227499 | validation: 0.6570028188919268]
	TIME [epoch: 1.36 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5265325152405848		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.5265325152405848 | validation: 0.5905476878657698]
	TIME [epoch: 1.35 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5952340071984908		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.5952340071984908 | validation: 0.7028250658321373]
	TIME [epoch: 1.35 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5132260674540775		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.5132260674540775 | validation: 0.5542035637135999]
	TIME [epoch: 1.35 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4408233704289646		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.4408233704289646 | validation: 0.4861686802892278]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_498.pth
	Model improved!!!
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4428337525719357		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.4428337525719357 | validation: 0.574084626549296]
	TIME [epoch: 1.36 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.443973561564191		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.443973561564191 | validation: 0.5537146029725113]
	TIME [epoch: 1.36 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4565013153201366		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.4565013153201366 | validation: 0.6355170733703541]
	TIME [epoch: 169 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47030227321205786		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.47030227321205786 | validation: 0.5730508370877168]
	TIME [epoch: 2.68 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4775191972108235		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.4775191972108235 | validation: 0.5886201965388013]
	TIME [epoch: 2.67 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46316920455565785		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.46316920455565785 | validation: 0.580876463179289]
	TIME [epoch: 2.67 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4542046929347879		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.4542046929347879 | validation: 0.5540300519282171]
	TIME [epoch: 2.67 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4452027129142924		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.4452027129142924 | validation: 0.5458509246013522]
	TIME [epoch: 2.67 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44756957372677636		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.44756957372677636 | validation: 0.5428778602600751]
	TIME [epoch: 2.67 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4629073033778235		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.4629073033778235 | validation: 0.5934107539142499]
	TIME [epoch: 2.67 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5140212087122698		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.5140212087122698 | validation: 0.7426847523312283]
	TIME [epoch: 2.67 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5802671299534332		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.5802671299534332 | validation: 0.5306158804910214]
	TIME [epoch: 2.67 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5368966011182306		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.5368966011182306 | validation: 0.6239570901021328]
	TIME [epoch: 2.66 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45695308523318745		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.45695308523318745 | validation: 0.5854183510339966]
	TIME [epoch: 2.66 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45662038084205864		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.45662038084205864 | validation: 0.5774361279623055]
	TIME [epoch: 2.67 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44600060637872874		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.44600060637872874 | validation: 0.5631994007942757]
	TIME [epoch: 2.67 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4410714212853277		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.4410714212853277 | validation: 0.520738389327169]
	TIME [epoch: 2.67 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4454186835973286		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.4454186835973286 | validation: 0.5953353240596672]
	TIME [epoch: 2.67 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4525963352576442		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.4525963352576442 | validation: 0.5392847475977897]
	TIME [epoch: 2.67 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4837832196509247		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.4837832196509247 | validation: 0.609154119703118]
	TIME [epoch: 2.67 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.474652342127019		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.474652342127019 | validation: 0.5197857697333715]
	TIME [epoch: 2.67 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4593287851276797		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.4593287851276797 | validation: 0.6030267583689082]
	TIME [epoch: 2.68 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44746727304257106		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.44746727304257106 | validation: 0.522607063190041]
	TIME [epoch: 2.67 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4405464576075343		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.4405464576075343 | validation: 0.6170761980478618]
	TIME [epoch: 2.67 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4614533040053405		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.4614533040053405 | validation: 0.5579799729141323]
	TIME [epoch: 2.67 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4809470118111122		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.4809470118111122 | validation: 0.572318583544782]
	TIME [epoch: 2.67 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47308421227720737		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.47308421227720737 | validation: 0.5631321079586682]
	TIME [epoch: 2.67 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46324830907165165		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.46324830907165165 | validation: 0.5433576616152987]
	TIME [epoch: 2.67 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4513221935936978		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.4513221935936978 | validation: 0.5839793928888204]
	TIME [epoch: 2.67 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44020623173984275		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.44020623173984275 | validation: 0.5153549035746109]
	TIME [epoch: 2.67 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4277608925826718		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.4277608925826718 | validation: 0.548309472322709]
	TIME [epoch: 2.67 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.428089440873207		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.428089440873207 | validation: 0.49532335507300007]
	TIME [epoch: 2.66 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4227254164080408		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.4227254164080408 | validation: 0.5963568229308466]
	TIME [epoch: 2.67 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44039741136777166		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.44039741136777166 | validation: 0.5347696201843559]
	TIME [epoch: 2.67 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47472270667508737		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.47472270667508737 | validation: 0.6650549902920588]
	TIME [epoch: 2.67 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4967540628586835		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.4967540628586835 | validation: 0.5595914820631425]
	TIME [epoch: 2.67 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49488392590166924		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.49488392590166924 | validation: 0.5634462478882434]
	TIME [epoch: 2.67 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4444701120562525		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.4444701120562525 | validation: 0.5613004355323449]
	TIME [epoch: 2.67 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4290307693716068		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.4290307693716068 | validation: 0.5240048494840864]
	TIME [epoch: 2.67 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43148073315302526		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.43148073315302526 | validation: 0.5847472100003291]
	TIME [epoch: 2.67 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4321871539580296		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.4321871539580296 | validation: 0.5311339933653838]
	TIME [epoch: 2.67 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43271292105733483		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.43271292105733483 | validation: 0.5456731627505491]
	TIME [epoch: 2.66 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.422030829281761		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.422030829281761 | validation: 0.5464653738031696]
	TIME [epoch: 2.67 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4146183169649807		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.4146183169649807 | validation: 0.48729597694849364]
	TIME [epoch: 2.67 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42502827952222716		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.42502827952222716 | validation: 0.638875793769473]
	TIME [epoch: 2.67 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4595154093398358		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.4595154093398358 | validation: 0.583987400766984]
	TIME [epoch: 2.67 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5253655949646482		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.5253655949646482 | validation: 0.6252564125964967]
	TIME [epoch: 2.66 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48230335218951687		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.48230335218951687 | validation: 0.5208907381848984]
	TIME [epoch: 2.67 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4366983005489555		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.4366983005489555 | validation: 0.5214149380662425]
	TIME [epoch: 2.67 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4168259539993543		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.4168259539993543 | validation: 0.5217988090223454]
	TIME [epoch: 2.66 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4167100635218585		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.4167100635218585 | validation: 0.5183771252598257]
	TIME [epoch: 2.67 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4238782251349328		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.4238782251349328 | validation: 0.5782957420300815]
	TIME [epoch: 2.67 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42752036755128275		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.42752036755128275 | validation: 0.5387467981982956]
	TIME [epoch: 2.67 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43518347145309266		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.43518347145309266 | validation: 0.5334809140044133]
	TIME [epoch: 2.67 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42215827972436176		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.42215827972436176 | validation: 0.5416569750815764]
	TIME [epoch: 2.67 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4198346184779272		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.4198346184779272 | validation: 0.4987498001694761]
	TIME [epoch: 2.67 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4308298230133048		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.4308298230133048 | validation: 0.6312669354732234]
	TIME [epoch: 2.67 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4608767314254486		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.4608767314254486 | validation: 0.5418614089801878]
	TIME [epoch: 2.67 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5076640870264538		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.5076640870264538 | validation: 0.6017736382982614]
	TIME [epoch: 2.67 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45039625515960635		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.45039625515960635 | validation: 0.5203346847927092]
	TIME [epoch: 2.67 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4116784692060618		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.4116784692060618 | validation: 0.4941440913458681]
	TIME [epoch: 2.67 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40377506774135663		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.40377506774135663 | validation: 0.5344517594384048]
	TIME [epoch: 2.67 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4071149658897673		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.4071149658897673 | validation: 0.49901274313923133]
	TIME [epoch: 2.67 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4135838614964468		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.4135838614964468 | validation: 0.5463480583953723]
	TIME [epoch: 2.67 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41906203257032604		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.41906203257032604 | validation: 0.554779236852733]
	TIME [epoch: 2.67 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42996726392300133		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.42996726392300133 | validation: 0.587473659607375]
	TIME [epoch: 2.67 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48264925221172666		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.48264925221172666 | validation: 0.6359995951380033]
	TIME [epoch: 2.67 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4748106883384057		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.4748106883384057 | validation: 0.4809731748058864]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_566.pth
	Model improved!!!
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43067253600286687		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.43067253600286687 | validation: 0.5275067525523929]
	TIME [epoch: 2.67 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4084788450079834		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.4084788450079834 | validation: 0.5061944974633849]
	TIME [epoch: 2.67 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4065450674473756		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.4065450674473756 | validation: 0.5402796133622244]
	TIME [epoch: 2.67 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40910372375492193		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.40910372375492193 | validation: 0.512382394979452]
	TIME [epoch: 2.67 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40218413550811277		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.40218413550811277 | validation: 0.523255365128834]
	TIME [epoch: 2.67 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3950459771817931		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.3950459771817931 | validation: 0.5110295881662513]
	TIME [epoch: 2.67 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4200541684747256		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.4200541684747256 | validation: 0.537486668504802]
	TIME [epoch: 2.66 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47706805618982445		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.47706805618982445 | validation: 0.6204658242995705]
	TIME [epoch: 2.67 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4431767167691656		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.4431767167691656 | validation: 0.5330664340084775]
	TIME [epoch: 2.67 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45094410544103397		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.45094410544103397 | validation: 0.5195983281064502]
	TIME [epoch: 2.67 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4085887562780993		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.4085887562780993 | validation: 0.4644615398210203]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_577.pth
	Model improved!!!
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3965784944789037		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.3965784944789037 | validation: 0.5319675471951815]
	TIME [epoch: 2.67 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3991300382055054		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.3991300382055054 | validation: 0.4890109687610192]
	TIME [epoch: 2.67 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4006177976739817		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.4006177976739817 | validation: 0.5411791686988462]
	TIME [epoch: 2.67 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.407729894697389		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.407729894697389 | validation: 0.5083158819888907]
	TIME [epoch: 2.67 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42816530962934296		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.42816530962934296 | validation: 0.6176861521783281]
	TIME [epoch: 2.67 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47874452481235225		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.47874452481235225 | validation: 0.5811103486269239]
	TIME [epoch: 2.67 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45796884335345606		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.45796884335345606 | validation: 0.5120682580814132]
	TIME [epoch: 2.67 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40412680574424586		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.40412680574424586 | validation: 0.5394294683897409]
	TIME [epoch: 2.67 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38930794515354394		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.38930794515354394 | validation: 0.4823879842711807]
	TIME [epoch: 2.67 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3901564657665256		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.3901564657665256 | validation: 0.5122242248892034]
	TIME [epoch: 2.67 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3927386051939871		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.3927386051939871 | validation: 0.4682049436121629]
	TIME [epoch: 2.67 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39521046169577423		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.39521046169577423 | validation: 0.5302647260468432]
	TIME [epoch: 2.66 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3983004986631737		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.3983004986631737 | validation: 0.4867057205874372]
	TIME [epoch: 2.67 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4055307879283953		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.4055307879283953 | validation: 0.5494633933477943]
	TIME [epoch: 2.67 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4055136244440882		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.4055136244440882 | validation: 0.5026046853585605]
	TIME [epoch: 2.67 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.418121113301445		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.418121113301445 | validation: 0.6066668549458355]
	TIME [epoch: 2.67 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4467465813088948		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.4467465813088948 | validation: 0.6168497183256392]
	TIME [epoch: 2.67 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48210793256551837		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.48210793256551837 | validation: 0.540111260522192]
	TIME [epoch: 2.67 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41599243839521116		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.41599243839521116 | validation: 0.5032338918554633]
	TIME [epoch: 2.67 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3910067874587202		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.3910067874587202 | validation: 0.4791231714460601]
	TIME [epoch: 2.67 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3795778359577136		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.3795778359577136 | validation: 0.49385379413668745]
	TIME [epoch: 2.67 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37172283168928416		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.37172283168928416 | validation: 0.4909228877800003]
	TIME [epoch: 2.67 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3770595725562649		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.3770595725562649 | validation: 0.47327962139382546]
	TIME [epoch: 2.67 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37624055750789853		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.37624055750789853 | validation: 0.5081878003760107]
	TIME [epoch: 2.67 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3842640948042737		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.3842640948042737 | validation: 0.4904133598708189]
	TIME [epoch: 2.67 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41891675622660535		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.41891675622660535 | validation: 0.615771329577409]
	TIME [epoch: 2.67 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4623661574751617		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.4623661574751617 | validation: 0.5344930348023724]
	TIME [epoch: 2.67 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43269990049536866		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.43269990049536866 | validation: 0.48737721401934897]
	TIME [epoch: 2.67 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41193266573923676		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.41193266573923676 | validation: 0.5179217847791037]
	TIME [epoch: 2.66 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3898085825778064		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.3898085825778064 | validation: 0.4591153054883333]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_607.pth
	Model improved!!!
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3729811200167141		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.3729811200167141 | validation: 0.4907661698216119]
	TIME [epoch: 2.67 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3741122621848936		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.3741122621848936 | validation: 0.47831210574280214]
	TIME [epoch: 2.67 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38148819675772244		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.38148819675772244 | validation: 0.48777757176184244]
	TIME [epoch: 2.67 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38241469894496505		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.38241469894496505 | validation: 0.4635213514527548]
	TIME [epoch: 2.66 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.392185537993672		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.392185537993672 | validation: 0.5490895836742538]
	TIME [epoch: 2.67 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3987339391124263		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.3987339391124263 | validation: 0.47335120855938695]
	TIME [epoch: 2.67 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3968619301931804		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.3968619301931804 | validation: 0.5509335287121562]
	TIME [epoch: 2.67 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41635540145023014		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.41635540145023014 | validation: 0.5853965102030962]
	TIME [epoch: 2.67 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47138837824767765		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.47138837824767765 | validation: 0.5340766426441775]
	TIME [epoch: 2.67 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41410784030379644		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.41410784030379644 | validation: 0.4900522459664274]
	TIME [epoch: 2.67 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38138954901971334		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.38138954901971334 | validation: 0.45387809446967]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_618.pth
	Model improved!!!
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3652764540407547		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.3652764540407547 | validation: 0.4454486195197859]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_619.pth
	Model improved!!!
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36333987395798983		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.36333987395798983 | validation: 0.46276518148981227]
	TIME [epoch: 2.67 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36505201064249376		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.36505201064249376 | validation: 0.4830193142755267]
	TIME [epoch: 2.67 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36808967915314966		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.36808967915314966 | validation: 0.483789130508983]
	TIME [epoch: 2.67 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40734177677611216		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.40734177677611216 | validation: 0.6003887628386562]
	TIME [epoch: 2.67 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43762304739836877		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.43762304739836877 | validation: 0.4994966825124763]
	TIME [epoch: 2.67 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4110429127301162		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.4110429127301162 | validation: 0.4662136836711584]
	TIME [epoch: 2.66 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3888524381194566		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.3888524381194566 | validation: 0.5010600125500341]
	TIME [epoch: 2.67 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38263438579711034		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.38263438579711034 | validation: 0.48178748128136206]
	TIME [epoch: 2.67 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3686120543089507		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.3686120543089507 | validation: 0.4788177096827491]
	TIME [epoch: 2.67 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36266788088898066		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.36266788088898066 | validation: 0.4707681672894314]
	TIME [epoch: 2.67 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.362421481053182		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.362421481053182 | validation: 0.4681225042053974]
	TIME [epoch: 2.67 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37924273922778684		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.37924273922778684 | validation: 0.5394454380902042]
	TIME [epoch: 2.67 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40916986839869085		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.40916986839869085 | validation: 0.5275480728538063]
	TIME [epoch: 2.67 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4287551094032931		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.4287551094032931 | validation: 0.5188184164110169]
	TIME [epoch: 2.68 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39740626659785355		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.39740626659785355 | validation: 0.46869850822624226]
	TIME [epoch: 2.67 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37136799812852156		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.37136799812852156 | validation: 0.44268652398653147]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_635.pth
	Model improved!!!
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3625031238811957		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.3625031238811957 | validation: 0.4406886522173137]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_636.pth
	Model improved!!!
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3590818935434315		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.3590818935434315 | validation: 0.5062782856876259]
	TIME [epoch: 2.67 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3737543390851563		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.3737543390851563 | validation: 0.46259281703092425]
	TIME [epoch: 2.67 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40437805857324904		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.40437805857324904 | validation: 0.5271418155996316]
	TIME [epoch: 2.67 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38374199035849144		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.38374199035849144 | validation: 0.4529661790358624]
	TIME [epoch: 2.67 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36576995993666517		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.36576995993666517 | validation: 0.4557496676977256]
	TIME [epoch: 2.67 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36710171232000705		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.36710171232000705 | validation: 0.46225987008693475]
	TIME [epoch: 2.67 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37078185491504173		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.37078185491504173 | validation: 0.5222183493865881]
	TIME [epoch: 2.67 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3863759895606219		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.3863759895606219 | validation: 0.5266986122940457]
	TIME [epoch: 2.67 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4088149088177589		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.4088149088177589 | validation: 0.5068604282146619]
	TIME [epoch: 2.67 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40825229259508056		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.40825229259508056 | validation: 0.47988963128968654]
	TIME [epoch: 2.67 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3725213679722312		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.3725213679722312 | validation: 0.46280265218561817]
	TIME [epoch: 2.66 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.351065209296242		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.351065209296242 | validation: 0.4571097116342516]
	TIME [epoch: 2.67 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.355201819456769		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.355201819456769 | validation: 0.46169144674645235]
	TIME [epoch: 2.67 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3511022796259687		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.3511022796259687 | validation: 0.43454182379518064]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_650.pth
	Model improved!!!
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35510897294557503		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.35510897294557503 | validation: 0.49002093559146254]
	TIME [epoch: 2.67 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35282102435198087		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.35282102435198087 | validation: 0.44431598538729955]
	TIME [epoch: 2.68 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36120482187479075		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.36120482187479075 | validation: 0.4831728617633293]
	TIME [epoch: 2.68 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3619041029036432		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.3619041029036432 | validation: 0.4543416663634475]
	TIME [epoch: 2.68 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3907130514756139		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.3907130514756139 | validation: 0.5833340481057626]
	TIME [epoch: 2.68 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43360267827646765		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.43360267827646765 | validation: 0.5350209474241229]
	TIME [epoch: 2.68 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3966346202852558		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.3966346202852558 | validation: 0.4509292015131514]
	TIME [epoch: 2.68 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36034391936629134		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.36034391936629134 | validation: 0.4526072248072377]
	TIME [epoch: 2.68 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3518516496491433		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.3518516496491433 | validation: 0.429978745330688]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_659.pth
	Model improved!!!
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3487526672434801		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.3487526672434801 | validation: 0.4714417937529784]
	TIME [epoch: 2.67 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3496929174429592		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.3496929174429592 | validation: 0.4670819700035983]
	TIME [epoch: 2.67 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3519032926786572		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.3519032926786572 | validation: 0.4607905294848865]
	TIME [epoch: 2.66 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36106551653526575		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.36106551653526575 | validation: 0.4962039808919741]
	TIME [epoch: 2.68 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3851485445853596		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.3851485445853596 | validation: 0.5158724599616828]
	TIME [epoch: 2.67 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39764979229775066		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.39764979229775066 | validation: 0.47151489463735596]
	TIME [epoch: 2.67 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3711021307585011		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.3711021307585011 | validation: 0.4506054781285393]
	TIME [epoch: 2.67 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3505608711417449		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.3505608711417449 | validation: 0.4205966429244319]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_667.pth
	Model improved!!!
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3466061858105949		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.3466061858105949 | validation: 0.44845028176071833]
	TIME [epoch: 2.67 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3406892450248825		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.3406892450248825 | validation: 0.4205645132895785]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_669.pth
	Model improved!!!
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34121120967953206		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.34121120967953206 | validation: 0.4659806468573888]
	TIME [epoch: 2.67 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3438025592079987		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.3438025592079987 | validation: 0.4308892948431005]
	TIME [epoch: 2.67 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3445320697972046		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.3445320697972046 | validation: 0.47667073804288757]
	TIME [epoch: 2.67 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34515809059467734		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.34515809059467734 | validation: 0.4254170729314815]
	TIME [epoch: 2.68 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35150638762919945		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.35150638762919945 | validation: 0.4991613684401374]
	TIME [epoch: 2.67 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38455768066611884		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.38455768066611884 | validation: 0.514796757972358]
	TIME [epoch: 2.67 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4255288653453068		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.4255288653453068 | validation: 0.502782103617142]
	TIME [epoch: 2.67 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3904511122051952		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.3904511122051952 | validation: 0.48595445623662803]
	TIME [epoch: 2.67 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3491783478546222		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.3491783478546222 | validation: 0.42427839349801444]
	TIME [epoch: 2.67 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3411616262163691		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.3411616262163691 | validation: 0.43893845220473793]
	TIME [epoch: 2.67 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3337395654561318		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.3337395654561318 | validation: 0.43043283674466565]
	TIME [epoch: 2.67 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34051941206530745		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.34051941206530745 | validation: 0.44906788010799165]
	TIME [epoch: 2.67 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3419392870713995		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.3419392870713995 | validation: 0.4409242553577465]
	TIME [epoch: 2.67 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3512362881725048		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.3512362881725048 | validation: 0.4693819253907176]
	TIME [epoch: 2.67 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3489061058636213		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.3489061058636213 | validation: 0.4616916560196077]
	TIME [epoch: 2.67 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35611186008683576		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.35611186008683576 | validation: 0.46649691123468373]
	TIME [epoch: 2.67 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3710688967707275		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.3710688967707275 | validation: 0.44999692434015803]
	TIME [epoch: 2.66 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35567931008168047		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.35567931008168047 | validation: 0.46831446010884037]
	TIME [epoch: 2.67 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.347729101898862		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.347729101898862 | validation: 0.44556316079021685]
	TIME [epoch: 2.67 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3326596647594542		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.3326596647594542 | validation: 0.42556522119292506]
	TIME [epoch: 2.67 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33119440396272987		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.33119440396272987 | validation: 0.4216749962020062]
	TIME [epoch: 2.67 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3420058251950451		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.3420058251950451 | validation: 0.460954327030732]
	TIME [epoch: 2.67 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3520118881424188		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.3520118881424188 | validation: 0.44292926228483076]
	TIME [epoch: 2.67 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3609426323869008		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.3609426323869008 | validation: 0.45751026160843256]
	TIME [epoch: 2.67 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34750276147936965		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.34750276147936965 | validation: 0.396209956327029]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_694.pth
	Model improved!!!
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3943594392195417		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.3943594392195417 | validation: 0.44453370140635107]
	TIME [epoch: 2.67 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33847545566093423		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.33847545566093423 | validation: 0.42161639565613407]
	TIME [epoch: 2.67 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.333811201096696		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.333811201096696 | validation: 0.40553848863833886]
	TIME [epoch: 2.67 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33262851732685017		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.33262851732685017 | validation: 0.4507618678942256]
	TIME [epoch: 2.67 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32737555153569003		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.32737555153569003 | validation: 0.4104123156079574]
	TIME [epoch: 2.67 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3238290786066037		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.3238290786066037 | validation: 0.43912522855617386]
	TIME [epoch: 2.67 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3306719883828332		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.3306719883828332 | validation: 0.40650337819256266]
	TIME [epoch: 2.67 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3341854810850919		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.3341854810850919 | validation: 0.4609455236045877]
	TIME [epoch: 2.67 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3320961091403281		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.3320961091403281 | validation: 0.44502922373792364]
	TIME [epoch: 2.67 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33382634942804834		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.33382634942804834 | validation: 0.4331996990299694]
	TIME [epoch: 2.67 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36261321040305405		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.36261321040305405 | validation: 0.5070593413313763]
	TIME [epoch: 2.67 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39231906605280986		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.39231906605280986 | validation: 0.43855825865083453]
	TIME [epoch: 2.67 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3545203855309885		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.3545203855309885 | validation: 0.4095918499370707]
	TIME [epoch: 2.67 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32286908384761537		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.32286908384761537 | validation: 0.41433929763339244]
	TIME [epoch: 2.67 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31965923322091483		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.31965923322091483 | validation: 0.412423421422047]
	TIME [epoch: 2.67 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32087117287636324		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.32087117287636324 | validation: 0.4185706278425412]
	TIME [epoch: 2.67 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3195965644060283		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.3195965644060283 | validation: 0.4076004814455706]
	TIME [epoch: 2.67 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32004035451414176		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.32004035451414176 | validation: 0.43108471178014585]
	TIME [epoch: 2.67 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33300801938240443		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.33300801938240443 | validation: 0.45389078216286827]
	TIME [epoch: 2.67 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.358522589007995		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.358522589007995 | validation: 0.45510368508633636]
	TIME [epoch: 2.67 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3571309883133837		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.3571309883133837 | validation: 0.4671500709212912]
	TIME [epoch: 2.67 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3471917924625308		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.3471917924625308 | validation: 0.42732070999969285]
	TIME [epoch: 2.67 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3330093220892028		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.3330093220892028 | validation: 0.421591119292456]
	TIME [epoch: 2.67 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3178750796145469		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.3178750796145469 | validation: 0.3971825636622585]
	TIME [epoch: 2.67 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31620739806602227		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.31620739806602227 | validation: 0.42230146728786205]
	TIME [epoch: 2.66 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3189913575315304		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.3189913575315304 | validation: 0.4088081684712692]
	TIME [epoch: 2.67 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33272805680148404		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.33272805680148404 | validation: 0.43650547490553077]
	TIME [epoch: 2.67 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3383764081378165		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.3383764081378165 | validation: 0.4040386264944525]
	TIME [epoch: 2.67 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3475792297696822		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.3475792297696822 | validation: 0.4524508017734723]
	TIME [epoch: 2.67 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3399877261856502		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.3399877261856502 | validation: 0.4398278622800058]
	TIME [epoch: 2.67 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33239005148324496		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.33239005148324496 | validation: 0.40165161709414915]
	TIME [epoch: 2.67 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32431148524071435		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.32431148524071435 | validation: 0.42729394766619433]
	TIME [epoch: 2.67 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3175476432731834		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.3175476432731834 | validation: 0.41476883180322316]
	TIME [epoch: 2.67 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31999954245234535		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.31999954245234535 | validation: 0.4108836184371154]
	TIME [epoch: 2.67 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3205390719629838		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.3205390719629838 | validation: 0.4093054668433105]
	TIME [epoch: 2.68 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3246048487513866		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.3246048487513866 | validation: 0.3964142267805378]
	TIME [epoch: 2.67 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3450628404286056		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.3450628404286056 | validation: 0.45869056012790294]
	TIME [epoch: 2.67 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3327033854658635		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.3327033854658635 | validation: 0.43710634184955915]
	TIME [epoch: 2.67 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3202066863166435		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.3202066863166435 | validation: 0.385717511993602]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_733.pth
	Model improved!!!
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3161668197464481		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.3161668197464481 | validation: 0.4299184375614319]
	TIME [epoch: 2.68 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3124218178212019		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.3124218178212019 | validation: 0.39197844452585356]
	TIME [epoch: 2.68 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.315779777777149		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.315779777777149 | validation: 0.43713626730533384]
	TIME [epoch: 2.68 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3231688940211278		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.3231688940211278 | validation: 0.41036225170039686]
	TIME [epoch: 2.67 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32455784461384457		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.32455784461384457 | validation: 0.3987219384987449]
	TIME [epoch: 2.67 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3323461004832271		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.3323461004832271 | validation: 0.4415315709404755]
	TIME [epoch: 2.68 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32237961726783687		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.32237961726783687 | validation: 0.41551216016290254]
	TIME [epoch: 2.68 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3272348169432266		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.3272348169432266 | validation: 0.40704969542091574]
	TIME [epoch: 2.68 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3286033851724217		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.3286033851724217 | validation: 0.42654411808433157]
	TIME [epoch: 2.68 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3241190121223002		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.3241190121223002 | validation: 0.4164431623768686]
	TIME [epoch: 2.68 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3102433403804399		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.3102433403804399 | validation: 0.3936884208284929]
	TIME [epoch: 2.68 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30627742819855647		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.30627742819855647 | validation: 0.39174776736134076]
	TIME [epoch: 2.67 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2993716658701732		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.2993716658701732 | validation: 0.40101272936458515]
	TIME [epoch: 2.68 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.312481058221318		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.312481058221318 | validation: 0.4031154578605836]
	TIME [epoch: 2.68 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3236964995735329		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.3236964995735329 | validation: 0.3686130561931089]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_748.pth
	Model improved!!!
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3637470546089209		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.3637470546089209 | validation: 0.4227367670651025]
	TIME [epoch: 2.68 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31703408931279453		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.31703408931279453 | validation: 0.4130738702008592]
	TIME [epoch: 2.68 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32365054686495404		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.32365054686495404 | validation: 0.38226989337787876]
	TIME [epoch: 2.68 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31862274749679587		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.31862274749679587 | validation: 0.3933944949196197]
	TIME [epoch: 2.68 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.297978999780942		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.297978999780942 | validation: 0.39812615052742095]
	TIME [epoch: 2.68 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2959361105586417		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.2959361105586417 | validation: 0.38958163854727024]
	TIME [epoch: 2.68 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3034241747407984		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.3034241747407984 | validation: 0.40212839942052114]
	TIME [epoch: 2.68 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31215078906504784		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.31215078906504784 | validation: 0.4367013167167512]
	TIME [epoch: 2.68 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.336408126452707		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.336408126452707 | validation: 0.4329828407530622]
	TIME [epoch: 2.68 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3468448888702909		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.3468448888702909 | validation: 0.4066212099729642]
	TIME [epoch: 2.68 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32021949545528056		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.32021949545528056 | validation: 0.3955688005240175]
	TIME [epoch: 2.68 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.298483416762821		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.298483416762821 | validation: 0.38030326917461316]
	TIME [epoch: 2.68 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.292980454289351		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.292980454289351 | validation: 0.38222915231601606]
	TIME [epoch: 2.68 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29435577074735936		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.29435577074735936 | validation: 0.38442846024361493]
	TIME [epoch: 2.67 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2953732507353946		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.2953732507353946 | validation: 0.38849331643932006]
	TIME [epoch: 2.68 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2932304339639659		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.2932304339639659 | validation: 0.37665216531868656]
	TIME [epoch: 2.68 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29182128325736606		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.29182128325736606 | validation: 0.3762307837554779]
	TIME [epoch: 2.67 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29973657774623513		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.29973657774623513 | validation: 0.3922024556254282]
	TIME [epoch: 2.68 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31844412162178976		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.31844412162178976 | validation: 0.4469946802715346]
	TIME [epoch: 2.68 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3480436251008834		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.3480436251008834 | validation: 0.405002027520319]
	TIME [epoch: 2.67 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33860623188254335		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.33860623188254335 | validation: 0.4033986219214325]
	TIME [epoch: 2.68 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31073756334131014		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.31073756334131014 | validation: 0.389342460736465]
	TIME [epoch: 2.68 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29188147337343656		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.29188147337343656 | validation: 0.37621427132311247]
	TIME [epoch: 2.68 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2959878202137293		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.2959878202137293 | validation: 0.4023741366610349]
	TIME [epoch: 2.69 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.295775826512498		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.295775826512498 | validation: 0.38708370229911493]
	TIME [epoch: 2.68 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3032056838665029		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.3032056838665029 | validation: 0.4046281272943187]
	TIME [epoch: 2.68 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30700442423125046		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.30700442423125046 | validation: 0.39421239100767624]
	TIME [epoch: 2.68 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3091477372783911		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.3091477372783911 | validation: 0.39452114602781996]
	TIME [epoch: 2.68 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31504757432768477		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.31504757432768477 | validation: 0.4041131646536948]
	TIME [epoch: 2.68 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31101497582637455		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.31101497582637455 | validation: 0.3794034294435621]
	TIME [epoch: 2.68 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2996022313866957		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.2996022313866957 | validation: 0.38045334262845865]
	TIME [epoch: 2.68 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.295436201701023		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.295436201701023 | validation: 0.3762751591689754]
	TIME [epoch: 2.68 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2908888259819092		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.2908888259819092 | validation: 0.38406264042180527]
	TIME [epoch: 2.68 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2904647989886319		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.2904647989886319 | validation: 0.3672112565787485]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_782.pth
	Model improved!!!
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29587552943327244		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.29587552943327244 | validation: 0.3879079030402732]
	TIME [epoch: 2.68 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29809117538947083		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.29809117538947083 | validation: 0.3526240884484019]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_784.pth
	Model improved!!!
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31032889452013207		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.31032889452013207 | validation: 0.398677938448381]
	TIME [epoch: 2.67 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2917913757564195		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.2917913757564195 | validation: 0.3698135693585556]
	TIME [epoch: 2.67 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28970171238023573		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.28970171238023573 | validation: 0.37306161870786314]
	TIME [epoch: 2.67 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29373410505509506		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.29373410505509506 | validation: 0.3923368109884333]
	TIME [epoch: 2.68 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3058855250575774		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.3058855250575774 | validation: 0.4179337890314009]
	TIME [epoch: 2.67 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3167955920203985		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.3167955920203985 | validation: 0.4295175939669437]
	TIME [epoch: 2.68 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31171128248863855		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.31171128248863855 | validation: 0.3767557950387921]
	TIME [epoch: 2.67 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28888705868734216		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.28888705868734216 | validation: 0.3728713814227641]
	TIME [epoch: 2.67 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27899312098312595		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.27899312098312595 | validation: 0.3709397926385298]
	TIME [epoch: 2.67 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2783054353331356		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.2783054353331356 | validation: 0.3552906628398828]
	TIME [epoch: 2.67 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28266910487009617		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.28266910487009617 | validation: 0.388743700163761]
	TIME [epoch: 2.67 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28576570257058304		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.28576570257058304 | validation: 0.38416082177946637]
	TIME [epoch: 2.67 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28712088318565554		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.28712088318565554 | validation: 0.3842289550654148]
	TIME [epoch: 2.67 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2943728990433135		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.2943728990433135 | validation: 0.3991541324663526]
	TIME [epoch: 2.67 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3363243355588033		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.3363243355588033 | validation: 0.3523905980133014]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_799.pth
	Model improved!!!
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3321935308273254		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.3321935308273254 | validation: 0.3748408753527083]
	TIME [epoch: 2.68 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27896889528312196		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.27896889528312196 | validation: 0.37785197046603547]
	TIME [epoch: 2.68 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28281557342055025		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.28281557342055025 | validation: 0.35239501290915776]
	TIME [epoch: 2.68 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28618310713724954		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.28618310713724954 | validation: 0.36537223163101007]
	TIME [epoch: 2.68 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28701530475049075		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.28701530475049075 | validation: 0.35648619664423586]
	TIME [epoch: 2.67 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2803717431300368		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.2803717431300368 | validation: 0.37330822827604915]
	TIME [epoch: 2.67 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28394665027377347		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.28394665027377347 | validation: 0.3677407415121248]
	TIME [epoch: 2.67 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28692085030257586		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.28692085030257586 | validation: 0.37607501850778546]
	TIME [epoch: 2.67 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2995614930285197		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.2995614930285197 | validation: 0.40444989645140583]
	TIME [epoch: 2.67 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31225352647536553		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.31225352647536553 | validation: 0.37800348226915104]
	TIME [epoch: 2.68 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.295826781391578		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.295826781391578 | validation: 0.35974744203227993]
	TIME [epoch: 2.68 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2847060000968779		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.2847060000968779 | validation: 0.37406765652524365]
	TIME [epoch: 2.67 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28109587370103223		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.28109587370103223 | validation: 0.34983037817337853]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_812.pth
	Model improved!!!
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28327364415293		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.28327364415293 | validation: 0.37002394516812925]
	TIME [epoch: 2.68 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27849345349781346		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.27849345349781346 | validation: 0.34004575633554174]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_814.pth
	Model improved!!!
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2785242269179832		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.2785242269179832 | validation: 0.3695702370571084]
	TIME [epoch: 2.68 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28195582269213787		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.28195582269213787 | validation: 0.3566439476737284]
	TIME [epoch: 2.67 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2891863859126036		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.2891863859126036 | validation: 0.37584126119337435]
	TIME [epoch: 2.68 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28600186723560345		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.28600186723560345 | validation: 0.3785198508438731]
	TIME [epoch: 2.67 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29542640833089373		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.29542640833089373 | validation: 0.38805677162303565]
	TIME [epoch: 2.67 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28906330905332644		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.28906330905332644 | validation: 0.3789547096486068]
	TIME [epoch: 2.67 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27895866877742415		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.27895866877742415 | validation: 0.345521967136114]
	TIME [epoch: 2.67 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.277212841455414		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.277212841455414 | validation: 0.3675628369161363]
	TIME [epoch: 2.68 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27749303363597405		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.27749303363597405 | validation: 0.3430065421193327]
	TIME [epoch: 2.68 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2729230377953189		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.2729230377953189 | validation: 0.3659811056963159]
	TIME [epoch: 2.67 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2729766750604975		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.2729766750604975 | validation: 0.3485232688778565]
	TIME [epoch: 2.68 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2725898750313987		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.2725898750313987 | validation: 0.35888882506354947]
	TIME [epoch: 2.67 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27423058121674554		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.27423058121674554 | validation: 0.3715490649253929]
	TIME [epoch: 2.67 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2824744617184686		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.2824744617184686 | validation: 0.4005615033315166]
	TIME [epoch: 2.67 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30400276682144856		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.30400276682144856 | validation: 0.3839201284594723]
	TIME [epoch: 2.68 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.308815593637484		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.308815593637484 | validation: 0.36393495069789894]
	TIME [epoch: 2.67 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27545771470143954		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.27545771470143954 | validation: 0.3437214640530805]
	TIME [epoch: 2.68 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2701465180260088		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.2701465180260088 | validation: 0.3635777843298478]
	TIME [epoch: 2.67 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26422386198212655		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.26422386198212655 | validation: 0.3310708230028788]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_833.pth
	Model improved!!!
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2772955562433182		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.2772955562433182 | validation: 0.35147787065814823]
	TIME [epoch: 2.68 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27377223320499744		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.27377223320499744 | validation: 0.34113754713033695]
	TIME [epoch: 2.68 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2706170157699455		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.2706170157699455 | validation: 0.3752161179689566]
	TIME [epoch: 2.68 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2754285905019012		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.2754285905019012 | validation: 0.33598345836507093]
	TIME [epoch: 2.68 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3013880700185237		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.3013880700185237 | validation: 0.36562050281548397]
	TIME [epoch: 2.68 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2777781102259956		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.2777781102259956 | validation: 0.3742551480325209]
	TIME [epoch: 2.68 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2792921651172334		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.2792921651172334 | validation: 0.35405399991476405]
	TIME [epoch: 2.67 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27278153269616207		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.27278153269616207 | validation: 0.35744314165210567]
	TIME [epoch: 2.68 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2609007612738479		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.2609007612738479 | validation: 0.3529461086493393]
	TIME [epoch: 2.68 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26242546053341864		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.26242546053341864 | validation: 0.34604438401928744]
	TIME [epoch: 2.68 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26463332055668515		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.26463332055668515 | validation: 0.3522512153905057]
	TIME [epoch: 2.68 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2661881395381872		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.2661881395381872 | validation: 0.3706529974059676]
	TIME [epoch: 2.68 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28633703772186797		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.28633703772186797 | validation: 0.35702037039255874]
	TIME [epoch: 2.68 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29569267552909995		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.29569267552909995 | validation: 0.368869296907131]
	TIME [epoch: 2.68 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2794689082760742		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.2794689082760742 | validation: 0.32844565164787515]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_848.pth
	Model improved!!!
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2605134139040529		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.2605134139040529 | validation: 0.3692033027038797]
	TIME [epoch: 2.67 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25967160045555265		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.25967160045555265 | validation: 0.3487615859260779]
	TIME [epoch: 2.67 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25450033304439335		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.25450033304439335 | validation: 0.34246073700892177]
	TIME [epoch: 2.66 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2537544782176366		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.2537544782176366 | validation: 0.35044372815326286]
	TIME [epoch: 2.67 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2593455277453884		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.2593455277453884 | validation: 0.34822256771511784]
	TIME [epoch: 2.67 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2601940331159443		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.2601940331159443 | validation: 0.3797832037665956]
	TIME [epoch: 2.67 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28031618360048194		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.28031618360048194 | validation: 0.33361349673668966]
	TIME [epoch: 2.67 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30110488219756265		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.30110488219756265 | validation: 0.3596116475401974]
	TIME [epoch: 2.67 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26955150702330644		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.26955150702330644 | validation: 0.34435788849633]
	TIME [epoch: 2.67 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25936198201915506		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.25936198201915506 | validation: 0.33639854649129936]
	TIME [epoch: 2.67 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2557320628175595		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.2557320628175595 | validation: 0.3532764246295306]
	TIME [epoch: 2.67 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2624930747978452		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.2624930747978452 | validation: 0.3322703930543669]
	TIME [epoch: 2.67 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26102051923604724		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.26102051923604724 | validation: 0.34445356730862225]
	TIME [epoch: 2.67 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25742956907047465		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.25742956907047465 | validation: 0.33006871735008575]
	TIME [epoch: 2.67 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2611058770880518		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.2611058770880518 | validation: 0.35925877831747555]
	TIME [epoch: 2.67 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26867276780465155		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.26867276780465155 | validation: 0.3441249870064554]
	TIME [epoch: 2.67 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26868685356781213		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.26868685356781213 | validation: 0.35935898730395355]
	TIME [epoch: 2.66 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2731256577699408		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.2731256577699408 | validation: 0.36708892265644444]
	TIME [epoch: 2.67 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26781366833720927		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.26781366833720927 | validation: 0.34792708650582665]
	TIME [epoch: 2.66 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2637455589196641		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.2637455589196641 | validation: 0.3275141952551254]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_868.pth
	Model improved!!!
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2674354175908959		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.2674354175908959 | validation: 0.34973134656135035]
	TIME [epoch: 2.68 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2588682684878323		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.2588682684878323 | validation: 0.31963054784840006]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_870.pth
	Model improved!!!
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25006812290433383		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.25006812290433383 | validation: 0.3435194685303941]
	TIME [epoch: 2.67 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24753364316212362		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.24753364316212362 | validation: 0.3490761136189366]
	TIME [epoch: 2.67 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24737560262564645		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.24737560262564645 | validation: 0.3285924958336391]
	TIME [epoch: 2.66 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2516301291283486		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.2516301291283486 | validation: 0.34574710008449844]
	TIME [epoch: 2.66 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2507104631927193		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.2507104631927193 | validation: 0.33016676113805316]
	TIME [epoch: 2.66 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26871278230609263		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.26871278230609263 | validation: 0.3626493883625028]
	TIME [epoch: 2.66 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26754233460516		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.26754233460516 | validation: 0.3449360442492775]
	TIME [epoch: 2.67 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2718247201052413		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.2718247201052413 | validation: 0.3536759456330866]
	TIME [epoch: 2.66 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2671500125383291		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.2671500125383291 | validation: 0.34699278222020324]
	TIME [epoch: 2.69 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2516808755191351		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.2516808755191351 | validation: 0.3406592803346373]
	TIME [epoch: 2.68 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24639814770420085		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.24639814770420085 | validation: 0.3217637368178893]
	TIME [epoch: 2.69 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2508209092565293		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.2508209092565293 | validation: 0.3476254796354805]
	TIME [epoch: 2.68 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24899850907655613		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.24899850907655613 | validation: 0.3164946417126018]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_883.pth
	Model improved!!!
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2531288242844725		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.2531288242844725 | validation: 0.3524864948386097]
	TIME [epoch: 2.67 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24492799786048536		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.24492799786048536 | validation: 0.32828494500161903]
	TIME [epoch: 2.67 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24759924971149397		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.24759924971149397 | validation: 0.34900333342330225]
	TIME [epoch: 2.67 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25446330468101086		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.25446330468101086 | validation: 0.34941126755765145]
	TIME [epoch: 2.66 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25898629835758347		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.25898629835758347 | validation: 0.35996601291259767]
	TIME [epoch: 2.67 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2576235794041375		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.2576235794041375 | validation: 0.33005051328935225]
	TIME [epoch: 2.67 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25704602468549104		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.25704602468549104 | validation: 0.3497641700111478]
	TIME [epoch: 2.67 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2503451072376954		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.2503451072376954 | validation: 0.3290456173560935]
	TIME [epoch: 2.67 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24372689664900044		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.24372689664900044 | validation: 0.34455584384265986]
	TIME [epoch: 2.67 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24809137818140645		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.24809137818140645 | validation: 0.316727600414579]
	TIME [epoch: 2.67 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25119899623228803		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.25119899623228803 | validation: 0.33889195095855673]
	TIME [epoch: 2.67 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24054212040307607		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.24054212040307607 | validation: 0.32656801436624344]
	TIME [epoch: 2.67 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24122771934685935		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.24122771934685935 | validation: 0.33930747572213815]
	TIME [epoch: 2.67 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23935537917008556		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.23935537917008556 | validation: 0.3379245375220371]
	TIME [epoch: 2.66 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24754847780321537		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.24754847780321537 | validation: 0.3512923208397513]
	TIME [epoch: 2.68 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25205085847979436		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.25205085847979436 | validation: 0.34646740890865424]
	TIME [epoch: 2.67 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2662690048675557		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.2662690048675557 | validation: 0.3407808390897376]
	TIME [epoch: 2.66 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25172932173222906		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.25172932173222906 | validation: 0.33854284799215045]
	TIME [epoch: 2.68 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2349811383323581		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.2349811383323581 | validation: 0.3291034013780766]
	TIME [epoch: 2.68 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23581573073986065		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.23581573073986065 | validation: 0.32867212620415726]
	TIME [epoch: 2.68 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23658587437388018		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.23658587437388018 | validation: 0.33842733338869124]
	TIME [epoch: 2.68 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23833377451422194		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.23833377451422194 | validation: 0.32753275250660147]
	TIME [epoch: 2.68 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24459214713080626		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.24459214713080626 | validation: 0.3484391142891055]
	TIME [epoch: 2.68 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24779070597630032		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.24779070597630032 | validation: 0.31427499956934285]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_907.pth
	Model improved!!!
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25883533892544924		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.25883533892544924 | validation: 0.33600857162435355]
	TIME [epoch: 2.68 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2411729172547616		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.2411729172547616 | validation: 0.3555885192828682]
	TIME [epoch: 2.68 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24405143096730642		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.24405143096730642 | validation: 0.3503441573372332]
	TIME [epoch: 2.68 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24162123785264436		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.24162123785264436 | validation: 0.32545026114480974]
	TIME [epoch: 2.67 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24061683102326575		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.24061683102326575 | validation: 0.3428544454166533]
	TIME [epoch: 2.66 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24011295454668904		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.24011295454668904 | validation: 0.3127490044155004]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_913.pth
	Model improved!!!
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24535933712109753		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.24535933712109753 | validation: 0.3343879493740676]
	TIME [epoch: 2.67 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23645245010987775		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.23645245010987775 | validation: 0.32286551728679]
	TIME [epoch: 2.66 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23053398746934506		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.23053398746934506 | validation: 0.33498387128505086]
	TIME [epoch: 2.66 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22988631909410293		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.22988631909410293 | validation: 0.33465178218303004]
	TIME [epoch: 2.67 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.233284828598368		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.233284828598368 | validation: 0.329057815506433]
	TIME [epoch: 2.66 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23107832322209354		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.23107832322209354 | validation: 0.32658899719835616]
	TIME [epoch: 2.67 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23054228067381538		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.23054228067381538 | validation: 0.3319471863651178]
	TIME [epoch: 2.67 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2347382002560007		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.2347382002560007 | validation: 0.3280393995254506]
	TIME [epoch: 2.67 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24010190787624344		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.24010190787624344 | validation: 0.3521562047009783]
	TIME [epoch: 2.67 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2518918009228398		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.2518918009228398 | validation: 0.3155560673501858]
	TIME [epoch: 2.67 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2512990610635012		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.2512990610635012 | validation: 0.3316200942704501]
	TIME [epoch: 2.66 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22926136879144557		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.22926136879144557 | validation: 0.3223478604981797]
	TIME [epoch: 2.67 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22400852429719822		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.22400852429719822 | validation: 0.3121178099514153]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_926.pth
	Model improved!!!
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22296989197336645		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.22296989197336645 | validation: 0.3284134785474421]
	TIME [epoch: 2.67 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22951530875956758		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.22951530875956758 | validation: 0.31030904197822634]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_928.pth
	Model improved!!!
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2279895154020502		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.2279895154020502 | validation: 0.3326311636772368]
	TIME [epoch: 2.66 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23352368807651203		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.23352368807651203 | validation: 0.3103132917595796]
	TIME [epoch: 2.67 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24041901737068772		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.24041901737068772 | validation: 0.3369556781411718]
	TIME [epoch: 2.67 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2280358309351286		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.2280358309351286 | validation: 0.31663455871152746]
	TIME [epoch: 2.66 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22827259013989726		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.22827259013989726 | validation: 0.34193094979486827]
	TIME [epoch: 2.66 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2328101421977027		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.2328101421977027 | validation: 0.3377321238388198]
	TIME [epoch: 2.66 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23463835891003815		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.23463835891003815 | validation: 0.33552324090056507]
	TIME [epoch: 2.67 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24037837410547067		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.24037837410547067 | validation: 0.3114765765396909]
	TIME [epoch: 2.67 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24202949344968586		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.24202949344968586 | validation: 0.33562628661593763]
	TIME [epoch: 2.67 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2265430877785407		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.2265430877785407 | validation: 0.3187817978385106]
	TIME [epoch: 2.67 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22319863272528806		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.22319863272528806 | validation: 0.3388065196157882]
	TIME [epoch: 2.67 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22134075825184138		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.22134075825184138 | validation: 0.318110965665483]
	TIME [epoch: 2.67 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22421055545540713		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.22421055545540713 | validation: 0.32588674957529834]
	TIME [epoch: 2.66 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2174859566096761		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.2174859566096761 | validation: 0.3267685055297447]
	TIME [epoch: 2.67 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21539519637658963		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.21539519637658963 | validation: 0.31785883273792614]
	TIME [epoch: 2.66 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2164265710083005		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.2164265710083005 | validation: 0.31679711988283543]
	TIME [epoch: 2.66 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21661762709902882		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.21661762709902882 | validation: 0.31773459418241573]
	TIME [epoch: 2.67 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21310105070628488		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.21310105070628488 | validation: 0.32713628623659113]
	TIME [epoch: 2.67 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23355999642313344		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.23355999642313344 | validation: 0.33504286353828205]
	TIME [epoch: 2.66 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2393471588374244		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.2393471588374244 | validation: 0.32286024663891255]
	TIME [epoch: 2.66 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25301578869433505		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.25301578869433505 | validation: 0.3482599767886417]
	TIME [epoch: 2.67 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22716454722815854		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.22716454722815854 | validation: 0.3291962762413775]
	TIME [epoch: 2.67 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21678316091097116		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.21678316091097116 | validation: 0.3152776469899952]
	TIME [epoch: 2.66 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21667812535967534		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.21667812535967534 | validation: 0.3287594781597839]
	TIME [epoch: 2.67 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21867548407425197		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.21867548407425197 | validation: 0.3074861075442275]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_953.pth
	Model improved!!!
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2250739079662357		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.2250739079662357 | validation: 0.33339801980323575]
	TIME [epoch: 2.67 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2205000262807733		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.2205000262807733 | validation: 0.3013748681554619]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_955.pth
	Model improved!!!
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2271011349152294		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.2271011349152294 | validation: 0.32926810865421824]
	TIME [epoch: 2.67 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21653881361766641		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.21653881361766641 | validation: 0.3210658714057674]
	TIME [epoch: 2.67 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2136591740790933		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.2136591740790933 | validation: 0.3254070452666123]
	TIME [epoch: 2.67 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2124610876270677		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.2124610876270677 | validation: 0.31600612373715886]
	TIME [epoch: 2.67 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21444984497211317		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.21444984497211317 | validation: 0.33373348528093444]
	TIME [epoch: 2.67 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21635305426643311		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.21635305426643311 | validation: 0.3074577364518537]
	TIME [epoch: 2.67 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22452882057829107		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.22452882057829107 | validation: 0.32852972968030575]
	TIME [epoch: 2.67 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22022649367987115		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.22022649367987115 | validation: 0.32437064876406585]
	TIME [epoch: 2.66 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22079365904161707		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.22079365904161707 | validation: 0.3360281587951531]
	TIME [epoch: 2.67 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2210569684082976		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.2210569684082976 | validation: 0.3207136189808071]
	TIME [epoch: 2.67 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21494472433687015		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.21494472433687015 | validation: 0.3290564720757583]
	TIME [epoch: 2.67 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21251080547429735		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.21251080547429735 | validation: 0.2989705540394889]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_967.pth
	Model improved!!!
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21755838723423807		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.21755838723423807 | validation: 0.33033115801343355]
	TIME [epoch: 2.67 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2142451073564553		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.2142451073564553 | validation: 0.31035958396495034]
	TIME [epoch: 2.66 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21577249281075173		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.21577249281075173 | validation: 0.3275003287774012]
	TIME [epoch: 2.67 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2098483004283801		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.2098483004283801 | validation: 0.313704701674375]
	TIME [epoch: 2.66 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21416579605479485		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.21416579605479485 | validation: 0.3331304996367915]
	TIME [epoch: 2.66 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2094487827078229		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.2094487827078229 | validation: 0.3038457980229469]
	TIME [epoch: 2.66 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21412913680102777		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.21412913680102777 | validation: 0.32799597192434576]
	TIME [epoch: 2.67 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21043380767936198		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.21043380767936198 | validation: 0.3115236498559057]
	TIME [epoch: 2.67 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20929062657400532		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.20929062657400532 | validation: 0.330410912932003]
	TIME [epoch: 2.67 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21030727054476706		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.21030727054476706 | validation: 0.31816348592213517]
	TIME [epoch: 2.67 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20951192089155274		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.20951192089155274 | validation: 0.3253801406710551]
	TIME [epoch: 2.66 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.212978959421739		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.212978959421739 | validation: 0.3159649917817684]
	TIME [epoch: 2.67 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2166493332743648		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.2166493332743648 | validation: 0.3285583020532154]
	TIME [epoch: 2.66 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20940247497053568		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.20940247497053568 | validation: 0.3102546192145549]
	TIME [epoch: 2.67 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20471388739886798		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.20471388739886798 | validation: 0.32380391200590264]
	TIME [epoch: 2.66 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20761364642390834		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.20761364642390834 | validation: 0.30074910947438876]
	TIME [epoch: 2.67 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21437680284169303		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.21437680284169303 | validation: 0.3233200084359815]
	TIME [epoch: 2.66 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21276808792719984		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.21276808792719984 | validation: 0.3032212595337887]
	TIME [epoch: 2.66 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21252121826091913		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.21252121826091913 | validation: 0.3199028192376582]
	TIME [epoch: 2.66 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2048512398492582		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.2048512398492582 | validation: 0.31444310872155656]
	TIME [epoch: 2.66 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20091057470541118		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.20091057470541118 | validation: 0.32344811694082937]
	TIME [epoch: 2.67 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20552838242747773		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.20552838242747773 | validation: 0.30803382152475195]
	TIME [epoch: 2.67 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.208815973373591		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.208815973373591 | validation: 0.32645892954193556]
	TIME [epoch: 2.67 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21681601701818495		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.21681601701818495 | validation: 0.2912585328716083]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_991.pth
	Model improved!!!
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22856077099579067		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.22856077099579067 | validation: 0.3148249497799948]
	TIME [epoch: 2.66 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19903050864196914		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.19903050864196914 | validation: 0.31879907952019854]
	TIME [epoch: 2.67 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2030680100821488		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.2030680100821488 | validation: 0.29942011508460337]
	TIME [epoch: 2.67 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20561186626138558		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.20561186626138558 | validation: 0.3201957503741718]
	TIME [epoch: 2.67 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19716097985645079		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.19716097985645079 | validation: 0.30851688470298344]
	TIME [epoch: 2.67 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19646740164583282		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.19646740164583282 | validation: 0.31745687350410895]
	TIME [epoch: 2.67 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1954654232952894		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.1954654232952894 | validation: 0.3145298418975946]
	TIME [epoch: 2.66 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19935440912248234		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.19935440912248234 | validation: 0.3222169700457209]
	TIME [epoch: 2.66 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2049513597021175		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.2049513597021175 | validation: 0.30471312336062545]
	TIME [epoch: 2.67 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20938560559625624		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.20938560559625624 | validation: 0.33119263287701844]
	TIME [epoch: 173 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21578203162055565		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.21578203162055565 | validation: 0.2829118272609207]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1002.pth
	Model improved!!!
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21088661907199183		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.21088661907199183 | validation: 0.31732942700564676]
	TIME [epoch: 5.71 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19588004714042404		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.19588004714042404 | validation: 0.31615617150869924]
	TIME [epoch: 5.7 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1978512327575675		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.1978512327575675 | validation: 0.2920796537436103]
	TIME [epoch: 5.7 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20301017060647764		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.20301017060647764 | validation: 0.31428013610160477]
	TIME [epoch: 5.7 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19934738722308518		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.19934738722308518 | validation: 0.30476655905200717]
	TIME [epoch: 5.71 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19947299310346112		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.19947299310346112 | validation: 0.308805721436231]
	TIME [epoch: 5.72 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19657191801592244		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.19657191801592244 | validation: 0.30175100129308013]
	TIME [epoch: 5.7 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1971062397758576		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.1971062397758576 | validation: 0.3211135952199189]
	TIME [epoch: 5.71 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19441372370259463		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.19441372370259463 | validation: 0.3132328391727781]
	TIME [epoch: 5.71 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19093843450908046		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.19093843450908046 | validation: 0.30702148410326147]
	TIME [epoch: 5.7 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19107985728833243		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.19107985728833243 | validation: 0.309821260964428]
	TIME [epoch: 5.71 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18782101726452294		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.18782101726452294 | validation: 0.30499209947150724]
	TIME [epoch: 5.71 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19032587508199697		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.19032587508199697 | validation: 0.3023392146159263]
	TIME [epoch: 5.7 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18979809737453943		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.18979809737453943 | validation: 0.3233519130846225]
	TIME [epoch: 5.71 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18807518276200286		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.18807518276200286 | validation: 0.29701146946921303]
	TIME [epoch: 5.71 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.205599717333162		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.205599717333162 | validation: 0.33372072667740604]
	TIME [epoch: 5.71 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21811806218247715		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.21811806218247715 | validation: 0.28703395170311513]
	TIME [epoch: 5.7 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21256547663583247		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.21256547663583247 | validation: 0.31534669508022956]
	TIME [epoch: 5.7 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18961828609203132		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.18961828609203132 | validation: 0.32094067793995273]
	TIME [epoch: 5.7 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19413728043640896		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.19413728043640896 | validation: 0.28885096841282193]
	TIME [epoch: 5.71 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20819178711044384		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.20819178711044384 | validation: 0.3039493904688718]
	TIME [epoch: 5.7 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1887061710091878		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.1887061710091878 | validation: 0.3134435305918197]
	TIME [epoch: 5.72 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19530222948986467		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.19530222948986467 | validation: 0.2948304960874952]
	TIME [epoch: 5.7 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19576817683855294		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.19576817683855294 | validation: 0.3084765903197063]
	TIME [epoch: 5.71 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18471303323648197		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.18471303323648197 | validation: 0.30969134501178613]
	TIME [epoch: 5.7 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1854635992277036		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.1854635992277036 | validation: 0.2992680192642618]
	TIME [epoch: 5.71 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18665605286706904		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.18665605286706904 | validation: 0.3116420699048963]
	TIME [epoch: 5.72 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.187732374294905		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.187732374294905 | validation: 0.3061691440335582]
	TIME [epoch: 5.7 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18517895720057165		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.18517895720057165 | validation: 0.30375215154209934]
	TIME [epoch: 5.71 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18250280645496791		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.18250280645496791 | validation: 0.3166861484669693]
	TIME [epoch: 5.71 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18504110566584903		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.18504110566584903 | validation: 0.30523814445989006]
	TIME [epoch: 5.7 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18921817249034723		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.18921817249034723 | validation: 0.3253401301715206]
	TIME [epoch: 5.71 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20486897510376303		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.20486897510376303 | validation: 0.28646922505301015]
	TIME [epoch: 5.7 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20721444468018227		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.20721444468018227 | validation: 0.3162228171452386]
	TIME [epoch: 5.7 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18031551076369495		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.18031551076369495 | validation: 0.30905111924451706]
	TIME [epoch: 5.7 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1832808859212065		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.1832808859212065 | validation: 0.29782600888497957]
	TIME [epoch: 5.7 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18144692436133994		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.18144692436133994 | validation: 0.3157787565607466]
	TIME [epoch: 5.71 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1857901250516574		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.1857901250516574 | validation: 0.2981816399733065]
	TIME [epoch: 5.7 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1821560729592477		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.1821560729592477 | validation: 0.30646635666459726]
	TIME [epoch: 5.71 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18316814155950886		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.18316814155950886 | validation: 0.29090219214735286]
	TIME [epoch: 5.71 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18442797595742022		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.18442797595742022 | validation: 0.3179458902710292]
	TIME [epoch: 5.71 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19340333592174508		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.19340333592174508 | validation: 0.2752768653804523]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1044.pth
	Model improved!!!
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20879998803204775		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.20879998803204775 | validation: 0.31295372725051973]
	TIME [epoch: 5.71 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17937651301697963		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.17937651301697963 | validation: 0.3126847440015801]
	TIME [epoch: 5.71 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19057810723428908		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.19057810723428908 | validation: 0.28680567380860705]
	TIME [epoch: 5.71 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18379683008588263		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.18379683008588263 | validation: 0.30652116093181453]
	TIME [epoch: 5.7 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18024339713107207		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.18024339713107207 | validation: 0.3116777592501569]
	TIME [epoch: 5.69 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17942391158129326		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.17942391158129326 | validation: 0.29930253539207347]
	TIME [epoch: 5.71 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18119250549629354		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.18119250549629354 | validation: 0.30213253225298164]
	TIME [epoch: 5.7 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17940553216279184		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.17940553216279184 | validation: 0.29173556028559916]
	TIME [epoch: 5.71 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17883644684900077		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.17883644684900077 | validation: 0.31046187174779316]
	TIME [epoch: 5.69 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1798158245187263		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.1798158245187263 | validation: 0.2913872695285989]
	TIME [epoch: 5.71 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18327741980272566		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.18327741980272566 | validation: 0.31573109219554757]
	TIME [epoch: 5.71 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18812145904837477		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.18812145904837477 | validation: 0.29458713416583526]
	TIME [epoch: 5.7 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18548004202146545		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.18548004202146545 | validation: 0.3121441666038334]
	TIME [epoch: 5.7 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18038321400134608		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.18038321400134608 | validation: 0.29565614812140256]
	TIME [epoch: 5.7 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17525074034979832		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.17525074034979832 | validation: 0.2986988790480953]
	TIME [epoch: 5.7 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17544581913376908		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.17544581913376908 | validation: 0.30007857328195175]
	TIME [epoch: 5.71 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1763058841784023		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.1763058841784023 | validation: 0.2970387958989779]
	TIME [epoch: 5.69 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17272510287534257		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.17272510287534257 | validation: 0.2902772179380791]
	TIME [epoch: 5.71 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1714163375912373		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.1714163375912373 | validation: 0.3052367621047358]
	TIME [epoch: 5.7 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17379510442616058		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.17379510442616058 | validation: 0.2834524560094947]
	TIME [epoch: 5.7 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1814894916272814		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.1814894916272814 | validation: 0.3201475159682584]
	TIME [epoch: 5.71 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18645785421617986		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.18645785421617986 | validation: 0.2850136789227923]
	TIME [epoch: 5.7 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19941356992732928		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.19941356992732928 | validation: 0.3123199094346418]
	TIME [epoch: 5.7 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17530118720398308		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.17530118720398308 | validation: 0.3110668872246103]
	TIME [epoch: 5.7 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17929498445029116		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.17929498445029116 | validation: 0.2940358758626365]
	TIME [epoch: 5.7 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17392393588977917		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.17392393588977917 | validation: 0.3040773178774192]
	TIME [epoch: 5.7 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17384009335671924		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.17384009335671924 | validation: 0.2993069502696185]
	TIME [epoch: 5.71 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17309151937124018		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.17309151937124018 | validation: 0.3021693362391479]
	TIME [epoch: 5.7 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17198343755872558		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.17198343755872558 | validation: 0.29081504934101615]
	TIME [epoch: 5.71 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1729506082404744		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.1729506082404744 | validation: 0.31372336267114864]
	TIME [epoch: 5.7 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1746512092926331		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.1746512092926331 | validation: 0.28649754289110557]
	TIME [epoch: 5.71 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1860776684241731		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.1860776684241731 | validation: 0.30642719123731593]
	TIME [epoch: 5.71 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1789192607673628		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.1789192607673628 | validation: 0.2883125898050626]
	TIME [epoch: 5.7 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1780204551751192		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.1780204551751192 | validation: 0.3057055408970108]
	TIME [epoch: 5.7 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16854332885026238		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.16854332885026238 | validation: 0.29779217063080277]
	TIME [epoch: 5.7 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1676728549717361		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.1676728549717361 | validation: 0.3059837952696134]
	TIME [epoch: 5.7 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16979027447235404		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.16979027447235404 | validation: 0.28603978213374287]
	TIME [epoch: 5.71 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16839165467982647		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.16839165467982647 | validation: 0.29916804680150444]
	TIME [epoch: 5.71 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1690962937866837		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.1690962937866837 | validation: 0.28661769875051507]
	TIME [epoch: 5.7 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16839196860102937		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.16839196860102937 | validation: 0.2983405266684285]
	TIME [epoch: 5.71 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17003235433363997		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.17003235433363997 | validation: 0.2906400489472986]
	TIME [epoch: 5.7 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16883609186317144		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.16883609186317144 | validation: 0.30967611602330536]
	TIME [epoch: 5.71 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17279707346896517		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.17279707346896517 | validation: 0.2774363092792547]
	TIME [epoch: 5.7 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17967786783154835		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.17967786783154835 | validation: 0.31828433283212465]
	TIME [epoch: 5.7 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1729717596502335		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.1729717596502335 | validation: 0.29083860419312624]
	TIME [epoch: 5.7 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17261887535857454		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.17261887535857454 | validation: 0.30663142157536866]
	TIME [epoch: 5.7 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16607750061581633		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.16607750061581633 | validation: 0.28999891605624906]
	TIME [epoch: 5.7 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17079334828198206		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.17079334828198206 | validation: 0.30025956773546236]
	TIME [epoch: 5.71 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17153777753347196		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.17153777753347196 | validation: 0.2897845590718517]
	TIME [epoch: 5.7 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17147600197395454		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.17147600197395454 | validation: 0.3027686523836665]
	TIME [epoch: 5.7 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16899339214086065		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.16899339214086065 | validation: 0.2876793013116146]
	TIME [epoch: 5.7 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17362064650755313		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.17362064650755313 | validation: 0.3013695036985322]
	TIME [epoch: 5.71 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16967997819791358		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.16967997819791358 | validation: 0.2904962000689012]
	TIME [epoch: 5.71 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1700026552704189		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.1700026552704189 | validation: 0.3069719352987666]
	TIME [epoch: 5.72 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1686085827584087		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.1686085827584087 | validation: 0.28702793491945444]
	TIME [epoch: 5.71 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1689911916387532		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.1689911916387532 | validation: 0.2975757112472973]
	TIME [epoch: 5.7 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16595955942437388		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.16595955942437388 | validation: 0.2958344420400855]
	TIME [epoch: 5.7 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1622011236051498		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.1622011236051498 | validation: 0.2893582995915413]
	TIME [epoch: 5.7 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16759257476044373		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.16759257476044373 | validation: 0.31482822652676995]
	TIME [epoch: 5.7 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16856459555354522		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.16856459555354522 | validation: 0.27789281159529067]
	TIME [epoch: 5.7 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1659071606812756		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.1659071606812756 | validation: 0.29942660098756757]
	TIME [epoch: 5.7 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16387881075767438		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.16387881075767438 | validation: 0.28183011865595714]
	TIME [epoch: 5.7 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16394728012575352		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.16394728012575352 | validation: 0.30378245900278134]
	TIME [epoch: 5.7 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1662203394519331		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.1662203394519331 | validation: 0.28457756820902375]
	TIME [epoch: 5.7 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16363118401174045		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.16363118401174045 | validation: 0.3027059004088759]
	TIME [epoch: 5.7 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1638431751853073		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.1638431751853073 | validation: 0.293679370705625]
	TIME [epoch: 5.7 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15840172413445328		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.15840172413445328 | validation: 0.2897573391265457]
	TIME [epoch: 5.7 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15934616816604597		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.15934616816604597 | validation: 0.2873214692183885]
	TIME [epoch: 5.7 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16202097335892782		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.16202097335892782 | validation: 0.27915936968932714]
	TIME [epoch: 5.7 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16393941991850366		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.16393941991850366 | validation: 0.3083234048906629]
	TIME [epoch: 5.7 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17932439076781187		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.17932439076781187 | validation: 0.2626971522756299]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1115.pth
	Model improved!!!
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18270555192842364		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.18270555192842364 | validation: 0.30000064050880665]
	TIME [epoch: 5.7 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16138287659547115		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.16138287659547115 | validation: 0.29896977600862684]
	TIME [epoch: 5.7 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16359967829825425		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.16359967829825425 | validation: 0.2828448051110441]
	TIME [epoch: 5.7 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16196114423187855		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.16196114423187855 | validation: 0.30117015082775306]
	TIME [epoch: 5.7 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1603268706067435		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.1603268706067435 | validation: 0.2902493162769583]
	TIME [epoch: 5.7 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1628430686901363		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.1628430686901363 | validation: 0.2903106712716183]
	TIME [epoch: 5.7 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15969078816666038		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.15969078816666038 | validation: 0.2864588757650943]
	TIME [epoch: 5.7 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16143944259515908		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.16143944259515908 | validation: 0.28432496358420833]
	TIME [epoch: 5.71 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15958407092649143		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.15958407092649143 | validation: 0.2920677970507617]
	TIME [epoch: 5.71 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1589166184485446		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.1589166184485446 | validation: 0.2896914495096119]
	TIME [epoch: 5.7 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1579030482120623		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.1579030482120623 | validation: 0.2790529696663283]
	TIME [epoch: 5.7 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15750653589925193		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.15750653589925193 | validation: 0.28565000365757703]
	TIME [epoch: 5.69 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1573306527997833		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.1573306527997833 | validation: 0.2908682530427878]
	TIME [epoch: 5.7 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15657030100708083		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.15657030100708083 | validation: 0.2779081565882619]
	TIME [epoch: 5.69 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15738324526761957		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.15738324526761957 | validation: 0.2934525917590036]
	TIME [epoch: 5.7 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1611032485796772		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.1611032485796772 | validation: 0.272011590825754]
	TIME [epoch: 5.7 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1717140400525042		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.1717140400525042 | validation: 0.30890168237346816]
	TIME [epoch: 5.7 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16614605838311444		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.16614605838311444 | validation: 0.2788373049247036]
	TIME [epoch: 5.7 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16029553736369054		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.16029553736369054 | validation: 0.2841812996226167]
	TIME [epoch: 5.69 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.153807468618898		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.153807468618898 | validation: 0.29447930837709313]
	TIME [epoch: 5.7 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1550321739579559		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.1550321739579559 | validation: 0.2780302718023662]
	TIME [epoch: 5.7 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1578171831746793		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.1578171831746793 | validation: 0.2991462107006433]
	TIME [epoch: 5.69 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15693991093310444		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.15693991093310444 | validation: 0.27138248423423633]
	TIME [epoch: 5.7 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16264497795692773		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.16264497795692773 | validation: 0.2871997267301531]
	TIME [epoch: 5.69 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15338322988354264		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.15338322988354264 | validation: 0.28967174318815053]
	TIME [epoch: 5.7 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15439466246904288		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.15439466246904288 | validation: 0.2851036507792139]
	TIME [epoch: 5.7 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15519997311131462		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.15519997311131462 | validation: 0.2875628283287332]
	TIME [epoch: 6.48 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15381734343385986		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.15381734343385986 | validation: 0.27419961175251173]
	TIME [epoch: 5.7 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15490411643990798		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.15490411643990798 | validation: 0.29513761792307963]
	TIME [epoch: 5.7 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1576371639065065		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.1576371639065065 | validation: 0.26963908286436516]
	TIME [epoch: 5.69 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1586787057580343		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.1586787057580343 | validation: 0.28334318935568176]
	TIME [epoch: 5.7 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15500397068768823		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.15500397068768823 | validation: 0.2780205630126864]
	TIME [epoch: 5.69 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15134133947960654		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.15134133947960654 | validation: 0.2815863432614976]
	TIME [epoch: 5.7 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15469719583006908		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.15469719583006908 | validation: 0.27684001359491567]
	TIME [epoch: 5.69 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15411619105558866		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.15411619105558866 | validation: 0.2862426183016149]
	TIME [epoch: 5.7 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15568794331550062		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.15568794331550062 | validation: 0.2787154765552942]
	TIME [epoch: 5.69 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1545801720207607		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.1545801720207607 | validation: 0.2928976649211285]
	TIME [epoch: 5.7 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15530349320807		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.15530349320807 | validation: 0.2711242972793512]
	TIME [epoch: 5.69 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15641393825343028		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.15641393825343028 | validation: 0.29912695528089683]
	TIME [epoch: 5.71 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15494416195163868		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.15494416195163868 | validation: 0.2741597466161973]
	TIME [epoch: 5.7 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15759100630801626		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.15759100630801626 | validation: 0.27930262099899394]
	TIME [epoch: 5.7 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15729590022414539		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.15729590022414539 | validation: 0.2718359643318628]
	TIME [epoch: 5.7 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1502413788327389		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.1502413788327389 | validation: 0.2909250738570803]
	TIME [epoch: 5.71 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15106217567448163		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.15106217567448163 | validation: 0.27899710681446543]
	TIME [epoch: 5.7 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14876834368248434		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.14876834368248434 | validation: 0.284468750580904]
	TIME [epoch: 5.71 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15385223602970058		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.15385223602970058 | validation: 0.26526968707989823]
	TIME [epoch: 5.7 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1551584532605375		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.1551584532605375 | validation: 0.29756861372522797]
	TIME [epoch: 5.71 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15771513154890354		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.15771513154890354 | validation: 0.2704050966722666]
	TIME [epoch: 5.69 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15321169508931554		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.15321169508931554 | validation: 0.2739176209972254]
	TIME [epoch: 5.71 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1510388112136091		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.1510388112136091 | validation: 0.28652281043451777]
	TIME [epoch: 5.7 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14831709892294762		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.14831709892294762 | validation: 0.2830978450582995]
	TIME [epoch: 5.74 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1507232473478238		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.1507232473478238 | validation: 0.2819901811558352]
	TIME [epoch: 5.7 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15281775668717928		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.15281775668717928 | validation: 0.2720513486622295]
	TIME [epoch: 5.71 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14714280761934911		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.14714280761934911 | validation: 0.2813407387078031]
	TIME [epoch: 5.69 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15127844488280393		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.15127844488280393 | validation: 0.2656570201206381]
	TIME [epoch: 5.71 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1504285275807173		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.1504285275807173 | validation: 0.29401154941308294]
	TIME [epoch: 5.69 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15041404747794462		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.15041404747794462 | validation: 0.27445039248182374]
	TIME [epoch: 5.7 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14928643670930392		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.14928643670930392 | validation: 0.2891073157940344]
	TIME [epoch: 5.7 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14764401872995234		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.14764401872995234 | validation: 0.2764831406643775]
	TIME [epoch: 5.7 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15274283691527152		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.15274283691527152 | validation: 0.28485098346824633]
	TIME [epoch: 5.7 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1508521892474796		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.1508521892474796 | validation: 0.27610519869539385]
	TIME [epoch: 5.71 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14897116016171089		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.14897116016171089 | validation: 0.2785286518256495]
	TIME [epoch: 5.7 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14673988133060847		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.14673988133060847 | validation: 0.2834952396298598]
	TIME [epoch: 5.71 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14824120812853606		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.14824120812853606 | validation: 0.2766032639597022]
	TIME [epoch: 5.7 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14780790764374005		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.14780790764374005 | validation: 0.280529336287968]
	TIME [epoch: 5.71 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1467035886156619		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.1467035886156619 | validation: 0.26775095330747006]
	TIME [epoch: 5.7 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15178511477506074		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.15178511477506074 | validation: 0.2882295942410476]
	TIME [epoch: 5.7 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1532175142682999		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.1532175142682999 | validation: 0.2758298341192269]
	TIME [epoch: 5.7 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1473532590727809		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.1473532590727809 | validation: 0.28679714248195254]
	TIME [epoch: 5.71 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1479918583809284		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.1479918583809284 | validation: 0.27165711784804775]
	TIME [epoch: 5.7 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14786503799657685		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.14786503799657685 | validation: 0.2835405173405636]
	TIME [epoch: 5.71 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14467503399794177		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.14467503399794177 | validation: 0.2697802661019538]
	TIME [epoch: 5.7 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14583909305260748		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.14583909305260748 | validation: 0.2809881084715345]
	TIME [epoch: 5.71 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14615957582136405		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.14615957582136405 | validation: 0.2724836042691706]
	TIME [epoch: 5.7 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14525653438757902		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.14525653438757902 | validation: 0.2777711610838471]
	TIME [epoch: 5.71 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14629455855412418		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.14629455855412418 | validation: 0.27106353421212526]
	TIME [epoch: 5.7 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1482979360031863		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.1482979360031863 | validation: 0.2789397312481587]
	TIME [epoch: 5.7 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1476750534082892		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.1476750534082892 | validation: 0.262417007334622]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1193.pth
	Model improved!!!
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14753974819384455		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.14753974819384455 | validation: 0.28614169321533484]
	TIME [epoch: 5.71 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14668277112793418		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.14668277112793418 | validation: 0.26886843226484863]
	TIME [epoch: 5.7 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14251717903126232		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.14251717903126232 | validation: 0.2801432953795347]
	TIME [epoch: 5.7 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14204165439429323		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.14204165439429323 | validation: 0.2722701139103308]
	TIME [epoch: 5.7 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14413045239860506		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.14413045239860506 | validation: 0.28452575533712743]
	TIME [epoch: 5.7 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14663584929821735		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.14663584929821735 | validation: 0.2600997270757367]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1199.pth
	Model improved!!!
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1532761002149554		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.1532761002149554 | validation: 0.27958969277425477]
	TIME [epoch: 5.7 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14657820361506282		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.14657820361506282 | validation: 0.26858678631397925]
	TIME [epoch: 5.7 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1465265941561413		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.1465265941561413 | validation: 0.285611749192444]
	TIME [epoch: 5.7 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14062555369745286		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.14062555369745286 | validation: 0.258614020676525]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1203.pth
	Model improved!!!
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14454631024081788		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.14454631024081788 | validation: 0.2812696092712236]
	TIME [epoch: 5.7 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14994438196253643		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.14994438196253643 | validation: 0.26089108855057946]
	TIME [epoch: 5.7 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14715613949242826		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.14715613949242826 | validation: 0.2825028074385586]
	TIME [epoch: 5.7 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1397216243955076		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.1397216243955076 | validation: 0.27432362418140493]
	TIME [epoch: 5.7 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14021656169702498		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.14021656169702498 | validation: 0.27140974982478233]
	TIME [epoch: 5.7 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14162602547260522		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.14162602547260522 | validation: 0.26642226694195265]
	TIME [epoch: 5.7 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14207282951858954		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.14207282951858954 | validation: 0.2820757100878743]
	TIME [epoch: 5.7 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14293898187883083		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.14293898187883083 | validation: 0.2594841782292182]
	TIME [epoch: 5.7 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14544144821292737		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.14544144821292737 | validation: 0.2902680017389326]
	TIME [epoch: 5.7 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14286970551079523		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.14286970551079523 | validation: 0.2615166417534023]
	TIME [epoch: 5.7 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1410666630829755		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.1410666630829755 | validation: 0.2780876557670374]
	TIME [epoch: 5.7 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13941946526654167		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.13941946526654167 | validation: 0.27447408586151845]
	TIME [epoch: 5.7 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1411691821989672		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.1411691821989672 | validation: 0.26663781078238247]
	TIME [epoch: 5.7 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13936006027947834		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.13936006027947834 | validation: 0.274517592505567]
	TIME [epoch: 5.7 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14244349479765148		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.14244349479765148 | validation: 0.26488967245789075]
	TIME [epoch: 5.7 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14003717696510135		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.14003717696510135 | validation: 0.27835411620815426]
	TIME [epoch: 5.7 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14265343413757334		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.14265343413757334 | validation: 0.26317306333415263]
	TIME [epoch: 5.7 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14216048774134873		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.14216048774134873 | validation: 0.27888325877135056]
	TIME [epoch: 5.7 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14332180315322485		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.14332180315322485 | validation: 0.2585606334126937]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1222.pth
	Model improved!!!
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13957726667070947		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.13957726667070947 | validation: 0.2753305181249779]
	TIME [epoch: 5.7 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13927762406128058		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.13927762406128058 | validation: 0.2523515110724911]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1224.pth
	Model improved!!!
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14913010954901013		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.14913010954901013 | validation: 0.2774124259331505]
	TIME [epoch: 5.7 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14056597571385834		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.14056597571385834 | validation: 0.27153050945577717]
	TIME [epoch: 5.71 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.139708399694445		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.139708399694445 | validation: 0.2714718838719821]
	TIME [epoch: 5.7 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13996259187026033		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.13996259187026033 | validation: 0.2704165792476176]
	TIME [epoch: 5.71 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13677856043010614		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.13677856043010614 | validation: 0.26501481708473845]
	TIME [epoch: 5.71 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13614560672255402		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.13614560672255402 | validation: 0.2668463778299377]
	TIME [epoch: 5.7 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13758837710643676		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.13758837710643676 | validation: 0.26887896354176427]
	TIME [epoch: 5.71 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13666894493132495		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.13666894493132495 | validation: 0.2657179595529881]
	TIME [epoch: 5.7 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13678921264684016		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.13678921264684016 | validation: 0.2691598606796907]
	TIME [epoch: 5.7 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13870806865062318		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.13870806865062318 | validation: 0.2591995918889365]
	TIME [epoch: 5.7 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13903823999153875		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.13903823999153875 | validation: 0.2823551236026267]
	TIME [epoch: 5.71 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14135817074253507		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.14135817074253507 | validation: 0.2492708987286222]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1236.pth
	Model improved!!!
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14546078892846578		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.14546078892846578 | validation: 0.2773126265480973]
	TIME [epoch: 5.7 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13922586902191575		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.13922586902191575 | validation: 0.2610960956557781]
	TIME [epoch: 5.7 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1374409583271174		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.1374409583271174 | validation: 0.2570559898913722]
	TIME [epoch: 5.7 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1356089239611925		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.1356089239611925 | validation: 0.2740036954924606]
	TIME [epoch: 5.7 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13630454178963822		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.13630454178963822 | validation: 0.26106388328018626]
	TIME [epoch: 5.7 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13465793462137682		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.13465793462137682 | validation: 0.2724632288861433]
	TIME [epoch: 5.71 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14105003358021276		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.14105003358021276 | validation: 0.24931502598005004]
	TIME [epoch: 5.7 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1420968041117134		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.1420968041117134 | validation: 0.27503833327996546]
	TIME [epoch: 5.7 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13772298094224814		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.13772298094224814 | validation: 0.2631481807332167]
	TIME [epoch: 5.7 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13590682740160723		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.13590682740160723 | validation: 0.26995382292461073]
	TIME [epoch: 5.69 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13533672285175705		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.13533672285175705 | validation: 0.25556477445986625]
	TIME [epoch: 5.7 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13478344268893705		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.13478344268893705 | validation: 0.26502557434150503]
	TIME [epoch: 5.7 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13366859143941753		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.13366859143941753 | validation: 0.25496424512581045]
	TIME [epoch: 5.71 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13554606987605233		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.13554606987605233 | validation: 0.2591611601325215]
	TIME [epoch: 5.71 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13422658285144143		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.13422658285144143 | validation: 0.27425968456137156]
	TIME [epoch: 5.7 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13413616659005598		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.13413616659005598 | validation: 0.2555396810568506]
	TIME [epoch: 5.71 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13263517829834168		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.13263517829834168 | validation: 0.2604808718026357]
	TIME [epoch: 5.7 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13264374317112318		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.13264374317112318 | validation: 0.25849340713429386]
	TIME [epoch: 5.7 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13430596519743446		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.13430596519743446 | validation: 0.2717032726134259]
	TIME [epoch: 5.7 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13521564661672295		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.13521564661672295 | validation: 0.26206293727173563]
	TIME [epoch: 5.7 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13367971532908896		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.13367971532908896 | validation: 0.2627143966587224]
	TIME [epoch: 5.7 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13090608119951008		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.13090608119951008 | validation: 0.26352666519914136]
	TIME [epoch: 5.7 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1338526327447098		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.1338526327447098 | validation: 0.2671512093234528]
	TIME [epoch: 5.7 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13683069898629757		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.13683069898629757 | validation: 0.2514035958162694]
	TIME [epoch: 5.7 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1377270019550202		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.1377270019550202 | validation: 0.26473684431783123]
	TIME [epoch: 5.7 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13504850173902067		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.13504850173902067 | validation: 0.260830862831439]
	TIME [epoch: 5.7 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13354232164690388		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.13354232164690388 | validation: 0.2681858273861978]
	TIME [epoch: 5.71 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13213725312216304		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.13213725312216304 | validation: 0.2707059207759381]
	TIME [epoch: 5.7 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13321510317930665		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.13321510317930665 | validation: 0.2664953862267018]
	TIME [epoch: 5.7 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13342073936667476		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.13342073936667476 | validation: 0.2581465555895052]
	TIME [epoch: 5.7 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13202189908310452		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.13202189908310452 | validation: 0.2614280202916885]
	TIME [epoch: 5.7 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13236997732679798		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.13236997732679798 | validation: 0.2677381776393623]
	TIME [epoch: 5.7 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1338387308926713		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.1338387308926713 | validation: 0.2537894706194887]
	TIME [epoch: 5.7 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13455652993235573		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.13455652993235573 | validation: 0.27227173837423496]
	TIME [epoch: 5.7 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13498584255062818		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.13498584255062818 | validation: 0.2458222903632683]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1271.pth
	Model improved!!!
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1360488062409995		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.1360488062409995 | validation: 0.2750193021110727]
	TIME [epoch: 5.73 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13351605956601698		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.13351605956601698 | validation: 0.25320244998190705]
	TIME [epoch: 5.74 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13132768696045705		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.13132768696045705 | validation: 0.2604614914186913]
	TIME [epoch: 5.7 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1318273633438166		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.1318273633438166 | validation: 0.26700699856516447]
	TIME [epoch: 5.7 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13158692240136088		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.13158692240136088 | validation: 0.2545889767242054]
	TIME [epoch: 5.7 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13152311854601867		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.13152311854601867 | validation: 0.255676079514282]
	TIME [epoch: 5.7 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13169420705593857		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.13169420705593857 | validation: 0.2683122231113519]
	TIME [epoch: 5.71 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.132224864991785		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.132224864991785 | validation: 0.26271385121606905]
	TIME [epoch: 5.7 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13324622566404976		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.13324622566404976 | validation: 0.27133513510288393]
	TIME [epoch: 5.7 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1348991361535411		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.1348991361535411 | validation: 0.242931643980489]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1281.pth
	Model improved!!!
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1366419599989583		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.1366419599989583 | validation: 0.26412792112840544]
	TIME [epoch: 5.73 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13243446101910217		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.13243446101910217 | validation: 0.24705379924597515]
	TIME [epoch: 5.73 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1326749470290364		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.1326749470290364 | validation: 0.25625517957495286]
	TIME [epoch: 5.73 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12892643320764904		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.12892643320764904 | validation: 0.2602882752416752]
	TIME [epoch: 5.72 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12882868321537597		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.12882868321537597 | validation: 0.2578795972392977]
	TIME [epoch: 5.73 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12997851824303147		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.12997851824303147 | validation: 0.26492477129255815]
	TIME [epoch: 5.73 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12896601499004987		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.12896601499004987 | validation: 0.2674465864410013]
	TIME [epoch: 5.7 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1296536297960642		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.1296536297960642 | validation: 0.24781197680645584]
	TIME [epoch: 5.71 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13149750113258438		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.13149750113258438 | validation: 0.269958238646675]
	TIME [epoch: 5.7 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13564332311290694		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.13564332311290694 | validation: 0.24019714913848755]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1291.pth
	Model improved!!!
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13295800660558604		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.13295800660558604 | validation: 0.25752226305068576]
	TIME [epoch: 5.73 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12981121167632684		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.12981121167632684 | validation: 0.25645226019083545]
	TIME [epoch: 5.73 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12872931596734483		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.12872931596734483 | validation: 0.24663598757433836]
	TIME [epoch: 5.73 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1303226701016984		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.1303226701016984 | validation: 0.25570306749746197]
	TIME [epoch: 5.72 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1266507841458493		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.1266507841458493 | validation: 0.2562398117324673]
	TIME [epoch: 5.74 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1289791233123441		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.1289791233123441 | validation: 0.25454250204981815]
	TIME [epoch: 5.75 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12928659404490947		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.12928659404490947 | validation: 0.26243306530339083]
	TIME [epoch: 5.73 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1287722211620238		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.1287722211620238 | validation: 0.2396334826798331]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1299.pth
	Model improved!!!
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1315404155316676		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.1315404155316676 | validation: 0.26453442006714123]
	TIME [epoch: 5.74 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1298989546223221		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.1298989546223221 | validation: 0.2391113092629453]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1301.pth
	Model improved!!!
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12978752608550845		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.12978752608550845 | validation: 0.2661789515669941]
	TIME [epoch: 5.74 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12824113443952487		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.12824113443952487 | validation: 0.250434648686066]
	TIME [epoch: 5.73 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12866555556249384		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.12866555556249384 | validation: 0.2485581843830342]
	TIME [epoch: 5.73 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1288613599198334		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.1288613599198334 | validation: 0.25716663077436114]
	TIME [epoch: 5.74 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1292332437947874		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.1292332437947874 | validation: 0.24583904402409315]
	TIME [epoch: 5.73 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12852356793954012		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.12852356793954012 | validation: 0.2663124010061509]
	TIME [epoch: 5.73 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12634671796989932		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.12634671796989932 | validation: 0.2552100206967644]
	TIME [epoch: 5.73 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1255729521336458		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.1255729521336458 | validation: 0.2573410112616026]
	TIME [epoch: 5.73 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1297219952847187		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.1297219952847187 | validation: 0.26229217278513045]
	TIME [epoch: 5.73 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12974646638557277		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.12974646638557277 | validation: 0.2533487507430639]
	TIME [epoch: 5.74 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1284661795486395		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.1284661795486395 | validation: 0.2632402520675187]
	TIME [epoch: 5.73 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12920188562471968		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.12920188562471968 | validation: 0.24977768449168633]
	TIME [epoch: 5.73 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12831661037188144		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.12831661037188144 | validation: 0.26100043902040887]
	TIME [epoch: 5.74 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12881535591844137		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.12881535591844137 | validation: 0.2532942254876815]
	TIME [epoch: 5.74 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1256981894231225		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.1256981894231225 | validation: 0.24564855055433263]
	TIME [epoch: 5.73 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12527988934546563		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.12527988934546563 | validation: 0.2548909043571203]
	TIME [epoch: 5.74 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1271342293636189		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.1271342293636189 | validation: 0.2480554761894542]
	TIME [epoch: 5.73 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1274446958790499		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.1274446958790499 | validation: 0.24941516856407042]
	TIME [epoch: 5.73 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1258970164079485		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.1258970164079485 | validation: 0.24986400604314116]
	TIME [epoch: 5.73 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12655077031735076		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.12655077031735076 | validation: 0.2584017781846149]
	TIME [epoch: 5.73 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12432183981488723		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.12432183981488723 | validation: 0.24495798174177807]
	TIME [epoch: 5.73 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12653627870642878		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.12653627870642878 | validation: 0.26106548586881917]
	TIME [epoch: 5.73 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12965714697717065		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.12965714697717065 | validation: 0.24995942266879176]
	TIME [epoch: 5.72 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12660813550334898		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.12660813550334898 | validation: 0.2549856531982046]
	TIME [epoch: 5.73 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12666888144320188		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.12666888144320188 | validation: 0.24229809972949423]
	TIME [epoch: 5.73 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12719028593051937		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.12719028593051937 | validation: 0.2526872237176341]
	TIME [epoch: 5.73 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1259566401336059		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.1259566401336059 | validation: 0.24542859311801105]
	TIME [epoch: 5.72 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12448723658877044		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.12448723658877044 | validation: 0.2466230593026184]
	TIME [epoch: 5.73 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12601705054090823		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.12601705054090823 | validation: 0.2485316290382219]
	TIME [epoch: 5.73 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12355787501644479		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.12355787501644479 | validation: 0.24086371838651202]
	TIME [epoch: 5.72 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12458357322728039		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.12458357322728039 | validation: 0.2425520459643322]
	TIME [epoch: 5.73 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12450965431947364		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.12450965431947364 | validation: 0.2505393021784951]
	TIME [epoch: 5.71 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12529814959794783		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.12529814959794783 | validation: 0.23675978381262192]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1334.pth
	Model improved!!!
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12800524441019248		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.12800524441019248 | validation: 0.25519456926231315]
	TIME [epoch: 5.73 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12479889154611941		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.12479889154611941 | validation: 0.24404963353475173]
	TIME [epoch: 5.73 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12819480838543793		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.12819480838543793 | validation: 0.25705081392507784]
	TIME [epoch: 5.73 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12276252327119384		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.12276252327119384 | validation: 0.2516270082142878]
	TIME [epoch: 5.73 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12265534090106982		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.12265534090106982 | validation: 0.2457271775918236]
	TIME [epoch: 5.72 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12259332133233146		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.12259332133233146 | validation: 0.25330960311552797]
	TIME [epoch: 5.73 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12038347791195542		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.12038347791195542 | validation: 0.22994273213976238]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1341.pth
	Model improved!!!
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12431706746671381		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.12431706746671381 | validation: 0.24295750305502342]
	TIME [epoch: 5.72 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12040127912295774		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.12040127912295774 | validation: 0.2483236133769866]
	TIME [epoch: 5.72 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12535398355030367		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.12535398355030367 | validation: 0.2478191900787428]
	TIME [epoch: 5.72 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12201065174874581		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.12201065174874581 | validation: 0.23956676484195089]
	TIME [epoch: 5.71 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12430300049424112		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.12430300049424112 | validation: 0.25290884984147155]
	TIME [epoch: 5.72 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12327853554971459		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.12327853554971459 | validation: 0.2431735619542046]
	TIME [epoch: 5.73 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12281342872113776		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.12281342872113776 | validation: 0.2612108489994673]
	TIME [epoch: 5.73 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12500042293106717		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.12500042293106717 | validation: 0.2324682196291481]
	TIME [epoch: 5.73 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1259867407888703		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.1259867407888703 | validation: 0.26349385116099705]
	TIME [epoch: 5.73 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.125509130435766		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.125509130435766 | validation: 0.2360329568349406]
	TIME [epoch: 5.74 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12435796761647883		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.12435796761647883 | validation: 0.2525844015215291]
	TIME [epoch: 5.73 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1231689558576901		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.1231689558576901 | validation: 0.24430590031965985]
	TIME [epoch: 5.73 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12308939505778074		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.12308939505778074 | validation: 0.24340305903707976]
	TIME [epoch: 5.73 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12282722633590461		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.12282722633590461 | validation: 0.24576783966312699]
	TIME [epoch: 5.73 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.120497711722449		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.120497711722449 | validation: 0.2485012998916632]
	TIME [epoch: 5.74 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12061375123681391		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.12061375123681391 | validation: 0.24679863239142438]
	TIME [epoch: 5.73 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12122699963811882		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.12122699963811882 | validation: 0.2449912300213736]
	TIME [epoch: 5.73 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1237012456571668		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.1237012456571668 | validation: 0.24441187077684862]
	TIME [epoch: 5.73 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12076124554354724		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.12076124554354724 | validation: 0.22650776778113935]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1360.pth
	Model improved!!!
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1231607720776983		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.1231607720776983 | validation: 0.26021886822865525]
	TIME [epoch: 5.73 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12381667271071102		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.12381667271071102 | validation: 0.22927387856193004]
	TIME [epoch: 5.72 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1245993539684234		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.1245993539684234 | validation: 0.2486721544672832]
	TIME [epoch: 5.72 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12294883962553745		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.12294883962553745 | validation: 0.25560415510351264]
	TIME [epoch: 5.71 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12131019022997958		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.12131019022997958 | validation: 0.24956857808751487]
	TIME [epoch: 5.71 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11786965591677089		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.11786965591677089 | validation: 0.24131221614506343]
	TIME [epoch: 5.72 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1207983178881063		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.1207983178881063 | validation: 0.23469344561074196]
	TIME [epoch: 5.71 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1219671010142573		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.1219671010142573 | validation: 0.25614411694115974]
	TIME [epoch: 5.71 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12222850504252708		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.12222850504252708 | validation: 0.24415951874682884]
	TIME [epoch: 5.71 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12333511808195263		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.12333511808195263 | validation: 0.24075294601102684]
	TIME [epoch: 5.73 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12270005441120868		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.12270005441120868 | validation: 0.2233152782294302]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1371.pth
	Model improved!!!
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12121354463389125		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.12121354463389125 | validation: 0.24200771478170235]
	TIME [epoch: 5.72 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12034392090898394		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.12034392090898394 | validation: 0.24802900990298796]
	TIME [epoch: 5.72 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1215111755551721		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.1215111755551721 | validation: 0.23199030968149625]
	TIME [epoch: 5.71 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11909659611571487		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.11909659611571487 | validation: 0.24758575093722587]
	TIME [epoch: 5.72 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12099242353342499		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.12099242353342499 | validation: 0.24354726704390864]
	TIME [epoch: 5.72 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11913529129189175		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.11913529129189175 | validation: 0.22976338293452514]
	TIME [epoch: 5.72 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11997342982015735		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.11997342982015735 | validation: 0.2436734837045986]
	TIME [epoch: 5.71 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12038125536935938		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.12038125536935938 | validation: 0.2499103887435343]
	TIME [epoch: 5.72 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12091116809383408		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.12091116809383408 | validation: 0.2348611694301268]
	TIME [epoch: 5.72 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11958644838575384		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.11958644838575384 | validation: 0.2532818375419742]
	TIME [epoch: 5.72 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12109086316112169		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.12109086316112169 | validation: 0.23157914459522086]
	TIME [epoch: 5.73 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12203450870033637		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.12203450870033637 | validation: 0.24696953544616848]
	TIME [epoch: 5.72 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1201448513380224		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.1201448513380224 | validation: 0.23366351258904594]
	TIME [epoch: 5.73 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12030434870407579		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.12030434870407579 | validation: 0.24189895990859256]
	TIME [epoch: 5.72 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11939337367485602		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.11939337367485602 | validation: 0.23534961646527253]
	TIME [epoch: 5.72 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11675411525883146		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.11675411525883146 | validation: 0.24653488632928236]
	TIME [epoch: 5.73 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1180834889148129		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.1180834889148129 | validation: 0.2370911628335436]
	TIME [epoch: 5.73 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11888884073973936		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.11888884073973936 | validation: 0.22543925123139125]
	TIME [epoch: 5.72 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11866119454664724		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.11866119454664724 | validation: 0.2491662058334761]
	TIME [epoch: 5.72 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11712063890791216		[learning rate: 8.6788e-05]
	Learning Rate: 8.67884e-05
	LOSS [training: 0.11712063890791216 | validation: 0.22626894089940236]
	TIME [epoch: 5.73 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1209520754038933		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.1209520754038933 | validation: 0.2402710617734102]
	TIME [epoch: 5.72 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11973054973529132		[learning rate: 8.6176e-05]
	Learning Rate: 8.61757e-05
	LOSS [training: 0.11973054973529132 | validation: 0.24082599077937303]
	TIME [epoch: 5.72 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12264130845341545		[learning rate: 8.5871e-05]
	Learning Rate: 8.5871e-05
	LOSS [training: 0.12264130845341545 | validation: 0.2132713279085232]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1394.pth
	Model improved!!!
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12636139687244188		[learning rate: 8.5567e-05]
	Learning Rate: 8.55673e-05
	LOSS [training: 0.12636139687244188 | validation: 0.23658394274133]
	TIME [epoch: 5.72 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11684040606915978		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.11684040606915978 | validation: 0.24670898285489226]
	TIME [epoch: 5.71 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12163893474203846		[learning rate: 8.4963e-05]
	Learning Rate: 8.49632e-05
	LOSS [training: 0.12163893474203846 | validation: 0.23172856189745758]
	TIME [epoch: 5.72 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11817071178106586		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.11817071178106586 | validation: 0.2356062812355798]
	TIME [epoch: 5.72 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11769345233559386		[learning rate: 8.4363e-05]
	Learning Rate: 8.43634e-05
	LOSS [training: 0.11769345233559386 | validation: 0.24280011134377882]
	TIME [epoch: 5.71 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11698802653603625		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.11698802653603625 | validation: 0.22715125793984506]
	TIME [epoch: 5.7 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1164253920101336		[learning rate: 8.3768e-05]
	Learning Rate: 8.37678e-05
	LOSS [training: 0.1164253920101336 | validation: 0.2460208557048696]
	TIME [epoch: 5.73 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11676617127991872		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.11676617127991872 | validation: 0.24288684987858586]
	TIME [epoch: 5.74 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11732876300181917		[learning rate: 8.3176e-05]
	Learning Rate: 8.31764e-05
	LOSS [training: 0.11732876300181917 | validation: 0.22930515563456366]
	TIME [epoch: 5.71 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11733370182020361		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.11733370182020361 | validation: 0.2328319020619524]
	TIME [epoch: 5.72 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11686680325537747		[learning rate: 8.2589e-05]
	Learning Rate: 8.25892e-05
	LOSS [training: 0.11686680325537747 | validation: 0.22728815680366377]
	TIME [epoch: 5.72 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11698609693219109		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.11698609693219109 | validation: 0.240219539964943]
	TIME [epoch: 5.71 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11519391545946932		[learning rate: 8.2006e-05]
	Learning Rate: 8.20061e-05
	LOSS [training: 0.11519391545946932 | validation: 0.23503396541166188]
	TIME [epoch: 5.71 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11817176743540657		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.11817176743540657 | validation: 0.2408109624068503]
	TIME [epoch: 5.71 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11713476014002805		[learning rate: 8.1427e-05]
	Learning Rate: 8.14272e-05
	LOSS [training: 0.11713476014002805 | validation: 0.23514850659403824]
	TIME [epoch: 5.71 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11778179945569736		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.11778179945569736 | validation: 0.23071915493590753]
	TIME [epoch: 5.7 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11871853994171762		[learning rate: 8.0852e-05]
	Learning Rate: 8.08523e-05
	LOSS [training: 0.11871853994171762 | validation: 0.24335217794700537]
	TIME [epoch: 5.71 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11931762400085023		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.11931762400085023 | validation: 0.23003742846005293]
	TIME [epoch: 5.71 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11696109360894717		[learning rate: 8.0281e-05]
	Learning Rate: 8.02815e-05
	LOSS [training: 0.11696109360894717 | validation: 0.23208966580313162]
	TIME [epoch: 5.72 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11649685597263351		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.11649685597263351 | validation: 0.2293475593924825]
	TIME [epoch: 5.71 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1165317412068025		[learning rate: 7.9715e-05]
	Learning Rate: 7.97147e-05
	LOSS [training: 0.1165317412068025 | validation: 0.24717378842002127]
	TIME [epoch: 5.72 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12157602285935194		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.12157602285935194 | validation: 0.2273861562608971]
	TIME [epoch: 5.71 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11776502273777326		[learning rate: 7.9152e-05]
	Learning Rate: 7.9152e-05
	LOSS [training: 0.11776502273777326 | validation: 0.2446456032529869]
	TIME [epoch: 5.71 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11728827049001601		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.11728827049001601 | validation: 0.23507631769402276]
	TIME [epoch: 5.71 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11614884027545458		[learning rate: 7.8593e-05]
	Learning Rate: 7.85931e-05
	LOSS [training: 0.11614884027545458 | validation: 0.22953230394536536]
	TIME [epoch: 5.71 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1164694602478371		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.1164694602478371 | validation: 0.24064830470550547]
	TIME [epoch: 5.71 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11531605781039606		[learning rate: 7.8038e-05]
	Learning Rate: 7.80383e-05
	LOSS [training: 0.11531605781039606 | validation: 0.23158364556074168]
	TIME [epoch: 5.71 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11639127218367795		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.11639127218367795 | validation: 0.22996491692642104]
	TIME [epoch: 5.7 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11817446163196225		[learning rate: 7.7487e-05]
	Learning Rate: 7.74873e-05
	LOSS [training: 0.11817446163196225 | validation: 0.21925201756757928]
	TIME [epoch: 5.71 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1155967120838826		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.1155967120838826 | validation: 0.2401263713328842]
	TIME [epoch: 5.71 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11537783054674357		[learning rate: 7.694e-05]
	Learning Rate: 7.69403e-05
	LOSS [training: 0.11537783054674357 | validation: 0.22048918148229504]
	TIME [epoch: 5.71 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11542449323716451		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.11542449323716451 | validation: 0.24381000751278836]
	TIME [epoch: 5.71 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11704884342140803		[learning rate: 7.6397e-05]
	Learning Rate: 7.63971e-05
	LOSS [training: 0.11704884342140803 | validation: 0.2219452916129727]
	TIME [epoch: 5.71 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11775324185629689		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.11775324185629689 | validation: 0.2389451222386415]
	TIME [epoch: 5.72 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11548373942003788		[learning rate: 7.5858e-05]
	Learning Rate: 7.58578e-05
	LOSS [training: 0.11548373942003788 | validation: 0.23048649601064014]
	TIME [epoch: 5.71 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11578320699561817		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.11578320699561817 | validation: 0.2331888757456568]
	TIME [epoch: 5.73 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11583740407903027		[learning rate: 7.5322e-05]
	Learning Rate: 7.53222e-05
	LOSS [training: 0.11583740407903027 | validation: 0.22617369681258112]
	TIME [epoch: 5.71 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11646801670745728		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.11646801670745728 | validation: 0.23452668742862315]
	TIME [epoch: 5.73 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11555012635677732		[learning rate: 7.479e-05]
	Learning Rate: 7.47905e-05
	LOSS [training: 0.11555012635677732 | validation: 0.2359634397800587]
	TIME [epoch: 5.74 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1147819659419135		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.1147819659419135 | validation: 0.22762459617356862]
	TIME [epoch: 5.74 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11482019950824973		[learning rate: 7.4262e-05]
	Learning Rate: 7.42625e-05
	LOSS [training: 0.11482019950824973 | validation: 0.2262116097293106]
	TIME [epoch: 5.73 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11638349333948732		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.11638349333948732 | validation: 0.22558119971616725]
	TIME [epoch: 5.73 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11489104753231752		[learning rate: 7.3738e-05]
	Learning Rate: 7.37382e-05
	LOSS [training: 0.11489104753231752 | validation: 0.23897503737322007]
	TIME [epoch: 5.73 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11424948147469334		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.11424948147469334 | validation: 0.21484500511659702]
	TIME [epoch: 5.73 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11687766358567131		[learning rate: 7.3218e-05]
	Learning Rate: 7.32176e-05
	LOSS [training: 0.11687766358567131 | validation: 0.2290043047585324]
	TIME [epoch: 5.74 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11516857536606238		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.11516857536606238 | validation: 0.22750766895319177]
	TIME [epoch: 5.73 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1135125479151195		[learning rate: 7.2701e-05]
	Learning Rate: 7.27007e-05
	LOSS [training: 0.1135125479151195 | validation: 0.2361602944551261]
	TIME [epoch: 5.73 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11426239542871276		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.11426239542871276 | validation: 0.23922291203032034]
	TIME [epoch: 5.73 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11574970027619685		[learning rate: 7.2187e-05]
	Learning Rate: 7.21874e-05
	LOSS [training: 0.11574970027619685 | validation: 0.21708512028175408]
	TIME [epoch: 5.73 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11861416435539159		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.11861416435539159 | validation: 0.2342966288311005]
	TIME [epoch: 5.73 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11497072929549985		[learning rate: 7.1678e-05]
	Learning Rate: 7.16778e-05
	LOSS [training: 0.11497072929549985 | validation: 0.22956669168420493]
	TIME [epoch: 5.73 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11535193352812709		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.11535193352812709 | validation: 0.22738095496349464]
	TIME [epoch: 5.73 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11281341740366842		[learning rate: 7.1172e-05]
	Learning Rate: 7.11718e-05
	LOSS [training: 0.11281341740366842 | validation: 0.2396321556623261]
	TIME [epoch: 5.72 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11367032715099866		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.11367032715099866 | validation: 0.23080816650942132]
	TIME [epoch: 5.74 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11407002221724813		[learning rate: 7.0669e-05]
	Learning Rate: 7.06693e-05
	LOSS [training: 0.11407002221724813 | validation: 0.2199642358360789]
	TIME [epoch: 5.73 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11517621857717616		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.11517621857717616 | validation: 0.23389307643088203]
	TIME [epoch: 5.72 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11525120889963475		[learning rate: 7.017e-05]
	Learning Rate: 7.01704e-05
	LOSS [training: 0.11525120889963475 | validation: 0.2277081298410769]
	TIME [epoch: 5.72 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11467114005827905		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.11467114005827905 | validation: 0.23565896606289527]
	TIME [epoch: 5.72 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11165548047369434		[learning rate: 6.9675e-05]
	Learning Rate: 6.9675e-05
	LOSS [training: 0.11165548047369434 | validation: 0.22984906944179048]
	TIME [epoch: 5.71 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11474799549936104		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.11474799549936104 | validation: 0.22710117697077747]
	TIME [epoch: 5.72 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11278394805737367		[learning rate: 6.9183e-05]
	Learning Rate: 6.91831e-05
	LOSS [training: 0.11278394805737367 | validation: 0.22449497620820422]
	TIME [epoch: 5.71 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11302185876761739		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.11302185876761739 | validation: 0.23479082965806786]
	TIME [epoch: 5.72 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11600189705648731		[learning rate: 6.8695e-05]
	Learning Rate: 6.86947e-05
	LOSS [training: 0.11600189705648731 | validation: 0.22029567860481217]
	TIME [epoch: 5.73 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11601071120680967		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.11601071120680967 | validation: 0.23531198200232295]
	TIME [epoch: 5.71 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11331488459776111		[learning rate: 6.821e-05]
	Learning Rate: 6.82097e-05
	LOSS [training: 0.11331488459776111 | validation: 0.2242792673255726]
	TIME [epoch: 5.73 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1146662436396787		[learning rate: 6.7969e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.1146662436396787 | validation: 0.21600081649891845]
	TIME [epoch: 5.72 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11370557116708108		[learning rate: 6.7728e-05]
	Learning Rate: 6.77282e-05
	LOSS [training: 0.11370557116708108 | validation: 0.22939818410119772]
	TIME [epoch: 5.72 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11363234419262512		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.11363234419262512 | validation: 0.22426700033856417]
	TIME [epoch: 5.72 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11425143221708836		[learning rate: 6.725e-05]
	Learning Rate: 6.725e-05
	LOSS [training: 0.11425143221708836 | validation: 0.22535587704055982]
	TIME [epoch: 5.72 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11181437571554527		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.11181437571554527 | validation: 0.22791741345970618]
	TIME [epoch: 5.72 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11481689098448991		[learning rate: 6.6775e-05]
	Learning Rate: 6.67752e-05
	LOSS [training: 0.11481689098448991 | validation: 0.22945440310137571]
	TIME [epoch: 5.72 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11299111851155497		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.11299111851155497 | validation: 0.22257015530340918]
	TIME [epoch: 5.71 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11289826258243175		[learning rate: 6.6304e-05]
	Learning Rate: 6.63038e-05
	LOSS [training: 0.11289826258243175 | validation: 0.2280576909361992]
	TIME [epoch: 5.72 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11303725601337312		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.11303725601337312 | validation: 0.24112740937067434]
	TIME [epoch: 5.73 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11419209116272429		[learning rate: 6.5836e-05]
	Learning Rate: 6.58357e-05
	LOSS [training: 0.11419209116272429 | validation: 0.22293220305502393]
	TIME [epoch: 5.73 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11393777648039075		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.11393777648039075 | validation: 0.21619201355637305]
	TIME [epoch: 5.73 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11144283859381054		[learning rate: 6.5371e-05]
	Learning Rate: 6.53709e-05
	LOSS [training: 0.11144283859381054 | validation: 0.23928649380684078]
	TIME [epoch: 5.72 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11118401496687148		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.11118401496687148 | validation: 0.22327673405200202]
	TIME [epoch: 5.72 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11296872627706911		[learning rate: 6.4909e-05]
	Learning Rate: 6.49094e-05
	LOSS [training: 0.11296872627706911 | validation: 0.2241285472381435]
	TIME [epoch: 5.72 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11401640815870887		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.11401640815870887 | validation: 0.22522031628652278]
	TIME [epoch: 5.72 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11235641897047692		[learning rate: 6.4451e-05]
	Learning Rate: 6.44512e-05
	LOSS [training: 0.11235641897047692 | validation: 0.21987625075531245]
	TIME [epoch: 5.72 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1129820846381342		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.1129820846381342 | validation: 0.2202369387957092]
	TIME [epoch: 5.72 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10982405167697826		[learning rate: 6.3996e-05]
	Learning Rate: 6.39962e-05
	LOSS [training: 0.10982405167697826 | validation: 0.2362933198017473]
	TIME [epoch: 5.72 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11259305410961425		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.11259305410961425 | validation: 0.22797075367048666]
	TIME [epoch: 5.72 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11117245284665711		[learning rate: 6.3544e-05]
	Learning Rate: 6.35444e-05
	LOSS [training: 0.11117245284665711 | validation: 0.23102190602567232]
	TIME [epoch: 5.72 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10963025725692696		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.10963025725692696 | validation: 0.22687562532820182]
	TIME [epoch: 5.72 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1088996581254095		[learning rate: 6.3096e-05]
	Learning Rate: 6.30958e-05
	LOSS [training: 0.1088996581254095 | validation: 0.21866036358188667]
	TIME [epoch: 5.72 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11022398619081364		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.11022398619081364 | validation: 0.22157508452205887]
	TIME [epoch: 5.72 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11245661229566527		[learning rate: 6.265e-05]
	Learning Rate: 6.26503e-05
	LOSS [training: 0.11245661229566527 | validation: 0.22811237084669944]
	TIME [epoch: 5.72 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11278690245805638		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.11278690245805638 | validation: 0.23016630020400855]
	TIME [epoch: 5.71 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11292905323583213		[learning rate: 6.2208e-05]
	Learning Rate: 6.2208e-05
	LOSS [training: 0.11292905323583213 | validation: 0.23007397169647664]
	TIME [epoch: 5.72 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11171895114105493		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.11171895114105493 | validation: 0.22208964555755195]
	TIME [epoch: 5.72 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10988386911416498		[learning rate: 6.1769e-05]
	Learning Rate: 6.17688e-05
	LOSS [training: 0.10988386911416498 | validation: 0.22938888905501298]
	TIME [epoch: 5.72 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11119902061697846		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.11119902061697846 | validation: 0.2245771936498202]
	TIME [epoch: 5.72 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11010557973438295		[learning rate: 6.1333e-05]
	Learning Rate: 6.13327e-05
	LOSS [training: 0.11010557973438295 | validation: 0.22462812081954633]
	TIME [epoch: 5.72 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11103762723290149		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.11103762723290149 | validation: 0.2286461593223705]
	TIME [epoch: 5.72 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11178894626678083		[learning rate: 6.09e-05]
	Learning Rate: 6.08998e-05
	LOSS [training: 0.11178894626678083 | validation: 0.21095434491216844]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1491.pth
	Model improved!!!
EPOCH 1492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11184766446371278		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.11184766446371278 | validation: 0.2246471241582264]
	TIME [epoch: 5.71 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11143939268706021		[learning rate: 6.047e-05]
	Learning Rate: 6.04698e-05
	LOSS [training: 0.11143939268706021 | validation: 0.23533000711296284]
	TIME [epoch: 5.71 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10910605154867144		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.10910605154867144 | validation: 0.21636827177092377]
	TIME [epoch: 5.71 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11622123040919402		[learning rate: 6.0043e-05]
	Learning Rate: 6.00429e-05
	LOSS [training: 0.11622123040919402 | validation: 0.23420596620002013]
	TIME [epoch: 5.71 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11206175581522093		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.11206175581522093 | validation: 0.23232004386000782]
	TIME [epoch: 5.71 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11241906773146852		[learning rate: 5.9619e-05]
	Learning Rate: 5.9619e-05
	LOSS [training: 0.11241906773146852 | validation: 0.22472201720275625]
	TIME [epoch: 5.72 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10912894307416712		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.10912894307416712 | validation: 0.22792836830259455]
	TIME [epoch: 5.71 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11175351538574421		[learning rate: 5.9198e-05]
	Learning Rate: 5.91981e-05
	LOSS [training: 0.11175351538574421 | validation: 0.22904911484890436]
	TIME [epoch: 5.71 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11110976032589438		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.11110976032589438 | validation: 0.2303599963076337]
	TIME [epoch: 5.71 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11136818310840442		[learning rate: 5.878e-05]
	Learning Rate: 5.87802e-05
	LOSS [training: 0.11136818310840442 | validation: 0.21716888965414466]
	TIME [epoch: 5.72 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11093259195579029		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.11093259195579029 | validation: 0.23230496242245222]
	TIME [epoch: 5.73 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11094010686670958		[learning rate: 5.8365e-05]
	Learning Rate: 5.83652e-05
	LOSS [training: 0.11094010686670958 | validation: 0.22379520305256034]
	TIME [epoch: 5.72 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10984717386029952		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.10984717386029952 | validation: 0.21901412131265172]
	TIME [epoch: 5.72 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11012514151867749		[learning rate: 5.7953e-05]
	Learning Rate: 5.79531e-05
	LOSS [training: 0.11012514151867749 | validation: 0.23049161839567767]
	TIME [epoch: 5.72 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11087688293992053		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.11087688293992053 | validation: 0.22169059204681557]
	TIME [epoch: 5.73 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10920569565634564		[learning rate: 5.7544e-05]
	Learning Rate: 5.7544e-05
	LOSS [training: 0.10920569565634564 | validation: 0.22109866114935897]
	TIME [epoch: 5.73 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11137540350007732		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.11137540350007732 | validation: 0.22136145227133627]
	TIME [epoch: 5.73 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11072364695898462		[learning rate: 5.7138e-05]
	Learning Rate: 5.71378e-05
	LOSS [training: 0.11072364695898462 | validation: 0.22713296617494053]
	TIME [epoch: 5.73 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11011679065443662		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.11011679065443662 | validation: 0.2268889961804285]
	TIME [epoch: 5.72 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11217319310138962		[learning rate: 5.6734e-05]
	Learning Rate: 5.67344e-05
	LOSS [training: 0.11217319310138962 | validation: 0.2157176489428194]
	TIME [epoch: 5.74 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11032854372731436		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.11032854372731436 | validation: 0.22824086928628204]
	TIME [epoch: 5.72 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11012032174898621		[learning rate: 5.6334e-05]
	Learning Rate: 5.63338e-05
	LOSS [training: 0.11012032174898621 | validation: 0.2172425585958762]
	TIME [epoch: 5.72 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10954084865647673		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.10954084865647673 | validation: 0.2317913318748123]
	TIME [epoch: 5.73 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11013218410930385		[learning rate: 5.5936e-05]
	Learning Rate: 5.59361e-05
	LOSS [training: 0.11013218410930385 | validation: 0.22185331961538513]
	TIME [epoch: 5.73 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10824753949883903		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.10824753949883903 | validation: 0.2250412154551107]
	TIME [epoch: 5.73 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1073488171372416		[learning rate: 5.5541e-05]
	Learning Rate: 5.55412e-05
	LOSS [training: 0.1073488171372416 | validation: 0.23127635381706302]
	TIME [epoch: 5.73 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10993035740892465		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.10993035740892465 | validation: 0.22281947231057406]
	TIME [epoch: 5.72 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10917987393816056		[learning rate: 5.5149e-05]
	Learning Rate: 5.51491e-05
	LOSS [training: 0.10917987393816056 | validation: 0.21790139551072887]
	TIME [epoch: 5.72 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11080408015593995		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.11080408015593995 | validation: 0.22614292912218434]
	TIME [epoch: 5.73 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1086913884233774		[learning rate: 5.476e-05]
	Learning Rate: 5.47598e-05
	LOSS [training: 0.1086913884233774 | validation: 0.2207023059365239]
	TIME [epoch: 5.72 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11083986632011636		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.11083986632011636 | validation: 0.22570211047219893]
	TIME [epoch: 5.73 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11052695403685082		[learning rate: 5.4373e-05]
	Learning Rate: 5.43732e-05
	LOSS [training: 0.11052695403685082 | validation: 0.22048383664531612]
	TIME [epoch: 5.72 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10897569303213998		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.10897569303213998 | validation: 0.23291990261623363]
	TIME [epoch: 5.73 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10631175744436529		[learning rate: 5.3989e-05]
	Learning Rate: 5.39893e-05
	LOSS [training: 0.10631175744436529 | validation: 0.2072266631433359]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1525.pth
	Model improved!!!
EPOCH 1526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10844443650657473		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.10844443650657473 | validation: 0.2190722543397644]
	TIME [epoch: 5.7 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10845130947141346		[learning rate: 5.3608e-05]
	Learning Rate: 5.36082e-05
	LOSS [training: 0.10845130947141346 | validation: 0.22453766768639133]
	TIME [epoch: 5.71 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11116357987032272		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.11116357987032272 | validation: 0.21216899128050387]
	TIME [epoch: 5.73 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10990346394589462		[learning rate: 5.323e-05]
	Learning Rate: 5.32297e-05
	LOSS [training: 0.10990346394589462 | validation: 0.2271118221448802]
	TIME [epoch: 5.72 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11036517936670581		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.11036517936670581 | validation: 0.22471631411852577]
	TIME [epoch: 5.74 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10839263185246328		[learning rate: 5.2854e-05]
	Learning Rate: 5.28539e-05
	LOSS [training: 0.10839263185246328 | validation: 0.21123246385799588]
	TIME [epoch: 5.73 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1110715545699266		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.1110715545699266 | validation: 0.22436779590197387]
	TIME [epoch: 5.74 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10903605472860642		[learning rate: 5.2481e-05]
	Learning Rate: 5.24807e-05
	LOSS [training: 0.10903605472860642 | validation: 0.22394140483517405]
	TIME [epoch: 5.72 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10813243701569926		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.10813243701569926 | validation: 0.21807919428474654]
	TIME [epoch: 5.73 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10728831074051925		[learning rate: 5.211e-05]
	Learning Rate: 5.21103e-05
	LOSS [training: 0.10728831074051925 | validation: 0.22268642876658196]
	TIME [epoch: 5.72 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10824809139368666		[learning rate: 5.1926e-05]
	Learning Rate: 5.1926e-05
	LOSS [training: 0.10824809139368666 | validation: 0.2179520362960468]
	TIME [epoch: 5.73 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10827416445771576		[learning rate: 5.1742e-05]
	Learning Rate: 5.17424e-05
	LOSS [training: 0.10827416445771576 | validation: 0.22457654566284901]
	TIME [epoch: 5.73 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10704598719716253		[learning rate: 5.1559e-05]
	Learning Rate: 5.15594e-05
	LOSS [training: 0.10704598719716253 | validation: 0.22233782216842088]
	TIME [epoch: 5.72 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10770882070259469		[learning rate: 5.1377e-05]
	Learning Rate: 5.13771e-05
	LOSS [training: 0.10770882070259469 | validation: 0.22470948861245985]
	TIME [epoch: 5.73 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10713617732118412		[learning rate: 5.1195e-05]
	Learning Rate: 5.11954e-05
	LOSS [training: 0.10713617732118412 | validation: 0.21546564793886913]
	TIME [epoch: 5.72 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10898064519927803		[learning rate: 5.1014e-05]
	Learning Rate: 5.10144e-05
	LOSS [training: 0.10898064519927803 | validation: 0.22078811934380718]
	TIME [epoch: 5.72 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10583544886966456		[learning rate: 5.0834e-05]
	Learning Rate: 5.0834e-05
	LOSS [training: 0.10583544886966456 | validation: 0.21686548168190348]
	TIME [epoch: 5.73 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10823669839882172		[learning rate: 5.0654e-05]
	Learning Rate: 5.06542e-05
	LOSS [training: 0.10823669839882172 | validation: 0.21864769445423904]
	TIME [epoch: 5.72 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10831754123129245		[learning rate: 5.0475e-05]
	Learning Rate: 5.04751e-05
	LOSS [training: 0.10831754123129245 | validation: 0.2220378133374321]
	TIME [epoch: 5.72 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1063114576079516		[learning rate: 5.0297e-05]
	Learning Rate: 5.02966e-05
	LOSS [training: 0.1063114576079516 | validation: 0.2152242582923011]
	TIME [epoch: 5.73 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10700161695223308		[learning rate: 5.0119e-05]
	Learning Rate: 5.01187e-05
	LOSS [training: 0.10700161695223308 | validation: 0.2171790003913575]
	TIME [epoch: 5.72 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10851515889344647		[learning rate: 4.9942e-05]
	Learning Rate: 4.99415e-05
	LOSS [training: 0.10851515889344647 | validation: 0.22059301820657443]
	TIME [epoch: 5.72 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10768123307829085		[learning rate: 4.9765e-05]
	Learning Rate: 4.97649e-05
	LOSS [training: 0.10768123307829085 | validation: 0.20921530209200576]
	TIME [epoch: 5.73 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11000233367310887		[learning rate: 4.9589e-05]
	Learning Rate: 4.95889e-05
	LOSS [training: 0.11000233367310887 | validation: 0.21870727183181443]
	TIME [epoch: 5.72 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10747406565930995		[learning rate: 4.9414e-05]
	Learning Rate: 4.94136e-05
	LOSS [training: 0.10747406565930995 | validation: 0.21606949197721673]
	TIME [epoch: 5.72 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1064353473747361		[learning rate: 4.9239e-05]
	Learning Rate: 4.92388e-05
	LOSS [training: 0.1064353473747361 | validation: 0.22055007598318316]
	TIME [epoch: 5.72 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10827379512276107		[learning rate: 4.9065e-05]
	Learning Rate: 4.90647e-05
	LOSS [training: 0.10827379512276107 | validation: 0.20567000884442932]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1552.pth
	Model improved!!!
EPOCH 1553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10469485106322961		[learning rate: 4.8891e-05]
	Learning Rate: 4.88912e-05
	LOSS [training: 0.10469485106322961 | validation: 0.21609000905347578]
	TIME [epoch: 5.84 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10788536474817662		[learning rate: 4.8718e-05]
	Learning Rate: 4.87183e-05
	LOSS [training: 0.10788536474817662 | validation: 0.21436786568603408]
	TIME [epoch: 5.7 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10729308751121368		[learning rate: 4.8546e-05]
	Learning Rate: 4.85461e-05
	LOSS [training: 0.10729308751121368 | validation: 0.22534872993526123]
	TIME [epoch: 5.71 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10865421926664379		[learning rate: 4.8374e-05]
	Learning Rate: 4.83744e-05
	LOSS [training: 0.10865421926664379 | validation: 0.2133619896578007]
	TIME [epoch: 5.7 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10742849919804819		[learning rate: 4.8203e-05]
	Learning Rate: 4.82033e-05
	LOSS [training: 0.10742849919804819 | validation: 0.20866155151421575]
	TIME [epoch: 5.72 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10788493075706672		[learning rate: 4.8033e-05]
	Learning Rate: 4.80329e-05
	LOSS [training: 0.10788493075706672 | validation: 0.22998638020125484]
	TIME [epoch: 5.71 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10927906669924178		[learning rate: 4.7863e-05]
	Learning Rate: 4.7863e-05
	LOSS [training: 0.10927906669924178 | validation: 0.21274946799809952]
	TIME [epoch: 5.71 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10754178605434998		[learning rate: 4.7694e-05]
	Learning Rate: 4.76938e-05
	LOSS [training: 0.10754178605434998 | validation: 0.21470925691023607]
	TIME [epoch: 5.7 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10554184768314327		[learning rate: 4.7525e-05]
	Learning Rate: 4.75251e-05
	LOSS [training: 0.10554184768314327 | validation: 0.22803516451283673]
	TIME [epoch: 5.71 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10676427206761588		[learning rate: 4.7357e-05]
	Learning Rate: 4.73571e-05
	LOSS [training: 0.10676427206761588 | validation: 0.2118112711826778]
	TIME [epoch: 5.7 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10481497846818362		[learning rate: 4.719e-05]
	Learning Rate: 4.71896e-05
	LOSS [training: 0.10481497846818362 | validation: 0.21682096044733656]
	TIME [epoch: 5.71 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10695922770288231		[learning rate: 4.7023e-05]
	Learning Rate: 4.70227e-05
	LOSS [training: 0.10695922770288231 | validation: 0.20565045671298715]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1564.pth
	Model improved!!!
EPOCH 1565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10735669787668291		[learning rate: 4.6856e-05]
	Learning Rate: 4.68564e-05
	LOSS [training: 0.10735669787668291 | validation: 0.20315296175602748]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1565.pth
	Model improved!!!
EPOCH 1566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10737871217078444		[learning rate: 4.6691e-05]
	Learning Rate: 4.66907e-05
	LOSS [training: 0.10737871217078444 | validation: 0.21801913492151243]
	TIME [epoch: 5.7 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10688720512141503		[learning rate: 4.6526e-05]
	Learning Rate: 4.65257e-05
	LOSS [training: 0.10688720512141503 | validation: 0.21909633143961954]
	TIME [epoch: 5.72 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10592498366840784		[learning rate: 4.6361e-05]
	Learning Rate: 4.63611e-05
	LOSS [training: 0.10592498366840784 | validation: 0.22063851720807157]
	TIME [epoch: 5.72 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10748122731087097		[learning rate: 4.6197e-05]
	Learning Rate: 4.61972e-05
	LOSS [training: 0.10748122731087097 | validation: 0.20975314195996664]
	TIME [epoch: 5.71 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10587735061831051		[learning rate: 4.6034e-05]
	Learning Rate: 4.60338e-05
	LOSS [training: 0.10587735061831051 | validation: 0.20971131924199504]
	TIME [epoch: 5.71 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1063519710356798		[learning rate: 4.5871e-05]
	Learning Rate: 4.5871e-05
	LOSS [training: 0.1063519710356798 | validation: 0.22524341774689455]
	TIME [epoch: 5.71 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10545747749970183		[learning rate: 4.5709e-05]
	Learning Rate: 4.57088e-05
	LOSS [training: 0.10545747749970183 | validation: 0.20956773067367396]
	TIME [epoch: 5.71 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10677442889762272		[learning rate: 4.5547e-05]
	Learning Rate: 4.55472e-05
	LOSS [training: 0.10677442889762272 | validation: 0.21716862691651384]
	TIME [epoch: 5.72 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10647924976966316		[learning rate: 4.5386e-05]
	Learning Rate: 4.53861e-05
	LOSS [training: 0.10647924976966316 | validation: 0.22485635611105503]
	TIME [epoch: 5.71 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1050425016115917		[learning rate: 4.5226e-05]
	Learning Rate: 4.52256e-05
	LOSS [training: 0.1050425016115917 | validation: 0.2118275687545398]
	TIME [epoch: 5.72 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10526471166306463		[learning rate: 4.5066e-05]
	Learning Rate: 4.50657e-05
	LOSS [training: 0.10526471166306463 | validation: 0.21959089898802944]
	TIME [epoch: 5.71 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1055432068485026		[learning rate: 4.4906e-05]
	Learning Rate: 4.49064e-05
	LOSS [training: 0.1055432068485026 | validation: 0.21625438984826426]
	TIME [epoch: 5.71 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1059568400524012		[learning rate: 4.4748e-05]
	Learning Rate: 4.47476e-05
	LOSS [training: 0.1059568400524012 | validation: 0.21211629267555063]
	TIME [epoch: 5.72 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10356603322184342		[learning rate: 4.4589e-05]
	Learning Rate: 4.45893e-05
	LOSS [training: 0.10356603322184342 | validation: 0.2132960843947018]
	TIME [epoch: 5.71 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10605160742473256		[learning rate: 4.4432e-05]
	Learning Rate: 4.44316e-05
	LOSS [training: 0.10605160742473256 | validation: 0.2228349421362853]
	TIME [epoch: 5.72 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10646221049986274		[learning rate: 4.4275e-05]
	Learning Rate: 4.42745e-05
	LOSS [training: 0.10646221049986274 | validation: 0.21816857314768212]
	TIME [epoch: 5.72 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1059224912918824		[learning rate: 4.4118e-05]
	Learning Rate: 4.4118e-05
	LOSS [training: 0.1059224912918824 | validation: 0.2127962314096526]
	TIME [epoch: 5.71 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10698065179686612		[learning rate: 4.3962e-05]
	Learning Rate: 4.3962e-05
	LOSS [training: 0.10698065179686612 | validation: 0.21452206669067597]
	TIME [epoch: 5.72 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10696069464303232		[learning rate: 4.3807e-05]
	Learning Rate: 4.38065e-05
	LOSS [training: 0.10696069464303232 | validation: 0.20923782191131926]
	TIME [epoch: 5.71 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10378766716796418		[learning rate: 4.3652e-05]
	Learning Rate: 4.36516e-05
	LOSS [training: 0.10378766716796418 | validation: 0.20591445815404189]
	TIME [epoch: 5.71 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10418836622724144		[learning rate: 4.3497e-05]
	Learning Rate: 4.34972e-05
	LOSS [training: 0.10418836622724144 | validation: 0.20768033678386308]
	TIME [epoch: 5.71 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10742863325577803		[learning rate: 4.3343e-05]
	Learning Rate: 4.33434e-05
	LOSS [training: 0.10742863325577803 | validation: 0.227658954103371]
	TIME [epoch: 5.71 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10453723080228859		[learning rate: 4.319e-05]
	Learning Rate: 4.31902e-05
	LOSS [training: 0.10453723080228859 | validation: 0.21441727716279368]
	TIME [epoch: 5.71 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1039537269346609		[learning rate: 4.3037e-05]
	Learning Rate: 4.30374e-05
	LOSS [training: 0.1039537269346609 | validation: 0.20935329289579985]
	TIME [epoch: 5.72 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10464437686510078		[learning rate: 4.2885e-05]
	Learning Rate: 4.28852e-05
	LOSS [training: 0.10464437686510078 | validation: 0.2203692132664731]
	TIME [epoch: 5.72 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.104966152905312		[learning rate: 4.2734e-05]
	Learning Rate: 4.27336e-05
	LOSS [training: 0.104966152905312 | validation: 0.2105932032179613]
	TIME [epoch: 5.71 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10636134569020125		[learning rate: 4.2582e-05]
	Learning Rate: 4.25825e-05
	LOSS [training: 0.10636134569020125 | validation: 0.21627573616918197]
	TIME [epoch: 5.72 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10614435774351831		[learning rate: 4.2432e-05]
	Learning Rate: 4.24319e-05
	LOSS [training: 0.10614435774351831 | validation: 0.21282566739376724]
	TIME [epoch: 5.71 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10198103656532237		[learning rate: 4.2282e-05]
	Learning Rate: 4.22819e-05
	LOSS [training: 0.10198103656532237 | validation: 0.2178053175588401]
	TIME [epoch: 5.72 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10688159068797344		[learning rate: 4.2132e-05]
	Learning Rate: 4.21323e-05
	LOSS [training: 0.10688159068797344 | validation: 0.21410899979946243]
	TIME [epoch: 5.71 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10452606059414014		[learning rate: 4.1983e-05]
	Learning Rate: 4.19833e-05
	LOSS [training: 0.10452606059414014 | validation: 0.20114619851614002]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1596.pth
	Model improved!!!
EPOCH 1597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10757071011415516		[learning rate: 4.1835e-05]
	Learning Rate: 4.18349e-05
	LOSS [training: 0.10757071011415516 | validation: 0.22766645367044155]
	TIME [epoch: 5.71 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1047319720516462		[learning rate: 4.1687e-05]
	Learning Rate: 4.1687e-05
	LOSS [training: 0.1047319720516462 | validation: 0.21612565402738806]
	TIME [epoch: 5.72 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10369248019804105		[learning rate: 4.154e-05]
	Learning Rate: 4.15395e-05
	LOSS [training: 0.10369248019804105 | validation: 0.20783337524418155]
	TIME [epoch: 5.72 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10599199530463814		[learning rate: 4.1393e-05]
	Learning Rate: 4.13926e-05
	LOSS [training: 0.10599199530463814 | validation: 0.21640561476301481]
	TIME [epoch: 5.72 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10421690185536635		[learning rate: 4.1246e-05]
	Learning Rate: 4.12463e-05
	LOSS [training: 0.10421690185536635 | validation: 0.20751204976384452]
	TIME [epoch: 5.72 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10510424282919076		[learning rate: 4.11e-05]
	Learning Rate: 4.11004e-05
	LOSS [training: 0.10510424282919076 | validation: 0.21295421049324423]
	TIME [epoch: 5.73 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1037958770187522		[learning rate: 4.0955e-05]
	Learning Rate: 4.09551e-05
	LOSS [training: 0.1037958770187522 | validation: 0.2160792648438138]
	TIME [epoch: 5.71 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10421516868400037		[learning rate: 4.081e-05]
	Learning Rate: 4.08103e-05
	LOSS [training: 0.10421516868400037 | validation: 0.23048761001557228]
	TIME [epoch: 5.72 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10467904884218002		[learning rate: 4.0666e-05]
	Learning Rate: 4.06659e-05
	LOSS [training: 0.10467904884218002 | validation: 0.21837549151932906]
	TIME [epoch: 5.71 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10497346955862956		[learning rate: 4.0522e-05]
	Learning Rate: 4.05221e-05
	LOSS [training: 0.10497346955862956 | validation: 0.21510710526573495]
	TIME [epoch: 5.72 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10501805999140845		[learning rate: 4.0379e-05]
	Learning Rate: 4.03788e-05
	LOSS [training: 0.10501805999140845 | validation: 0.2132008673000601]
	TIME [epoch: 5.71 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10404767712461296		[learning rate: 4.0236e-05]
	Learning Rate: 4.02361e-05
	LOSS [training: 0.10404767712461296 | validation: 0.20408042556872386]
	TIME [epoch: 5.72 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10521519623791153		[learning rate: 4.0094e-05]
	Learning Rate: 4.00938e-05
	LOSS [training: 0.10521519623791153 | validation: 0.2090564897269286]
	TIME [epoch: 5.72 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10421663878120213		[learning rate: 3.9952e-05]
	Learning Rate: 3.9952e-05
	LOSS [training: 0.10421663878120213 | validation: 0.20709617330778235]
	TIME [epoch: 5.72 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10450117777671049		[learning rate: 3.9811e-05]
	Learning Rate: 3.98107e-05
	LOSS [training: 0.10450117777671049 | validation: 0.21108228750945732]
	TIME [epoch: 5.71 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10355474205769133		[learning rate: 3.967e-05]
	Learning Rate: 3.967e-05
	LOSS [training: 0.10355474205769133 | validation: 0.206608108798338]
	TIME [epoch: 5.72 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10616578960169187		[learning rate: 3.953e-05]
	Learning Rate: 3.95297e-05
	LOSS [training: 0.10616578960169187 | validation: 0.2137988267235371]
	TIME [epoch: 5.71 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10362808104474618		[learning rate: 3.939e-05]
	Learning Rate: 3.93899e-05
	LOSS [training: 0.10362808104474618 | validation: 0.21353331157304456]
	TIME [epoch: 5.72 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10298570283396692		[learning rate: 3.9251e-05]
	Learning Rate: 3.92506e-05
	LOSS [training: 0.10298570283396692 | validation: 0.21344708697897563]
	TIME [epoch: 5.71 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10576590708484006		[learning rate: 3.9112e-05]
	Learning Rate: 3.91118e-05
	LOSS [training: 0.10576590708484006 | validation: 0.2027190422475214]
	TIME [epoch: 5.72 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10443678209416675		[learning rate: 3.8973e-05]
	Learning Rate: 3.89735e-05
	LOSS [training: 0.10443678209416675 | validation: 0.20317871874759563]
	TIME [epoch: 5.71 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10290509135272449		[learning rate: 3.8836e-05]
	Learning Rate: 3.88357e-05
	LOSS [training: 0.10290509135272449 | validation: 0.21897399149606955]
	TIME [epoch: 5.71 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10395285315520042		[learning rate: 3.8698e-05]
	Learning Rate: 3.86983e-05
	LOSS [training: 0.10395285315520042 | validation: 0.20537173875570228]
	TIME [epoch: 5.71 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10458101034744155		[learning rate: 3.8561e-05]
	Learning Rate: 3.85615e-05
	LOSS [training: 0.10458101034744155 | validation: 0.20952239639084774]
	TIME [epoch: 5.72 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10245327016090006		[learning rate: 3.8425e-05]
	Learning Rate: 3.84251e-05
	LOSS [training: 0.10245327016090006 | validation: 0.20917786468185923]
	TIME [epoch: 5.71 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10386420493357297		[learning rate: 3.8289e-05]
	Learning Rate: 3.82893e-05
	LOSS [training: 0.10386420493357297 | validation: 0.21185109455654436]
	TIME [epoch: 5.71 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10457652192986341		[learning rate: 3.8154e-05]
	Learning Rate: 3.81539e-05
	LOSS [training: 0.10457652192986341 | validation: 0.1959896772787053]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1623.pth
	Model improved!!!
EPOCH 1624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1047088915033448		[learning rate: 3.8019e-05]
	Learning Rate: 3.8019e-05
	LOSS [training: 0.1047088915033448 | validation: 0.21536708447340458]
	TIME [epoch: 5.72 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10373173798730285		[learning rate: 3.7885e-05]
	Learning Rate: 3.78845e-05
	LOSS [training: 0.10373173798730285 | validation: 0.20395086326442563]
	TIME [epoch: 5.72 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10226841436221068		[learning rate: 3.7751e-05]
	Learning Rate: 3.77505e-05
	LOSS [training: 0.10226841436221068 | validation: 0.21081672231857276]
	TIME [epoch: 5.72 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10391232088780637		[learning rate: 3.7617e-05]
	Learning Rate: 3.7617e-05
	LOSS [training: 0.10391232088780637 | validation: 0.20700548291104887]
	TIME [epoch: 5.72 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10441311695556539		[learning rate: 3.7484e-05]
	Learning Rate: 3.7484e-05
	LOSS [training: 0.10441311695556539 | validation: 0.21639759133176573]
	TIME [epoch: 5.72 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10115596072000266		[learning rate: 3.7351e-05]
	Learning Rate: 3.73515e-05
	LOSS [training: 0.10115596072000266 | validation: 0.20673926820752442]
	TIME [epoch: 5.71 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10420673220429388		[learning rate: 3.7219e-05]
	Learning Rate: 3.72194e-05
	LOSS [training: 0.10420673220429388 | validation: 0.1957326854063114]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1630.pth
	Model improved!!!
EPOCH 1631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10304749289560912		[learning rate: 3.7088e-05]
	Learning Rate: 3.70878e-05
	LOSS [training: 0.10304749289560912 | validation: 0.20493545120046297]
	TIME [epoch: 5.71 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10373068230645932		[learning rate: 3.6957e-05]
	Learning Rate: 3.69566e-05
	LOSS [training: 0.10373068230645932 | validation: 0.19834783675463777]
	TIME [epoch: 5.71 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10098234708909645		[learning rate: 3.6826e-05]
	Learning Rate: 3.68259e-05
	LOSS [training: 0.10098234708909645 | validation: 0.20991987873289764]
	TIME [epoch: 5.71 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10191684822578762		[learning rate: 3.6696e-05]
	Learning Rate: 3.66957e-05
	LOSS [training: 0.10191684822578762 | validation: 0.21152353294353896]
	TIME [epoch: 5.72 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10131088771561222		[learning rate: 3.6566e-05]
	Learning Rate: 3.6566e-05
	LOSS [training: 0.10131088771561222 | validation: 0.21625070156118006]
	TIME [epoch: 5.72 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10471882703018551		[learning rate: 3.6437e-05]
	Learning Rate: 3.64367e-05
	LOSS [training: 0.10471882703018551 | validation: 0.20157967874067376]
	TIME [epoch: 5.72 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10097644855341358		[learning rate: 3.6308e-05]
	Learning Rate: 3.63078e-05
	LOSS [training: 0.10097644855341358 | validation: 0.21659965283079813]
	TIME [epoch: 5.71 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10178991485691025		[learning rate: 3.6179e-05]
	Learning Rate: 3.61794e-05
	LOSS [training: 0.10178991485691025 | validation: 0.21273887662749422]
	TIME [epoch: 5.72 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10228631594838258		[learning rate: 3.6051e-05]
	Learning Rate: 3.60515e-05
	LOSS [training: 0.10228631594838258 | validation: 0.21363741707727013]
	TIME [epoch: 5.72 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10171891480864952		[learning rate: 3.5924e-05]
	Learning Rate: 3.5924e-05
	LOSS [training: 0.10171891480864952 | validation: 0.2163669941803005]
	TIME [epoch: 5.72 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10200522970859652		[learning rate: 3.5797e-05]
	Learning Rate: 3.5797e-05
	LOSS [training: 0.10200522970859652 | validation: 0.20960688807363398]
	TIME [epoch: 5.71 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10547273714184399		[learning rate: 3.567e-05]
	Learning Rate: 3.56704e-05
	LOSS [training: 0.10547273714184399 | validation: 0.2049027243136235]
	TIME [epoch: 5.72 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10233561501254518		[learning rate: 3.5544e-05]
	Learning Rate: 3.55442e-05
	LOSS [training: 0.10233561501254518 | validation: 0.2058122759255792]
	TIME [epoch: 5.72 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10223109902046558		[learning rate: 3.5419e-05]
	Learning Rate: 3.54186e-05
	LOSS [training: 0.10223109902046558 | validation: 0.21247396522371462]
	TIME [epoch: 5.72 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10226640470267702		[learning rate: 3.5293e-05]
	Learning Rate: 3.52933e-05
	LOSS [training: 0.10226640470267702 | validation: 0.20720662235301643]
	TIME [epoch: 5.72 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10432132576056428		[learning rate: 3.5169e-05]
	Learning Rate: 3.51685e-05
	LOSS [training: 0.10432132576056428 | validation: 0.20269308604659467]
	TIME [epoch: 5.71 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10229807559354274		[learning rate: 3.5044e-05]
	Learning Rate: 3.50441e-05
	LOSS [training: 0.10229807559354274 | validation: 0.21209750034509894]
	TIME [epoch: 5.71 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10187049244999731		[learning rate: 3.492e-05]
	Learning Rate: 3.49202e-05
	LOSS [training: 0.10187049244999731 | validation: 0.21515465633589131]
	TIME [epoch: 5.72 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10351345411139708		[learning rate: 3.4797e-05]
	Learning Rate: 3.47967e-05
	LOSS [training: 0.10351345411139708 | validation: 0.19721161200061965]
	TIME [epoch: 5.71 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.103311749325961		[learning rate: 3.4674e-05]
	Learning Rate: 3.46737e-05
	LOSS [training: 0.103311749325961 | validation: 0.21133809771339457]
	TIME [epoch: 5.71 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10330786930712456		[learning rate: 3.4551e-05]
	Learning Rate: 3.45511e-05
	LOSS [training: 0.10330786930712456 | validation: 0.2112102119137733]
	TIME [epoch: 5.72 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10220906830540592		[learning rate: 3.4429e-05]
	Learning Rate: 3.44289e-05
	LOSS [training: 0.10220906830540592 | validation: 0.19655703068042396]
	TIME [epoch: 5.71 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10115591299144011		[learning rate: 3.4307e-05]
	Learning Rate: 3.43072e-05
	LOSS [training: 0.10115591299144011 | validation: 0.19880211766524913]
	TIME [epoch: 5.71 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10528754092815397		[learning rate: 3.4186e-05]
	Learning Rate: 3.41858e-05
	LOSS [training: 0.10528754092815397 | validation: 0.20843324627883866]
	TIME [epoch: 5.71 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1054945622745898		[learning rate: 3.4065e-05]
	Learning Rate: 3.4065e-05
	LOSS [training: 0.1054945622745898 | validation: 0.20487463475941103]
	TIME [epoch: 5.71 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.101739272176094		[learning rate: 3.3944e-05]
	Learning Rate: 3.39445e-05
	LOSS [training: 0.101739272176094 | validation: 0.21191164547275732]
	TIME [epoch: 5.72 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09945919203675171		[learning rate: 3.3824e-05]
	Learning Rate: 3.38245e-05
	LOSS [training: 0.09945919203675171 | validation: 0.20307764347052615]
	TIME [epoch: 5.71 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10238123318748735		[learning rate: 3.3705e-05]
	Learning Rate: 3.37049e-05
	LOSS [training: 0.10238123318748735 | validation: 0.21463782187230385]
	TIME [epoch: 5.71 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10136505752826716		[learning rate: 3.3586e-05]
	Learning Rate: 3.35857e-05
	LOSS [training: 0.10136505752826716 | validation: 0.20862464135268627]
	TIME [epoch: 5.71 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1055314947828488		[learning rate: 3.3467e-05]
	Learning Rate: 3.34669e-05
	LOSS [training: 0.1055314947828488 | validation: 0.2095518339062415]
	TIME [epoch: 5.71 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10239586832135228		[learning rate: 3.3349e-05]
	Learning Rate: 3.33486e-05
	LOSS [training: 0.10239586832135228 | validation: 0.2132220170276715]
	TIME [epoch: 5.72 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1017943295666332		[learning rate: 3.3231e-05]
	Learning Rate: 3.32306e-05
	LOSS [training: 0.1017943295666332 | validation: 0.21588128338611196]
	TIME [epoch: 5.72 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1010369987118449		[learning rate: 3.3113e-05]
	Learning Rate: 3.31131e-05
	LOSS [training: 0.1010369987118449 | validation: 0.20147656473401915]
	TIME [epoch: 5.71 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10110751262800509		[learning rate: 3.2996e-05]
	Learning Rate: 3.2996e-05
	LOSS [training: 0.10110751262800509 | validation: 0.20887633078828652]
	TIME [epoch: 5.72 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1015747380434645		[learning rate: 3.2879e-05]
	Learning Rate: 3.28794e-05
	LOSS [training: 0.1015747380434645 | validation: 0.20340116850192655]
	TIME [epoch: 5.72 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10186244709165714		[learning rate: 3.2763e-05]
	Learning Rate: 3.27631e-05
	LOSS [training: 0.10186244709165714 | validation: 0.21069035121790317]
	TIME [epoch: 5.71 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10435835954993777		[learning rate: 3.2647e-05]
	Learning Rate: 3.26472e-05
	LOSS [training: 0.10435835954993777 | validation: 0.202155550739504]
	TIME [epoch: 5.71 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10147896743755354		[learning rate: 3.2532e-05]
	Learning Rate: 3.25318e-05
	LOSS [training: 0.10147896743755354 | validation: 0.20212081974215473]
	TIME [epoch: 5.71 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1021790206908739		[learning rate: 3.2417e-05]
	Learning Rate: 3.24167e-05
	LOSS [training: 0.1021790206908739 | validation: 0.20945248068804415]
	TIME [epoch: 5.72 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10244947889415268		[learning rate: 3.2302e-05]
	Learning Rate: 3.23021e-05
	LOSS [training: 0.10244947889415268 | validation: 0.19675092778832357]
	TIME [epoch: 5.72 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1006491014704709		[learning rate: 3.2188e-05]
	Learning Rate: 3.21879e-05
	LOSS [training: 0.1006491014704709 | validation: 0.2105357692640018]
	TIME [epoch: 5.72 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10136140407312262		[learning rate: 3.2074e-05]
	Learning Rate: 3.20741e-05
	LOSS [training: 0.10136140407312262 | validation: 0.20078131171276803]
	TIME [epoch: 5.72 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10241100733837201		[learning rate: 3.1961e-05]
	Learning Rate: 3.19606e-05
	LOSS [training: 0.10241100733837201 | validation: 0.20006336237989492]
	TIME [epoch: 5.71 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10121474561926963		[learning rate: 3.1848e-05]
	Learning Rate: 3.18476e-05
	LOSS [training: 0.10121474561926963 | validation: 0.20529187775316357]
	TIME [epoch: 5.72 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10231329680011379		[learning rate: 3.1735e-05]
	Learning Rate: 3.1735e-05
	LOSS [training: 0.10231329680011379 | validation: 0.20242406290253614]
	TIME [epoch: 5.71 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10078948963637566		[learning rate: 3.1623e-05]
	Learning Rate: 3.16228e-05
	LOSS [training: 0.10078948963637566 | validation: 0.20965867610133213]
	TIME [epoch: 5.71 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10187432455195375		[learning rate: 3.1511e-05]
	Learning Rate: 3.1511e-05
	LOSS [training: 0.10187432455195375 | validation: 0.21824236646798414]
	TIME [epoch: 5.72 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10151359531806657		[learning rate: 3.14e-05]
	Learning Rate: 3.13995e-05
	LOSS [training: 0.10151359531806657 | validation: 0.2081845204261228]
	TIME [epoch: 5.71 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10157873030895335		[learning rate: 3.1288e-05]
	Learning Rate: 3.12885e-05
	LOSS [training: 0.10157873030895335 | validation: 0.19401823665470908]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1679.pth
	Model improved!!!
EPOCH 1680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0998247284410071		[learning rate: 3.1178e-05]
	Learning Rate: 3.11779e-05
	LOSS [training: 0.0998247284410071 | validation: 0.20475809282623142]
	TIME [epoch: 5.71 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10154349219680864		[learning rate: 3.1068e-05]
	Learning Rate: 3.10676e-05
	LOSS [training: 0.10154349219680864 | validation: 0.20352289920597408]
	TIME [epoch: 5.71 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10055266255252356		[learning rate: 3.0958e-05]
	Learning Rate: 3.09577e-05
	LOSS [training: 0.10055266255252356 | validation: 0.20445296761680645]
	TIME [epoch: 5.72 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09871609510143053		[learning rate: 3.0848e-05]
	Learning Rate: 3.08483e-05
	LOSS [training: 0.09871609510143053 | validation: 0.19749045769464454]
	TIME [epoch: 5.71 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10124470485157333		[learning rate: 3.0739e-05]
	Learning Rate: 3.07392e-05
	LOSS [training: 0.10124470485157333 | validation: 0.21089613327903375]
	TIME [epoch: 5.71 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10192646293093498		[learning rate: 3.063e-05]
	Learning Rate: 3.06305e-05
	LOSS [training: 0.10192646293093498 | validation: 0.19115777014796126]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1685.pth
	Model improved!!!
EPOCH 1686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10102959485896858		[learning rate: 3.0522e-05]
	Learning Rate: 3.05222e-05
	LOSS [training: 0.10102959485896858 | validation: 0.21244078784126086]
	TIME [epoch: 5.71 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09996776535636351		[learning rate: 3.0414e-05]
	Learning Rate: 3.04142e-05
	LOSS [training: 0.09996776535636351 | validation: 0.19776173382451104]
	TIME [epoch: 5.71 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10049367790011485		[learning rate: 3.0307e-05]
	Learning Rate: 3.03067e-05
	LOSS [training: 0.10049367790011485 | validation: 0.20541127352193614]
	TIME [epoch: 5.71 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10132721943657348		[learning rate: 3.02e-05]
	Learning Rate: 3.01995e-05
	LOSS [training: 0.10132721943657348 | validation: 0.2099504340909495]
	TIME [epoch: 5.71 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10128936640877267		[learning rate: 3.0093e-05]
	Learning Rate: 3.00927e-05
	LOSS [training: 0.10128936640877267 | validation: 0.19954312248914657]
	TIME [epoch: 5.71 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1011318925324444		[learning rate: 2.9986e-05]
	Learning Rate: 2.99863e-05
	LOSS [training: 0.1011318925324444 | validation: 0.20385372920308506]
	TIME [epoch: 5.71 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10033028035443899		[learning rate: 2.988e-05]
	Learning Rate: 2.98803e-05
	LOSS [training: 0.10033028035443899 | validation: 0.20838864582199662]
	TIME [epoch: 5.72 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10030422967957325		[learning rate: 2.9775e-05]
	Learning Rate: 2.97746e-05
	LOSS [training: 0.10030422967957325 | validation: 0.20729953510528612]
	TIME [epoch: 5.71 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10105340722286865		[learning rate: 2.9669e-05]
	Learning Rate: 2.96693e-05
	LOSS [training: 0.10105340722286865 | validation: 0.21398820870476332]
	TIME [epoch: 5.71 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10416689509631283		[learning rate: 2.9564e-05]
	Learning Rate: 2.95644e-05
	LOSS [training: 0.10416689509631283 | validation: 0.19675654106113713]
	TIME [epoch: 5.72 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10163663211123654		[learning rate: 2.946e-05]
	Learning Rate: 2.94599e-05
	LOSS [training: 0.10163663211123654 | validation: 0.1980719186299586]
	TIME [epoch: 5.72 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10244379597228837		[learning rate: 2.9356e-05]
	Learning Rate: 2.93557e-05
	LOSS [training: 0.10244379597228837 | validation: 0.19811249727473845]
	TIME [epoch: 5.74 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10296040588357162		[learning rate: 2.9252e-05]
	Learning Rate: 2.92519e-05
	LOSS [training: 0.10296040588357162 | validation: 0.20364376932975145]
	TIME [epoch: 5.72 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10066018089294769		[learning rate: 2.9148e-05]
	Learning Rate: 2.91485e-05
	LOSS [training: 0.10066018089294769 | validation: 0.2140711794808628]
	TIME [epoch: 5.71 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10169396914013572		[learning rate: 2.9045e-05]
	Learning Rate: 2.90454e-05
	LOSS [training: 0.10169396914013572 | validation: 0.199818518841964]
	TIME [epoch: 5.72 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10067788397748718		[learning rate: 2.8943e-05]
	Learning Rate: 2.89427e-05
	LOSS [training: 0.10067788397748718 | validation: 0.20193286134388036]
	TIME [epoch: 5.71 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10123464456046265		[learning rate: 2.884e-05]
	Learning Rate: 2.88403e-05
	LOSS [training: 0.10123464456046265 | validation: 0.20708283633047594]
	TIME [epoch: 5.72 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09822827463755118		[learning rate: 2.8738e-05]
	Learning Rate: 2.87383e-05
	LOSS [training: 0.09822827463755118 | validation: 0.20660931014986825]
	TIME [epoch: 5.71 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09984252804248028		[learning rate: 2.8637e-05]
	Learning Rate: 2.86367e-05
	LOSS [training: 0.09984252804248028 | validation: 0.20362318978906116]
	TIME [epoch: 5.71 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09911312878430799		[learning rate: 2.8535e-05]
	Learning Rate: 2.85355e-05
	LOSS [training: 0.09911312878430799 | validation: 0.20833619266623638]
	TIME [epoch: 5.71 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09829983237577214		[learning rate: 2.8435e-05]
	Learning Rate: 2.84345e-05
	LOSS [training: 0.09829983237577214 | validation: 0.18710114929785635]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1706.pth
	Model improved!!!
EPOCH 1707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09959103928629581		[learning rate: 2.8334e-05]
	Learning Rate: 2.8334e-05
	LOSS [training: 0.09959103928629581 | validation: 0.20968069100310924]
	TIME [epoch: 5.71 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09881367804286378		[learning rate: 2.8234e-05]
	Learning Rate: 2.82338e-05
	LOSS [training: 0.09881367804286378 | validation: 0.20482145857009604]
	TIME [epoch: 5.72 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10093514823261752		[learning rate: 2.8134e-05]
	Learning Rate: 2.8134e-05
	LOSS [training: 0.10093514823261752 | validation: 0.19528214084216522]
	TIME [epoch: 5.72 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09976542015382682		[learning rate: 2.8034e-05]
	Learning Rate: 2.80345e-05
	LOSS [training: 0.09976542015382682 | validation: 0.202964822638819]
	TIME [epoch: 5.71 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09980048373359804		[learning rate: 2.7935e-05]
	Learning Rate: 2.79353e-05
	LOSS [training: 0.09980048373359804 | validation: 0.20003706551810574]
	TIME [epoch: 5.71 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09910132840181846		[learning rate: 2.7837e-05]
	Learning Rate: 2.78366e-05
	LOSS [training: 0.09910132840181846 | validation: 0.21247413049334166]
	TIME [epoch: 5.71 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1013835906648768		[learning rate: 2.7738e-05]
	Learning Rate: 2.77381e-05
	LOSS [training: 0.1013835906648768 | validation: 0.2044574157113937]
	TIME [epoch: 5.71 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09960318377075482		[learning rate: 2.764e-05]
	Learning Rate: 2.764e-05
	LOSS [training: 0.09960318377075482 | validation: 0.2054824665608027]
	TIME [epoch: 5.71 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09946747385169985		[learning rate: 2.7542e-05]
	Learning Rate: 2.75423e-05
	LOSS [training: 0.09946747385169985 | validation: 0.2134772419500166]
	TIME [epoch: 5.71 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09905529974941266		[learning rate: 2.7445e-05]
	Learning Rate: 2.74449e-05
	LOSS [training: 0.09905529974941266 | validation: 0.2037259467598081]
	TIME [epoch: 5.71 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09956629088315794		[learning rate: 2.7348e-05]
	Learning Rate: 2.73478e-05
	LOSS [training: 0.09956629088315794 | validation: 0.19758985590177972]
	TIME [epoch: 5.71 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09922932721674094		[learning rate: 2.7251e-05]
	Learning Rate: 2.72511e-05
	LOSS [training: 0.09922932721674094 | validation: 0.2051752564838124]
	TIME [epoch: 5.72 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10041013676748914		[learning rate: 2.7155e-05]
	Learning Rate: 2.71548e-05
	LOSS [training: 0.10041013676748914 | validation: 0.19807627290773772]
	TIME [epoch: 5.71 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10135479842101106		[learning rate: 2.7059e-05]
	Learning Rate: 2.70587e-05
	LOSS [training: 0.10135479842101106 | validation: 0.20413068451255123]
	TIME [epoch: 5.71 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09931196149245856		[learning rate: 2.6963e-05]
	Learning Rate: 2.69631e-05
	LOSS [training: 0.09931196149245856 | validation: 0.2102995170606596]
	TIME [epoch: 5.71 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0991670410464625		[learning rate: 2.6868e-05]
	Learning Rate: 2.68677e-05
	LOSS [training: 0.0991670410464625 | validation: 0.19929840269616506]
	TIME [epoch: 5.71 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09956062993836656		[learning rate: 2.6773e-05]
	Learning Rate: 2.67727e-05
	LOSS [training: 0.09956062993836656 | validation: 0.20057204906745937]
	TIME [epoch: 5.71 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09918625712926503		[learning rate: 2.6678e-05]
	Learning Rate: 2.6678e-05
	LOSS [training: 0.09918625712926503 | validation: 0.19565459220132764]
	TIME [epoch: 5.71 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09770272365466735		[learning rate: 2.6584e-05]
	Learning Rate: 2.65837e-05
	LOSS [training: 0.09770272365466735 | validation: 0.2021553745431481]
	TIME [epoch: 5.71 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09994463742607756		[learning rate: 2.649e-05]
	Learning Rate: 2.64897e-05
	LOSS [training: 0.09994463742607756 | validation: 0.19859296936960025]
	TIME [epoch: 5.71 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09849840766691563		[learning rate: 2.6396e-05]
	Learning Rate: 2.6396e-05
	LOSS [training: 0.09849840766691563 | validation: 0.20506172315956683]
	TIME [epoch: 5.71 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09827868013868539		[learning rate: 2.6303e-05]
	Learning Rate: 2.63027e-05
	LOSS [training: 0.09827868013868539 | validation: 0.20221909149590972]
	TIME [epoch: 5.72 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09891956619361776		[learning rate: 2.621e-05]
	Learning Rate: 2.62097e-05
	LOSS [training: 0.09891956619361776 | validation: 0.20619959896358175]
	TIME [epoch: 5.7 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09881812219077162		[learning rate: 2.6117e-05]
	Learning Rate: 2.6117e-05
	LOSS [training: 0.09881812219077162 | validation: 0.1953776198011975]
	TIME [epoch: 5.71 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10013723347487334		[learning rate: 2.6025e-05]
	Learning Rate: 2.60246e-05
	LOSS [training: 0.10013723347487334 | validation: 0.2016327021392498]
	TIME [epoch: 5.7 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09902488057703167		[learning rate: 2.5933e-05]
	Learning Rate: 2.59326e-05
	LOSS [training: 0.09902488057703167 | validation: 0.2017088776770212]
	TIME [epoch: 5.71 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10150985245805029		[learning rate: 2.5841e-05]
	Learning Rate: 2.58409e-05
	LOSS [training: 0.10150985245805029 | validation: 0.19664300376465266]
	TIME [epoch: 5.71 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0991005358363864		[learning rate: 2.575e-05]
	Learning Rate: 2.57495e-05
	LOSS [training: 0.0991005358363864 | validation: 0.21616709163791709]
	TIME [epoch: 5.71 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10005514023452051		[learning rate: 2.5658e-05]
	Learning Rate: 2.56585e-05
	LOSS [training: 0.10005514023452051 | validation: 0.20135801949074691]
	TIME [epoch: 5.7 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0982211007252981		[learning rate: 2.5568e-05]
	Learning Rate: 2.55677e-05
	LOSS [training: 0.0982211007252981 | validation: 0.1970356988852756]
	TIME [epoch: 5.71 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10020969646875515		[learning rate: 2.5477e-05]
	Learning Rate: 2.54773e-05
	LOSS [training: 0.10020969646875515 | validation: 0.20560628584439622]
	TIME [epoch: 5.71 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09915863396547504		[learning rate: 2.5387e-05]
	Learning Rate: 2.53872e-05
	LOSS [training: 0.09915863396547504 | validation: 0.19680284576853993]
	TIME [epoch: 5.71 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09708402303813285		[learning rate: 2.5297e-05]
	Learning Rate: 2.52975e-05
	LOSS [training: 0.09708402303813285 | validation: 0.19451777639470322]
	TIME [epoch: 5.71 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09905879018489526		[learning rate: 2.5208e-05]
	Learning Rate: 2.5208e-05
	LOSS [training: 0.09905879018489526 | validation: 0.19836163487961056]
	TIME [epoch: 5.71 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09965896301455487		[learning rate: 2.5119e-05]
	Learning Rate: 2.51189e-05
	LOSS [training: 0.09965896301455487 | validation: 0.2040625162966257]
	TIME [epoch: 5.71 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09972233063083778		[learning rate: 2.503e-05]
	Learning Rate: 2.503e-05
	LOSS [training: 0.09972233063083778 | validation: 0.19692265577330492]
	TIME [epoch: 5.71 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10061155900720133		[learning rate: 2.4942e-05]
	Learning Rate: 2.49415e-05
	LOSS [training: 0.10061155900720133 | validation: 0.20692796944806147]
	TIME [epoch: 5.71 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09938979469661043		[learning rate: 2.4853e-05]
	Learning Rate: 2.48533e-05
	LOSS [training: 0.09938979469661043 | validation: 0.2009143515755791]
	TIME [epoch: 5.71 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09922430197057491		[learning rate: 2.4765e-05]
	Learning Rate: 2.47655e-05
	LOSS [training: 0.09922430197057491 | validation: 0.20398050102665322]
	TIME [epoch: 5.7 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10013836519991344		[learning rate: 2.4678e-05]
	Learning Rate: 2.46779e-05
	LOSS [training: 0.10013836519991344 | validation: 0.19191933200443312]
	TIME [epoch: 5.71 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10037262233329103		[learning rate: 2.4591e-05]
	Learning Rate: 2.45906e-05
	LOSS [training: 0.10037262233329103 | validation: 0.19636101523090776]
	TIME [epoch: 5.71 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09789599099890939		[learning rate: 2.4504e-05]
	Learning Rate: 2.45037e-05
	LOSS [training: 0.09789599099890939 | validation: 0.20379418715074388]
	TIME [epoch: 5.71 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10015554579751176		[learning rate: 2.4417e-05]
	Learning Rate: 2.4417e-05
	LOSS [training: 0.10015554579751176 | validation: 0.20085764972002496]
	TIME [epoch: 5.71 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09932751389592698		[learning rate: 2.4331e-05]
	Learning Rate: 2.43307e-05
	LOSS [training: 0.09932751389592698 | validation: 0.19465543627905918]
	TIME [epoch: 5.71 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09923377512172382		[learning rate: 2.4245e-05]
	Learning Rate: 2.42446e-05
	LOSS [training: 0.09923377512172382 | validation: 0.19306974873183588]
	TIME [epoch: 5.71 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1005579763049895		[learning rate: 2.4159e-05]
	Learning Rate: 2.41589e-05
	LOSS [training: 0.1005579763049895 | validation: 0.20589326266938032]
	TIME [epoch: 5.71 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09704210496296108		[learning rate: 2.4073e-05]
	Learning Rate: 2.40735e-05
	LOSS [training: 0.09704210496296108 | validation: 0.2063670424378551]
	TIME [epoch: 5.71 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09803518506985605		[learning rate: 2.3988e-05]
	Learning Rate: 2.39883e-05
	LOSS [training: 0.09803518506985605 | validation: 0.2045131506885416]
	TIME [epoch: 5.72 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09746055860999027		[learning rate: 2.3904e-05]
	Learning Rate: 2.39035e-05
	LOSS [training: 0.09746055860999027 | validation: 0.1881263027638465]
	TIME [epoch: 5.71 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10013003666379328		[learning rate: 2.3819e-05]
	Learning Rate: 2.3819e-05
	LOSS [training: 0.10013003666379328 | validation: 0.19424405129494984]
	TIME [epoch: 5.71 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09945804953140457		[learning rate: 2.3735e-05]
	Learning Rate: 2.37347e-05
	LOSS [training: 0.09945804953140457 | validation: 0.20146618063455024]
	TIME [epoch: 5.71 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09858086987103257		[learning rate: 2.3651e-05]
	Learning Rate: 2.36508e-05
	LOSS [training: 0.09858086987103257 | validation: 0.1956395387249786]
	TIME [epoch: 5.71 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10051107905909692		[learning rate: 2.3567e-05]
	Learning Rate: 2.35672e-05
	LOSS [training: 0.10051107905909692 | validation: 0.1952119196219356]
	TIME [epoch: 5.71 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09887279754508342		[learning rate: 2.3484e-05]
	Learning Rate: 2.34838e-05
	LOSS [training: 0.09887279754508342 | validation: 0.20517502102136576]
	TIME [epoch: 5.71 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09783831278026531		[learning rate: 2.3401e-05]
	Learning Rate: 2.34008e-05
	LOSS [training: 0.09783831278026531 | validation: 0.1996082411237874]
	TIME [epoch: 5.71 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09863140958680426		[learning rate: 2.3318e-05]
	Learning Rate: 2.33181e-05
	LOSS [training: 0.09863140958680426 | validation: 0.19608768020452233]
	TIME [epoch: 5.71 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09882145876004855		[learning rate: 2.3236e-05]
	Learning Rate: 2.32356e-05
	LOSS [training: 0.09882145876004855 | validation: 0.20121288221664715]
	TIME [epoch: 5.71 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09767108199852352		[learning rate: 2.3153e-05]
	Learning Rate: 2.31534e-05
	LOSS [training: 0.09767108199852352 | validation: 0.19784206766072931]
	TIME [epoch: 5.71 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09843857710593829		[learning rate: 2.3072e-05]
	Learning Rate: 2.30716e-05
	LOSS [training: 0.09843857710593829 | validation: 0.1951039007268487]
	TIME [epoch: 5.71 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0986667019512371		[learning rate: 2.299e-05]
	Learning Rate: 2.299e-05
	LOSS [training: 0.0986667019512371 | validation: 0.20438946137983693]
	TIME [epoch: 5.71 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0991471850180029		[learning rate: 2.2909e-05]
	Learning Rate: 2.29087e-05
	LOSS [training: 0.0991471850180029 | validation: 0.2010885174324423]
	TIME [epoch: 5.71 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09788820669149682		[learning rate: 2.2828e-05]
	Learning Rate: 2.28277e-05
	LOSS [training: 0.09788820669149682 | validation: 0.19816016184633634]
	TIME [epoch: 5.71 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09776786080237987		[learning rate: 2.2747e-05]
	Learning Rate: 2.27469e-05
	LOSS [training: 0.09776786080237987 | validation: 0.20160873985142064]
	TIME [epoch: 5.7 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09763670305171576		[learning rate: 2.2667e-05]
	Learning Rate: 2.26665e-05
	LOSS [training: 0.09763670305171576 | validation: 0.1982388661033514]
	TIME [epoch: 5.72 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09816244172235992		[learning rate: 2.2586e-05]
	Learning Rate: 2.25864e-05
	LOSS [training: 0.09816244172235992 | validation: 0.20262267444909693]
	TIME [epoch: 5.7 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09881439471245956		[learning rate: 2.2506e-05]
	Learning Rate: 2.25065e-05
	LOSS [training: 0.09881439471245956 | validation: 0.20256606757588363]
	TIME [epoch: 5.71 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09794536258333093		[learning rate: 2.2427e-05]
	Learning Rate: 2.24269e-05
	LOSS [training: 0.09794536258333093 | validation: 0.20310303707419558]
	TIME [epoch: 5.71 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10069179927069388		[learning rate: 2.2348e-05]
	Learning Rate: 2.23476e-05
	LOSS [training: 0.10069179927069388 | validation: 0.20009119318971136]
	TIME [epoch: 5.71 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09868613124731651		[learning rate: 2.2269e-05]
	Learning Rate: 2.22686e-05
	LOSS [training: 0.09868613124731651 | validation: 0.19377816256377914]
	TIME [epoch: 5.71 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09858770967165971		[learning rate: 2.219e-05]
	Learning Rate: 2.21898e-05
	LOSS [training: 0.09858770967165971 | validation: 0.19783262729431347]
	TIME [epoch: 5.71 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09768527785150316		[learning rate: 2.2111e-05]
	Learning Rate: 2.21114e-05
	LOSS [training: 0.09768527785150316 | validation: 0.192615981876613]
	TIME [epoch: 5.7 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09737274769761664		[learning rate: 2.2033e-05]
	Learning Rate: 2.20332e-05
	LOSS [training: 0.09737274769761664 | validation: 0.19505068342627585]
	TIME [epoch: 5.71 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09805671158399076		[learning rate: 2.1955e-05]
	Learning Rate: 2.19553e-05
	LOSS [training: 0.09805671158399076 | validation: 0.20302996606889423]
	TIME [epoch: 5.71 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09830935424389775		[learning rate: 2.1878e-05]
	Learning Rate: 2.18776e-05
	LOSS [training: 0.09830935424389775 | validation: 0.19828033718104332]
	TIME [epoch: 5.72 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0962779134511569		[learning rate: 2.18e-05]
	Learning Rate: 2.18003e-05
	LOSS [training: 0.0962779134511569 | validation: 0.18883548932148014]
	TIME [epoch: 5.71 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09584892320398632		[learning rate: 2.1723e-05]
	Learning Rate: 2.17232e-05
	LOSS [training: 0.09584892320398632 | validation: 0.19870309544127884]
	TIME [epoch: 5.71 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09798995622094711		[learning rate: 2.1646e-05]
	Learning Rate: 2.16464e-05
	LOSS [training: 0.09798995622094711 | validation: 0.20479232432156788]
	TIME [epoch: 5.71 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0990580929199037		[learning rate: 2.157e-05]
	Learning Rate: 2.15698e-05
	LOSS [training: 0.0990580929199037 | validation: 0.19878810499822325]
	TIME [epoch: 5.71 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09780652874905268		[learning rate: 2.1494e-05]
	Learning Rate: 2.14935e-05
	LOSS [training: 0.09780652874905268 | validation: 0.20207061153830352]
	TIME [epoch: 5.71 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09727195355421053		[learning rate: 2.1418e-05]
	Learning Rate: 2.14175e-05
	LOSS [training: 0.09727195355421053 | validation: 0.18892863682840916]
	TIME [epoch: 5.72 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0977150401804024		[learning rate: 2.1342e-05]
	Learning Rate: 2.13418e-05
	LOSS [training: 0.0977150401804024 | validation: 0.19632230006646215]
	TIME [epoch: 5.71 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09886295492830169		[learning rate: 2.1266e-05]
	Learning Rate: 2.12663e-05
	LOSS [training: 0.09886295492830169 | validation: 0.2085900480198255]
	TIME [epoch: 5.71 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09839367293222949		[learning rate: 2.1191e-05]
	Learning Rate: 2.11911e-05
	LOSS [training: 0.09839367293222949 | validation: 0.19541739681807332]
	TIME [epoch: 5.71 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09699379768432975		[learning rate: 2.1116e-05]
	Learning Rate: 2.11162e-05
	LOSS [training: 0.09699379768432975 | validation: 0.19504375113690567]
	TIME [epoch: 5.71 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09696364740549943		[learning rate: 2.1042e-05]
	Learning Rate: 2.10415e-05
	LOSS [training: 0.09696364740549943 | validation: 0.1893690079239559]
	TIME [epoch: 5.71 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09561161219758577		[learning rate: 2.0967e-05]
	Learning Rate: 2.09671e-05
	LOSS [training: 0.09561161219758577 | validation: 0.2006360148330514]
	TIME [epoch: 5.71 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09875563409897162		[learning rate: 2.0893e-05]
	Learning Rate: 2.0893e-05
	LOSS [training: 0.09875563409897162 | validation: 0.19603196438931975]
	TIME [epoch: 5.7 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09799328100635375		[learning rate: 2.0819e-05]
	Learning Rate: 2.08191e-05
	LOSS [training: 0.09799328100635375 | validation: 0.18921076567562456]
	TIME [epoch: 5.71 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09808397011732474		[learning rate: 2.0745e-05]
	Learning Rate: 2.07455e-05
	LOSS [training: 0.09808397011732474 | validation: 0.2038528730866987]
	TIME [epoch: 5.7 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09650520821601148		[learning rate: 2.0672e-05]
	Learning Rate: 2.06721e-05
	LOSS [training: 0.09650520821601148 | validation: 0.19963412569696037]
	TIME [epoch: 5.71 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09953061845615879		[learning rate: 2.0599e-05]
	Learning Rate: 2.0599e-05
	LOSS [training: 0.09953061845615879 | validation: 0.20497813742531357]
	TIME [epoch: 5.71 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10050008912561778		[learning rate: 2.0526e-05]
	Learning Rate: 2.05262e-05
	LOSS [training: 0.10050008912561778 | validation: 0.19510224142025157]
	TIME [epoch: 5.71 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09707925010341742		[learning rate: 2.0454e-05]
	Learning Rate: 2.04536e-05
	LOSS [training: 0.09707925010341742 | validation: 0.19863455789065593]
	TIME [epoch: 5.71 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09733269519885981		[learning rate: 2.0381e-05]
	Learning Rate: 2.03812e-05
	LOSS [training: 0.09733269519885981 | validation: 0.2032198083353746]
	TIME [epoch: 5.71 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09776716617053585		[learning rate: 2.0309e-05]
	Learning Rate: 2.03092e-05
	LOSS [training: 0.09776716617053585 | validation: 0.1905772345485215]
	TIME [epoch: 5.71 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09790685122027708		[learning rate: 2.0237e-05]
	Learning Rate: 2.02374e-05
	LOSS [training: 0.09790685122027708 | validation: 0.19954801623769547]
	TIME [epoch: 5.71 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0964333826890994		[learning rate: 2.0166e-05]
	Learning Rate: 2.01658e-05
	LOSS [training: 0.0964333826890994 | validation: 0.19832754107807432]
	TIME [epoch: 5.71 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09600263290038537		[learning rate: 2.0094e-05]
	Learning Rate: 2.00945e-05
	LOSS [training: 0.09600263290038537 | validation: 0.2013331375532204]
	TIME [epoch: 5.71 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0966192214411328		[learning rate: 2.0023e-05]
	Learning Rate: 2.00234e-05
	LOSS [training: 0.0966192214411328 | validation: 0.19828526578796557]
	TIME [epoch: 5.7 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09718138687557513		[learning rate: 1.9953e-05]
	Learning Rate: 1.99526e-05
	LOSS [training: 0.09718138687557513 | validation: 0.19792069889469488]
	TIME [epoch: 5.72 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.097935468262673		[learning rate: 1.9882e-05]
	Learning Rate: 1.98821e-05
	LOSS [training: 0.097935468262673 | validation: 0.19294135785577315]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_1_v_mmd4_1807.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 7282.202 seconds.
