Args:
Namespace(name='model_phi1_1a_distortion_v1r_2_v_mmd1', outdir='out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1', training_data='data/training_data/distortions/paraboloids/data_phi1_1a_distortion_v1r_2/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_1a_distortion_v1r_2/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.06624465, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3379854051

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.541419532706302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.541419532706302 | validation: 7.154823436444078]
	TIME [epoch: 412 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.01970225917994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.01970225917994 | validation: 6.860759015459686]
	TIME [epoch: 6.02 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.630360118730485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.630360118730485 | validation: 6.2518093348275645]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.071827474185863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.071827474185863 | validation: 5.644123329528124]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.600485806605095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.600485806605095 | validation: 5.153030377106302]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.15868621191595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.15868621191595 | validation: 5.170822227853069]
	TIME [epoch: 5.99 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.865593015054014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.865593015054014 | validation: 4.572377440461565]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.4756591999103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4756591999103 | validation: 4.418770346817415]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.4302673993028465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4302673993028465 | validation: 4.277303761978551]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.29613869639508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.29613869639508 | validation: 4.124839428658589]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.123210304891065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.123210304891065 | validation: 4.044800294810912]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.062056586572837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.062056586572837 | validation: 4.084967720705112]
	TIME [epoch: 5.99 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.957798551103104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.957798551103104 | validation: 3.9120569576711146]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8684021456971927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8684021456971927 | validation: 3.812318964801956]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.771303659505507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.771303659505507 | validation: 3.7158176175547712]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6680297741815022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6680297741815022 | validation: 3.8457205991455874]
	TIME [epoch: 5.99 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6828131198075598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6828131198075598 | validation: 3.6415273914518886]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5893752757455513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5893752757455513 | validation: 3.4454232947599817]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4244676562277148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4244676562277148 | validation: 3.431977659155773]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6384113036767483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6384113036767483 | validation: 3.5694929518152767]
	TIME [epoch: 5.98 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.39901603365159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.39901603365159 | validation: 3.312774036527367]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3905380041296054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3905380041296054 | validation: 3.4080688685799334]
	TIME [epoch: 5.99 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3242610171226366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3242610171226366 | validation: 3.269434288067037]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2812046839387836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2812046839387836 | validation: 3.2573178808687815]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.232449190528457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.232449190528457 | validation: 3.230978246356506]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.257130678535562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.257130678535562 | validation: 3.286903898224109]
	TIME [epoch: 5.99 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210989369793459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.210989369793459 | validation: 3.15585775434736]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2863661422375174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2863661422375174 | validation: 3.191205965913557]
	TIME [epoch: 6.23 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.141494139822697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.141494139822697 | validation: 3.187453387367853]
	TIME [epoch: 5.98 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.092684205152443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.092684205152443 | validation: 3.0718057691000205]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.170188376816159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.170188376816159 | validation: 3.6491104561660226]
	TIME [epoch: 6 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.35679174743695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.35679174743695 | validation: 3.218594145195045]
	TIME [epoch: 5.98 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0991214984096103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0991214984096103 | validation: 3.0651336936967146]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.281356411336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.281356411336 | validation: 3.2233511857602326]
	TIME [epoch: 6.19 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.216928715977397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.216928715977397 | validation: 3.1074894609546693]
	TIME [epoch: 5.98 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.041421519007612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.041421519007612 | validation: 3.1177764969587405]
	TIME [epoch: 5.98 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0407006559749536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0407006559749536 | validation: 3.0764433589394256]
	TIME [epoch: 5.98 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0948915668745887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0948915668745887 | validation: 2.989939474958229]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0172858156593314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0172858156593314 | validation: 2.9740751300364994]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1252217661199895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1252217661199895 | validation: 3.099039188253001]
	TIME [epoch: 6 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.003087202668712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.003087202668712 | validation: 2.955277784882034]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9745682132509086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9745682132509086 | validation: 3.1481390314001514]
	TIME [epoch: 5.98 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1076519715329796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1076519715329796 | validation: 2.979315444558635]
	TIME [epoch: 5.98 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.956281799284109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.956281799284109 | validation: 2.991372695829968]
	TIME [epoch: 5.99 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.984999044114231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.984999044114231 | validation: 2.9380830132159534]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1688618224933136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1688618224933136 | validation: 3.072921090495634]
	TIME [epoch: 6.26 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.108515160178974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.108515160178974 | validation: 3.082176349778726]
	TIME [epoch: 5.98 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.950678269200359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.950678269200359 | validation: 2.8936149345173288]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5192696162055253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5192696162055253 | validation: 3.3564417238611557]
	TIME [epoch: 5.99 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0965280462046394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0965280462046394 | validation: 2.948772259774792]
	TIME [epoch: 5.98 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.921448124354576		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 2.921448124354576 | validation: 2.8823885543568677]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9934525950632915		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 2.9934525950632915 | validation: 2.9754029888343436]
	TIME [epoch: 5.99 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.933448716847948		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 2.933448716847948 | validation: 2.8499448718139617]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8477682984021264		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 2.8477682984021264 | validation: 2.882855974345194]
	TIME [epoch: 5.99 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2380099434746548		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 3.2380099434746548 | validation: 3.100619391342298]
	TIME [epoch: 5.98 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.993817051513321		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 2.993817051513321 | validation: 3.4719189419690855]
	TIME [epoch: 5.98 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.446635576956559		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 3.446635576956559 | validation: 3.163405006619633]
	TIME [epoch: 5.98 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1621951357407463		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 3.1621951357407463 | validation: 3.1034246263521963]
	TIME [epoch: 5.98 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.109327890191052		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 3.109327890191052 | validation: 3.0615288459551824]
	TIME [epoch: 5.98 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.072891323171014		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 3.072891323171014 | validation: 3.0949204098240473]
	TIME [epoch: 5.98 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1119726695797		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 3.1119726695797 | validation: 3.067780726036479]
	TIME [epoch: 5.98 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.060994321943352		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 3.060994321943352 | validation: 2.9610735240909203]
	TIME [epoch: 5.99 sec]
EPOCH 63/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 0.010000000000000002
		[batch 4/4] avg loss: 3.0199590384640187		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 3.0199590384640187 | validation: 2.8605516293089144]
	TIME [epoch: 434 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.774668456093891		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 2.774668456093891 | validation: 2.5708054163754315]
	TIME [epoch: 6.03 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6008494694054374		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 2.6008494694054374 | validation: 2.4721915018149967]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5360360944966773		[learning rate: 0.0094573]
	Learning Rate: 0.00945735
	LOSS [training: 2.5360360944966773 | validation: 2.370869055001259]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.482436358260305		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 2.482436358260305 | validation: 2.13860707438518]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4736051254916642		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 2.4736051254916642 | validation: 5.560423260656011]
	TIME [epoch: 5.99 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.85760668257799		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 5.85760668257799 | validation: 5.41995771359602]
	TIME [epoch: 5.99 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.113501128990608		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 5.113501128990608 | validation: 6.7042654001249655]
	TIME [epoch: 6 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.655792860100602		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 6.655792860100602 | validation: 6.285627277126456]
	TIME [epoch: 6.07 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.050226816972519		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 6.050226816972519 | validation: 6.463434725443097]
	TIME [epoch: 5.98 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.062589572060253		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 6.062589572060253 | validation: 6.468164852152426]
	TIME [epoch: 5.98 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.324992045467704		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 6.324992045467704 | validation: 5.916330776630369]
	TIME [epoch: 5.98 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.424213815430412		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 5.424213815430412 | validation: 5.529765149384415]
	TIME [epoch: 6.16 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.18229156296186		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 6.18229156296186 | validation: 6.324109928820038]
	TIME [epoch: 5.99 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.213206611214282		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 6.213206611214282 | validation: 5.858020407511164]
	TIME [epoch: 5.99 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.173453074861802		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 5.173453074861802 | validation: 5.232105701757035]
	TIME [epoch: 5.98 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.937756601413132		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 4.937756601413132 | validation: 4.3762508130977995]
	TIME [epoch: 5.98 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.510041221992511		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 4.510041221992511 | validation: 4.239605244914237]
	TIME [epoch: 5.99 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.149167166598248		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 4.149167166598248 | validation: 3.7283296313702152]
	TIME [epoch: 5.98 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.782835955490646		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 3.782835955490646 | validation: 3.5072098175779765]
	TIME [epoch: 5.98 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5820609266515815		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 3.5820609266515815 | validation: 3.3044952218636467]
	TIME [epoch: 5.98 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3893101150296956		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 3.3893101150296956 | validation: 3.0981537698617005]
	TIME [epoch: 5.98 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1999559086138247		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 3.1999559086138247 | validation: 2.966478264908408]
	TIME [epoch: 5.99 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.080394635245509		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 3.080394635245509 | validation: 2.89001731712379]
	TIME [epoch: 5.99 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0167535323259465		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 3.0167535323259465 | validation: 2.8270635084559643]
	TIME [epoch: 5.98 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9699184594285084		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 2.9699184594285084 | validation: 2.754693264992019]
	TIME [epoch: 5.98 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9185465154198456		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 2.9185465154198456 | validation: 2.6616019464147485]
	TIME [epoch: 5.98 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8582452011676334		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 2.8582452011676334 | validation: 2.590022927106215]
	TIME [epoch: 5.99 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7881777621106716		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 2.7881777621106716 | validation: 2.474420868755189]
	TIME [epoch: 5.99 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7166061549698775		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 2.7166061549698775 | validation: 2.3562192738860874]
	TIME [epoch: 5.99 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.648563093408897		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 2.648563093408897 | validation: 2.2105329631815955]
	TIME [epoch: 6.29 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.559268028141686		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 2.559268028141686 | validation: 2.021422365126795]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4773933395198		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 2.4773933395198 | validation: 1.9387463379228937]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8359093594780838		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 2.8359093594780838 | validation: 4.584035023602706]
	TIME [epoch: 5.99 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.519528552848351		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 4.519528552848351 | validation: 3.7458806153551643]
	TIME [epoch: 5.98 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.146415812028341		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 4.146415812028341 | validation: 4.285204338285494]
	TIME [epoch: 5.99 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.070221611874091		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 4.070221611874091 | validation: 3.990704320744544]
	TIME [epoch: 6 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.388753198212853		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 4.388753198212853 | validation: 4.328787474033268]
	TIME [epoch: 5.99 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.325271893739584		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 4.325271893739584 | validation: 4.150425399344591]
	TIME [epoch: 6.01 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.208693182327788		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 4.208693182327788 | validation: 5.414047333859193]
	TIME [epoch: 5.99 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.325940118394218		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 6.325940118394218 | validation: 6.607247413802231]
	TIME [epoch: 5.99 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.36243064968638		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 6.36243064968638 | validation: 5.146895970253357]
	TIME [epoch: 5.98 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.7058518715882105		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 5.7058518715882105 | validation: 5.61658328140503]
	TIME [epoch: 5.99 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.03588168680395		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 5.03588168680395 | validation: 5.389174530923235]
	TIME [epoch: 5.98 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.307510931770944		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 5.307510931770944 | validation: 5.993216586010071]
	TIME [epoch: 5.98 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.711611815599429		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 5.711611815599429 | validation: 5.369338243226194]
	TIME [epoch: 5.98 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.170718057695707		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 5.170718057695707 | validation: 5.773053511081458]
	TIME [epoch: 5.99 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.201651004957396		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 5.201651004957396 | validation: 3.3814018564683272]
	TIME [epoch: 5.99 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8934307934360755		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 3.8934307934360755 | validation: 5.886851387802709]
	TIME [epoch: 5.99 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.959169986552974		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 5.959169986552974 | validation: 5.853391954131496]
	TIME [epoch: 5.97 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.359495996530004		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 5.359495996530004 | validation: 3.8012323708878863]
	TIME [epoch: 5.99 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7446577283175184		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 3.7446577283175184 | validation: 3.524001400409726]
	TIME [epoch: 5.99 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6150867591944493		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 3.6150867591944493 | validation: 3.467913511083444]
	TIME [epoch: 5.99 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.525108896480025		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 3.525108896480025 | validation: 3.524946427538892]
	TIME [epoch: 6.31 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4418207596351102		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 3.4418207596351102 | validation: 3.289752884910643]
	TIME [epoch: 5.98 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3844718244865035		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 3.3844718244865035 | validation: 3.1551105668840322]
	TIME [epoch: 5.98 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2837392424481227		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 3.2837392424481227 | validation: 3.082531878819147]
	TIME [epoch: 5.98 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.211044570979728		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 3.211044570979728 | validation: 3.0008512004242394]
	TIME [epoch: 5.98 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1424960335724705		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 3.1424960335724705 | validation: 2.9436183582700384]
	TIME [epoch: 5.99 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0846050362108755		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 3.0846050362108755 | validation: 2.922017118534813]
	TIME [epoch: 5.98 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0373598476968833		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 3.0373598476968833 | validation: 2.9608859713456583]
	TIME [epoch: 5.98 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0165076872857646		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 3.0165076872857646 | validation: 2.7166263721567456]
	TIME [epoch: 5.99 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9413257166995534		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 2.9413257166995534 | validation: 2.7181249728595214]
	TIME [epoch: 5.99 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.856651143060086		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 2.856651143060086 | validation: 2.4918825540094667]
	TIME [epoch: 5.98 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.766105538459748		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 2.766105538459748 | validation: 2.7057093446733598]
	TIME [epoch: 5.97 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.17915471528028		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 4.17915471528028 | validation: 3.937484231453158]
	TIME [epoch: 6.27 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.485381015319069		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 3.485381015319069 | validation: 2.5316338879582325]
	TIME [epoch: 5.98 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8066566821923464		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 2.8066566821923464 | validation: 2.379748949458471]
	TIME [epoch: 5.98 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.685172229593355		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 2.685172229593355 | validation: 2.2441325680362243]
	TIME [epoch: 5.99 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6247309359213586		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 2.6247309359213586 | validation: 2.1096786358649684]
	TIME [epoch: 5.99 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5144309023446123		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 2.5144309023446123 | validation: 1.9297628653847931]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.530466098650376		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 2.530466098650376 | validation: 1.9098402465902762]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.444528039598179		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 2.444528039598179 | validation: 1.8370599810072321]
	TIME [epoch: 6.05 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.362523874728177		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 2.362523874728177 | validation: 1.8067441925183712]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.302085390260494		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 2.302085390260494 | validation: 1.8815552477045394]
	TIME [epoch: 6.01 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4468503426507318		[learning rate: 0.0073282]
	Learning Rate: 0.00732824
	LOSS [training: 2.4468503426507318 | validation: 1.73559596984266]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3851239748327107		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 2.3851239748327107 | validation: 2.477469827733069]
	TIME [epoch: 5.99 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0433698374914115		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 3.0433698374914115 | validation: 3.838211240594904]
	TIME [epoch: 6 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.674124373209881		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 3.674124373209881 | validation: 2.5755978822198173]
	TIME [epoch: 6 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7462818182681814		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 2.7462818182681814 | validation: 2.4500811263040196]
	TIME [epoch: 6 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6579698715692652		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 2.6579698715692652 | validation: 2.374849349216218]
	TIME [epoch: 5.99 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.608832396384556		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 2.608832396384556 | validation: 2.290434632477111]
	TIME [epoch: 5.99 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.565623595072193		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 2.565623595072193 | validation: 2.203941657210343]
	TIME [epoch: 6.31 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.508357501485932		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 2.508357501485932 | validation: 2.061748609937861]
	TIME [epoch: 6 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.501225669291631		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 2.501225669291631 | validation: 1.9743681839793497]
	TIME [epoch: 5.99 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.452525345612522		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 2.452525345612522 | validation: 1.9636230178104528]
	TIME [epoch: 5.99 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.410988753533372		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 2.410988753533372 | validation: 2.0952956280798514]
	TIME [epoch: 5.99 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4167992398685416		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 2.4167992398685416 | validation: 1.994635518716716]
	TIME [epoch: 5.99 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.391752190519071		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 2.391752190519071 | validation: 1.7667185354042203]
	TIME [epoch: 5.99 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3323153205548284		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 2.3323153205548284 | validation: 1.8251985565450637]
	TIME [epoch: 5.99 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4302216372981142		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 2.4302216372981142 | validation: 1.9023776631977605]
	TIME [epoch: 5.99 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3337316634055054		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 2.3337316634055054 | validation: 1.6428098549858399]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2080331640621598		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 2.2080331640621598 | validation: 2.2799063740860825]
	TIME [epoch: 6.01 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7095379683013876		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 2.7095379683013876 | validation: 1.7222892258251208]
	TIME [epoch: 5.99 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0858645412000496		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 2.0858645412000496 | validation: 1.9209044763095466]
	TIME [epoch: 5.99 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9569468474229677		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 1.9569468474229677 | validation: 2.161254809298433]
	TIME [epoch: 5.98 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1389419875234603		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 2.1389419875234603 | validation: 1.6887238358451855]
	TIME [epoch: 5.99 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7237289868894607		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 1.7237289868894607 | validation: 2.330502303510066]
	TIME [epoch: 6 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.011825242092328		[learning rate: 0.0067548]
	Learning Rate: 0.00675484
	LOSS [training: 2.011825242092328 | validation: 1.6312773655438244]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_161.pth
	Model improved!!!
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7091544775061456		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 1.7091544775061456 | validation: 1.6536721389342688]
	TIME [epoch: 6.69 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5946141348464984		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 1.5946141348464984 | validation: 1.5140942790350846]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5793964280028412		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 1.5793964280028412 | validation: 1.6373655408361527]
	TIME [epoch: 6 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7657889660053683		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 1.7657889660053683 | validation: 1.6038384404287187]
	TIME [epoch: 5.98 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5339348154645613		[learning rate: 0.0066363]
	Learning Rate: 0.00663626
	LOSS [training: 1.5339348154645613 | validation: 1.8906978045514835]
	TIME [epoch: 5.98 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7603681186484896		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 1.7603681186484896 | validation: 1.8930750496311943]
	TIME [epoch: 5.98 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.509442796656465		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 1.509442796656465 | validation: 1.4938829377363636]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5198416618764852		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 1.5198416618764852 | validation: 1.433574799326795]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4783130622458374		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 1.4783130622458374 | validation: 1.493023214925582]
	TIME [epoch: 6 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4693694811701319		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 1.4693694811701319 | validation: 1.3698448589106182]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_171.pth
	Model improved!!!
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3542234406784817		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 1.3542234406784817 | validation: 1.61502317542414]
	TIME [epoch: 6.01 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4688029950948298		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 1.4688029950948298 | validation: 1.392699141995518]
	TIME [epoch: 5.99 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.260504588382075		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 1.260504588382075 | validation: 1.195260869551113]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0679842878708565		[learning rate: 0.006428]
	Learning Rate: 0.00642802
	LOSS [training: 1.0679842878708565 | validation: 1.921566923180341]
	TIME [epoch: 6 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.61507078882693		[learning rate: 0.0064053]
	Learning Rate: 0.00640528
	LOSS [training: 1.61507078882693 | validation: 1.6035598324700122]
	TIME [epoch: 5.99 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.167081405665236		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 1.167081405665236 | validation: 1.2556023257661892]
	TIME [epoch: 6 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.137140702052694		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 1.137140702052694 | validation: 1.5784376925417702]
	TIME [epoch: 6.09 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3735700427407513		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 1.3735700427407513 | validation: 1.0252682478457718]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9873861496296268		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.9873861496296268 | validation: 3.125826498188456]
	TIME [epoch: 6.02 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8518497269355296		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 1.8518497269355296 | validation: 1.1369925679418638]
	TIME [epoch: 5.99 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.059513681216623		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 1.059513681216623 | validation: 0.8435905838953195]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8515782605605109		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.8515782605605109 | validation: 0.8962985039599141]
	TIME [epoch: 5.99 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8318266329916424		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.8318266329916424 | validation: 1.2379052288176777]
	TIME [epoch: 6.04 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0598555043485811		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 1.0598555043485811 | validation: 0.9268063132880272]
	TIME [epoch: 6 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9034914699589585		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.9034914699589585 | validation: 1.1429502666130302]
	TIME [epoch: 6 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.992847740714537		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.992847740714537 | validation: 0.8564220762631048]
	TIME [epoch: 5.99 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4681950652996825		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 1.4681950652996825 | validation: 0.9298352774676737]
	TIME [epoch: 5.99 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.948013705581958		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.948013705581958 | validation: 0.8974538543192514]
	TIME [epoch: 6 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8292038190843695		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.8292038190843695 | validation: 0.9955460067912204]
	TIME [epoch: 5.99 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.962551530980263		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.962551530980263 | validation: 0.5769198647415528]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0053858980519155		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 1.0053858980519155 | validation: 0.9270290145554575]
	TIME [epoch: 6 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8019287475860898		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.8019287475860898 | validation: 0.7882253712096075]
	TIME [epoch: 5.98 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7834766961707698		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.7834766961707698 | validation: 0.7253367592519535]
	TIME [epoch: 5.99 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7774584340507253		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.7774584340507253 | validation: 1.2086235464037247]
	TIME [epoch: 5.99 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8217969183744482		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.8217969183744482 | validation: 0.7597685587131394]
	TIME [epoch: 5.98 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7059616137389292		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.7059616137389292 | validation: 0.9598314838529827]
	TIME [epoch: 6 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0623448265692785		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 1.0623448265692785 | validation: 0.6349095459275627]
	TIME [epoch: 5.97 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7809461772813058		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.7809461772813058 | validation: 0.6578444954000279]
	TIME [epoch: 5.97 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7590968984437741		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.7590968984437741 | validation: 1.0123625127559635]
	TIME [epoch: 5.98 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8654219816441686		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.8654219816441686 | validation: 0.8242326073463846]
	TIME [epoch: 442 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8774385660770028		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.8774385660770028 | validation: 0.8176461996165676]
	TIME [epoch: 11.8 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7845114655634037		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.7845114655634037 | validation: 0.7726190877279737]
	TIME [epoch: 11.8 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9110855422947308		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 1.9110855422947308 | validation: 1.7014597042476522]
	TIME [epoch: 11.8 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0459891723121504		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 1.0459891723121504 | validation: 0.8827094968072928]
	TIME [epoch: 11.8 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.709073137216008		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.709073137216008 | validation: 0.7447992258877959]
	TIME [epoch: 11.8 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7057593247191936		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.7057593247191936 | validation: 0.640042909542921]
	TIME [epoch: 11.8 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.841973011393587		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.841973011393587 | validation: 0.5589660123929714]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6961861188271461		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.6961861188271461 | validation: 0.5245822447384733]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_209.pth
	Model improved!!!
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6908330610908855		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.6908330610908855 | validation: 0.6660991031614272]
	TIME [epoch: 11.8 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7903753936098041		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.7903753936098041 | validation: 0.8733949732702482]
	TIME [epoch: 11.8 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6741028504714104		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.6741028504714104 | validation: 0.8678012011305734]
	TIME [epoch: 11.9 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7525453405816932		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.7525453405816932 | validation: 0.9835340676761685]
	TIME [epoch: 11.8 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7466165481225571		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.7466165481225571 | validation: 0.6296171642241044]
	TIME [epoch: 11.8 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6547238963077189		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.6547238963077189 | validation: 0.8313099061572629]
	TIME [epoch: 11.8 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7221271833481997		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.7221271833481997 | validation: 0.7205338662283594]
	TIME [epoch: 11.8 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9197130120654622		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.9197130120654622 | validation: 0.688158501637308]
	TIME [epoch: 11.8 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7297224313276389		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.7297224313276389 | validation: 0.518520611318003]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6766292821585711		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.6766292821585711 | validation: 0.620285845052174]
	TIME [epoch: 11.8 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7122167658268108		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.7122167658268108 | validation: 0.5142116431578463]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6717710793937762		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.6717710793937762 | validation: 0.9984755497369173]
	TIME [epoch: 11.8 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8050191902175504		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.8050191902175504 | validation: 0.5370783282666522]
	TIME [epoch: 11.8 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6385088843978088		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.6385088843978088 | validation: 3.007486858318291]
	TIME [epoch: 11.8 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6360839502142766		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 1.6360839502142766 | validation: 0.8229740176956692]
	TIME [epoch: 11.8 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7765262074466052		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.7765262074466052 | validation: 0.7067149802633422]
	TIME [epoch: 11.8 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6279647696329852		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.6279647696329852 | validation: 0.5318335696648075]
	TIME [epoch: 11.8 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5727298535231352		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.5727298535231352 | validation: 0.7202074334426438]
	TIME [epoch: 11.8 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6898548508399975		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.6898548508399975 | validation: 0.8534615425043706]
	TIME [epoch: 11.8 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6249183763141063		[learning rate: 0.0053088]
	Learning Rate: 0.00530884
	LOSS [training: 0.6249183763141063 | validation: 0.7140332459088146]
	TIME [epoch: 11.8 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8788625673032002		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.8788625673032002 | validation: 0.6594442976428727]
	TIME [epoch: 11.8 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5951085033165594		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.5951085033165594 | validation: 0.5449452805493323]
	TIME [epoch: 11.8 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6522663564540749		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.6522663564540749 | validation: 3.0364319570953184]
	TIME [epoch: 11.8 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8581760304730368		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 1.8581760304730368 | validation: 0.920253765030815]
	TIME [epoch: 11.8 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8348850669153196		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.8348850669153196 | validation: 0.9763876382134635]
	TIME [epoch: 11.8 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6513339780039393		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.6513339780039393 | validation: 1.2683562416363978]
	TIME [epoch: 11.8 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8470519714881785		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.8470519714881785 | validation: 0.4273497803762515]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5723143384353413		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.5723143384353413 | validation: 0.4990082919505651]
	TIME [epoch: 11.8 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7471452606725096		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.7471452606725096 | validation: 0.524931058059064]
	TIME [epoch: 11.8 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6528913423809888		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.6528913423809888 | validation: 0.4084367055206446]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6591382881281136		[learning rate: 0.005106]
	Learning Rate: 0.00510595
	LOSS [training: 0.6591382881281136 | validation: 0.3934844063724956]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5836087339832812		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.5836087339832812 | validation: 0.8120725742195682]
	TIME [epoch: 11.8 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6346853151428196		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.6346853151428196 | validation: 0.5087319081108118]
	TIME [epoch: 11.8 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6220961351721965		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.6220961351721965 | validation: 0.46940787422474894]
	TIME [epoch: 11.8 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47556807114923366		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.47556807114923366 | validation: 0.8554022495832181]
	TIME [epoch: 11.8 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6414441120230522		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.6414441120230522 | validation: 0.6059277860728727]
	TIME [epoch: 11.8 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6818163632943214		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.6818163632943214 | validation: 0.7341057390330397]
	TIME [epoch: 11.8 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5900703787676804		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.5900703787676804 | validation: 0.44971785389702035]
	TIME [epoch: 11.8 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.566907788426971		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.566907788426971 | validation: 0.43456263524629996]
	TIME [epoch: 11.8 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5311363640238465		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.5311363640238465 | validation: 0.5199891752333203]
	TIME [epoch: 11.8 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5060010854883251		[learning rate: 0.0049282]
	Learning Rate: 0.00492825
	LOSS [training: 0.5060010854883251 | validation: 0.4235797080379846]
	TIME [epoch: 11.8 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5303743124650406		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.5303743124650406 | validation: 0.4102946150130806]
	TIME [epoch: 11.8 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8359207601081422		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.8359207601081422 | validation: 0.5738528531157427]
	TIME [epoch: 11.8 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.846512208883281		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.846512208883281 | validation: 0.5602305316064146]
	TIME [epoch: 11.8 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5757988112697531		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.5757988112697531 | validation: 0.4695464606375227]
	TIME [epoch: 11.8 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47701043793580333		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.47701043793580333 | validation: 0.5824291333991991]
	TIME [epoch: 11.8 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5864503270703081		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.5864503270703081 | validation: 0.6848498801267697]
	TIME [epoch: 11.8 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5576963590214999		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.5576963590214999 | validation: 0.7497566979011419]
	TIME [epoch: 11.8 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47224963604239106		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.47224963604239106 | validation: 0.6340840070111029]
	TIME [epoch: 11.8 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6181355827787269		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.6181355827787269 | validation: 0.5911890989558617]
	TIME [epoch: 11.8 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5415844088725685		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.5415844088725685 | validation: 0.578265908430332]
	TIME [epoch: 11.8 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5790935274913366		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.5790935274913366 | validation: 0.5746569306629137]
	TIME [epoch: 11.8 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49818035951198403		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.49818035951198403 | validation: 0.7440223002134103]
	TIME [epoch: 11.8 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46750224889404735		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.46750224889404735 | validation: 0.5946688094998984]
	TIME [epoch: 11.8 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47590505816496237		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.47590505816496237 | validation: 0.5272026089197067]
	TIME [epoch: 11.8 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4448481255421335		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.4448481255421335 | validation: 0.7620776337410663]
	TIME [epoch: 11.8 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4661619338503287		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.4661619338503287 | validation: 0.6602361676721733]
	TIME [epoch: 11.8 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5532694159519433		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.5532694159519433 | validation: 0.9094136199487649]
	TIME [epoch: 11.8 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7904682562719022		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.7904682562719022 | validation: 0.520459134104893]
	TIME [epoch: 11.8 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.498298091459667		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.498298091459667 | validation: 0.48089757681228484]
	TIME [epoch: 11.8 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4456375698895141		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.4456375698895141 | validation: 0.4337370955961889]
	TIME [epoch: 11.8 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4918168284287185		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.4918168284287185 | validation: 0.37204594333210045]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_271.pth
	Model improved!!!
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5035984534031406		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.5035984534031406 | validation: 0.29912696207420286]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_272.pth
	Model improved!!!
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4386060872162132		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.4386060872162132 | validation: 0.3561032957493594]
	TIME [epoch: 11.8 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4600738869050009		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.4600738869050009 | validation: 0.32790278439203735]
	TIME [epoch: 11.8 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.603870728115821		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.603870728115821 | validation: 0.38737675993651133]
	TIME [epoch: 11.8 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5511349757977795		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.5511349757977795 | validation: 0.2731252939185891]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_276.pth
	Model improved!!!
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4685735985086992		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.4685735985086992 | validation: 0.2769244983209531]
	TIME [epoch: 11.8 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3944388481848544		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.3944388481848544 | validation: 0.38987180363738577]
	TIME [epoch: 11.8 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41966168588417985		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.41966168588417985 | validation: 0.42559178440964185]
	TIME [epoch: 11.8 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42123453183585535		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.42123453183585535 | validation: 0.582786975881441]
	TIME [epoch: 11.8 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46310681924548225		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.46310681924548225 | validation: 0.5441081448947367]
	TIME [epoch: 11.8 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4883968823501487		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.4883968823501487 | validation: 0.5590834978112803]
	TIME [epoch: 11.8 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5104406998146416		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.5104406998146416 | validation: 0.3040611942199727]
	TIME [epoch: 11.8 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37118988424402743		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.37118988424402743 | validation: 0.39098941602856363]
	TIME [epoch: 11.8 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3835565429426076		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.3835565429426076 | validation: 0.6879914318750011]
	TIME [epoch: 11.8 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.566301644649755		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.566301644649755 | validation: 0.5756617918691742]
	TIME [epoch: 11.8 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45798139855603476		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.45798139855603476 | validation: 0.4959105968708373]
	TIME [epoch: 11.8 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41896053995868976		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.41896053995868976 | validation: 0.41188198773376483]
	TIME [epoch: 11.8 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43530205667209687		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.43530205667209687 | validation: 0.34291105839316116]
	TIME [epoch: 11.8 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40609420302026045		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.40609420302026045 | validation: 0.41785663811524515]
	TIME [epoch: 11.8 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4242353016382072		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.4242353016382072 | validation: 0.3851810900815864]
	TIME [epoch: 11.8 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4224435805322688		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.4224435805322688 | validation: 0.400318687449171]
	TIME [epoch: 11.8 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4217279355015379		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.4217279355015379 | validation: 0.5149561665118492]
	TIME [epoch: 11.8 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36744452830506424		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.36744452830506424 | validation: 0.41227829684639405]
	TIME [epoch: 11.8 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.410364237417438		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.410364237417438 | validation: 0.8215836631002102]
	TIME [epoch: 11.8 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5229704843834956		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.5229704843834956 | validation: 0.3295308992692154]
	TIME [epoch: 11.8 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41953734671161347		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.41953734671161347 | validation: 0.360369958249981]
	TIME [epoch: 11.8 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4428733027728673		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.4428733027728673 | validation: 0.4525669708670538]
	TIME [epoch: 11.8 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39371669509723084		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.39371669509723084 | validation: 0.6314490032960098]
	TIME [epoch: 11.8 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.553234395092097		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.553234395092097 | validation: 0.35826189219377197]
	TIME [epoch: 11.8 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45250930631631575		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.45250930631631575 | validation: 0.3020756969821086]
	TIME [epoch: 11.8 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4925725441066544		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.4925725441066544 | validation: 0.599250321849972]
	TIME [epoch: 11.8 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37141203156805236		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.37141203156805236 | validation: 0.4600097649295142]
	TIME [epoch: 11.8 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4387842270228568		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.4387842270228568 | validation: 0.46448436136949783]
	TIME [epoch: 11.8 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44964729639365886		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.44964729639365886 | validation: 0.2633002578850653]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46314538857674936		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.46314538857674936 | validation: 0.5507819495222828]
	TIME [epoch: 11.8 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37477856547391		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.37477856547391 | validation: 0.2895854255608771]
	TIME [epoch: 11.8 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3559806216363891		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.3559806216363891 | validation: 0.5942107606778743]
	TIME [epoch: 11.8 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36262625345138744		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.36262625345138744 | validation: 0.4570772858913207]
	TIME [epoch: 11.8 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3715009429999756		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.3715009429999756 | validation: 0.350075738854825]
	TIME [epoch: 11.8 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4155467294710161		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.4155467294710161 | validation: 0.3953835198439375]
	TIME [epoch: 11.8 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3488685784496225		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.3488685784496225 | validation: 0.46410673992469864]
	TIME [epoch: 11.8 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3370451439886574		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.3370451439886574 | validation: 0.3678857683317365]
	TIME [epoch: 11.8 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3466843384594943		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.3466843384594943 | validation: 0.4009398443896659]
	TIME [epoch: 11.8 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4467001266798836		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.4467001266798836 | validation: 0.2139212953003947]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_315.pth
	Model improved!!!
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3536893209812566		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.3536893209812566 | validation: 0.35128961436207806]
	TIME [epoch: 11.8 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7098487023994137		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.7098487023994137 | validation: 0.9351259291117433]
	TIME [epoch: 11.8 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5498322311890022		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.5498322311890022 | validation: 0.4492669180112384]
	TIME [epoch: 11.8 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4230987542922245		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.4230987542922245 | validation: 0.3528433505635784]
	TIME [epoch: 11.8 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3243892252923277		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.3243892252923277 | validation: 0.39458889565288024]
	TIME [epoch: 11.8 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3654697867238641		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.3654697867238641 | validation: 0.6372001107457266]
	TIME [epoch: 11.8 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5373145840084366		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.5373145840084366 | validation: 0.48345655585853453]
	TIME [epoch: 11.8 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.358696344994515		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.358696344994515 | validation: 0.3743571350880541]
	TIME [epoch: 11.8 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3040498326039337		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.3040498326039337 | validation: 0.41558745994233415]
	TIME [epoch: 11.8 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45151438412193307		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.45151438412193307 | validation: 0.22177081916829758]
	TIME [epoch: 11.8 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32740467106463833		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.32740467106463833 | validation: 0.3910559644033184]
	TIME [epoch: 11.8 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3658164562361851		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.3658164562361851 | validation: 0.5814862702118431]
	TIME [epoch: 11.8 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38036886814574566		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.38036886814574566 | validation: 0.5361548519166837]
	TIME [epoch: 11.8 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.339934852520445		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.339934852520445 | validation: 0.40212282948618017]
	TIME [epoch: 11.8 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3328772769924146		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.3328772769924146 | validation: 0.22177887260836407]
	TIME [epoch: 11.8 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46164993270373067		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.46164993270373067 | validation: 0.3308122659231822]
	TIME [epoch: 11.8 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3604962121230454		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.3604962121230454 | validation: 0.35673758996121485]
	TIME [epoch: 11.8 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36394390724833126		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.36394390724833126 | validation: 0.46531152989873004]
	TIME [epoch: 11.8 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3608338916835986		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.3608338916835986 | validation: 0.39598349835207225]
	TIME [epoch: 11.8 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4077835682049252		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.4077835682049252 | validation: 0.43172395352713006]
	TIME [epoch: 11.8 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3334984842981712		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.3334984842981712 | validation: 0.4025978013952647]
	TIME [epoch: 11.8 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32762278591424715		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.32762278591424715 | validation: 0.3187678345970718]
	TIME [epoch: 11.8 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6018657144597003		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.6018657144597003 | validation: 0.2527126048734796]
	TIME [epoch: 11.8 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3308098069966551		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.3308098069966551 | validation: 0.23440571534789467]
	TIME [epoch: 11.8 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32423899396700157		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.32423899396700157 | validation: 0.4126747347257397]
	TIME [epoch: 11.8 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3661673356704429		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.3661673356704429 | validation: 0.336605177934358]
	TIME [epoch: 11.8 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3729743159546278		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.3729743159546278 | validation: 0.2061783623440019]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_342.pth
	Model improved!!!
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3156162716988317		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.3156162716988317 | validation: 0.20609400984409543]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5842806893987565		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.5842806893987565 | validation: 0.4448955106125166]
	TIME [epoch: 11.8 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48066932793708883		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.48066932793708883 | validation: 0.3129012858218965]
	TIME [epoch: 11.8 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33986473580479265		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.33986473580479265 | validation: 0.3799342500471918]
	TIME [epoch: 11.8 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4854576384386624		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.4854576384386624 | validation: 0.3814612049575361]
	TIME [epoch: 11.8 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3639737879015782		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.3639737879015782 | validation: 0.24370279437099168]
	TIME [epoch: 11.8 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31466785496084315		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.31466785496084315 | validation: 0.22460719917032063]
	TIME [epoch: 11.8 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4219576148544189		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.4219576148544189 | validation: 0.5507126322795662]
	TIME [epoch: 11.8 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3946852934775983		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.3946852934775983 | validation: 0.5076671893625551]
	TIME [epoch: 11.8 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3496949759561563		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.3496949759561563 | validation: 0.3445127397306463]
	TIME [epoch: 11.8 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36426970628693983		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.36426970628693983 | validation: 0.32641638069730305]
	TIME [epoch: 11.8 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3608241912830046		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.3608241912830046 | validation: 0.3610963240570573]
	TIME [epoch: 11.8 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32632045689412936		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.32632045689412936 | validation: 0.31372236502232276]
	TIME [epoch: 11.8 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3063753450549809		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.3063753450549809 | validation: 0.6834446355181284]
	TIME [epoch: 11.8 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37560550147366933		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.37560550147366933 | validation: 0.2413316032341593]
	TIME [epoch: 11.8 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30864385219033674		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.30864385219033674 | validation: 0.539126242237979]
	TIME [epoch: 11.8 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4336025989642137		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.4336025989642137 | validation: 0.5392745820799707]
	TIME [epoch: 11.8 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3506796726984916		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.3506796726984916 | validation: 0.3664986899042265]
	TIME [epoch: 11.8 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3256561496453328		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.3256561496453328 | validation: 0.34798677911980985]
	TIME [epoch: 11.8 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37237039432574626		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.37237039432574626 | validation: 0.2504569349938835]
	TIME [epoch: 11.8 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29729383973266316		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.29729383973266316 | validation: 0.3448559564367889]
	TIME [epoch: 11.8 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2532702087869282		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.2532702087869282 | validation: 0.5781423904817854]
	TIME [epoch: 11.8 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8352118086931191		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.8352118086931191 | validation: 0.49056699751227806]
	TIME [epoch: 11.8 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9041849481535611		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.9041849481535611 | validation: 0.4983308635700069]
	TIME [epoch: 11.8 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.335576856444407		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.335576856444407 | validation: 0.25128420840172166]
	TIME [epoch: 11.8 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3186235424402394		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.3186235424402394 | validation: 0.7371574412118225]
	TIME [epoch: 11.8 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7116542714257913		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.7116542714257913 | validation: 0.6745627212914115]
	TIME [epoch: 11.8 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43826955767446835		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.43826955767446835 | validation: 0.4424221644511952]
	TIME [epoch: 11.8 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3404333379083304		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.3404333379083304 | validation: 0.302381900474444]
	TIME [epoch: 11.8 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29211234841987044		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.29211234841987044 | validation: 0.32641354444735554]
	TIME [epoch: 11.8 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.352252879490888		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.352252879490888 | validation: 0.2564896011079819]
	TIME [epoch: 11.8 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31881834101964646		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.31881834101964646 | validation: 0.47663177864686584]
	TIME [epoch: 11.8 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3659821177929447		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.3659821177929447 | validation: 0.32580162380163014]
	TIME [epoch: 11.8 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32702801405368126		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.32702801405368126 | validation: 0.23878537935572153]
	TIME [epoch: 11.8 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.286382107463302		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.286382107463302 | validation: 0.38398640289764374]
	TIME [epoch: 11.8 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28843671913073754		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.28843671913073754 | validation: 0.45159886559447715]
	TIME [epoch: 11.8 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3251742991351546		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.3251742991351546 | validation: 0.34153457516182295]
	TIME [epoch: 11.8 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42991669222794265		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.42991669222794265 | validation: 0.35835111234504285]
	TIME [epoch: 11.8 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34413185949755953		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.34413185949755953 | validation: 0.2537573696137351]
	TIME [epoch: 11.8 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3086474423876851		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.3086474423876851 | validation: 0.3722401555782816]
	TIME [epoch: 11.8 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.302006375500002		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.302006375500002 | validation: 0.2696406116184682]
	TIME [epoch: 11.8 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2343342939623007		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.2343342939623007 | validation: 0.2086185480011961]
	TIME [epoch: 11.8 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2972358502014989		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.2972358502014989 | validation: 0.39464541594968217]
	TIME [epoch: 11.8 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2983763038657279		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.2983763038657279 | validation: 0.3689692279951158]
	TIME [epoch: 11.8 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33604870834267153		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.33604870834267153 | validation: 0.272923943294761]
	TIME [epoch: 11.8 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2591310488515169		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.2591310488515169 | validation: 0.381350552017547]
	TIME [epoch: 11.8 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30412039363426363		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.30412039363426363 | validation: 0.2137571728407216]
	TIME [epoch: 11.8 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.316039632555339		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.316039632555339 | validation: 0.627668794637749]
	TIME [epoch: 11.8 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3691123958392536		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.3691123958392536 | validation: 0.2712009515408093]
	TIME [epoch: 11.8 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3003257602920431		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.3003257602920431 | validation: 0.22931601849538746]
	TIME [epoch: 11.8 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2627302410053073		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.2627302410053073 | validation: 0.2177460437692459]
	TIME [epoch: 11.8 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2672782362546806		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.2672782362546806 | validation: 0.3342419613349247]
	TIME [epoch: 11.8 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3214186154344529		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.3214186154344529 | validation: 0.2879395998398703]
	TIME [epoch: 11.8 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30761628124692175		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.30761628124692175 | validation: 0.29864234054289446]
	TIME [epoch: 11.8 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30520536956173283		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.30520536956173283 | validation: 0.3241258943873987]
	TIME [epoch: 11.8 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23546858343014665		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.23546858343014665 | validation: 0.185041388835711]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_398.pth
	Model improved!!!
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26043577572942517		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.26043577572942517 | validation: 0.31341032268931357]
	TIME [epoch: 11.8 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37465307697425954		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.37465307697425954 | validation: 0.20471371988897413]
	TIME [epoch: 11.8 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.266009164840773		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.266009164840773 | validation: 0.3854486090607442]
	TIME [epoch: 11.8 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28896873228653924		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.28896873228653924 | validation: 0.3993488911724653]
	TIME [epoch: 11.8 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27026326129210876		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.27026326129210876 | validation: 0.32801860615239165]
	TIME [epoch: 11.8 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3357135433559072		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.3357135433559072 | validation: 2.0944443525952483]
	TIME [epoch: 11.8 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2351573023353524		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 1.2351573023353524 | validation: 0.2786981252854328]
	TIME [epoch: 11.8 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2839921073593273		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.2839921073593273 | validation: 0.3030883172219372]
	TIME [epoch: 11.8 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2547198814443811		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.2547198814443811 | validation: 0.3426710156652396]
	TIME [epoch: 12 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2758060893440411		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.2758060893440411 | validation: 0.25866643624169494]
	TIME [epoch: 11.8 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25439883654582457		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.25439883654582457 | validation: 0.27837989729340973]
	TIME [epoch: 11.8 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.307762087089087		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.307762087089087 | validation: 0.4928428687837854]
	TIME [epoch: 11.8 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2684127384693469		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.2684127384693469 | validation: 0.32700400676221464]
	TIME [epoch: 11.8 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27259370209470357		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.27259370209470357 | validation: 0.18813268336988948]
	TIME [epoch: 11.8 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32008309419481046		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.32008309419481046 | validation: 0.18653447911798005]
	TIME [epoch: 11.8 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37531686958706933		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.37531686958706933 | validation: 0.5513042268117077]
	TIME [epoch: 11.8 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2994607850261347		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.2994607850261347 | validation: 0.2385284870851001]
	TIME [epoch: 11.8 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25252417750758493		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.25252417750758493 | validation: 0.16061058236643116]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_416.pth
	Model improved!!!
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23526636905974313		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.23526636905974313 | validation: 0.22617661093190833]
	TIME [epoch: 11.8 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2386123702632643		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.2386123702632643 | validation: 0.2724494561914995]
	TIME [epoch: 11.8 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.256286104206209		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.256286104206209 | validation: 0.2873334789659213]
	TIME [epoch: 11.8 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25440152277591543		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.25440152277591543 | validation: 0.17247032228660156]
	TIME [epoch: 11.8 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2573716903179319		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.2573716903179319 | validation: 0.330561230701967]
	TIME [epoch: 11.8 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2649512147769541		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.2649512147769541 | validation: 0.30357282711094324]
	TIME [epoch: 11.8 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2522017878540885		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.2522017878540885 | validation: 0.8157320099328743]
	TIME [epoch: 11.8 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37930581170188243		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.37930581170188243 | validation: 0.22197633091223568]
	TIME [epoch: 11.8 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30699693197557937		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.30699693197557937 | validation: 0.2722366773646465]
	TIME [epoch: 11.8 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2272146331585333		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.2272146331585333 | validation: 0.29136696009559093]
	TIME [epoch: 11.8 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2208858600884827		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.2208858600884827 | validation: 0.2928199830199544]
	TIME [epoch: 11.8 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.233178317619061		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.233178317619061 | validation: 0.14747462159538444]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_428.pth
	Model improved!!!
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25906625463921645		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.25906625463921645 | validation: 0.17859317606184838]
	TIME [epoch: 11.8 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23383631242570846		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.23383631242570846 | validation: 0.21655118441192167]
	TIME [epoch: 11.8 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23326631549490004		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.23326631549490004 | validation: 0.42358417634620615]
	TIME [epoch: 11.8 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25654058769120347		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.25654058769120347 | validation: 0.2169319469525994]
	TIME [epoch: 11.8 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24192134002303056		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.24192134002303056 | validation: 0.27295409698005196]
	TIME [epoch: 11.8 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21610608774603396		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.21610608774603396 | validation: 0.3084889720328313]
	TIME [epoch: 11.8 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3081289099010823		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.3081289099010823 | validation: 0.2891761499482108]
	TIME [epoch: 11.8 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31690785633047586		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.31690785633047586 | validation: 0.1533613560308436]
	TIME [epoch: 11.8 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23087303141959026		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.23087303141959026 | validation: 0.24016216800836004]
	TIME [epoch: 11.8 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24372279983500744		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.24372279983500744 | validation: 0.29314673528094576]
	TIME [epoch: 11.8 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2410214580917494		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.2410214580917494 | validation: 0.1441404017535564]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_439.pth
	Model improved!!!
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2192115448397443		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.2192115448397443 | validation: 0.34313451548302243]
	TIME [epoch: 11.8 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3222206186184348		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.3222206186184348 | validation: 0.22701425854611493]
	TIME [epoch: 11.8 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3057709611178224		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.3057709611178224 | validation: 0.3404751548425017]
	TIME [epoch: 11.8 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2509027292528149		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.2509027292528149 | validation: 0.2884591649877877]
	TIME [epoch: 11.8 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24297960839114685		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.24297960839114685 | validation: 0.2525492756586316]
	TIME [epoch: 11.8 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3856214112518027		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.3856214112518027 | validation: 0.3243231684074359]
	TIME [epoch: 11.8 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27778381649707573		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.27778381649707573 | validation: 0.16310867026495607]
	TIME [epoch: 11.8 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2460345396863997		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.2460345396863997 | validation: 0.1589267439494081]
	TIME [epoch: 11.8 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23718207597620267		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.23718207597620267 | validation: 0.2541056561327193]
	TIME [epoch: 11.8 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2243879326395377		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.2243879326395377 | validation: 0.21016217461155043]
	TIME [epoch: 11.8 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20583841294755817		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.20583841294755817 | validation: 0.20830874077359654]
	TIME [epoch: 11.8 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19784038362141748		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.19784038362141748 | validation: 0.24294168352187936]
	TIME [epoch: 11.8 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2820508112535506		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.2820508112535506 | validation: 0.26396272207696997]
	TIME [epoch: 11.8 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24565040608167524		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.24565040608167524 | validation: 0.2074099591969133]
	TIME [epoch: 11.8 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25769242091655453		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.25769242091655453 | validation: 0.18412665170780002]
	TIME [epoch: 11.8 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2195496457383594		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.2195496457383594 | validation: 0.14350809653678265]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_455.pth
	Model improved!!!
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24605962749272522		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.24605962749272522 | validation: 0.36188315764191875]
	TIME [epoch: 11.8 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22960746599304627		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.22960746599304627 | validation: 0.16811322428671266]
	TIME [epoch: 11.8 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18040412515681195		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.18040412515681195 | validation: 0.22515813825171474]
	TIME [epoch: 11.8 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20534408154927342		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.20534408154927342 | validation: 0.18089329211363597]
	TIME [epoch: 11.8 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2526016459292151		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.2526016459292151 | validation: 0.22665175704408907]
	TIME [epoch: 11.8 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23231343520653347		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.23231343520653347 | validation: 0.3793747931134475]
	TIME [epoch: 11.8 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3075290411221046		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.3075290411221046 | validation: 0.23300989480572287]
	TIME [epoch: 11.8 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25642314044933295		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.25642314044933295 | validation: 0.28367849050203625]
	TIME [epoch: 11.8 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22552836974188745		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.22552836974188745 | validation: 0.21785882382350857]
	TIME [epoch: 11.8 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32471450134576163		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.32471450134576163 | validation: 0.19126479745159383]
	TIME [epoch: 11.8 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21277148844696955		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.21277148844696955 | validation: 0.8603753606207485]
	TIME [epoch: 11.8 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4610732069562198		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.4610732069562198 | validation: 0.22677390895141358]
	TIME [epoch: 11.8 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16848368674376638		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.16848368674376638 | validation: 0.13774064107363793]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_468.pth
	Model improved!!!
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16893113535267476		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.16893113535267476 | validation: 0.2612282555355499]
	TIME [epoch: 11.8 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21031742804110976		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.21031742804110976 | validation: 0.19265860856735]
	TIME [epoch: 11.8 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18486188004132867		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.18486188004132867 | validation: 0.2621581111174185]
	TIME [epoch: 11.8 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19944742046443006		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.19944742046443006 | validation: 0.252348639128682]
	TIME [epoch: 11.8 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2203375144720169		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.2203375144720169 | validation: 0.2028135147731418]
	TIME [epoch: 11.8 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1726485109507294		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.1726485109507294 | validation: 0.1355458291666397]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_474.pth
	Model improved!!!
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21099856990064877		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.21099856990064877 | validation: 0.18673714390383406]
	TIME [epoch: 11.8 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24433962655109523		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.24433962655109523 | validation: 0.19638094286618307]
	TIME [epoch: 11.8 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21394338457759438		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.21394338457759438 | validation: 0.40472131620538376]
	TIME [epoch: 11.8 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23284325985814716		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.23284325985814716 | validation: 0.1631308749049976]
	TIME [epoch: 11.8 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19542501941696847		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.19542501941696847 | validation: 0.6320585878649863]
	TIME [epoch: 11.8 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30234862046704725		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.30234862046704725 | validation: 0.20482475572196793]
	TIME [epoch: 11.8 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1999846266697036		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.1999846266697036 | validation: 0.30447115927016005]
	TIME [epoch: 11.8 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2633055752110095		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.2633055752110095 | validation: 0.19219282731369772]
	TIME [epoch: 11.8 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31667516389643524		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.31667516389643524 | validation: 0.2282705703320631]
	TIME [epoch: 11.8 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22202492631519274		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.22202492631519274 | validation: 0.16551764530264224]
	TIME [epoch: 11.8 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22321376083528846		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.22321376083528846 | validation: 0.15321898096675868]
	TIME [epoch: 11.8 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17704563949928515		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.17704563949928515 | validation: 0.14687740645483544]
	TIME [epoch: 11.8 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.219941299745887		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.219941299745887 | validation: 0.15822159122055202]
	TIME [epoch: 11.8 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1831101768354002		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.1831101768354002 | validation: 0.16288457983652133]
	TIME [epoch: 11.8 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20790426717652422		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.20790426717652422 | validation: 0.14908685500168178]
	TIME [epoch: 11.8 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18159687939351435		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.18159687939351435 | validation: 0.18401818570315354]
	TIME [epoch: 11.8 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23660480370364298		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.23660480370364298 | validation: 0.17960180775330753]
	TIME [epoch: 11.8 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2717295759017187		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.2717295759017187 | validation: 0.2529256900290116]
	TIME [epoch: 11.8 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19749327980851536		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.19749327980851536 | validation: 0.18992654700335027]
	TIME [epoch: 11.8 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2011177645509624		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.2011177645509624 | validation: 0.2104264867350057]
	TIME [epoch: 11.8 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2538041797946626		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.2538041797946626 | validation: 0.22872863902645335]
	TIME [epoch: 11.8 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19341042053847896		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.19341042053847896 | validation: 0.25947203405798536]
	TIME [epoch: 11.8 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20661865783096967		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.20661865783096967 | validation: 0.21531542085960287]
	TIME [epoch: 11.8 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1939009633059649		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.1939009633059649 | validation: 0.24245130495502426]
	TIME [epoch: 11.8 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1879760028916913		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.1879760028916913 | validation: 0.24575782155117215]
	TIME [epoch: 11.8 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16862052092550167		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.16862052092550167 | validation: 0.15161988260856724]
	TIME [epoch: 11.8 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22997988681588696		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.22997988681588696 | validation: 0.2415544105988775]
	TIME [epoch: 461 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20800552065176686		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.20800552065176686 | validation: 0.3414053954695915]
	TIME [epoch: 25.5 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18597810966785389		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.18597810966785389 | validation: 0.20407570527035157]
	TIME [epoch: 25.5 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1879837875882154		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.1879837875882154 | validation: 0.231626550737501]
	TIME [epoch: 25.5 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2657267446342261		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.2657267446342261 | validation: 0.30401604054417397]
	TIME [epoch: 25.5 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1939979022939987		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.1939979022939987 | validation: 0.31506296508454945]
	TIME [epoch: 25.5 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32774399201455473		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.32774399201455473 | validation: 0.2608492917664663]
	TIME [epoch: 25.5 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24204351857820927		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.24204351857820927 | validation: 0.16141104923666183]
	TIME [epoch: 25.5 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16686380903593917		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.16686380903593917 | validation: 0.4132492242254845]
	TIME [epoch: 25.5 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45766855586099026		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.45766855586099026 | validation: 0.2801037292234685]
	TIME [epoch: 25.5 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2733611274937859		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.2733611274937859 | validation: 0.22003817340802234]
	TIME [epoch: 25.5 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2723387071759836		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.2723387071759836 | validation: 0.23608170730060035]
	TIME [epoch: 25.5 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1828675536948896		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.1828675536948896 | validation: 0.18964162960318903]
	TIME [epoch: 25.5 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1788770178081206		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.1788770178081206 | validation: 0.22792739992708927]
	TIME [epoch: 25.5 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1766322944010632		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.1766322944010632 | validation: 0.2051339294773772]
	TIME [epoch: 25.5 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1824767896647262		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.1824767896647262 | validation: 0.22698555737098652]
	TIME [epoch: 25.5 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18055438387300216		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.18055438387300216 | validation: 0.18255820359348715]
	TIME [epoch: 25.5 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21059898604607738		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.21059898604607738 | validation: 0.15813221356023563]
	TIME [epoch: 25.5 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16152751760295445		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.16152751760295445 | validation: 0.21812040126269436]
	TIME [epoch: 25.5 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17624766237253028		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.17624766237253028 | validation: 0.13536860581102939]
	TIME [epoch: 25.5 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_520.pth
	Model improved!!!
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2094307804722823		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.2094307804722823 | validation: 0.2162698263670304]
	TIME [epoch: 25.5 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20056559752596614		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.20056559752596614 | validation: 0.1487801445346667]
	TIME [epoch: 25.5 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15891756212544536		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.15891756212544536 | validation: 0.27406570270413666]
	TIME [epoch: 25.5 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20416588127397464		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.20416588127397464 | validation: 0.2070785956059752]
	TIME [epoch: 25.5 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19435792498801494		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.19435792498801494 | validation: 0.12492410845304865]
	TIME [epoch: 25.5 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_525.pth
	Model improved!!!
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15289656219415587		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.15289656219415587 | validation: 0.22956752909834213]
	TIME [epoch: 25.5 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17909167370300444		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.17909167370300444 | validation: 0.2075348557673386]
	TIME [epoch: 25.5 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16461019526906		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.16461019526906 | validation: 0.1397732197636325]
	TIME [epoch: 25.5 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16199427752164608		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.16199427752164608 | validation: 0.2396446119777333]
	TIME [epoch: 25.5 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15669441473902285		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.15669441473902285 | validation: 0.18711342763411698]
	TIME [epoch: 25.5 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1964711618084816		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.1964711618084816 | validation: 0.18293680715781768]
	TIME [epoch: 25.5 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21156532911302686		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.21156532911302686 | validation: 0.21922367232118023]
	TIME [epoch: 25.5 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16484074150797776		[learning rate: 0.0018085]
	Learning Rate: 0.00180846
	LOSS [training: 0.16484074150797776 | validation: 0.18598621063435516]
	TIME [epoch: 25.5 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17407485861638097		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.17407485861638097 | validation: 0.1389328729052448]
	TIME [epoch: 25.5 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.189847682585008		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.189847682585008 | validation: 0.4953432807839786]
	TIME [epoch: 25.5 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23134059802067308		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.23134059802067308 | validation: 0.1757698845237332]
	TIME [epoch: 25.5 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17316597157285363		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.17316597157285363 | validation: 0.22127372829327155]
	TIME [epoch: 25.5 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14307190064415282		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.14307190064415282 | validation: 0.13835559723205432]
	TIME [epoch: 25.5 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18236707134481592		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.18236707134481592 | validation: 0.13304279279535364]
	TIME [epoch: 25.5 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1583608860052113		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.1583608860052113 | validation: 0.17823541266126625]
	TIME [epoch: 25.5 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15611275200014452		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.15611275200014452 | validation: 0.10950684539333197]
	TIME [epoch: 25.5 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_541.pth
	Model improved!!!
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.177824407315327		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.177824407315327 | validation: 0.2443367219807625]
	TIME [epoch: 25.5 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19404927327207527		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.19404927327207527 | validation: 0.18897557315771335]
	TIME [epoch: 25.5 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1921743976716353		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.1921743976716353 | validation: 0.181204817630565]
	TIME [epoch: 25.5 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17101314010984836		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.17101314010984836 | validation: 0.22634192456923335]
	TIME [epoch: 25.5 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24974823634135904		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.24974823634135904 | validation: 0.2409552528301597]
	TIME [epoch: 25.5 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24070763030139775		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.24070763030139775 | validation: 0.19198280163767736]
	TIME [epoch: 25.5 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1551692359925348		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.1551692359925348 | validation: 0.20874659731220746]
	TIME [epoch: 25.5 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14558131593830728		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.14558131593830728 | validation: 0.2400728703771911]
	TIME [epoch: 25.5 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14686276871077897		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.14686276871077897 | validation: 0.2086257287907714]
	TIME [epoch: 25.5 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13466394333600148		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.13466394333600148 | validation: 1.1797650894730674]
	TIME [epoch: 25.5 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4886058190133921		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.4886058190133921 | validation: 0.1560900157296506]
	TIME [epoch: 25.5 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11855339390000882		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.11855339390000882 | validation: 0.15856651593132587]
	TIME [epoch: 25.5 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14097154562072686		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.14097154562072686 | validation: 0.19018834661980077]
	TIME [epoch: 25.5 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14402966100128478		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.14402966100128478 | validation: 0.2210731428243966]
	TIME [epoch: 25.5 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12172598568804292		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.12172598568804292 | validation: 0.1206679814870855]
	TIME [epoch: 25.5 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15869812439737635		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.15869812439737635 | validation: 0.15649471077409477]
	TIME [epoch: 25.5 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18221323209991183		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.18221323209991183 | validation: 0.1508450490522969]
	TIME [epoch: 25.5 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13042470143529655		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.13042470143529655 | validation: 0.20157432370249423]
	TIME [epoch: 25.5 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16869454920893806		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.16869454920893806 | validation: 0.16397698453805742]
	TIME [epoch: 25.5 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1727295259720829		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.1727295259720829 | validation: 0.21688143277889804]
	TIME [epoch: 25.5 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14320592151903333		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.14320592151903333 | validation: 0.23109180773061117]
	TIME [epoch: 25.5 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22202799920138744		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.22202799920138744 | validation: 0.16945179625879847]
	TIME [epoch: 25.5 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16584327220196896		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.16584327220196896 | validation: 0.1264771795647673]
	TIME [epoch: 25.5 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13227497096776764		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.13227497096776764 | validation: 0.19417652088334908]
	TIME [epoch: 25.5 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12673121995650338		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.12673121995650338 | validation: 0.18505383683839566]
	TIME [epoch: 25.5 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14076890441896772		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.14076890441896772 | validation: 0.1913179714905583]
	TIME [epoch: 25.5 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14217928371026783		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.14217928371026783 | validation: 0.22891796225373218]
	TIME [epoch: 25.5 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14850280650764297		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.14850280650764297 | validation: 0.13646218916312594]
	TIME [epoch: 25.5 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1255354976888754		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.1255354976888754 | validation: 0.1702395292814849]
	TIME [epoch: 25.5 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15128857689078032		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.15128857689078032 | validation: 0.19194237355346672]
	TIME [epoch: 25.5 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19924285549487017		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.19924285549487017 | validation: 0.09347076464852777]
	TIME [epoch: 25.5 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_572.pth
	Model improved!!!
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1200824268582902		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.1200824268582902 | validation: 0.1895896152567148]
	TIME [epoch: 25.5 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1531860067042536		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.1531860067042536 | validation: 0.15425219527609474]
	TIME [epoch: 25.5 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12432813061276328		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.12432813061276328 | validation: 0.11034432732171512]
	TIME [epoch: 25.5 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15904612795418172		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.15904612795418172 | validation: 0.13862111003705982]
	TIME [epoch: 25.5 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15682888738675696		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.15682888738675696 | validation: 0.15817671874081934]
	TIME [epoch: 25.5 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16576074717470413		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.16576074717470413 | validation: 0.11176794459380385]
	TIME [epoch: 25.5 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1250647682670312		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.1250647682670312 | validation: 0.1189987134117791]
	TIME [epoch: 25.5 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16279456553586502		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.16279456553586502 | validation: 0.1634290749594536]
	TIME [epoch: 25.5 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11863785121550309		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.11863785121550309 | validation: 0.1240678040691627]
	TIME [epoch: 25.5 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16103737386812805		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.16103737386812805 | validation: 0.11355648970135476]
	TIME [epoch: 25.5 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15103183854347735		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.15103183854347735 | validation: 0.12254772233756962]
	TIME [epoch: 25.5 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10725840381599029		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.10725840381599029 | validation: 0.21196218412423218]
	TIME [epoch: 25.5 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16289237645592877		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.16289237645592877 | validation: 0.10622153053870423]
	TIME [epoch: 25.5 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1335894671066109		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.1335894671066109 | validation: 0.1131061827428586]
	TIME [epoch: 25.5 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13764878151946586		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.13764878151946586 | validation: 0.14598551798915566]
	TIME [epoch: 25.5 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1605525917821075		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.1605525917821075 | validation: 0.16469306155587024]
	TIME [epoch: 25.5 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15410074599477475		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.15410074599477475 | validation: 0.14460888495420798]
	TIME [epoch: 25.5 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12937089879224134		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.12937089879224134 | validation: 0.2296140350730497]
	TIME [epoch: 25.5 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14925630978834661		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.14925630978834661 | validation: 0.15047123002203408]
	TIME [epoch: 25.5 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12510021795679818		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.12510021795679818 | validation: 0.21298602708074368]
	TIME [epoch: 25.5 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13948953500098643		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.13948953500098643 | validation: 0.9041527876698474]
	TIME [epoch: 25.5 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42107306488207225		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.42107306488207225 | validation: 0.14445517126747304]
	TIME [epoch: 25.5 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1283798857146969		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.1283798857146969 | validation: 0.1057579337691543]
	TIME [epoch: 25.5 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11835804915958562		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.11835804915958562 | validation: 0.10896022672243488]
	TIME [epoch: 25.5 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12247244046975457		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.12247244046975457 | validation: 0.10583000499160808]
	TIME [epoch: 25.5 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1259461182757323		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.1259461182757323 | validation: 0.11746408641349156]
	TIME [epoch: 25.5 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11870008767174678		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.11870008767174678 | validation: 0.08052729156945929]
	TIME [epoch: 25.5 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_599.pth
	Model improved!!!
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12267997035360087		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.12267997035360087 | validation: 0.11599384669567961]
	TIME [epoch: 25.5 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12762798510252804		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.12762798510252804 | validation: 0.12571039678624127]
	TIME [epoch: 25.5 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1440709675969185		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.1440709675969185 | validation: 0.0970534883885151]
	TIME [epoch: 25.5 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15065839338634324		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.15065839338634324 | validation: 0.22728736901907715]
	TIME [epoch: 25.5 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11485833558594771		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.11485833558594771 | validation: 0.1961795528882841]
	TIME [epoch: 25.5 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14059819199798954		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.14059819199798954 | validation: 0.10898017958376828]
	TIME [epoch: 25.5 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12806217676570258		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.12806217676570258 | validation: 0.23805867502657496]
	TIME [epoch: 25.5 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13860769908217443		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.13860769908217443 | validation: 0.14302644688471977]
	TIME [epoch: 25.5 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15187955650976523		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.15187955650976523 | validation: 0.12652107806132712]
	TIME [epoch: 25.5 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09572992258998916		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.09572992258998916 | validation: 0.22097404736118562]
	TIME [epoch: 25.5 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13566611868478434		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.13566611868478434 | validation: 0.10324390280653956]
	TIME [epoch: 25.5 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11828452577396983		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.11828452577396983 | validation: 0.1845519989027551]
	TIME [epoch: 25.5 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11719617469443641		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.11719617469443641 | validation: 0.12018362498119883]
	TIME [epoch: 25.5 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15094779717226137		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.15094779717226137 | validation: 0.08443227089653317]
	TIME [epoch: 25.5 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1113279703634944		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.1113279703634944 | validation: 0.14019751799929522]
	TIME [epoch: 25.5 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12633827065999625		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.12633827065999625 | validation: 0.11528255466310856]
	TIME [epoch: 25.5 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0988954313822944		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.0988954313822944 | validation: 0.16360287877990423]
	TIME [epoch: 25.5 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10178105500961407		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.10178105500961407 | validation: 0.1183432915739759]
	TIME [epoch: 25.5 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12558680093672248		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.12558680093672248 | validation: 0.1460526949621876]
	TIME [epoch: 25.5 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10943560172333444		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.10943560172333444 | validation: 0.17632621428567363]
	TIME [epoch: 25.5 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11931160341649644		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.11931160341649644 | validation: 0.1830514211101486]
	TIME [epoch: 25.5 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16906176069764717		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.16906176069764717 | validation: 0.16848846696612343]
	TIME [epoch: 25.5 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11698167046528156		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.11698167046528156 | validation: 0.07953492402037574]
	TIME [epoch: 25.5 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_622.pth
	Model improved!!!
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10570877200724452		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.10570877200724452 | validation: 0.0979736053155037]
	TIME [epoch: 25.5 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12550552612029742		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.12550552612029742 | validation: 0.11440212538400851]
	TIME [epoch: 25.5 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0973033106425668		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.0973033106425668 | validation: 0.0815413229274696]
	TIME [epoch: 25.5 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11271946849919541		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.11271946849919541 | validation: 0.11296082601507021]
	TIME [epoch: 25.5 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10756706509199414		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.10756706509199414 | validation: 0.08910249928681038]
	TIME [epoch: 25.5 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10569894137628973		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.10569894137628973 | validation: 0.15098822568762918]
	TIME [epoch: 25.5 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11102534501946312		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.11102534501946312 | validation: 0.15224465096728612]
	TIME [epoch: 25.5 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11419992570366252		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.11419992570366252 | validation: 0.08354547013482073]
	TIME [epoch: 25.5 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09546122165311692		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.09546122165311692 | validation: 0.17774248933698988]
	TIME [epoch: 25.5 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15230002098103365		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.15230002098103365 | validation: 0.11430734646905438]
	TIME [epoch: 25.5 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10795146037173445		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.10795146037173445 | validation: 0.2414524285264475]
	TIME [epoch: 25.5 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12276925507194492		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.12276925507194492 | validation: 0.08527043896379324]
	TIME [epoch: 25.5 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08191726188070832		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.08191726188070832 | validation: 0.12009067118410452]
	TIME [epoch: 25.5 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12016236424767729		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.12016236424767729 | validation: 0.12208865015303591]
	TIME [epoch: 25.5 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1052736725477172		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.1052736725477172 | validation: 0.0831492126257048]
	TIME [epoch: 25.5 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10956933440990162		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.10956933440990162 | validation: 0.2266820044892602]
	TIME [epoch: 25.5 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10676852617348662		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.10676852617348662 | validation: 0.08650718191628357]
	TIME [epoch: 25.5 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09045329763272429		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.09045329763272429 | validation: 0.103111850695915]
	TIME [epoch: 25.5 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11250356640902258		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.11250356640902258 | validation: 0.059966560407131]
	TIME [epoch: 25.5 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_641.pth
	Model improved!!!
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07906066839932377		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.07906066839932377 | validation: 0.16915255107471103]
	TIME [epoch: 25.5 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15261521863498081		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.15261521863498081 | validation: 0.07388001565324562]
	TIME [epoch: 25.5 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09066760873582133		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.09066760873582133 | validation: 0.12682377906590092]
	TIME [epoch: 25.5 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11330822575079612		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.11330822575079612 | validation: 0.08257000488011887]
	TIME [epoch: 25.5 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1132768403705979		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.1132768403705979 | validation: 0.07802824131149474]
	TIME [epoch: 25.5 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10499584604599303		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.10499584604599303 | validation: 0.2397265676337325]
	TIME [epoch: 25.5 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11943079652122289		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.11943079652122289 | validation: 0.07609767667536937]
	TIME [epoch: 25.5 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10627833003334489		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.10627833003334489 | validation: 0.11722435602075003]
	TIME [epoch: 25.5 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09086307301474789		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.09086307301474789 | validation: 0.07466083503058833]
	TIME [epoch: 25.4 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21513910827690574		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.21513910827690574 | validation: 0.0818545171042461]
	TIME [epoch: 25.5 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09921787070492619		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.09921787070492619 | validation: 0.08108180396680513]
	TIME [epoch: 25.4 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10500545223610865		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.10500545223610865 | validation: 0.14423132259149668]
	TIME [epoch: 25.5 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10283127268356552		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.10283127268356552 | validation: 0.16882037348038412]
	TIME [epoch: 25.4 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13662791912035926		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.13662791912035926 | validation: 0.12275808184377313]
	TIME [epoch: 25.5 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09702112721945602		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.09702112721945602 | validation: 0.08823543062831331]
	TIME [epoch: 25.4 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11759145413335578		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.11759145413335578 | validation: 0.06369826765767957]
	TIME [epoch: 25.5 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08284328547311307		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.08284328547311307 | validation: 0.14621218574559455]
	TIME [epoch: 25.4 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11376300537503455		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.11376300537503455 | validation: 0.06427042554188793]
	TIME [epoch: 25.5 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10142583242883763		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.10142583242883763 | validation: 0.1917314635527147]
	TIME [epoch: 25.5 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10786575502880241		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.10786575502880241 | validation: 0.06668402338513685]
	TIME [epoch: 25.5 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10382239537098825		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.10382239537098825 | validation: 0.06785855314113465]
	TIME [epoch: 25.4 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08238342981246088		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.08238342981246088 | validation: 0.06714456227313036]
	TIME [epoch: 25.5 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08373816324026857		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.08373816324026857 | validation: 0.11734861666333912]
	TIME [epoch: 25.4 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09433224114343387		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.09433224114343387 | validation: 0.07125868240499425]
	TIME [epoch: 25.5 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08189345014586867		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.08189345014586867 | validation: 0.1278008975780032]
	TIME [epoch: 25.4 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11227243112197025		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.11227243112197025 | validation: 0.1029188105538471]
	TIME [epoch: 25.5 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.091754607435193		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.091754607435193 | validation: 0.0904104770208097]
	TIME [epoch: 25.4 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10935509795821889		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.10935509795821889 | validation: 0.0794037716739664]
	TIME [epoch: 25.5 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0857938556140757		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.0857938556140757 | validation: 0.08957839524532346]
	TIME [epoch: 25.4 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10272915648939733		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.10272915648939733 | validation: 0.08127176047444323]
	TIME [epoch: 25.5 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11775878867637055		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.11775878867637055 | validation: 0.29952989670135455]
	TIME [epoch: 25.4 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15606925516627576		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.15606925516627576 | validation: 0.12088607333234766]
	TIME [epoch: 25.5 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10982111088527395		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.10982111088527395 | validation: 0.1662169150358782]
	TIME [epoch: 25.4 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09533456286766984		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.09533456286766984 | validation: 0.08578755504851959]
	TIME [epoch: 25.5 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12130825288258437		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.12130825288258437 | validation: 0.06482663187105916]
	TIME [epoch: 25.4 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08569118699154128		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.08569118699154128 | validation: 0.09478325975841181]
	TIME [epoch: 25.5 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10423726240308946		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.10423726240308946 | validation: 0.1372090679638223]
	TIME [epoch: 25.4 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12122229504260426		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.12122229504260426 | validation: 0.11800296241843254]
	TIME [epoch: 25.4 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1603729785876855		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.1603729785876855 | validation: 0.09459854551817964]
	TIME [epoch: 25.5 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09278587983687359		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.09278587983687359 | validation: 0.07897605495374854]
	TIME [epoch: 25.5 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09287866410319989		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.09287866410319989 | validation: 0.06542196515445042]
	TIME [epoch: 25.4 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15541034301927573		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.15541034301927573 | validation: 0.24128009854370963]
	TIME [epoch: 25.4 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15468592202360934		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.15468592202360934 | validation: 0.25069327178175776]
	TIME [epoch: 25.4 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12365823161332958		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.12365823161332958 | validation: 0.08927680555562167]
	TIME [epoch: 25.5 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08002934512244557		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.08002934512244557 | validation: 0.09787335663691661]
	TIME [epoch: 25.4 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10304251173379193		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.10304251173379193 | validation: 0.08783277982253407]
	TIME [epoch: 25.5 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08365911703511675		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.08365911703511675 | validation: 0.14544843281929898]
	TIME [epoch: 25.4 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10996055211208527		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.10996055211208527 | validation: 0.10392217583139322]
	TIME [epoch: 25.4 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08424366573995988		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.08424366573995988 | validation: 0.08612237023838692]
	TIME [epoch: 25.4 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07825797687359046		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.07825797687359046 | validation: 0.07162989944047225]
	TIME [epoch: 25.4 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08411914738157607		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.08411914738157607 | validation: 0.10755052725293934]
	TIME [epoch: 25.4 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10837733814241865		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.10837733814241865 | validation: 0.1140211732553625]
	TIME [epoch: 25.5 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07861698169160071		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.07861698169160071 | validation: 0.11713744579074238]
	TIME [epoch: 25.6 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13524699097478554		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.13524699097478554 | validation: 0.08471012942235101]
	TIME [epoch: 25.5 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08551890760519694		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.08551890760519694 | validation: 0.06752126365755709]
	TIME [epoch: 25.5 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0864311214332266		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.0864311214332266 | validation: 0.14590948222202116]
	TIME [epoch: 25.4 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0891677511703261		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.0891677511703261 | validation: 0.09835976595932887]
	TIME [epoch: 25.4 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07692104394953546		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.07692104394953546 | validation: 0.08364678548545101]
	TIME [epoch: 25.4 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10006321211531284		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.10006321211531284 | validation: 0.06188395632173013]
	TIME [epoch: 25.5 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09311856255431825		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.09311856255431825 | validation: 0.12614202739181435]
	TIME [epoch: 25.5 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0873303752745338		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.0873303752745338 | validation: 0.10508025174379024]
	TIME [epoch: 25.5 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07680341325489802		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.07680341325489802 | validation: 0.07438602948370612]
	TIME [epoch: 25.4 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11821241783509825		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.11821241783509825 | validation: 0.10447471478125897]
	TIME [epoch: 25.4 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11455877157574362		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.11455877157574362 | validation: 0.11181543959785802]
	TIME [epoch: 25.4 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10114294053762651		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.10114294053762651 | validation: 0.13401796977342945]
	TIME [epoch: 25.5 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08616630815080403		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.08616630815080403 | validation: 0.08845614540550306]
	TIME [epoch: 25.5 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08992459394793466		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.08992459394793466 | validation: 0.07515359226583407]
	TIME [epoch: 25.5 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07101177249347304		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.07101177249347304 | validation: 0.09928048739965309]
	TIME [epoch: 25.4 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.080456750763492		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.080456750763492 | validation: 0.08515445282054171]
	TIME [epoch: 25.4 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08697268537650395		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.08697268537650395 | validation: 0.06084347410212076]
	TIME [epoch: 25.4 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08390242603509214		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.08390242603509214 | validation: 0.05692030748793762]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_712.pth
	Model improved!!!
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08009317830865201		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.08009317830865201 | validation: 0.1498139412133398]
	TIME [epoch: 25.4 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32341452450609		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.32341452450609 | validation: 0.15477469772408992]
	TIME [epoch: 25.4 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14388405026465537		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.14388405026465537 | validation: 0.08404848521826483]
	TIME [epoch: 25.4 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1255466995217876		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.1255466995217876 | validation: 0.06629814076061286]
	TIME [epoch: 25.5 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07693391325671915		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.07693391325671915 | validation: 0.06155231014225662]
	TIME [epoch: 25.4 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07874460876223914		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.07874460876223914 | validation: 0.06335996149011688]
	TIME [epoch: 25.4 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07455799930674292		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.07455799930674292 | validation: 0.06209904271480127]
	TIME [epoch: 25.4 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08126223710489767		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.08126223710489767 | validation: 0.08165026415240398]
	TIME [epoch: 25.4 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2999066470196241		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.2999066470196241 | validation: 0.25468565665562026]
	TIME [epoch: 25.4 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1347420903265605		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.1347420903265605 | validation: 0.06323664287072857]
	TIME [epoch: 25.4 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0662782922511206		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.0662782922511206 | validation: 0.08869960157687723]
	TIME [epoch: 25.4 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0883303963365567		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.0883303963365567 | validation: 0.06843901643903959]
	TIME [epoch: 25.4 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08244709938648898		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.08244709938648898 | validation: 0.06496603988283187]
	TIME [epoch: 25.4 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08973274584527839		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.08973274584527839 | validation: 0.05700852460140885]
	TIME [epoch: 25.4 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06697115121228693		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.06697115121228693 | validation: 0.0867802342990936]
	TIME [epoch: 25.4 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0856270715403013		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.0856270715403013 | validation: 0.06899090845071082]
	TIME [epoch: 25.4 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.072357626444996		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.072357626444996 | validation: 0.08196783756399371]
	TIME [epoch: 25.4 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08007501752025871		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.08007501752025871 | validation: 0.09131620631124436]
	TIME [epoch: 25.4 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07840683968669378		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.07840683968669378 | validation: 0.07984680490913107]
	TIME [epoch: 25.4 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08900292665507344		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.08900292665507344 | validation: 0.10139290517940833]
	TIME [epoch: 25.4 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09874474719221991		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.09874474719221991 | validation: 0.08365327960962984]
	TIME [epoch: 25.4 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08712894552694035		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.08712894552694035 | validation: 0.12736474975760917]
	TIME [epoch: 25.4 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09998253143205166		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.09998253143205166 | validation: 0.07515654862863759]
	TIME [epoch: 25.4 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10521862411726775		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.10521862411726775 | validation: 0.057074103962159506]
	TIME [epoch: 25.4 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06953442084666356		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.06953442084666356 | validation: 0.07475493525822047]
	TIME [epoch: 25.4 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08241966190604275		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.08241966190604275 | validation: 0.06550721177549604]
	TIME [epoch: 25.4 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07205032952812078		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.07205032952812078 | validation: 0.05516066599005849]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_739.pth
	Model improved!!!
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09965677622007196		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.09965677622007196 | validation: 0.09535270091102685]
	TIME [epoch: 25.4 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09876269585711107		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.09876269585711107 | validation: 0.11615306282140789]
	TIME [epoch: 25.4 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08447690938435783		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.08447690938435783 | validation: 0.10125992812107948]
	TIME [epoch: 25.4 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07519146286325629		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.07519146286325629 | validation: 0.09913938745575218]
	TIME [epoch: 25.4 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19837324847797727		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.19837324847797727 | validation: 0.12407517300375207]
	TIME [epoch: 25.4 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08125162264111617		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.08125162264111617 | validation: 0.04970487536568618]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_745.pth
	Model improved!!!
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07483665978937885		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.07483665978937885 | validation: 0.08170643318334786]
	TIME [epoch: 25.4 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07132988551377524		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.07132988551377524 | validation: 0.09037420679103594]
	TIME [epoch: 25.4 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06572784955360489		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.06572784955360489 | validation: 0.05941296935268991]
	TIME [epoch: 25.4 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1594856136536612		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.1594856136536612 | validation: 0.36979056947472116]
	TIME [epoch: 25.4 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19504736540639359		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.19504736540639359 | validation: 0.05382176389752323]
	TIME [epoch: 25.4 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06442772263749488		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.06442772263749488 | validation: 0.0477513318158043]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_751.pth
	Model improved!!!
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0649722069093087		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.0649722069093087 | validation: 0.05933047341292265]
	TIME [epoch: 25.4 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0705386108948246		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.0705386108948246 | validation: 0.06489792448164974]
	TIME [epoch: 25.4 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07148319499102901		[learning rate: 0.00082662]
	Learning Rate: 0.000826624
	LOSS [training: 0.07148319499102901 | validation: 0.048025271914560914]
	TIME [epoch: 25.4 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07070617926599018		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.07070617926599018 | validation: 0.07588699910249627]
	TIME [epoch: 25.4 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0747574275568999		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.0747574275568999 | validation: 0.39121834192852234]
	TIME [epoch: 25.4 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2181970070965805		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.2181970070965805 | validation: 0.05959813122414376]
	TIME [epoch: 25.4 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07339306177823167		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.07339306177823167 | validation: 0.058607948577018815]
	TIME [epoch: 25.4 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06408925911871087		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.06408925911871087 | validation: 0.06670377747559504]
	TIME [epoch: 25.4 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07007161487816244		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.07007161487816244 | validation: 0.13824240358231815]
	TIME [epoch: 25.4 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07832002733597422		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.07832002733597422 | validation: 0.0811502632945792]
	TIME [epoch: 25.4 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07746820899317317		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.07746820899317317 | validation: 0.09052867407955535]
	TIME [epoch: 25.4 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09104778523661486		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.09104778523661486 | validation: 0.10552555291045948]
	TIME [epoch: 25.4 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16649672947778024		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.16649672947778024 | validation: 0.13362416723826043]
	TIME [epoch: 25.4 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10549735646468693		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.10549735646468693 | validation: 0.08702542949981251]
	TIME [epoch: 25.4 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07312543714298422		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.07312543714298422 | validation: 0.0657299206969034]
	TIME [epoch: 25.4 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06403656549355664		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.06403656549355664 | validation: 0.04582015571252432]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_767.pth
	Model improved!!!
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057443831493029805		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.057443831493029805 | validation: 0.06208617891587377]
	TIME [epoch: 25.4 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07660972959337924		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.07660972959337924 | validation: 0.16714532728567702]
	TIME [epoch: 25.4 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10065728551293694		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.10065728551293694 | validation: 0.08861257120054547]
	TIME [epoch: 25.4 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09920987043976741		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.09920987043976741 | validation: 0.06863847308152923]
	TIME [epoch: 25.4 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09371579256949007		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.09371579256949007 | validation: 0.05706685530911616]
	TIME [epoch: 25.4 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06417755610599721		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.06417755610599721 | validation: 0.049755175995183414]
	TIME [epoch: 25.4 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07769075644245867		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.07769075644245867 | validation: 0.07686230733693578]
	TIME [epoch: 25.4 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07055578750723979		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.07055578750723979 | validation: 0.09560444325929919]
	TIME [epoch: 25.4 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08747453794486504		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.08747453794486504 | validation: 0.0779379540873201]
	TIME [epoch: 25.4 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08504654357440461		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.08504654357440461 | validation: 0.06389810052531229]
	TIME [epoch: 25.4 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0769887313825249		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.0769887313825249 | validation: 0.0930357010056412]
	TIME [epoch: 25.4 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06952487475570121		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.06952487475570121 | validation: 0.0500242411190677]
	TIME [epoch: 25.4 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07241457557907399		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.07241457557907399 | validation: 0.07829515149979178]
	TIME [epoch: 25.4 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07937279777620392		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.07937279777620392 | validation: 0.07448888076677995]
	TIME [epoch: 25.4 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07066569917062009		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.07066569917062009 | validation: 0.04855426985280026]
	TIME [epoch: 25.4 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06859090398939674		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.06859090398939674 | validation: 0.06316783583561392]
	TIME [epoch: 25.4 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06861639155797788		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.06861639155797788 | validation: 0.06441798540062045]
	TIME [epoch: 25.4 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3001032151485846		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.3001032151485846 | validation: 0.113429316454011]
	TIME [epoch: 25.4 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08296710815624879		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.08296710815624879 | validation: 0.04999902154348186]
	TIME [epoch: 25.4 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07539307337423988		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.07539307337423988 | validation: 0.04747350244195642]
	TIME [epoch: 25.4 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061699447402408715		[learning rate: 0.00073282]
	Learning Rate: 0.000732825
	LOSS [training: 0.061699447402408715 | validation: 0.053550830221186105]
	TIME [epoch: 25.4 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0602974875148226		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.0602974875148226 | validation: 0.07105747390334147]
	TIME [epoch: 25.4 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07534124005158596		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.07534124005158596 | validation: 0.0793013181274585]
	TIME [epoch: 25.4 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06537593170515564		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.06537593170515564 | validation: 0.07251436070038679]
	TIME [epoch: 25.4 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061229436493008604		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.061229436493008604 | validation: 0.0442042885878571]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_792.pth
	Model improved!!!
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06787725763175972		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.06787725763175972 | validation: 0.07034326612786387]
	TIME [epoch: 25.4 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10193360132461615		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.10193360132461615 | validation: 0.08693623321513828]
	TIME [epoch: 25.4 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07198842535807365		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.07198842535807365 | validation: 0.06105311335023311]
	TIME [epoch: 25.4 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061073476743746566		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.061073476743746566 | validation: 0.054499425271927915]
	TIME [epoch: 25.4 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.096234326985281		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.096234326985281 | validation: 0.06200915324302876]
	TIME [epoch: 25.4 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06758661372737816		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.06758661372737816 | validation: 0.043394149005661076]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_798.pth
	Model improved!!!
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07059519139471661		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.07059519139471661 | validation: 0.11368190391644573]
	TIME [epoch: 25.4 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07739499462087043		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.07739499462087043 | validation: 0.06494727993757111]
	TIME [epoch: 25.4 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09284625307925173		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.09284625307925173 | validation: 0.10014031818778944]
	TIME [epoch: 25.5 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08030805246441601		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.08030805246441601 | validation: 0.0470014544379718]
	TIME [epoch: 25.4 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07062396315311563		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.07062396315311563 | validation: 0.051248155609907434]
	TIME [epoch: 25.4 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05719907438116553		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.05719907438116553 | validation: 0.05333874965524536]
	TIME [epoch: 25.4 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07248775995921448		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.07248775995921448 | validation: 0.08039322547953404]
	TIME [epoch: 25.4 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11438827473561973		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.11438827473561973 | validation: 0.0998928357423991]
	TIME [epoch: 25.4 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08161534787450313		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.08161534787450313 | validation: 0.052559146921902285]
	TIME [epoch: 25.4 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0593577404654197		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.0593577404654197 | validation: 0.06079644929822982]
	TIME [epoch: 25.4 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06406468529422517		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.06406468529422517 | validation: 0.05308626876389924]
	TIME [epoch: 25.4 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07332458989228746		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.07332458989228746 | validation: 0.049295028755670225]
	TIME [epoch: 25.4 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06157646077620592		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.06157646077620592 | validation: 0.07554115531894534]
	TIME [epoch: 25.5 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06559514649163672		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.06559514649163672 | validation: 0.08641476975843188]
	TIME [epoch: 25.4 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06643710033563724		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.06643710033563724 | validation: 0.05254715922680154]
	TIME [epoch: 25.4 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06180631460706367		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.06180631460706367 | validation: 0.0874154266515953]
	TIME [epoch: 25.4 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07387906173370323		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.07387906173370323 | validation: 0.05224456855722473]
	TIME [epoch: 25.4 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07112013425435484		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.07112013425435484 | validation: 0.06491814474660343]
	TIME [epoch: 25.4 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06464172745377131		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.06464172745377131 | validation: 0.06608808829474774]
	TIME [epoch: 25.4 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06314934056315176		[learning rate: 0.00065894]
	Learning Rate: 0.00065894
	LOSS [training: 0.06314934056315176 | validation: 0.06533820664297765]
	TIME [epoch: 25.4 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07989073430654488		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.07989073430654488 | validation: 0.06386866546807786]
	TIME [epoch: 25.4 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06928866512965288		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.06928866512965288 | validation: 0.05676931050008149]
	TIME [epoch: 25.4 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056736958224646854		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.056736958224646854 | validation: 0.04506741216011666]
	TIME [epoch: 25.4 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07536137937856609		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.07536137937856609 | validation: 0.08037082884643909]
	TIME [epoch: 25.4 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06711617651935073		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.06711617651935073 | validation: 0.08475904453940394]
	TIME [epoch: 25.4 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07099991752215629		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.07099991752215629 | validation: 0.12971016889576706]
	TIME [epoch: 25.5 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09264876645321662		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.09264876645321662 | validation: 0.06005786124795434]
	TIME [epoch: 25.4 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062170815884714105		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.062170815884714105 | validation: 0.05530528759171738]
	TIME [epoch: 25.4 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05835747259095715		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.05835747259095715 | validation: 0.06689922635882628]
	TIME [epoch: 25.4 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06470543125924966		[learning rate: 0.00063601]
	Learning Rate: 0.000636007
	LOSS [training: 0.06470543125924966 | validation: 0.09058393892048441]
	TIME [epoch: 25.4 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07083097092274993		[learning rate: 0.00063376]
	Learning Rate: 0.000633758
	LOSS [training: 0.07083097092274993 | validation: 0.26814574781430656]
	TIME [epoch: 25.4 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15943616919899842		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.15943616919899842 | validation: 0.03885361192358151]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_830.pth
	Model improved!!!
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06253625296018092		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.06253625296018092 | validation: 0.054038579390746425]
	TIME [epoch: 25.5 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05503568415138575		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.05503568415138575 | validation: 0.04215718225503861]
	TIME [epoch: 25.4 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06601417086515411		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.06601417086515411 | validation: 0.05155283559921667]
	TIME [epoch: 25.4 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06107205667256208		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.06107205667256208 | validation: 0.05018123296908626]
	TIME [epoch: 25.4 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06430020643173794		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.06430020643173794 | validation: 0.0468420662021153]
	TIME [epoch: 25.4 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062045424698858445		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.062045424698858445 | validation: 0.07925012217695923]
	TIME [epoch: 25.4 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0828598857291324		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.0828598857291324 | validation: 0.06884730428398946]
	TIME [epoch: 25.4 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06039730154841576		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.06039730154841576 | validation: 0.09929696501903429]
	TIME [epoch: 25.4 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08255298560031642		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.08255298560031642 | validation: 0.09539077680508978]
	TIME [epoch: 25.4 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08295689064098893		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.08295689064098893 | validation: 0.05965516444949996]
	TIME [epoch: 25.4 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06585031891583637		[learning rate: 0.00060738]
	Learning Rate: 0.000607382
	LOSS [training: 0.06585031891583637 | validation: 0.12090091626054064]
	TIME [epoch: 25.4 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08867348361535064		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.08867348361535064 | validation: 0.045543062687759275]
	TIME [epoch: 25.4 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06214153806862891		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.06214153806862891 | validation: 0.04820908614826905]
	TIME [epoch: 25.5 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07203010383924256		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.07203010383924256 | validation: 0.04862079488212792]
	TIME [epoch: 25.4 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05783612654650429		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.05783612654650429 | validation: 0.05877744245672409]
	TIME [epoch: 25.4 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10548932795955716		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.10548932795955716 | validation: 0.0704256770625476]
	TIME [epoch: 25.4 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06415336682091216		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.06415336682091216 | validation: 0.12768846400440248]
	TIME [epoch: 25.4 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07679937461980485		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.07679937461980485 | validation: 0.22571745245758312]
	TIME [epoch: 25.4 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1323168011143128		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.1323168011143128 | validation: 0.05948117593679622]
	TIME [epoch: 25.4 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06089934328701433		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.06089934328701433 | validation: 0.07249345226598519]
	TIME [epoch: 25.5 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06996668816183636		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.06996668816183636 | validation: 0.0660165838688411]
	TIME [epoch: 25.4 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06353145148188548		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.06353145148188548 | validation: 0.0442882683555286]
	TIME [epoch: 25.4 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053425973244436625		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.053425973244436625 | validation: 0.04077454984205576]
	TIME [epoch: 25.4 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052724868191669105		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.052724868191669105 | validation: 0.05718012732964089]
	TIME [epoch: 25.4 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06174506950829764		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.06174506950829764 | validation: 0.07327595921073915]
	TIME [epoch: 25.4 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06344385492042823		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.06344385492042823 | validation: 0.06227859140450248]
	TIME [epoch: 25.4 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07478476564448933		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.07478476564448933 | validation: 0.06466653775967017]
	TIME [epoch: 25.4 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062293709058724896		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.062293709058724896 | validation: 0.07214547897688386]
	TIME [epoch: 25.4 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06708371164131204		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.06708371164131204 | validation: 0.08512995821779731]
	TIME [epoch: 25.4 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06914694630013346		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.06914694630013346 | validation: 0.05073624003359103]
	TIME [epoch: 25.4 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057076676230151675		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.057076676230151675 | validation: 0.1501155811645558]
	TIME [epoch: 25.4 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09328684816380606		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.09328684816380606 | validation: 0.06967290032415654]
	TIME [epoch: 25.4 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06933074495728997		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.06933074495728997 | validation: 0.053126121432630694]
	TIME [epoch: 25.5 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061806193955439695		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.061806193955439695 | validation: 0.05217616604469401]
	TIME [epoch: 25.5 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06212396942623213		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.06212396942623213 | validation: 0.06658198498543852]
	TIME [epoch: 25.6 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08664923891705836		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.08664923891705836 | validation: 0.05674336442869815]
	TIME [epoch: 25.5 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05846423477883415		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.05846423477883415 | validation: 0.07158087850929795]
	TIME [epoch: 25.5 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08174410056596443		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.08174410056596443 | validation: 0.07440460937537707]
	TIME [epoch: 25.5 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08262724199177225		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.08262724199177225 | validation: 0.08040808016574827]
	TIME [epoch: 25.5 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06376626567090536		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.06376626567090536 | validation: 0.04798958319122562]
	TIME [epoch: 25.5 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05901715373120439		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.05901715373120439 | validation: 0.04798781862173342]
	TIME [epoch: 25.5 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06293292771936174		[learning rate: 0.00054421]
	Learning Rate: 0.000544214
	LOSS [training: 0.06293292771936174 | validation: 0.04520268279084143]
	TIME [epoch: 25.5 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06251794999351348		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.06251794999351348 | validation: 0.16821485874561531]
	TIME [epoch: 25.5 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08083186704541387		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.08083186704541387 | validation: 0.11951406846451434]
	TIME [epoch: 25.5 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08127969687402219		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.08127969687402219 | validation: 0.06619311690575422]
	TIME [epoch: 25.5 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056739422632773984		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.056739422632773984 | validation: 0.05664475669629955]
	TIME [epoch: 25.5 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061748077435456475		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.061748077435456475 | validation: 0.04723092281468129]
	TIME [epoch: 25.5 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05472264211488161		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.05472264211488161 | validation: 0.04501338627123612]
	TIME [epoch: 25.5 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06142661843413072		[learning rate: 0.00053088]
	Learning Rate: 0.000530885
	LOSS [training: 0.06142661843413072 | validation: 0.06513677882884075]
	TIME [epoch: 25.5 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06893431675453862		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.06893431675453862 | validation: 0.06627709523594033]
	TIME [epoch: 25.5 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0685736190732305		[learning rate: 0.00052714]
	Learning Rate: 0.000527137
	LOSS [training: 0.0685736190732305 | validation: 0.10735259254227045]
	TIME [epoch: 25.5 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0763630280255169		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.0763630280255169 | validation: 0.060604745358414806]
	TIME [epoch: 25.5 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06019749833999734		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.06019749833999734 | validation: 0.0516794012618749]
	TIME [epoch: 25.5 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059673911886297754		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.059673911886297754 | validation: 0.05597323663364548]
	TIME [epoch: 25.5 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0673902815382001		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.0673902815382001 | validation: 0.04709215272777901]
	TIME [epoch: 25.5 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06155547791295567		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.06155547791295567 | validation: 0.045879665247572664]
	TIME [epoch: 25.5 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26614996673796		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.26614996673796 | validation: 0.11594903094736744]
	TIME [epoch: 25.5 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08057371858202449		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.08057371858202449 | validation: 0.045410809221189063]
	TIME [epoch: 25.5 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07105855759525667		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.07105855759525667 | validation: 0.04418763116204277]
	TIME [epoch: 25.5 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05070601982097728		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.05070601982097728 | validation: 0.04176660663789181]
	TIME [epoch: 25.5 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05125707236195936		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.05125707236195936 | validation: 0.041884408081431526]
	TIME [epoch: 25.5 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.067225711133951		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.067225711133951 | validation: 0.06692882950242171]
	TIME [epoch: 25.4 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07835301622365348		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.07835301622365348 | validation: 0.06791972043302327]
	TIME [epoch: 25.4 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07176863217795786		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.07176863217795786 | validation: 0.09748454472973617]
	TIME [epoch: 25.5 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07844853461936532		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.07844853461936532 | validation: 0.07458173045496028]
	TIME [epoch: 25.5 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05896329674439543		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.05896329674439543 | validation: 0.06704017983151224]
	TIME [epoch: 25.5 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05822145321025772		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.05822145321025772 | validation: 0.052182519005955975]
	TIME [epoch: 25.5 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05909291605559116		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.05909291605559116 | validation: 0.04119251962280128]
	TIME [epoch: 25.4 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05975494840849639		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.05975494840849639 | validation: 0.055734248381393146]
	TIME [epoch: 25.5 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06330922515995743		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.06330922515995743 | validation: 0.06986492259339706]
	TIME [epoch: 25.6 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05985802188716631		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.05985802188716631 | validation: 0.0467479765663978]
	TIME [epoch: 25.5 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05404180046625398		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.05404180046625398 | validation: 0.07410487913402528]
	TIME [epoch: 25.4 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06516921367123994		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.06516921367123994 | validation: 0.05275106682952341]
	TIME [epoch: 25.5 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06048149050046281		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.06048149050046281 | validation: 0.04508324260886466]
	TIME [epoch: 25.5 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051381895563207075		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.051381895563207075 | validation: 0.04738381581057273]
	TIME [epoch: 25.5 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05774069275643451		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.05774069275643451 | validation: 0.0417722732986373]
	TIME [epoch: 25.5 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05953429451296938		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.05953429451296938 | validation: 0.04402154506412477]
	TIME [epoch: 25.4 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06113224626396642		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.06113224626396642 | validation: 0.0670925818346467]
	TIME [epoch: 25.5 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05542058445242815		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.05542058445242815 | validation: 0.03989371073385423]
	TIME [epoch: 25.4 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05202695296800824		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.05202695296800824 | validation: 0.066003454850635]
	TIME [epoch: 25.4 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053788974741984186		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.053788974741984186 | validation: 0.047765190125758576]
	TIME [epoch: 25.5 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057626328156723486		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.057626328156723486 | validation: 0.053409586717376424]
	TIME [epoch: 25.4 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05141909525832306		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.05141909525832306 | validation: 0.07618320689308047]
	TIME [epoch: 25.4 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06554889569115561		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.06554889569115561 | validation: 0.05639494670231457]
	TIME [epoch: 25.4 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06721063972496184		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.06721063972496184 | validation: 0.05610183540767569]
	TIME [epoch: 25.4 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053304475220938785		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.053304475220938785 | validation: 0.06238870517275828]
	TIME [epoch: 25.4 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06951451370175885		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.06951451370175885 | validation: 0.09380725476630024]
	TIME [epoch: 25.4 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06775238271763487		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.06775238271763487 | validation: 0.05327975145556554]
	TIME [epoch: 25.4 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05889183081373404		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.05889183081373404 | validation: 0.042426856715101026]
	TIME [epoch: 25.5 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04889789543967171		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.04889789543967171 | validation: 0.054503162073639616]
	TIME [epoch: 25.4 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05188840090627063		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.05188840090627063 | validation: 0.04245880001046427]
	TIME [epoch: 25.4 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06609594267531878		[learning rate: 0.00045588]
	Learning Rate: 0.000455876
	LOSS [training: 0.06609594267531878 | validation: 0.049100910605497]
	TIME [epoch: 25.5 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05611668728459479		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.05611668728459479 | validation: 0.045267538523980036]
	TIME [epoch: 25.4 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06748479880232436		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.06748479880232436 | validation: 0.053403818410480745]
	TIME [epoch: 25.4 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06603543694900782		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.06603543694900782 | validation: 0.04487765023122995]
	TIME [epoch: 25.5 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06075509862560171		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.06075509862560171 | validation: 0.044258045544389946]
	TIME [epoch: 25.4 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05409235406155134		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.05409235406155134 | validation: 0.04445899810879246]
	TIME [epoch: 25.5 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05930499824776764		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.05930499824776764 | validation: 0.060446614463516186]
	TIME [epoch: 25.4 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05784262150840069		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.05784262150840069 | validation: 0.044262844519696525]
	TIME [epoch: 25.5 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060272968839671835		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.060272968839671835 | validation: 0.090322470677332]
	TIME [epoch: 25.4 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06303576337802216		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.06303576337802216 | validation: 0.04299678524079352]
	TIME [epoch: 25.5 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_2_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_2_v_mmd1_931.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 18190.755 seconds.
