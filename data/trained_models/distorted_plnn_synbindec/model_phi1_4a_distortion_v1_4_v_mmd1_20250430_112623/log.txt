Args:
Namespace(name='model_phi1_4a_distortion_v1_4_v_mmd1', outdir='out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_4/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_4/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2910507219

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.6580261073034785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.6580261073034785 | validation: 6.6359777378329206]
	TIME [epoch: 148 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.145267960840191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.145267960840191 | validation: 6.381445292281986]
	TIME [epoch: 0.679 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.291159313405456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.291159313405456 | validation: 6.530797870718806]
	TIME [epoch: 0.674 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.177321139445254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.177321139445254 | validation: 6.471646965162609]
	TIME [epoch: 0.668 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.780908008096203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.780908008096203 | validation: 6.142257363250568]
	TIME [epoch: 0.673 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.637114768895889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.637114768895889 | validation: 6.03032264767133]
	TIME [epoch: 0.677 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.466757294247654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.466757294247654 | validation: 6.078463165690656]
	TIME [epoch: 0.668 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.35802272713598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.35802272713598 | validation: 6.112723624881578]
	TIME [epoch: 0.67 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.476468966698425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.476468966698425 | validation: 5.787232079105595]
	TIME [epoch: 0.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0718069139437345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0718069139437345 | validation: 5.996928376785857]
	TIME [epoch: 0.668 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9359720938284104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9359720938284104 | validation: 5.540839776156059]
	TIME [epoch: 0.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8091692836269857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8091692836269857 | validation: 5.766433614206843]
	TIME [epoch: 0.673 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.712389325881416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.712389325881416 | validation: 5.439079658446899]
	TIME [epoch: 0.673 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7886893902259833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7886893902259833 | validation: 5.7627714145438205]
	TIME [epoch: 0.673 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.386992572059519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.386992572059519 | validation: 5.656726314671367]
	TIME [epoch: 0.672 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.555557124553203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.555557124553203 | validation: 5.477805569727775]
	TIME [epoch: 0.67 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.799451654182295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.799451654182295 | validation: 5.515402182431304]
	TIME [epoch: 0.672 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4423543628359687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4423543628359687 | validation: 5.539301402226594]
	TIME [epoch: 0.671 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4358372155841095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4358372155841095 | validation: 5.288790604415651]
	TIME [epoch: 0.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2841440586339856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2841440586339856 | validation: 5.243180147224414]
	TIME [epoch: 0.672 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1507835466289063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1507835466289063 | validation: 5.112243089562343]
	TIME [epoch: 0.664 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2394985882077965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2394985882077965 | validation: 5.318657309574739]
	TIME [epoch: 0.676 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8120436525836063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8120436525836063 | validation: 5.047668515806236]
	TIME [epoch: 0.672 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.093209377526366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.093209377526366 | validation: 5.148645374011043]
	TIME [epoch: 0.668 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1229287909563137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1229287909563137 | validation: 4.98277770651384]
	TIME [epoch: 0.668 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1176737700162245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1176737700162245 | validation: 5.063835800947095]
	TIME [epoch: 0.67 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0715006269056673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0715006269056673 | validation: 4.989362120194421]
	TIME [epoch: 0.666 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2020235973870497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2020235973870497 | validation: 5.030687383423223]
	TIME [epoch: 0.666 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1535965996875213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1535965996875213 | validation: 4.92299121277116]
	TIME [epoch: 0.664 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.930118966454352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.930118966454352 | validation: 4.883982859740451]
	TIME [epoch: 0.674 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8623366524944767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8623366524944767 | validation: 4.84133108218669]
	TIME [epoch: 0.668 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8931228056703926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8931228056703926 | validation: 4.879901085090836]
	TIME [epoch: 0.674 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.070210181874195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.070210181874195 | validation: 4.857750349788473]
	TIME [epoch: 0.669 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.034409164771852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.034409164771852 | validation: 4.812207655083083]
	TIME [epoch: 0.664 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.877354960924056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.877354960924056 | validation: 4.75138666978043]
	TIME [epoch: 0.669 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8445924010533337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8445924010533337 | validation: 4.755494902325789]
	TIME [epoch: 0.669 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8678215277901353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8678215277901353 | validation: 4.751424910563251]
	TIME [epoch: 0.669 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8534157508442455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8534157508442455 | validation: 4.7106168708364]
	TIME [epoch: 0.669 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8220927871946913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8220927871946913 | validation: 4.691640615271367]
	TIME [epoch: 0.667 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7782577365048695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7782577365048695 | validation: 4.654456064692342]
	TIME [epoch: 0.669 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7863583443968936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7863583443968936 | validation: 4.69426484010917]
	TIME [epoch: 0.668 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8659262241234558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8659262241234558 | validation: 4.6582432517434125]
	TIME [epoch: 0.666 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.878609436487334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.878609436487334 | validation: 4.6268898426061495]
	TIME [epoch: 0.665 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.797128410152017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.797128410152017 | validation: 4.592600422209947]
	TIME [epoch: 0.667 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.731912348454077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.731912348454077 | validation: 4.591717635873978]
	TIME [epoch: 0.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7700721784992344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7700721784992344 | validation: 4.566119220500416]
	TIME [epoch: 0.669 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7854608969101453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7854608969101453 | validation: 4.55276009455401]
	TIME [epoch: 0.671 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7043260749972773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7043260749972773 | validation: 4.509361378382709]
	TIME [epoch: 0.667 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6666045749554566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6666045749554566 | validation: 4.520026480816521]
	TIME [epoch: 0.667 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6814877217775215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6814877217775215 | validation: 4.481499646955051]
	TIME [epoch: 0.668 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7198069782437746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7198069782437746 | validation: 4.514355973044023]
	TIME [epoch: 0.668 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7447211259774797		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 2.7447211259774797 | validation: 4.441109075831633]
	TIME [epoch: 0.667 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.711483975839394		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 2.711483975839394 | validation: 4.447251538318466]
	TIME [epoch: 0.668 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6599877843657533		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 2.6599877843657533 | validation: 4.4009345285549575]
	TIME [epoch: 0.667 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.647221942826151		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 2.647221942826151 | validation: 4.414469480278092]
	TIME [epoch: 0.668 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6465918940497115		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 2.6465918940497115 | validation: 4.3745973329038135]
	TIME [epoch: 0.666 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6506915384401966		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 2.6506915384401966 | validation: 4.368755673367871]
	TIME [epoch: 0.669 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.629598450685312		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 2.629598450685312 | validation: 4.33188818325384]
	TIME [epoch: 0.668 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5972949681143063		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 2.5972949681143063 | validation: 4.3302050777591665]
	TIME [epoch: 0.671 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5798715295055015		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 2.5798715295055015 | validation: 4.286893895150178]
	TIME [epoch: 0.669 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.591179609231667		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 2.591179609231667 | validation: 4.316207651663182]
	TIME [epoch: 0.671 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5962609657549853		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 2.5962609657549853 | validation: 4.262558033296826]
	TIME [epoch: 0.668 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.592864564682327		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 2.592864564682327 | validation: 4.294918194009915]
	TIME [epoch: 0.669 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6132560189550738		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 2.6132560189550738 | validation: 4.213836193892559]
	TIME [epoch: 0.667 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6154187636325115		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 2.6154187636325115 | validation: 4.247109394280797]
	TIME [epoch: 0.67 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.57827172522272		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 2.57827172522272 | validation: 4.175623644914128]
	TIME [epoch: 0.668 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.546101791463878		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 2.546101791463878 | validation: 4.203624355069413]
	TIME [epoch: 0.67 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.539221906350195		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 2.539221906350195 | validation: 4.179942020163568]
	TIME [epoch: 0.669 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.568911798054929		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 2.568911798054929 | validation: 4.173489359545643]
	TIME [epoch: 0.667 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5566879463281835		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 2.5566879463281835 | validation: 4.157535871466988]
	TIME [epoch: 0.667 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.537148775919925		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 2.537148775919925 | validation: 4.130846687211345]
	TIME [epoch: 0.667 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5063960648881793		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 2.5063960648881793 | validation: 4.133344712978833]
	TIME [epoch: 0.67 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4967046240318287		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 2.4967046240318287 | validation: 4.090028858509301]
	TIME [epoch: 0.672 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4974203215702584		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 2.4974203215702584 | validation: 4.103972708436078]
	TIME [epoch: 0.67 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4917621646258907		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 2.4917621646258907 | validation: 4.0512412431013365]
	TIME [epoch: 0.669 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.501931126254206		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 2.501931126254206 | validation: 4.06796531464727]
	TIME [epoch: 0.674 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4953485237575688		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 2.4953485237575688 | validation: 4.010400369403561]
	TIME [epoch: 0.669 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.486211094436429		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 2.486211094436429 | validation: 4.049084551270183]
	TIME [epoch: 0.671 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.480666491682547		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 2.480666491682547 | validation: 3.960011558427823]
	TIME [epoch: 0.668 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.487591640550276		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 2.487591640550276 | validation: 4.029683456648108]
	TIME [epoch: 0.667 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4827369953433784		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 2.4827369953433784 | validation: 3.961579598477366]
	TIME [epoch: 0.668 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4738998680273743		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 2.4738998680273743 | validation: 3.985086471897116]
	TIME [epoch: 0.668 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.455730932069771		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 2.455730932069771 | validation: 3.9482099493748324]
	TIME [epoch: 0.666 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4485350794927223		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 2.4485350794927223 | validation: 3.897448849641776]
	TIME [epoch: 0.669 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.451191147722565		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 2.451191147722565 | validation: 3.8972076873640646]
	TIME [epoch: 0.667 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.443901447595235		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 2.443901447595235 | validation: 3.8730399076572866]
	TIME [epoch: 0.668 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4376185618341664		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 2.4376185618341664 | validation: 3.8876429962071786]
	TIME [epoch: 0.667 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4329266762756028		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 2.4329266762756028 | validation: 3.8506443760016604]
	TIME [epoch: 0.671 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4266582970778128		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 2.4266582970778128 | validation: 3.8409457685531807]
	TIME [epoch: 0.669 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4220413226139694		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 2.4220413226139694 | validation: 3.7997570894753467]
	TIME [epoch: 0.673 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.410443216145049		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 2.410443216145049 | validation: 3.7989600365439666]
	TIME [epoch: 0.671 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.409198215099673		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 2.409198215099673 | validation: 3.7391374034667244]
	TIME [epoch: 0.672 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.402491267460691		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 2.402491267460691 | validation: 3.7435010787624132]
	TIME [epoch: 0.673 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3996394098627727		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 2.3996394098627727 | validation: 3.684876241993422]
	TIME [epoch: 0.671 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3974866202270233		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 2.3974866202270233 | validation: 3.688611840420292]
	TIME [epoch: 0.671 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3932053178224026		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 2.3932053178224026 | validation: 3.648829279766561]
	TIME [epoch: 0.666 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3856350372438677		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 2.3856350372438677 | validation: 3.627779500097285]
	TIME [epoch: 0.669 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.376741621873403		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 2.376741621873403 | validation: 3.6157178595747412]
	TIME [epoch: 0.671 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3741498908261742		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 2.3741498908261742 | validation: 3.5994288186005883]
	TIME [epoch: 0.669 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3723722309062873		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 2.3723722309062873 | validation: 3.5695113820112647]
	TIME [epoch: 0.666 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3737410180639085		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 2.3737410180639085 | validation: 3.5748150272248895]
	TIME [epoch: 0.669 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3699546292226126		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 2.3699546292226126 | validation: 3.5355471052703713]
	TIME [epoch: 0.665 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3745437541265484		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 2.3745437541265484 | validation: 3.559102121461347]
	TIME [epoch: 0.669 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.365835845795644		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 2.365835845795644 | validation: 3.5067305981525565]
	TIME [epoch: 0.666 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.36921065060862		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 2.36921065060862 | validation: 3.517954241454838]
	TIME [epoch: 0.667 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.361991154057169		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 2.361991154057169 | validation: 3.5070189228700572]
	TIME [epoch: 0.666 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.364485686400147		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 2.364485686400147 | validation: 3.499764496522291]
	TIME [epoch: 0.668 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.372687796770935		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 2.372687796770935 | validation: 3.500629445252608]
	TIME [epoch: 0.665 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3696864932388353		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 2.3696864932388353 | validation: 3.4738935482486895]
	TIME [epoch: 0.667 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3694074793505586		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 2.3694074793505586 | validation: 3.493961434357402]
	TIME [epoch: 0.67 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3650854278767275		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 2.3650854278767275 | validation: 3.4682680856553563]
	TIME [epoch: 0.668 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.370234123669694		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 2.370234123669694 | validation: 3.4704423877187978]
	TIME [epoch: 0.668 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3706833805997585		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 2.3706833805997585 | validation: 3.462437926271359]
	TIME [epoch: 0.669 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3726978452376675		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 2.3726978452376675 | validation: 3.454869105661375]
	TIME [epoch: 0.672 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3671331096317236		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 2.3671331096317236 | validation: 3.480811201623253]
	TIME [epoch: 0.671 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3677339659462424		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 2.3677339659462424 | validation: 3.4354129713590527]
	TIME [epoch: 0.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.371168244274013		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 2.371168244274013 | validation: 3.47102980208566]
	TIME [epoch: 0.669 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.377624554185426		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 2.377624554185426 | validation: 3.439510192206585]
	TIME [epoch: 0.668 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.365802066618549		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 2.365802066618549 | validation: 3.4285615004897743]
	TIME [epoch: 0.672 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3725950462994554		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 2.3725950462994554 | validation: 3.4574120238299297]
	TIME [epoch: 0.67 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.364351397809381		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 2.364351397809381 | validation: 3.425966023957883]
	TIME [epoch: 0.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.36632310655174		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 2.36632310655174 | validation: 3.4425596456002503]
	TIME [epoch: 0.669 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3700074230157693		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 2.3700074230157693 | validation: 3.414882094480879]
	TIME [epoch: 0.669 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.365170975852753		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 2.365170975852753 | validation: 3.421143245148075]
	TIME [epoch: 0.67 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3634060146413396		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 2.3634060146413396 | validation: 3.4154788616989316]
	TIME [epoch: 0.667 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3679045559257417		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 2.3679045559257417 | validation: 3.4070084523273936]
	TIME [epoch: 0.669 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3609623706875773		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 2.3609623706875773 | validation: 3.3775045574243094]
	TIME [epoch: 0.669 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3751446989042853		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 2.3751446989042853 | validation: 3.459710403760785]
	TIME [epoch: 0.67 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3895681946728042		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 2.3895681946728042 | validation: 3.3864528330129704]
	TIME [epoch: 0.671 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3515388537839175		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 2.3515388537839175 | validation: 3.3459652588040436]
	TIME [epoch: 0.668 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.367107025999482		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 2.367107025999482 | validation: 3.3832570269546913]
	TIME [epoch: 0.668 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3617701264714834		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 2.3617701264714834 | validation: 3.3854775531938475]
	TIME [epoch: 0.67 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3656993908485173		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 2.3656993908485173 | validation: 3.364083321051625]
	TIME [epoch: 0.669 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.362534232675983		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 2.362534232675983 | validation: 3.3569401657831976]
	TIME [epoch: 0.669 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3598137582308216		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 2.3598137582308216 | validation: 3.365665646974618]
	TIME [epoch: 0.666 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3553779039083484		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 2.3553779039083484 | validation: 3.350126988994437]
	TIME [epoch: 0.668 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3633325742895788		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 2.3633325742895788 | validation: 3.377560080711589]
	TIME [epoch: 0.665 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.36695671281527		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 2.36695671281527 | validation: 3.2986220818662946]
	TIME [epoch: 0.668 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3669299913602804		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 2.3669299913602804 | validation: 3.431628918178518]
	TIME [epoch: 0.67 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3793767861802073		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 2.3793767861802073 | validation: 3.3188186742637407]
	TIME [epoch: 0.667 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.351594841435674		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 2.351594841435674 | validation: 3.289391142602682]
	TIME [epoch: 0.666 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3597730138785242		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 2.3597730138785242 | validation: 3.343626940211788]
	TIME [epoch: 0.667 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.35361560593155		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 2.35361560593155 | validation: 3.33211641956426]
	TIME [epoch: 0.668 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3619826889435025		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 2.3619826889435025 | validation: 3.3146107980340775]
	TIME [epoch: 0.667 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3518042443497946		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 2.3518042443497946 | validation: 3.317072198130275]
	TIME [epoch: 0.668 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3498358670794204		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 2.3498358670794204 | validation: 3.329152445249136]
	TIME [epoch: 0.665 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3567976856101653		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 2.3567976856101653 | validation: 3.3039069592317754]
	TIME [epoch: 0.665 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.344363232263349		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 2.344363232263349 | validation: 3.3096918946453657]
	TIME [epoch: 0.666 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3453636723611657		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 2.3453636723611657 | validation: 3.2680869399033727]
	TIME [epoch: 0.667 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3311742640964366		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 2.3311742640964366 | validation: 3.293535815980114]
	TIME [epoch: 0.666 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3181607923300964		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 2.3181607923300964 | validation: 3.4177224022320054]
	TIME [epoch: 0.667 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.506874117278843		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 3.506874117278843 | validation: 3.0021697080903524]
	TIME [epoch: 0.668 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_152.pth
	Model improved!!!
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5368698664667826		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 2.5368698664667826 | validation: 3.7580888598721054]
	TIME [epoch: 0.668 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6794418234655364		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 2.6794418234655364 | validation: 3.5272495793742906]
	TIME [epoch: 0.67 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.442405567738336		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 2.442405567738336 | validation: 3.2767657931115175]
	TIME [epoch: 0.671 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3462730617734078		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 2.3462730617734078 | validation: 3.199283114293802]
	TIME [epoch: 0.666 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.359466951341415		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 2.359466951341415 | validation: 3.177579358636842]
	TIME [epoch: 0.665 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3480907568279963		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 2.3480907568279963 | validation: 3.2023204435754544]
	TIME [epoch: 0.667 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.34125556609972		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 2.34125556609972 | validation: 3.2154023982614257]
	TIME [epoch: 0.667 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.337133585903877		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 2.337133585903877 | validation: 3.1997200565654524]
	TIME [epoch: 0.665 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.340643247334926		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 2.340643247334926 | validation: 3.2173804520049014]
	TIME [epoch: 0.666 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.332968375991115		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 2.332968375991115 | validation: 3.21133075563214]
	TIME [epoch: 0.666 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.330630419788866		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 2.330630419788866 | validation: 3.1880950092939937]
	TIME [epoch: 0.666 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.328323315148737		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 2.328323315148737 | validation: 3.1851466658355543]
	TIME [epoch: 0.669 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.321946331627184		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 2.321946331627184 | validation: 3.158920429619803]
	TIME [epoch: 0.67 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3194899854341346		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 2.3194899854341346 | validation: 3.1646676706351644]
	TIME [epoch: 0.666 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.321912381187054		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 2.321912381187054 | validation: 3.1468933616302652]
	TIME [epoch: 0.666 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.315524094637026		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 2.315524094637026 | validation: 3.1269634702935534]
	TIME [epoch: 0.666 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3100642404331926		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 2.3100642404331926 | validation: 3.113307038242118]
	TIME [epoch: 0.666 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.301577165406393		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 2.301577165406393 | validation: 3.0980237070526355]
	TIME [epoch: 0.666 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.301538999024529		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 2.301538999024529 | validation: 3.0202107814632946]
	TIME [epoch: 0.665 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2862399645087534		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 2.2862399645087534 | validation: 3.1342770122028156]
	TIME [epoch: 0.667 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2769949556434903		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 2.2769949556434903 | validation: 2.784402061446011]
	TIME [epoch: 0.667 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4559004237134423		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 2.4559004237134423 | validation: 3.4021491466131404]
	TIME [epoch: 0.668 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4242538510488165		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 2.4242538510488165 | validation: 3.009702262066577]
	TIME [epoch: 0.668 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2572666553552443		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 2.2572666553552443 | validation: 2.8658090232889606]
	TIME [epoch: 0.666 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.286792314210684		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 2.286792314210684 | validation: 2.96561711403134]
	TIME [epoch: 0.667 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2402405298356958		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 2.2402405298356958 | validation: 2.9391131379627016]
	TIME [epoch: 0.666 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2034439339677045		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 2.2034439339677045 | validation: 2.6620455273305614]
	TIME [epoch: 0.668 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1962191531347157		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 2.1962191531347157 | validation: 3.1753036033247746]
	TIME [epoch: 0.668 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3482965026726697		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 2.3482965026726697 | validation: 2.4182747986390987]
	TIME [epoch: 0.669 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3661551417137585		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 2.3661551417137585 | validation: 2.8853107611992903]
	TIME [epoch: 0.67 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1275884365080144		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 2.1275884365080144 | validation: 2.576910441631375]
	TIME [epoch: 0.668 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.024538655059101		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 2.024538655059101 | validation: 2.1640286888960856]
	TIME [epoch: 0.668 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_184.pth
	Model improved!!!
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9743256179308972		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.9743256179308972 | validation: 2.500192290681878]
	TIME [epoch: 0.671 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9523554673537828		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.9523554673537828 | validation: 1.6508725530305313]
	TIME [epoch: 0.666 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8051526921853605		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.8051526921853605 | validation: 1.9209638384114598]
	TIME [epoch: 0.673 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6467498197707833		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.6467498197707833 | validation: 1.2480305400609912]
	TIME [epoch: 0.666 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3320290579850567		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.3320290579850567 | validation: 1.3733467754809272]
	TIME [epoch: 0.666 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2584370198341643		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.2584370198341643 | validation: 2.1918745584195314]
	TIME [epoch: 0.667 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.843049713076107		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.843049713076107 | validation: 1.6691995955018393]
	TIME [epoch: 0.662 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.407629396372896		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.407629396372896 | validation: 1.6367929331407038]
	TIME [epoch: 0.664 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.372809663073092		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.372809663073092 | validation: 1.6972559202377722]
	TIME [epoch: 0.663 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.415252288262252		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.415252288262252 | validation: 1.173053315842969]
	TIME [epoch: 0.668 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.151592687073826		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.151592687073826 | validation: 1.1535661134353556]
	TIME [epoch: 0.669 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1253299500599674		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.1253299500599674 | validation: 1.3354577629382085]
	TIME [epoch: 0.666 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1485648474376695		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.1485648474376695 | validation: 1.0576740887435623]
	TIME [epoch: 0.671 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_197.pth
	Model improved!!!
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2461154604923728		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.2461154604923728 | validation: 1.7631507700713862]
	TIME [epoch: 0.669 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.418877032465353		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.418877032465353 | validation: 1.7403980068668448]
	TIME [epoch: 0.668 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3543783507777487		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.3543783507777487 | validation: 1.3459084389829403]
	TIME [epoch: 0.668 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1926933497558205		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.1926933497558205 | validation: 1.1139230717306885]
	TIME [epoch: 156 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0623503382644492		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.0623503382644492 | validation: 1.3281243367311477]
	TIME [epoch: 1.34 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1179385804062023		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.1179385804062023 | validation: 1.0800029285713624]
	TIME [epoch: 1.31 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.119661476346756		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.119661476346756 | validation: 1.630318322056545]
	TIME [epoch: 1.31 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3451240144019045		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.3451240144019045 | validation: 1.2997605374749586]
	TIME [epoch: 1.31 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.164328644212762		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.164328644212762 | validation: 1.1586286914430992]
	TIME [epoch: 1.31 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.08372427109409		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.08372427109409 | validation: 1.0332891162334945]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_207.pth
	Model improved!!!
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0528208169310755		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.0528208169310755 | validation: 1.604264174904663]
	TIME [epoch: 1.31 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.406684111205384		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.406684111205384 | validation: 1.2339981328129772]
	TIME [epoch: 1.31 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.056603489026089		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.056603489026089 | validation: 0.8936143839902944]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_210.pth
	Model improved!!!
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.297057602596108		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.297057602596108 | validation: 1.6028729956105523]
	TIME [epoch: 1.31 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2470755719962419		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.2470755719962419 | validation: 1.6341536059095472]
	TIME [epoch: 1.31 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2964830288608926		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 1.2964830288608926 | validation: 1.5184717187198045]
	TIME [epoch: 1.31 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.234641791440835		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.234641791440835 | validation: 1.4639166758785176]
	TIME [epoch: 1.31 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1853331201251587		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 1.1853331201251587 | validation: 1.5051896515307184]
	TIME [epoch: 1.31 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1844591563705869		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.1844591563705869 | validation: 1.5357628803691707]
	TIME [epoch: 1.31 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2276187713421487		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.2276187713421487 | validation: 1.6539923347789172]
	TIME [epoch: 1.32 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2918262869297639		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.2918262869297639 | validation: 1.6139667968337688]
	TIME [epoch: 1.31 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2621096586100204		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.2621096586100204 | validation: 1.4389712323072004]
	TIME [epoch: 1.31 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1633480315012181		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.1633480315012181 | validation: 1.4046735148249203]
	TIME [epoch: 1.31 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1354540016391004		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 1.1354540016391004 | validation: 1.3774455435899151]
	TIME [epoch: 1.31 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.125432707543126		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.125432707543126 | validation: 1.3545395628492827]
	TIME [epoch: 1.3 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1128251955637753		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 1.1128251955637753 | validation: 1.3627000961684896]
	TIME [epoch: 1.31 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1109197059708529		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.1109197059708529 | validation: 1.3233896789909776]
	TIME [epoch: 1.31 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1159759390028874		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 1.1159759390028874 | validation: 1.4087005527467398]
	TIME [epoch: 1.3 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.129371051476718		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 1.129371051476718 | validation: 0.8864592308586812]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2086601118992488		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.2086601118992488 | validation: 1.5766833723205818]
	TIME [epoch: 1.31 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2743300603586483		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.2743300603586483 | validation: 1.4807857311487682]
	TIME [epoch: 1.31 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1726233037501828		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 1.1726233037501828 | validation: 1.4871346757677855]
	TIME [epoch: 1.31 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2845152432064497		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.2845152432064497 | validation: 1.484736695853484]
	TIME [epoch: 1.31 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1922603470064799		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 1.1922603470064799 | validation: 1.5052397655584027]
	TIME [epoch: 1.31 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1362978455092465		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.1362978455092465 | validation: 1.3295171451008558]
	TIME [epoch: 1.31 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.101825880554819		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 1.101825880554819 | validation: 1.298641215235367]
	TIME [epoch: 1.31 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.100509890592445		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.100509890592445 | validation: 1.2588411862039113]
	TIME [epoch: 1.31 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0398235500802282		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 1.0398235500802282 | validation: 0.7915247023114761]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_235.pth
	Model improved!!!
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9590473689030149		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.9590473689030149 | validation: 1.304666051971184]
	TIME [epoch: 1.31 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0763857468242273		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 1.0763857468242273 | validation: 0.9884320271031479]
	TIME [epoch: 1.31 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8885979689569341		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.8885979689569341 | validation: 0.8106589292339255]
	TIME [epoch: 1.31 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9796208588915275		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.9796208588915275 | validation: 1.2468064460194226]
	TIME [epoch: 1.31 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0277579865418378		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 1.0277579865418378 | validation: 1.1677880538990821]
	TIME [epoch: 1.31 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9604181066886258		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.9604181066886258 | validation: 0.7996867325880281]
	TIME [epoch: 1.31 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9000846417773184		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.9000846417773184 | validation: 0.8631036651128579]
	TIME [epoch: 1.3 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8116522807597099		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.8116522807597099 | validation: 1.0302473786628947]
	TIME [epoch: 1.31 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8502513522990194		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.8502513522990194 | validation: 0.8761819864138172]
	TIME [epoch: 1.31 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3147158314617273		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 1.3147158314617273 | validation: 1.452821488780362]
	TIME [epoch: 1.31 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1700157817458208		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.1700157817458208 | validation: 1.219136590690569]
	TIME [epoch: 1.31 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.008879006475412		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 1.008879006475412 | validation: 0.8009695931256222]
	TIME [epoch: 1.31 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8318156722699184		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.8318156722699184 | validation: 0.693897301656871]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9475370920289493		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.9475370920289493 | validation: 1.1042926065748662]
	TIME [epoch: 1.31 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9228683203173188		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.9228683203173188 | validation: 0.9713099291218708]
	TIME [epoch: 1.31 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8237427947972203		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.8237427947972203 | validation: 0.7491947753822741]
	TIME [epoch: 1.31 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9388205341251364		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.9388205341251364 | validation: 1.0692028967408496]
	TIME [epoch: 1.31 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8637759157028047		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.8637759157028047 | validation: 0.8669737415888336]
	TIME [epoch: 1.31 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7738977092524977		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.7738977092524977 | validation: 0.7329793824605343]
	TIME [epoch: 1.31 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7790107859504145		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.7790107859504145 | validation: 0.9217903049810569]
	TIME [epoch: 1.31 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7771514307127311		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.7771514307127311 | validation: 0.7380535158247722]
	TIME [epoch: 1.31 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7232252367276852		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.7232252367276852 | validation: 0.765628386096185]
	TIME [epoch: 1.31 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.707869471355373		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.707869471355373 | validation: 0.8473403340973319]
	TIME [epoch: 1.31 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7307159148252542		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.7307159148252542 | validation: 0.8479026323510701]
	TIME [epoch: 1.31 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7875661015314923		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.7875661015314923 | validation: 1.1487234890727678]
	TIME [epoch: 1.31 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8853490371060813		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.8853490371060813 | validation: 0.6485922861431476]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_261.pth
	Model improved!!!
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8613894146036828		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.8613894146036828 | validation: 1.111473256622144]
	TIME [epoch: 1.31 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9004628066449678		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.9004628066449678 | validation: 0.8213872612015956]
	TIME [epoch: 1.31 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.715483635934157		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.715483635934157 | validation: 0.5947217781354489]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_264.pth
	Model improved!!!
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9944359294624258		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.9944359294624258 | validation: 1.1751613802082077]
	TIME [epoch: 1.31 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9724902884519296		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.9724902884519296 | validation: 1.1367629495852183]
	TIME [epoch: 1.31 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8898576262180684		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.8898576262180684 | validation: 0.7279557899652186]
	TIME [epoch: 1.31 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8795368905798694		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.8795368905798694 | validation: 0.8594658891862044]
	TIME [epoch: 1.31 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.703997056972928		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.703997056972928 | validation: 0.7556100991472938]
	TIME [epoch: 1.32 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6625812850285064		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.6625812850285064 | validation: 0.6506406687287145]
	TIME [epoch: 1.31 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6789425684230146		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.6789425684230146 | validation: 0.8705348123450083]
	TIME [epoch: 1.31 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7280899048809767		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.7280899048809767 | validation: 0.7970876980888614]
	TIME [epoch: 1.31 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7123894826843992		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.7123894826843992 | validation: 0.8249068022051267]
	TIME [epoch: 1.31 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7062842712863341		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.7062842712863341 | validation: 0.7723399085314475]
	TIME [epoch: 1.31 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6463813267639614		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.6463813267639614 | validation: 0.6902572853536908]
	TIME [epoch: 1.31 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6247649303740408		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.6247649303740408 | validation: 0.7311325978020145]
	TIME [epoch: 1.31 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6255563424614733		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.6255563424614733 | validation: 0.6875795789682073]
	TIME [epoch: 1.31 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6333249852312524		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.6333249852312524 | validation: 0.8600971431949118]
	TIME [epoch: 1.31 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6583198953975873		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.6583198953975873 | validation: 0.6592645309053587]
	TIME [epoch: 1.32 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7666697341855826		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.7666697341855826 | validation: 1.386900420124472]
	TIME [epoch: 1.31 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0723550439009		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 1.0723550439009 | validation: 1.0916497924315782]
	TIME [epoch: 1.31 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8783396418487397		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.8783396418487397 | validation: 0.648191497096409]
	TIME [epoch: 1.31 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6722557393141355		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.6722557393141355 | validation: 0.6144978628944382]
	TIME [epoch: 1.31 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0442845845805084		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 1.0442845845805084 | validation: 1.440642118992277]
	TIME [epoch: 1.31 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2565607748520347		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 1.2565607748520347 | validation: 1.2893589634566387]
	TIME [epoch: 1.31 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9807410332734933		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.9807410332734933 | validation: 0.6353997918636065]
	TIME [epoch: 1.31 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6629688040291972		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.6629688040291972 | validation: 0.5448324608080919]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_287.pth
	Model improved!!!
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7209622523435576		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.7209622523435576 | validation: 0.871044627519276]
	TIME [epoch: 1.32 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7188002896872188		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.7188002896872188 | validation: 0.6036790592708572]
	TIME [epoch: 1.32 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6753295507617101		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.6753295507617101 | validation: 0.8117331033649778]
	TIME [epoch: 1.32 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6652186765108604		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.6652186765108604 | validation: 0.6307197153762766]
	TIME [epoch: 1.32 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6056163708072544		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.6056163708072544 | validation: 0.6787863377768297]
	TIME [epoch: 1.32 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6319627425035246		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.6319627425035246 | validation: 0.9048131070829353]
	TIME [epoch: 1.31 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6775405073868543		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.6775405073868543 | validation: 0.6832591844353613]
	TIME [epoch: 1.32 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6185467800787622		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.6185467800787622 | validation: 0.7177868344273375]
	TIME [epoch: 1.32 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5824785891989724		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.5824785891989724 | validation: 0.6760403606024818]
	TIME [epoch: 1.32 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5851428545980423		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.5851428545980423 | validation: 0.6286843744933068]
	TIME [epoch: 1.32 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6141124808722064		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.6141124808722064 | validation: 0.9155692805193898]
	TIME [epoch: 1.32 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.721496340113489		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.721496340113489 | validation: 0.5815726668098834]
	TIME [epoch: 1.32 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6775513497090944		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.6775513497090944 | validation: 0.9063453606581118]
	TIME [epoch: 1.32 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7310401560535569		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.7310401560535569 | validation: 0.5574886057391563]
	TIME [epoch: 1.32 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5695222219159765		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.5695222219159765 | validation: 0.6711187856186673]
	TIME [epoch: 1.32 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5547966122517479		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.5547966122517479 | validation: 0.5983523799824361]
	TIME [epoch: 1.31 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5756858003862452		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.5756858003862452 | validation: 0.8503711984120379]
	TIME [epoch: 1.32 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7038442358861073		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.7038442358861073 | validation: 0.6357725678639666]
	TIME [epoch: 1.31 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6869180863439478		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.6869180863439478 | validation: 0.8644541969534929]
	TIME [epoch: 1.32 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6873105402443846		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.6873105402443846 | validation: 0.5957868123799702]
	TIME [epoch: 1.31 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5491275220292896		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.5491275220292896 | validation: 0.6779813401926695]
	TIME [epoch: 1.32 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5422870291519357		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.5422870291519357 | validation: 0.5505831476995405]
	TIME [epoch: 1.31 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5601016114095211		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.5601016114095211 | validation: 0.805922536199163]
	TIME [epoch: 1.31 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6408111816567496		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.6408111816567496 | validation: 0.5374324932916539]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.688250580067076		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.688250580067076 | validation: 0.9700544681012006]
	TIME [epoch: 1.32 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.781928968062303		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.781928968062303 | validation: 0.7068172473769581]
	TIME [epoch: 1.31 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6099802853772401		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.6099802853772401 | validation: 0.513113051210426]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_314.pth
	Model improved!!!
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7764600077856437		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.7764600077856437 | validation: 1.2643636927211643]
	TIME [epoch: 1.3 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9148697810485241		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.9148697810485241 | validation: 1.1400678272562321]
	TIME [epoch: 1.31 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8741744953088619		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.8741744953088619 | validation: 0.8073131338243912]
	TIME [epoch: 1.3 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7546360228787993		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.7546360228787993 | validation: 0.5358377749161441]
	TIME [epoch: 1.3 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5919848907588394		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.5919848907588394 | validation: 0.6946054260597094]
	TIME [epoch: 1.31 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5898187839561535		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.5898187839561535 | validation: 0.8132582912369799]
	TIME [epoch: 1.3 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.643007918467491		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.643007918467491 | validation: 0.6268600777282485]
	TIME [epoch: 1.31 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5762078807191525		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.5762078807191525 | validation: 0.6921929370946918]
	TIME [epoch: 1.31 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5605319864972028		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.5605319864972028 | validation: 0.5842159612566266]
	TIME [epoch: 1.31 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5211390264295601		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.5211390264295601 | validation: 0.5497783616130185]
	TIME [epoch: 1.31 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5204512485391201		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.5204512485391201 | validation: 0.9474129388660568]
	TIME [epoch: 1.3 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7143410825233107		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.7143410825233107 | validation: 0.6039166764220741]
	TIME [epoch: 1.3 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5731399346518691		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.5731399346518691 | validation: 0.6783828642290717]
	TIME [epoch: 1.3 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.583261933504028		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.583261933504028 | validation: 0.7413409200489505]
	TIME [epoch: 1.3 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5809414245491603		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.5809414245491603 | validation: 0.5204005180554591]
	TIME [epoch: 1.3 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.612283654670167		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.612283654670167 | validation: 0.9095356712657922]
	TIME [epoch: 1.3 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7210632188253813		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.7210632188253813 | validation: 0.6288262900638378]
	TIME [epoch: 1.3 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5383426713620757		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.5383426713620757 | validation: 0.4770522373695689]
	TIME [epoch: 1.29 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_332.pth
	Model improved!!!
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6267587496094097		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.6267587496094097 | validation: 1.1382491625178521]
	TIME [epoch: 1.32 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8663912036837077		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.8663912036837077 | validation: 0.9112648193814213]
	TIME [epoch: 1.32 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7413058458725809		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.7413058458725809 | validation: 0.5537991075586679]
	TIME [epoch: 1.3 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5938752014235724		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.5938752014235724 | validation: 0.5599768125481571]
	TIME [epoch: 1.3 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5418221174180042		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.5418221174180042 | validation: 0.76712214036468]
	TIME [epoch: 1.31 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.571609179241257		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.571609179241257 | validation: 0.5256870596490891]
	TIME [epoch: 1.3 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9685426142947832		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.9685426142947832 | validation: 0.6940314240922181]
	TIME [epoch: 1.31 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5495415493044885		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.5495415493044885 | validation: 0.6962927080151162]
	TIME [epoch: 1.3 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5361569199959072		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.5361569199959072 | validation: 0.5394607201569661]
	TIME [epoch: 1.31 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5541210661085806		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.5541210661085806 | validation: 0.7113145692928946]
	TIME [epoch: 1.3 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5536148564802524		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.5536148564802524 | validation: 0.6464806117077335]
	TIME [epoch: 1.31 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5281609094173081		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.5281609094173081 | validation: 0.5899505376665186]
	TIME [epoch: 1.3 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5193387453253567		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.5193387453253567 | validation: 0.6221224267075584]
	TIME [epoch: 1.32 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5195063814161154		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.5195063814161154 | validation: 0.5710191235242631]
	TIME [epoch: 1.3 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5416505850860045		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.5416505850860045 | validation: 0.7451825219875955]
	TIME [epoch: 1.31 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5670531757801991		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.5670531757801991 | validation: 0.5211924565412116]
	TIME [epoch: 1.31 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5106755769372204		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.5106755769372204 | validation: 0.6240047001503521]
	TIME [epoch: 1.31 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5140639751159504		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.5140639751159504 | validation: 0.5385605843820026]
	TIME [epoch: 1.31 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4898379058553835		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.4898379058553835 | validation: 0.6889340023823728]
	TIME [epoch: 1.31 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5484614006570068		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.5484614006570068 | validation: 0.5160122613586545]
	TIME [epoch: 1.32 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.564553543350239		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.564553543350239 | validation: 0.7687335743848229]
	TIME [epoch: 1.31 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6021684568664935		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.6021684568664935 | validation: 0.5571116432923245]
	TIME [epoch: 1.32 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47390821224867535		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.47390821224867535 | validation: 0.5391384563226301]
	TIME [epoch: 1.31 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48723464546710477		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.48723464546710477 | validation: 0.8328457767557338]
	TIME [epoch: 1.31 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.599787796801296		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.599787796801296 | validation: 0.4996888945969392]
	TIME [epoch: 1.31 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5730044578306196		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.5730044578306196 | validation: 0.7849563937137662]
	TIME [epoch: 1.31 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5871774662745263		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.5871774662745263 | validation: 0.5038690201837395]
	TIME [epoch: 1.31 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46333271875113896		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.46333271875113896 | validation: 0.5166749556425642]
	TIME [epoch: 1.31 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4741545261454755		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.4741545261454755 | validation: 0.6822537504089224]
	TIME [epoch: 1.3 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5367678829664425		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.5367678829664425 | validation: 0.537671349892714]
	TIME [epoch: 1.31 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5950068608941846		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.5950068608941846 | validation: 0.9247510741358154]
	TIME [epoch: 1.31 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7297520315154478		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.7297520315154478 | validation: 0.7342263510711585]
	TIME [epoch: 1.31 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5851057471352765		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.5851057471352765 | validation: 0.42217240624098884]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_365.pth
	Model improved!!!
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5266381019383026		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.5266381019383026 | validation: 0.6442763954046304]
	TIME [epoch: 1.31 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48943906015566385		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.48943906015566385 | validation: 0.5875899426277237]
	TIME [epoch: 1.32 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4834934641907697		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.4834934641907697 | validation: 0.5460574108518653]
	TIME [epoch: 1.31 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.559698331127284		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.559698331127284 | validation: 0.7355801008618528]
	TIME [epoch: 1.31 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5751588663665641		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.5751588663665641 | validation: 0.5412615590956867]
	TIME [epoch: 1.31 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.459125306023483		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.459125306023483 | validation: 0.4871793639418323]
	TIME [epoch: 1.31 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4604551170718279		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.4604551170718279 | validation: 0.6933573450158964]
	TIME [epoch: 1.31 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5190989481853278		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.5190989481853278 | validation: 0.4939453933747855]
	TIME [epoch: 1.31 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4940381349209028		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.4940381349209028 | validation: 0.704615668823392]
	TIME [epoch: 1.31 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5116936522410362		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.5116936522410362 | validation: 0.5018824327685935]
	TIME [epoch: 1.31 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49511977222721615		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.49511977222721615 | validation: 0.844846625484923]
	TIME [epoch: 1.31 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6351468742203309		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.6351468742203309 | validation: 0.4893848275746267]
	TIME [epoch: 1.31 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44801443694323534		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.44801443694323534 | validation: 0.4138900802386443]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_378.pth
	Model improved!!!
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4904882923354545		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.4904882923354545 | validation: 0.8061340811216141]
	TIME [epoch: 1.32 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6371173100817705		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.6371173100817705 | validation: 0.5928278781563273]
	TIME [epoch: 1.32 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4637204598666547		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.4637204598666547 | validation: 0.5336874619669569]
	TIME [epoch: 1.32 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5877195769284886		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.5877195769284886 | validation: 0.8377273548965537]
	TIME [epoch: 1.31 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5969619835258321		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.5969619835258321 | validation: 0.5127508271107226]
	TIME [epoch: 1.31 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4345316831741877		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.4345316831741877 | validation: 0.42518858546731314]
	TIME [epoch: 1.3 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47183458715227616		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.47183458715227616 | validation: 0.6561952335628343]
	TIME [epoch: 1.3 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5077779761024099		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.5077779761024099 | validation: 0.585841000852357]
	TIME [epoch: 1.3 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4975088532160937		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.4975088532160937 | validation: 0.6339789530545903]
	TIME [epoch: 1.31 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.493879205414753		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.493879205414753 | validation: 0.5059643872135652]
	TIME [epoch: 1.3 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4357646942113429		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.4357646942113429 | validation: 0.5079693887001907]
	TIME [epoch: 1.31 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4260029345561344		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.4260029345561344 | validation: 0.5106584147655531]
	TIME [epoch: 1.31 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4265443123304641		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.4265443123304641 | validation: 0.47649536752376803]
	TIME [epoch: 1.31 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.423487565019478		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.423487565019478 | validation: 0.5035293855140147]
	TIME [epoch: 1.3 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4520961139368943		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.4520961139368943 | validation: 0.7572898786925906]
	TIME [epoch: 1.31 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.553962310859858		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.553962310859858 | validation: 0.47314951649269255]
	TIME [epoch: 1.3 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5769569495595226		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.5769569495595226 | validation: 0.7882888057085532]
	TIME [epoch: 1.31 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6200185338850615		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.6200185338850615 | validation: 0.48443366511166375]
	TIME [epoch: 1.3 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4193397785015426		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.4193397785015426 | validation: 0.401087086398091]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_397.pth
	Model improved!!!
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4849568482083379		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.4849568482083379 | validation: 0.7501024094985662]
	TIME [epoch: 1.32 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5868164821559562		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.5868164821559562 | validation: 0.425486023390083]
	TIME [epoch: 1.32 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9159477553450879		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.9159477553450879 | validation: 0.4688705645283552]
	TIME [epoch: 1.32 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.454253764275835		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.454253764275835 | validation: 0.679528037393452]
	TIME [epoch: 1.3 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5140115473349348		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.5140115473349348 | validation: 0.49983944196908753]
	TIME [epoch: 1.3 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4373312500338158		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.4373312500338158 | validation: 0.41385275633781093]
	TIME [epoch: 1.3 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4764650221254146		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.4764650221254146 | validation: 0.6021985094982054]
	TIME [epoch: 1.3 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4858602301835262		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.4858602301835262 | validation: 0.655050807495384]
	TIME [epoch: 1.3 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47648440614639676		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.47648440614639676 | validation: 0.4836054835608845]
	TIME [epoch: 1.3 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48517906236607455		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.48517906236607455 | validation: 0.5099490921382975]
	TIME [epoch: 1.3 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42843247781453747		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.42843247781453747 | validation: 0.5062477684253041]
	TIME [epoch: 1.3 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41449172292995934		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.41449172292995934 | validation: 0.4515352443121692]
	TIME [epoch: 1.3 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4243196075880141		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.4243196075880141 | validation: 0.4906723473557875]
	TIME [epoch: 1.3 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4134727051915783		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.4134727051915783 | validation: 0.524368375532206]
	TIME [epoch: 1.3 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42684913335919505		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.42684913335919505 | validation: 0.5015934307730737]
	TIME [epoch: 1.3 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43751183008967287		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.43751183008967287 | validation: 0.6856805517528033]
	TIME [epoch: 1.29 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49295719737118265		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.49295719737118265 | validation: 0.4088870455537231]
	TIME [epoch: 1.3 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4756734369012082		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.4756734369012082 | validation: 0.6171301713318489]
	TIME [epoch: 1.3 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46565427021563066		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.46565427021563066 | validation: 0.43623491738378617]
	TIME [epoch: 1.3 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39673832915503876		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.39673832915503876 | validation: 0.44686026774920995]
	TIME [epoch: 1.3 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3938781451106465		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.3938781451106465 | validation: 0.4372568583640897]
	TIME [epoch: 1.3 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4453743998337383		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.4453743998337383 | validation: 0.7648003048221013]
	TIME [epoch: 1.3 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5643237043619145		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.5643237043619145 | validation: 0.5446699170036439]
	TIME [epoch: 1.3 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.470428156581361		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.470428156581361 | validation: 0.5326570957398111]
	TIME [epoch: 1.3 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4318318148180714		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.4318318148180714 | validation: 0.4632362945864406]
	TIME [epoch: 1.3 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40703620742811797		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.40703620742811797 | validation: 0.4825030084156861]
	TIME [epoch: 1.3 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4059927168593551		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.4059927168593551 | validation: 0.4666024580564164]
	TIME [epoch: 1.3 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4041801440472952		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.4041801440472952 | validation: 0.5245836846746033]
	TIME [epoch: 1.29 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4201055985075837		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.4201055985075837 | validation: 0.43779852242338235]
	TIME [epoch: 1.3 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4965153845262724		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.4965153845262724 | validation: 0.5701954265256174]
	TIME [epoch: 1.3 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43966814669864795		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.43966814669864795 | validation: 0.4347467988546879]
	TIME [epoch: 1.3 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4184422078026852		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.4184422078026852 | validation: 0.6513502335438397]
	TIME [epoch: 1.3 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4968204370572367		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.4968204370572367 | validation: 0.3862489465677196]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_430.pth
	Model improved!!!
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6288915208039902		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.6288915208039902 | validation: 0.6881051293638278]
	TIME [epoch: 1.3 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.526412765984999		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.526412765984999 | validation: 0.5207342829928967]
	TIME [epoch: 1.3 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4089295833056379		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.4089295833056379 | validation: 0.363676175124595]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_433.pth
	Model improved!!!
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5765313718552065		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.5765313718552065 | validation: 0.6003139110894826]
	TIME [epoch: 1.31 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4701451748390437		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.4701451748390437 | validation: 0.535125102210469]
	TIME [epoch: 1.31 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4014001258070239		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.4014001258070239 | validation: 0.3779233851155541]
	TIME [epoch: 1.31 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46395309401762846		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.46395309401762846 | validation: 0.6210598032864477]
	TIME [epoch: 1.3 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44713767386141795		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.44713767386141795 | validation: 0.5589021535164401]
	TIME [epoch: 1.31 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.448027266688262		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.448027266688262 | validation: 0.4680746224187393]
	TIME [epoch: 1.3 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43318935499410754		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.43318935499410754 | validation: 0.49795925542981145]
	TIME [epoch: 1.3 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4041369665231981		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.4041369665231981 | validation: 0.45397703761328123]
	TIME [epoch: 1.3 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3824914913863979		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.3824914913863979 | validation: 0.3819428413899381]
	TIME [epoch: 1.31 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4349090607167743		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.4349090607167743 | validation: 0.6612272852728149]
	TIME [epoch: 1.31 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48850784994247276		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.48850784994247276 | validation: 0.40233933683951373]
	TIME [epoch: 1.3 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4007921838825834		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.4007921838825834 | validation: 0.4103329237627307]
	TIME [epoch: 1.31 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3835772593511853		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.3835772593511853 | validation: 0.520919596963371]
	TIME [epoch: 1.31 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4103607844189815		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.4103607844189815 | validation: 0.5286321735497442]
	TIME [epoch: 1.3 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41744455406373265		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.41744455406373265 | validation: 0.5064992631546692]
	TIME [epoch: 1.3 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44612823804515134		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.44612823804515134 | validation: 0.6062475587721556]
	TIME [epoch: 1.3 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4247562970091461		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.4247562970091461 | validation: 0.48644061385199144]
	TIME [epoch: 1.3 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39401226828998986		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.39401226828998986 | validation: 0.3787604643458695]
	TIME [epoch: 1.3 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41079146767326175		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.41079146767326175 | validation: 0.6470581130743565]
	TIME [epoch: 1.3 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5022090229994752		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.5022090229994752 | validation: 0.40406002015864273]
	TIME [epoch: 1.3 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.411066736033759		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.411066736033759 | validation: 0.5111416286650349]
	TIME [epoch: 1.3 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40810165781287944		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.40810165781287944 | validation: 0.45871443736874584]
	TIME [epoch: 1.3 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40372879025521247		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.40372879025521247 | validation: 0.5529966839137586]
	TIME [epoch: 1.3 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4316863801666719		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.4316863801666719 | validation: 0.48557864360724906]
	TIME [epoch: 1.3 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3872239842302137		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.3872239842302137 | validation: 0.38772423934129246]
	TIME [epoch: 1.3 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38269268263986805		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.38269268263986805 | validation: 0.5802987032018538]
	TIME [epoch: 1.3 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4283157373354615		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.4283157373354615 | validation: 0.3656326863399363]
	TIME [epoch: 1.3 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3919773999294528		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.3919773999294528 | validation: 0.5404049173955098]
	TIME [epoch: 1.3 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3982643892560854		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.3982643892560854 | validation: 0.3810211414741666]
	TIME [epoch: 1.3 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38225360398682123		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.38225360398682123 | validation: 0.4632217319712972]
	TIME [epoch: 1.3 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3833712946736034		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.3833712946736034 | validation: 0.4475170198733263]
	TIME [epoch: 1.31 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3987626065360695		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.3987626065360695 | validation: 0.5902315010058142]
	TIME [epoch: 1.3 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41790196625635406		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.41790196625635406 | validation: 0.41638214088198205]
	TIME [epoch: 1.3 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39119208997345994		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.39119208997345994 | validation: 0.6310932308363704]
	TIME [epoch: 1.3 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4652194850414867		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.4652194850414867 | validation: 0.3325143276757658]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_468.pth
	Model improved!!!
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41851590782891124		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.41851590782891124 | validation: 0.6775049034669274]
	TIME [epoch: 1.31 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5155789575526843		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.5155789575526843 | validation: 0.5275283763085792]
	TIME [epoch: 1.31 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40393880571049123		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.40393880571049123 | validation: 0.39211750076445945]
	TIME [epoch: 1.31 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46178360865535417		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.46178360865535417 | validation: 0.6256292071541785]
	TIME [epoch: 1.31 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44212872298647554		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.44212872298647554 | validation: 0.41580311659529223]
	TIME [epoch: 1.3 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3604394350250654		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.3604394350250654 | validation: 0.3607851025754498]
	TIME [epoch: 1.31 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3837168449732448		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.3837168449732448 | validation: 0.5778902557045006]
	TIME [epoch: 1.3 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44837193271903203		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.44837193271903203 | validation: 0.42354245761748266]
	TIME [epoch: 1.31 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38781796106547783		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.38781796106547783 | validation: 0.4963107982857647]
	TIME [epoch: 1.3 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4108189420786492		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.4108189420786492 | validation: 0.49374807983032004]
	TIME [epoch: 1.31 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3925846338933954		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.3925846338933954 | validation: 0.37028086904466495]
	TIME [epoch: 1.3 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3795509310933673		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.3795509310933673 | validation: 0.44573206401919735]
	TIME [epoch: 1.31 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36073534118135187		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.36073534118135187 | validation: 0.349194405701676]
	TIME [epoch: 1.31 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37634933114209873		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.37634933114209873 | validation: 0.5126186033341339]
	TIME [epoch: 1.31 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3949359720157004		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.3949359720157004 | validation: 0.39714117273557753]
	TIME [epoch: 1.31 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4239842337046665		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.4239842337046665 | validation: 0.7177111612751139]
	TIME [epoch: 1.31 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5390303705328635		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.5390303705328635 | validation: 0.4821946353538919]
	TIME [epoch: 1.3 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38851473833247985		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.38851473833247985 | validation: 0.31742279794711786]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_486.pth
	Model improved!!!
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4399627377706269		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.4399627377706269 | validation: 0.6452948277433608]
	TIME [epoch: 1.31 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4799722271539572		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.4799722271539572 | validation: 0.4460162261431993]
	TIME [epoch: 1.31 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3612834855372766		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.3612834855372766 | validation: 0.3334552797047282]
	TIME [epoch: 1.31 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42713493383005563		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.42713493383005563 | validation: 0.5860588311004961]
	TIME [epoch: 1.31 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43473636878540134		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.43473636878540134 | validation: 0.41493806846427655]
	TIME [epoch: 1.31 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3635707295396318		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.3635707295396318 | validation: 0.313200525840536]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_492.pth
	Model improved!!!
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4953365767642858		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.4953365767642858 | validation: 0.6025545728008945]
	TIME [epoch: 1.31 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4632621547261416		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.4632621547261416 | validation: 0.5939487085918286]
	TIME [epoch: 1.3 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4285582705122242		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.4285582705122242 | validation: 0.41115513060858033]
	TIME [epoch: 1.31 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4123087849795541		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.4123087849795541 | validation: 0.4047709726073582]
	TIME [epoch: 1.31 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3765250342439734		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.3765250342439734 | validation: 0.5968630370056044]
	TIME [epoch: 1.3 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45828265025506354		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.45828265025506354 | validation: 0.41989280367361365]
	TIME [epoch: 1.31 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3517902435148426		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.3517902435148426 | validation: 0.32847969219823087]
	TIME [epoch: 1.31 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4075417664709168		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.4075417664709168 | validation: 0.5203271694125337]
	TIME [epoch: 1.32 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4041501129799383		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.4041501129799383 | validation: 0.45265207361578597]
	TIME [epoch: 160 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36482068751521163		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.36482068751521163 | validation: 0.37601404426795176]
	TIME [epoch: 2.62 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3680158527437989		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.3680158527437989 | validation: 0.42180156663127627]
	TIME [epoch: 2.61 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37587343519382305		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.37587343519382305 | validation: 0.4910203811705472]
	TIME [epoch: 2.6 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38875293704461156		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.38875293704461156 | validation: 0.39160643718942945]
	TIME [epoch: 2.6 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3806514281472213		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.3806514281472213 | validation: 0.34731251789568135]
	TIME [epoch: 2.61 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39579029749248357		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.39579029749248357 | validation: 0.5403714729003923]
	TIME [epoch: 2.6 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41103965373822215		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.41103965373822215 | validation: 0.3929888290420778]
	TIME [epoch: 2.6 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3477562734650928		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.3477562734650928 | validation: 0.3516071386892479]
	TIME [epoch: 2.61 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35098409852701934		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.35098409852701934 | validation: 0.4352727064909938]
	TIME [epoch: 2.6 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3577466474893082		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.3577466474893082 | validation: 0.3458890062583752]
	TIME [epoch: 2.6 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3587570961351		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.3587570961351 | validation: 0.4071308158573623]
	TIME [epoch: 2.61 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34916476616200554		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.34916476616200554 | validation: 0.4286201771667095]
	TIME [epoch: 2.6 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37142415634644793		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.37142415634644793 | validation: 0.5277661618919937]
	TIME [epoch: 2.6 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3931330772968138		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.3931330772968138 | validation: 0.4158451745490004]
	TIME [epoch: 2.6 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37321737406887645		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.37321737406887645 | validation: 0.4302860445344412]
	TIME [epoch: 2.6 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35009771097396075		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.35009771097396075 | validation: 0.3288952922324423]
	TIME [epoch: 2.61 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36545450340151886		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.36545450340151886 | validation: 0.6116836095308407]
	TIME [epoch: 2.6 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4616673317195359		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.4616673317195359 | validation: 0.3791368685249956]
	TIME [epoch: 2.61 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37426959740073995		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.37426959740073995 | validation: 0.4132924946775884]
	TIME [epoch: 2.62 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3649176422901975		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.3649176422901975 | validation: 0.40468026885439823]
	TIME [epoch: 2.6 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.369919381085364		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.369919381085364 | validation: 0.47460085725085027]
	TIME [epoch: 2.6 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37430383462600897		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.37430383462600897 | validation: 0.3356136243245889]
	TIME [epoch: 2.61 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39218799362662227		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.39218799362662227 | validation: 0.5518613378707049]
	TIME [epoch: 2.61 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42032644564268795		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.42032644564268795 | validation: 0.35722119265885877]
	TIME [epoch: 2.61 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33953236778925244		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.33953236778925244 | validation: 0.41021696324507345]
	TIME [epoch: 2.61 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3438862198152441		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.3438862198152441 | validation: 0.33650931301946746]
	TIME [epoch: 2.6 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35102221761122343		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.35102221761122343 | validation: 0.5158003725431002]
	TIME [epoch: 2.61 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4024543228858585		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.4024543228858585 | validation: 0.39284682631699336]
	TIME [epoch: 2.61 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3509239665225584		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.3509239665225584 | validation: 0.3626269942331674]
	TIME [epoch: 2.62 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3568231714571061		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.3568231714571061 | validation: 0.504785511247649]
	TIME [epoch: 2.61 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38569540684570397		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.38569540684570397 | validation: 0.36104805644836513]
	TIME [epoch: 2.61 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35143689759338015		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.35143689759338015 | validation: 0.34101235157067933]
	TIME [epoch: 2.61 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35393420975190054		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.35393420975190054 | validation: 0.486838945338108]
	TIME [epoch: 2.61 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3771179800515899		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.3771179800515899 | validation: 0.3556807974219012]
	TIME [epoch: 2.61 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3433948774384909		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.3433948774384909 | validation: 0.3505419091260633]
	TIME [epoch: 2.61 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33160484763933923		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.33160484763933923 | validation: 0.4205580107849275]
	TIME [epoch: 2.6 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3419433363246294		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.3419433363246294 | validation: 0.3039786724662196]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_538.pth
	Model improved!!!
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3832056363343592		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.3832056363343592 | validation: 0.5212029706566893]
	TIME [epoch: 2.6 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3964831633495015		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.3964831633495015 | validation: 0.33635137435651313]
	TIME [epoch: 2.6 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3364732960456145		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.3364732960456145 | validation: 0.3321719924922018]
	TIME [epoch: 2.61 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3689569896739247		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.3689569896739247 | validation: 0.5891283412028783]
	TIME [epoch: 2.6 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4657548833383103		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.4657548833383103 | validation: 0.3982444476925835]
	TIME [epoch: 2.61 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35293382952464314		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.35293382952464314 | validation: 0.3292397140685289]
	TIME [epoch: 2.59 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4068701577174778		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.4068701577174778 | validation: 0.5693920408309462]
	TIME [epoch: 2.61 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4258247491191064		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.4258247491191064 | validation: 0.36019754041787644]
	TIME [epoch: 2.6 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32825375662620304		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.32825375662620304 | validation: 0.4027653394875212]
	TIME [epoch: 2.61 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3414515789554676		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.3414515789554676 | validation: 0.38749546570858906]
	TIME [epoch: 2.59 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3472092679074953		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.3472092679074953 | validation: 0.4336879193314003]
	TIME [epoch: 2.59 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35719385281718646		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.35719385281718646 | validation: 0.5798687294676128]
	TIME [epoch: 2.59 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43454303641402653		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.43454303641402653 | validation: 0.38782394687929245]
	TIME [epoch: 2.59 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36829849605990006		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.36829849605990006 | validation: 0.3204554541142151]
	TIME [epoch: 2.59 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3506459169448769		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.3506459169448769 | validation: 0.3997564317279577]
	TIME [epoch: 2.6 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3528174804506068		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.3528174804506068 | validation: 0.341413984922961]
	TIME [epoch: 2.6 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3494476213395302		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.3494476213395302 | validation: 0.4520014233876486]
	TIME [epoch: 2.6 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35291357817529007		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.35291357817529007 | validation: 0.3464369635273061]
	TIME [epoch: 2.59 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33458943573905375		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.33458943573905375 | validation: 0.3321127170847539]
	TIME [epoch: 2.61 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3220535691480798		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.3220535691480798 | validation: 0.4936803169982203]
	TIME [epoch: 2.59 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3993957250701585		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.3993957250701585 | validation: 0.3414060626060516]
	TIME [epoch: 2.6 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34138942811874456		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.34138942811874456 | validation: 0.39244636511859915]
	TIME [epoch: 2.6 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34901402529967557		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.34901402529967557 | validation: 0.4801193186524702]
	TIME [epoch: 2.6 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3665052442937753		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.3665052442937753 | validation: 0.3390915361104653]
	TIME [epoch: 2.6 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36238471806814576		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.36238471806814576 | validation: 0.4243666189082858]
	TIME [epoch: 2.6 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34491071794441325		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.34491071794441325 | validation: 0.3303582182474122]
	TIME [epoch: 2.6 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3316380677152671		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.3316380677152671 | validation: 0.3830614070759062]
	TIME [epoch: 2.6 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.336291138650071		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.336291138650071 | validation: 0.3767475381926697]
	TIME [epoch: 2.6 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.339156529546356		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.339156529546356 | validation: 0.4219823295856953]
	TIME [epoch: 2.61 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33638343240324703		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.33638343240324703 | validation: 0.3248591559704068]
	TIME [epoch: 2.6 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3523201213144654		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.3523201213144654 | validation: 0.5014751145806551]
	TIME [epoch: 2.6 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38854717584135784		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.38854717584135784 | validation: 0.3385604874050747]
	TIME [epoch: 2.6 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3353568970474102		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.3353568970474102 | validation: 0.36930529177529103]
	TIME [epoch: 2.6 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3218817110507217		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.3218817110507217 | validation: 0.31584451148627457]
	TIME [epoch: 2.6 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3411693396474539		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.3411693396474539 | validation: 0.4568910332949931]
	TIME [epoch: 2.6 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35532255988322126		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.35532255988322126 | validation: 0.30482057528535006]
	TIME [epoch: 2.6 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34026199471477797		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.34026199471477797 | validation: 0.40686471069379104]
	TIME [epoch: 2.61 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33123957750475386		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.33123957750475386 | validation: 0.3652853840128758]
	TIME [epoch: 2.6 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33211448864146403		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.33211448864146403 | validation: 0.34152791708758246]
	TIME [epoch: 2.62 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3392063547901307		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.3392063547901307 | validation: 0.4719481955297205]
	TIME [epoch: 2.6 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3508462297391048		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.3508462297391048 | validation: 0.3163998608800767]
	TIME [epoch: 2.61 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33779498694576376		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.33779498694576376 | validation: 0.4477094183322386]
	TIME [epoch: 2.61 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34648985188169595		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.34648985188169595 | validation: 0.3147758721242276]
	TIME [epoch: 2.61 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32496582652669675		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.32496582652669675 | validation: 0.4222807026251803]
	TIME [epoch: 2.6 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34224497610135585		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.34224497610135585 | validation: 0.3432745045614316]
	TIME [epoch: 2.61 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3339960825740532		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.3339960825740532 | validation: 0.4244172412786211]
	TIME [epoch: 2.61 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3531601321346183		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.3531601321346183 | validation: 0.33074072955477585]
	TIME [epoch: 2.62 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34162016183513394		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.34162016183513394 | validation: 0.4435236652945972]
	TIME [epoch: 2.6 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3544415340606098		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.3544415340606098 | validation: 0.33404365922863943]
	TIME [epoch: 2.61 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3208174803222048		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.3208174803222048 | validation: 0.3217868520090826]
	TIME [epoch: 2.61 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31877228959739934		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.31877228959739934 | validation: 0.36467608503421484]
	TIME [epoch: 2.61 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.317522584623025		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.317522584623025 | validation: 0.29059478563673696]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_590.pth
	Model improved!!!
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34369574841383865		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.34369574841383865 | validation: 0.715387457048092]
	TIME [epoch: 2.6 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5761637497912965		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.5761637497912965 | validation: 0.5983386639192078]
	TIME [epoch: 2.61 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46465995867839527		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.46465995867839527 | validation: 0.3524695586146558]
	TIME [epoch: 2.61 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37414075273823366		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.37414075273823366 | validation: 0.31333976564030963]
	TIME [epoch: 2.61 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47958431081149394		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.47958431081149394 | validation: 0.4527444939806663]
	TIME [epoch: 2.61 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39305273417628017		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.39305273417628017 | validation: 1.2042047110632546]
	TIME [epoch: 2.61 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9761117268022433		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.9761117268022433 | validation: 1.1986273651442463]
	TIME [epoch: 2.6 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8769331952060498		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.8769331952060498 | validation: 1.1016946451239733]
	TIME [epoch: 2.62 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8037478799929543		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.8037478799929543 | validation: 1.0035068839782963]
	TIME [epoch: 2.61 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7757488932956315		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.7757488932956315 | validation: 0.8409605522220753]
	TIME [epoch: 2.61 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.676488800641541		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.676488800641541 | validation: 0.6610298981817868]
	TIME [epoch: 2.6 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5271054728537545		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.5271054728537545 | validation: 0.41171380109522]
	TIME [epoch: 2.61 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37463337308213557		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.37463337308213557 | validation: 0.30776339797944846]
	TIME [epoch: 2.6 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3504206982020921		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.3504206982020921 | validation: 0.32623272689029775]
	TIME [epoch: 2.61 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3303978248907504		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.3303978248907504 | validation: 0.37250503057666434]
	TIME [epoch: 2.6 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3339219968825975		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.3339219968825975 | validation: 0.3499622679054361]
	TIME [epoch: 2.61 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32347987288286206		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.32347987288286206 | validation: 0.3284155686743956]
	TIME [epoch: 2.59 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3276453374873542		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.3276453374873542 | validation: 0.34001161905471516]
	TIME [epoch: 2.61 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31556089626432		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.31556089626432 | validation: 0.34818753117641693]
	TIME [epoch: 2.6 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31668508950535595		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.31668508950535595 | validation: 0.46597267425919764]
	TIME [epoch: 2.61 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3766039867872826		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.3766039867872826 | validation: 0.8954259649326293]
	TIME [epoch: 2.61 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6916145767774767		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.6916145767774767 | validation: 0.8808171282468742]
	TIME [epoch: 2.61 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6761846856026		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.6761846856026 | validation: 0.6586534472260696]
	TIME [epoch: 2.61 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5585526441580727		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.5585526441580727 | validation: 0.46187274998044026]
	TIME [epoch: 2.61 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43129003682697814		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.43129003682697814 | validation: 0.3962572027791808]
	TIME [epoch: 2.61 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3616692075807223		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.3616692075807223 | validation: 0.3344163405502716]
	TIME [epoch: 2.61 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3456626863955776		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.3456626863955776 | validation: 0.3377100524153611]
	TIME [epoch: 2.6 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33682248883793997		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.33682248883793997 | validation: 0.34524081224436237]
	TIME [epoch: 2.61 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3264135666403019		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.3264135666403019 | validation: 0.37097885959411414]
	TIME [epoch: 2.61 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3232814827130728		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.3232814827130728 | validation: 0.3684576477844299]
	TIME [epoch: 2.61 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3209353193035814		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.3209353193035814 | validation: 0.33267718368124405]
	TIME [epoch: 2.61 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32778295760638115		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.32778295760638115 | validation: 0.3682662250464195]
	TIME [epoch: 2.61 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3210356162039166		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.3210356162039166 | validation: 0.3604685366911641]
	TIME [epoch: 2.61 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3175702894216556		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.3175702894216556 | validation: 0.3366291906656944]
	TIME [epoch: 2.61 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3234658406436601		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.3234658406436601 | validation: 0.4089189960861023]
	TIME [epoch: 2.59 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33706492203049565		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.33706492203049565 | validation: 0.32228081664650077]
	TIME [epoch: 2.6 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32987171936278087		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.32987171936278087 | validation: 0.34744001984254547]
	TIME [epoch: 2.61 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31527353925533597		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.31527353925533597 | validation: 0.37416454948184624]
	TIME [epoch: 2.6 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31134273950624614		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.31134273950624614 | validation: 0.2820380165147371]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_629.pth
	Model improved!!!
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3396599876289196		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.3396599876289196 | validation: 0.4162475052373261]
	TIME [epoch: 2.6 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3433890081643741		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.3433890081643741 | validation: 0.32837388226819136]
	TIME [epoch: 2.61 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31316729677665406		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.31316729677665406 | validation: 0.32576150980071533]
	TIME [epoch: 2.61 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3168155675015917		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.3168155675015917 | validation: 0.36715730418913195]
	TIME [epoch: 2.61 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3248787650212481		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.3248787650212481 | validation: 0.3534042289750743]
	TIME [epoch: 2.61 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3238149471001927		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.3238149471001927 | validation: 0.3449810410589848]
	TIME [epoch: 2.61 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3266320283064847		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.3266320283064847 | validation: 0.36541487573650844]
	TIME [epoch: 2.6 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32058286638013983		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.32058286638013983 | validation: 0.32305326542533824]
	TIME [epoch: 2.61 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32479493462999415		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.32479493462999415 | validation: 0.35252351102043467]
	TIME [epoch: 2.6 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9404692070301411		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.9404692070301411 | validation: 0.32083030631424647]
	TIME [epoch: 2.6 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45189413009252494		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.45189413009252494 | validation: 0.5929957717585389]
	TIME [epoch: 2.6 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5007791266981805		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.5007791266981805 | validation: 0.4909996453176422]
	TIME [epoch: 2.6 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3718765894189286		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.3718765894189286 | validation: 0.31600217516711215]
	TIME [epoch: 2.6 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3303521376346778		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.3303521376346778 | validation: 0.28865734430584594]
	TIME [epoch: 2.6 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38224686608490954		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.38224686608490954 | validation: 0.3648752322167768]
	TIME [epoch: 2.6 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31736214727627454		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.31736214727627454 | validation: 0.38122673250481576]
	TIME [epoch: 2.61 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3192149698658917		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.3192149698658917 | validation: 0.34306926192582143]
	TIME [epoch: 2.61 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30855495368574337		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.30855495368574337 | validation: 0.3040778230081886]
	TIME [epoch: 2.6 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31566109612588117		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.31566109612588117 | validation: 0.34181601326432437]
	TIME [epoch: 2.6 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30701627997000464		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.30701627997000464 | validation: 0.3463437697955134]
	TIME [epoch: 2.6 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30574831793464363		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.30574831793464363 | validation: 0.3503907603904365]
	TIME [epoch: 2.6 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3061921997946373		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.3061921997946373 | validation: 0.30023229149081726]
	TIME [epoch: 2.61 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42386086148188457		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.42386086148188457 | validation: 0.35769728290798425]
	TIME [epoch: 2.6 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33827461688489907		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.33827461688489907 | validation: 0.3806337112532381]
	TIME [epoch: 2.6 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32526960536309674		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.32526960536309674 | validation: 0.33374997332435485]
	TIME [epoch: 2.61 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3091884197292876		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.3091884197292876 | validation: 0.3123238735337858]
	TIME [epoch: 2.6 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3178523210004088		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.3178523210004088 | validation: 0.3715295911304044]
	TIME [epoch: 2.61 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31514071237774643		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.31514071237774643 | validation: 0.29865270400758315]
	TIME [epoch: 2.6 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3169775486703315		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.3169775486703315 | validation: 0.33391352992510365]
	TIME [epoch: 2.6 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3097650407126741		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.3097650407126741 | validation: 0.3322699667477603]
	TIME [epoch: 2.59 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3097330660271983		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.3097330660271983 | validation: 0.47001765342471064]
	TIME [epoch: 2.6 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37173571178798864		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.37173571178798864 | validation: 0.3705630825863398]
	TIME [epoch: 2.6 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32871885185766153		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.32871885185766153 | validation: 0.31414386875268313]
	TIME [epoch: 2.6 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3111574685618043		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.3111574685618043 | validation: 0.37301295622428765]
	TIME [epoch: 2.61 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32369982952035764		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.32369982952035764 | validation: 0.32475499902842975]
	TIME [epoch: 2.61 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3034308389771411		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.3034308389771411 | validation: 0.3130562601965583]
	TIME [epoch: 2.6 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3066952119183573		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.3066952119183573 | validation: 0.4058142476153524]
	TIME [epoch: 2.6 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3388378170705491		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.3388378170705491 | validation: 0.30822082324656647]
	TIME [epoch: 2.6 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30664971053010764		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.30664971053010764 | validation: 0.2953299446440517]
	TIME [epoch: 2.6 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31337280921167926		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.31337280921167926 | validation: 0.3940272975775673]
	TIME [epoch: 2.59 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3291734747674516		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.3291734747674516 | validation: 0.3307229561314022]
	TIME [epoch: 2.6 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30694619111120186		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.30694619111120186 | validation: 0.292885337768498]
	TIME [epoch: 2.59 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3184970268890506		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.3184970268890506 | validation: 0.4080409096775659]
	TIME [epoch: 2.61 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3334327476184092		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.3334327476184092 | validation: 0.3391252071260759]
	TIME [epoch: 2.59 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.312644311230176		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.312644311230176 | validation: 0.31065421399121784]
	TIME [epoch: 2.61 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3176522510633724		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.3176522510633724 | validation: 0.3733013267013363]
	TIME [epoch: 2.59 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3155883655439318		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.3155883655439318 | validation: 0.3091965154402756]
	TIME [epoch: 2.6 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3164267951056383		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.3164267951056383 | validation: 0.40414840316986667]
	TIME [epoch: 2.6 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32840215804562023		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.32840215804562023 | validation: 0.3133278941173718]
	TIME [epoch: 2.6 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31137610937784743		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.31137610937784743 | validation: 0.30482353901737996]
	TIME [epoch: 2.6 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3074116035754751		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.3074116035754751 | validation: 0.35950178423714535]
	TIME [epoch: 2.61 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3142006044761638		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.3142006044761638 | validation: 0.3003810089303497]
	TIME [epoch: 2.6 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30103610053569463		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.30103610053569463 | validation: 0.3794987386379314]
	TIME [epoch: 2.6 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3148935655989851		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.3148935655989851 | validation: 0.29572134379067716]
	TIME [epoch: 2.59 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30925637684003676		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.30925637684003676 | validation: 0.4284100969763893]
	TIME [epoch: 2.61 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3524947185741113		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.3524947185741113 | validation: 0.3449396240471469]
	TIME [epoch: 2.6 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31470299459563095		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.31470299459563095 | validation: 0.2925705618833198]
	TIME [epoch: 2.6 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32652235600635254		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.32652235600635254 | validation: 0.4327041126046391]
	TIME [epoch: 2.6 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34355999231405954		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.34355999231405954 | validation: 0.32463027199223726]
	TIME [epoch: 2.6 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2931557701208921		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.2931557701208921 | validation: 0.27472716009210413]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_689.pth
	Model improved!!!
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.330094021863093		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.330094021863093 | validation: 0.34390247270965396]
	TIME [epoch: 2.59 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3104889304418678		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.3104889304418678 | validation: 0.3400532484028881]
	TIME [epoch: 2.59 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2989709477922511		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.2989709477922511 | validation: 0.33988497275861057]
	TIME [epoch: 2.57 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3115084345081626		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.3115084345081626 | validation: 0.3494115641739704]
	TIME [epoch: 2.59 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3112640970451235		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.3112640970451235 | validation: 0.3543383399807414]
	TIME [epoch: 2.58 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31183403414395894		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.31183403414395894 | validation: 0.29702206664114167]
	TIME [epoch: 2.58 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3028017856142672		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.3028017856142672 | validation: 0.31157158109232425]
	TIME [epoch: 2.58 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3010271549337589		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.3010271549337589 | validation: 0.30828794838669227]
	TIME [epoch: 2.58 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3014912880527075		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.3014912880527075 | validation: 0.3026345836064327]
	TIME [epoch: 2.58 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30256017768140064		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.30256017768140064 | validation: 0.4075624102078328]
	TIME [epoch: 2.58 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3257798121454248		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.3257798121454248 | validation: 0.3102825438743452]
	TIME [epoch: 2.58 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3121496361342203		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.3121496361342203 | validation: 0.34900746656646975]
	TIME [epoch: 2.6 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2998668924167723		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.2998668924167723 | validation: 0.31321819983965826]
	TIME [epoch: 2.59 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30793292869302036		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.30793292869302036 | validation: 0.39104668299702844]
	TIME [epoch: 2.59 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3190434110825467		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.3190434110825467 | validation: 0.30724790340861163]
	TIME [epoch: 2.6 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29711789584257814		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.29711789584257814 | validation: 0.2996589322168219]
	TIME [epoch: 2.59 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2968780005987712		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.2968780005987712 | validation: 0.4286418074398623]
	TIME [epoch: 2.6 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3407947607396464		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.3407947607396464 | validation: 0.3254199277127552]
	TIME [epoch: 2.6 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3037267008840419		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.3037267008840419 | validation: 0.29069218253637275]
	TIME [epoch: 2.59 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3111559922348751		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.3111559922348751 | validation: 0.4054422854678567]
	TIME [epoch: 2.6 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3209578339086904		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.3209578339086904 | validation: 0.32719698958619875]
	TIME [epoch: 2.58 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2926468529950234		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.2926468529950234 | validation: 0.2994478395643285]
	TIME [epoch: 2.59 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3101004714233545		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.3101004714233545 | validation: 0.35654169729126717]
	TIME [epoch: 2.58 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30909981368428424		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.30909981368428424 | validation: 0.3265826917437649]
	TIME [epoch: 2.59 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30300861417504943		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.30300861417504943 | validation: 0.30404124885727324]
	TIME [epoch: 2.58 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30858389255531327		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.30858389255531327 | validation: 0.33108869243372185]
	TIME [epoch: 2.59 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29745465967608575		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.29745465967608575 | validation: 0.33010485251633676]
	TIME [epoch: 2.57 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2929359713261253		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.2929359713261253 | validation: 0.29264909687523843]
	TIME [epoch: 2.58 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30085581188596633		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.30085581188596633 | validation: 0.34822158745387627]
	TIME [epoch: 2.6 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29993192464826157		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.29993192464826157 | validation: 0.29207329821352496]
	TIME [epoch: 2.58 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2946386905318095		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.2946386905318095 | validation: 0.30576719268413594]
	TIME [epoch: 2.6 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2957185439277965		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.2957185439277965 | validation: 0.30204630076278793]
	TIME [epoch: 2.58 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29651868860111724		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.29651868860111724 | validation: 0.313264568645886]
	TIME [epoch: 2.6 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2957405687381968		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.2957405687381968 | validation: 0.29361141265592305]
	TIME [epoch: 2.59 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3098397259528979		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.3098397259528979 | validation: 0.6189601201572943]
	TIME [epoch: 2.6 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49672592639323826		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.49672592639323826 | validation: 0.575595424347222]
	TIME [epoch: 2.58 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42390292486081826		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.42390292486081826 | validation: 0.36530273958542425]
	TIME [epoch: 2.6 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3587789879649928		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.3587789879649928 | validation: 0.318836174864214]
	TIME [epoch: 2.58 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3155559845184887		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.3155559845184887 | validation: 0.30504858429060155]
	TIME [epoch: 2.6 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2982874103401333		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.2982874103401333 | validation: 0.31377923017729586]
	TIME [epoch: 2.58 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29854904200165044		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.29854904200165044 | validation: 0.2798846921021687]
	TIME [epoch: 2.59 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3113649507776807		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.3113649507776807 | validation: 0.3168003334747579]
	TIME [epoch: 2.58 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2925982811322887		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.2925982811322887 | validation: 0.3195181867298869]
	TIME [epoch: 2.58 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2945209517349819		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.2945209517349819 | validation: 0.2937096766047328]
	TIME [epoch: 2.58 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29900279168771543		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.29900279168771543 | validation: 0.345588469814553]
	TIME [epoch: 2.58 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29392495219782006		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.29392495219782006 | validation: 0.34423381274123893]
	TIME [epoch: 2.57 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2981803992325458		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.2981803992325458 | validation: 0.2834792980724827]
	TIME [epoch: 2.59 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30595621017950975		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.30595621017950975 | validation: 0.3362311400481782]
	TIME [epoch: 2.59 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29010941098799564		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.29010941098799564 | validation: 0.33165192101673546]
	TIME [epoch: 2.58 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29330500189742087		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.29330500189742087 | validation: 0.30151542431274614]
	TIME [epoch: 2.6 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29477149527568086		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.29477149527568086 | validation: 0.3605902254972674]
	TIME [epoch: 2.58 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30730732102861913		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.30730732102861913 | validation: 0.301678658648891]
	TIME [epoch: 2.6 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3016962799012455		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.3016962799012455 | validation: 0.29390496325764015]
	TIME [epoch: 2.58 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2957816742497509		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.2957816742497509 | validation: 0.3277118063740956]
	TIME [epoch: 2.6 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3021150120467748		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.3021150120467748 | validation: 0.31827907055776394]
	TIME [epoch: 2.58 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29225464153597325		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.29225464153597325 | validation: 0.29809111550479045]
	TIME [epoch: 2.6 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3051539981406803		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.3051539981406803 | validation: 0.3791330457513744]
	TIME [epoch: 2.58 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3132775665916016		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.3132775665916016 | validation: 0.3093823026737054]
	TIME [epoch: 2.6 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2880003461965891		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.2880003461965891 | validation: 0.284047301430547]
	TIME [epoch: 2.59 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2928195096236865		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.2928195096236865 | validation: 0.3524262320824909]
	TIME [epoch: 2.59 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30474588081900705		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.30474588081900705 | validation: 0.3041054657621467]
	TIME [epoch: 2.59 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2922115878293769		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.2922115878293769 | validation: 0.2965070331887657]
	TIME [epoch: 2.6 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28977330343151486		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.28977330343151486 | validation: 0.351456075077531]
	TIME [epoch: 2.59 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29848375689013146		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.29848375689013146 | validation: 0.48734840198344864]
	TIME [epoch: 2.6 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36307989930786866		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.36307989930786866 | validation: 0.3215154974435494]
	TIME [epoch: 2.58 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32230529788693246		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.32230529788693246 | validation: 0.297968111672599]
	TIME [epoch: 2.6 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2991865428976289		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.2991865428976289 | validation: 0.33927388802739733]
	TIME [epoch: 2.59 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.294756034963011		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.294756034963011 | validation: 0.29198867573863757]
	TIME [epoch: 2.6 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29788984114739786		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.29788984114739786 | validation: 0.30340237600040654]
	TIME [epoch: 2.59 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.291283325850669		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.291283325850669 | validation: 0.3076242401799416]
	TIME [epoch: 2.61 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28329786660722905		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.28329786660722905 | validation: 0.32344481686062687]
	TIME [epoch: 2.6 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29287471559689215		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.29287471559689215 | validation: 0.31833987554499843]
	TIME [epoch: 2.61 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2878390460608094		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.2878390460608094 | validation: 0.3052094604956481]
	TIME [epoch: 2.59 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28885048830267546		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.28885048830267546 | validation: 0.345446032925232]
	TIME [epoch: 2.6 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3056604506688899		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.3056604506688899 | validation: 0.27336478359918975]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_764.pth
	Model improved!!!
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32488397997368823		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.32488397997368823 | validation: 0.37403111778585246]
	TIME [epoch: 2.58 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31292973522228096		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.31292973522228096 | validation: 0.3363483315136117]
	TIME [epoch: 2.59 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29688077281815395		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.29688077281815395 | validation: 0.2699287268605018]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_767.pth
	Model improved!!!
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30968244204354395		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.30968244204354395 | validation: 0.3166747057648476]
	TIME [epoch: 2.6 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2948828317844895		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.2948828317844895 | validation: 0.3187044649976317]
	TIME [epoch: 2.61 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28759520372541814		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.28759520372541814 | validation: 0.27306774285081153]
	TIME [epoch: 2.61 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2988671955056341		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.2988671955056341 | validation: 0.32092920077506254]
	TIME [epoch: 2.58 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2918313154021482		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.2918313154021482 | validation: 0.3114412429244317]
	TIME [epoch: 2.58 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2848477418844561		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.2848477418844561 | validation: 0.2954134331909483]
	TIME [epoch: 2.61 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28844660125838373		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.28844660125838373 | validation: 0.2890337131798914]
	TIME [epoch: 2.58 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.287718263304656		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.287718263304656 | validation: 0.3478969454218259]
	TIME [epoch: 2.6 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29455778641565816		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.29455778641565816 | validation: 0.30040258076468196]
	TIME [epoch: 2.58 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2920119058622398		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.2920119058622398 | validation: 0.3722414796815911]
	TIME [epoch: 2.59 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3060239566340113		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.3060239566340113 | validation: 0.3077787205759015]
	TIME [epoch: 2.58 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28708778007948027		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.28708778007948027 | validation: 0.28800234227958177]
	TIME [epoch: 2.6 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2940608234312373		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.2940608234312373 | validation: 0.29942365719738673]
	TIME [epoch: 2.58 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2925240435510097		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.2925240435510097 | validation: 0.31251465658481536]
	TIME [epoch: 2.6 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28865068810634353		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.28865068810634353 | validation: 0.3149815663749324]
	TIME [epoch: 2.6 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2866508448358702		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.2866508448358702 | validation: 0.29761260512647286]
	TIME [epoch: 2.6 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28380659466551644		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.28380659466551644 | validation: 0.2827912676369841]
	TIME [epoch: 2.6 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28464081489214205		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.28464081489214205 | validation: 0.2678050339660411]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_785.pth
	Model improved!!!
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2975023195966146		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.2975023195966146 | validation: 0.35564523089877825]
	TIME [epoch: 2.58 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30482090382038524		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.30482090382038524 | validation: 0.2836917569347555]
	TIME [epoch: 2.6 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2966800939334272		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.2966800939334272 | validation: 0.2923340987683533]
	TIME [epoch: 2.61 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28787431815176606		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.28787431815176606 | validation: 0.3515663760029396]
	TIME [epoch: 2.61 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3018408085983395		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.3018408085983395 | validation: 0.27748856425580354]
	TIME [epoch: 2.6 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2979802027083213		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.2979802027083213 | validation: 0.3544766148390693]
	TIME [epoch: 2.6 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29587159801793134		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.29587159801793134 | validation: 0.32206735104082373]
	TIME [epoch: 2.61 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28603851530514024		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.28603851530514024 | validation: 0.2932574421449218]
	TIME [epoch: 2.6 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2851800793636027		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.2851800793636027 | validation: 0.30181043242022015]
	TIME [epoch: 2.57 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2858472136188084		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.2858472136188084 | validation: 0.31894297361815704]
	TIME [epoch: 2.6 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28940825671993753		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.28940825671993753 | validation: 0.2873282554961087]
	TIME [epoch: 2.57 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3121642934093493		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.3121642934093493 | validation: 0.33989468555129543]
	TIME [epoch: 2.6 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2952340422111322		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.2952340422111322 | validation: 0.3049452363270107]
	TIME [epoch: 2.56 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28068524187569505		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.28068524187569505 | validation: 0.26755439310432794]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_799.pth
	Model improved!!!
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3003182768121442		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.3003182768121442 | validation: 0.3292747504986667]
	TIME [epoch: 2.6 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29385480043152234		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.29385480043152234 | validation: 0.2994909746968703]
	TIME [epoch: 2.6 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2795171068397245		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.2795171068397245 | validation: 0.28282538761962733]
	TIME [epoch: 2.59 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28836803974842545		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.28836803974842545 | validation: 0.3583198512912807]
	TIME [epoch: 2.61 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3024172832641428		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.3024172832641428 | validation: 0.30294210005794886]
	TIME [epoch: 2.58 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2871681174897885		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.2871681174897885 | validation: 0.26685520163834087]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_805.pth
	Model improved!!!
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29744877657878593		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.29744877657878593 | validation: 0.42953026881286444]
	TIME [epoch: 2.59 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3190286630940636		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.3190286630940636 | validation: 0.35660498567382426]
	TIME [epoch: 2.59 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2907226681361673		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.2907226681361673 | validation: 0.27737684334792784]
	TIME [epoch: 2.59 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.293777134911922		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.293777134911922 | validation: 0.30133652366024166]
	TIME [epoch: 2.59 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2787847065464043		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.2787847065464043 | validation: 0.28732103482019467]
	TIME [epoch: 2.59 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.284379253675799		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.284379253675799 | validation: 0.28378732261023226]
	TIME [epoch: 2.59 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27941523823144254		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.27941523823144254 | validation: 0.30615336522502556]
	TIME [epoch: 2.58 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28396426943585584		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.28396426943585584 | validation: 0.28749732194905114]
	TIME [epoch: 2.59 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27928677098609617		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.27928677098609617 | validation: 0.3617335237859983]
	TIME [epoch: 2.58 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2971312462383598		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.2971312462383598 | validation: 0.29486485019408737]
	TIME [epoch: 2.59 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2861316016745765		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.2861316016745765 | validation: 0.2950208214314464]
	TIME [epoch: 2.58 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28440859030274007		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.28440859030274007 | validation: 0.320690914169456]
	TIME [epoch: 2.59 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29033178378362784		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.29033178378362784 | validation: 0.27921926007704656]
	TIME [epoch: 2.59 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2866205855727216		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.2866205855727216 | validation: 0.26554716986057986]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_819.pth
	Model improved!!!
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3107569470357592		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.3107569470357592 | validation: 0.3373497184982883]
	TIME [epoch: 2.59 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2927264182359791		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.2927264182359791 | validation: 0.31658026270308126]
	TIME [epoch: 2.6 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2800540715200927		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.2800540715200927 | validation: 0.28883506878144605]
	TIME [epoch: 2.59 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2810644092557724		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.2810644092557724 | validation: 0.33926282843186994]
	TIME [epoch: 2.59 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2940274629523681		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.2940274629523681 | validation: 0.294803252802794]
	TIME [epoch: 2.59 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2818397334651182		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.2818397334651182 | validation: 0.27698799334332813]
	TIME [epoch: 2.58 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2777417493348023		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.2777417493348023 | validation: 0.3053515042000787]
	TIME [epoch: 2.6 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27712542154504394		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.27712542154504394 | validation: 0.27176563093580447]
	TIME [epoch: 2.59 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2826099020706468		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.2826099020706468 | validation: 0.3662180768983794]
	TIME [epoch: 2.59 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29652998796296065		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.29652998796296065 | validation: 0.30298025367287096]
	TIME [epoch: 2.59 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2811123433664748		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.2811123433664748 | validation: 0.2834750236429578]
	TIME [epoch: 2.6 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28529538587366465		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.28529538587366465 | validation: 0.3391367631188696]
	TIME [epoch: 2.58 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28732228168714113		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.28732228168714113 | validation: 0.2892468254905521]
	TIME [epoch: 2.59 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27715425516472547		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.27715425516472547 | validation: 0.294490295669473]
	TIME [epoch: 2.59 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28110692695459905		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.28110692695459905 | validation: 0.2827523357282569]
	TIME [epoch: 2.6 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27804316145965374		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.27804316145965374 | validation: 0.2710790552887547]
	TIME [epoch: 2.59 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2864488695987532		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.2864488695987532 | validation: 0.36799386655733346]
	TIME [epoch: 2.6 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3036854959382829		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.3036854959382829 | validation: 0.2930597539624822]
	TIME [epoch: 2.6 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2880623503046216		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.2880623503046216 | validation: 0.29306431707969205]
	TIME [epoch: 2.6 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2769557843761703		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.2769557843761703 | validation: 0.3317620939091732]
	TIME [epoch: 2.59 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29164317131919526		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.29164317131919526 | validation: 0.2729625082379013]
	TIME [epoch: 2.61 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2836169515148144		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.2836169515148144 | validation: 0.30939380773284847]
	TIME [epoch: 2.61 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27798752904027085		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.27798752904027085 | validation: 0.2864457423909974]
	TIME [epoch: 2.61 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2755971906467731		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.2755971906467731 | validation: 0.284824463067296]
	TIME [epoch: 2.6 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27256978608570215		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.27256978608570215 | validation: 0.32067542250748743]
	TIME [epoch: 2.6 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2840189401069639		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.2840189401069639 | validation: 0.2683647599550322]
	TIME [epoch: 2.6 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2854245616439139		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.2854245616439139 | validation: 0.29978958175913806]
	TIME [epoch: 2.6 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2817146846131021		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.2817146846131021 | validation: 0.32114434973422123]
	TIME [epoch: 2.6 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.282153086569891		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.282153086569891 | validation: 0.2754464388293618]
	TIME [epoch: 2.6 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2810781174322143		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.2810781174322143 | validation: 0.29852418543646697]
	TIME [epoch: 2.61 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2728543288504906		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.2728543288504906 | validation: 0.2867733846255776]
	TIME [epoch: 2.59 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2827602943503664		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.2827602943503664 | validation: 0.2717529424366165]
	TIME [epoch: 2.6 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28810947028234074		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.28810947028234074 | validation: 0.3454793143630877]
	TIME [epoch: 2.59 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2914050740026608		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.2914050740026608 | validation: 0.2729126580058336]
	TIME [epoch: 2.59 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2827525138920025		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.2827525138920025 | validation: 0.3320249390727517]
	TIME [epoch: 2.59 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2814803397796445		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.2814803397796445 | validation: 0.2972519102755808]
	TIME [epoch: 2.6 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2786171103535297		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.2786171103535297 | validation: 0.2675120406491485]
	TIME [epoch: 2.6 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2784341941902402		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.2784341941902402 | validation: 0.287593940895866]
	TIME [epoch: 2.59 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2762022847031455		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.2762022847031455 | validation: 0.32017100375109736]
	TIME [epoch: 2.6 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2745393462863812		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.2745393462863812 | validation: 0.2838025535894499]
	TIME [epoch: 2.59 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2740208154717972		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.2740208154717972 | validation: 0.3141058937126827]
	TIME [epoch: 2.6 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27399403570025327		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.27399403570025327 | validation: 0.27078464454806794]
	TIME [epoch: 2.59 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27685136996154086		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.27685136996154086 | validation: 0.295634108819535]
	TIME [epoch: 2.6 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2755225467725347		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.2755225467725347 | validation: 0.29048953010151485]
	TIME [epoch: 2.59 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27171543673130566		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.27171543673130566 | validation: 0.37323731814248506]
	TIME [epoch: 2.6 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31519454691845106		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.31519454691845106 | validation: 0.3102058827145818]
	TIME [epoch: 2.59 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2888843387995335		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.2888843387995335 | validation: 0.2747358173568145]
	TIME [epoch: 2.6 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28186784637762485		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.28186784637762485 | validation: 0.29045050335296074]
	TIME [epoch: 2.58 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2749741607041488		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.2749741607041488 | validation: 0.3025085999831897]
	TIME [epoch: 2.59 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28008885617526497		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.28008885617526497 | validation: 0.2626224210290353]
	TIME [epoch: 2.58 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_869.pth
	Model improved!!!
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28874961598502685		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.28874961598502685 | validation: 0.30028111250056044]
	TIME [epoch: 2.58 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2761314562619665		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.2761314562619665 | validation: 0.27130977247213356]
	TIME [epoch: 2.61 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2747435145501081		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.2747435145501081 | validation: 0.2895664546147142]
	TIME [epoch: 2.61 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2727735062976028		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.2727735062976028 | validation: 0.2895879889735885]
	TIME [epoch: 2.59 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2771383090281272		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.2771383090281272 | validation: 0.2843470239813429]
	TIME [epoch: 2.6 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27450811151730514		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.27450811151730514 | validation: 0.30348687418915987]
	TIME [epoch: 2.59 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2787228781050958		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.2787228781050958 | validation: 0.27321372369428537]
	TIME [epoch: 2.6 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2831435198733088		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.2831435198733088 | validation: 0.3409275988612246]
	TIME [epoch: 2.59 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28801161351553733		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.28801161351553733 | validation: 0.2928961247230744]
	TIME [epoch: 2.61 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27376217937084535		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.27376217937084535 | validation: 0.26464834236463114]
	TIME [epoch: 2.59 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2886356009993083		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.2886356009993083 | validation: 0.2931911133086877]
	TIME [epoch: 2.6 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2722451021917093		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.2722451021917093 | validation: 0.3000136555604698]
	TIME [epoch: 2.59 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2721776134522402		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.2721776134522402 | validation: 0.28499329793689016]
	TIME [epoch: 2.6 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2732852390290293		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.2732852390290293 | validation: 0.29013407171257455]
	TIME [epoch: 2.59 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2743522237847117		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.2743522237847117 | validation: 0.3263230109505749]
	TIME [epoch: 2.6 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2802971344122551		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.2802971344122551 | validation: 0.27230336503747543]
	TIME [epoch: 2.59 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2810881621025649		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.2810881621025649 | validation: 0.2977605370438187]
	TIME [epoch: 2.6 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27198193658791825		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.27198193658791825 | validation: 0.3066547658187155]
	TIME [epoch: 2.6 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2711667672677359		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.2711667672677359 | validation: 0.2841820495158268]
	TIME [epoch: 2.6 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2768040293294867		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.2768040293294867 | validation: 0.29775314078732246]
	TIME [epoch: 2.6 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27582269350704386		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.27582269350704386 | validation: 0.26482970036002224]
	TIME [epoch: 2.6 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2744527352998323		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.2744527352998323 | validation: 0.327348511681646]
	TIME [epoch: 2.59 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2806995639559793		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.2806995639559793 | validation: 0.2761924965687065]
	TIME [epoch: 2.6 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27693997737575926		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.27693997737575926 | validation: 0.2872692684625821]
	TIME [epoch: 2.59 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2703526398399488		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.2703526398399488 | validation: 0.2564725826866984]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_894.pth
	Model improved!!!
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2722137103011981		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.2722137103011981 | validation: 0.3229264030471113]
	TIME [epoch: 2.59 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28121124692421623		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.28121124692421623 | validation: 0.29714137721568007]
	TIME [epoch: 2.6 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27449599621214094		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.27449599621214094 | validation: 0.25992600912926545]
	TIME [epoch: 2.6 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.283430206857687		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.283430206857687 | validation: 0.29428044272883014]
	TIME [epoch: 2.6 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28150210244887525		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.28150210244887525 | validation: 0.30299006206881657]
	TIME [epoch: 2.59 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26898289046657703		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.26898289046657703 | validation: 0.266568180789574]
	TIME [epoch: 2.61 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2780976339620468		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.2780976339620468 | validation: 0.27996003797857094]
	TIME [epoch: 2.59 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2737193584038565		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.2737193584038565 | validation: 0.28144177199111714]
	TIME [epoch: 2.6 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2678125774997913		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.2678125774997913 | validation: 0.2836135831028697]
	TIME [epoch: 2.6 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2721862635872525		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.2721862635872525 | validation: 0.24730091111951175]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_904.pth
	Model improved!!!
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29233317020919336		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.29233317020919336 | validation: 0.3229769069757002]
	TIME [epoch: 2.61 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27928551340249314		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.27928551340249314 | validation: 0.2783135988683078]
	TIME [epoch: 2.6 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2792474962707044		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.2792474962707044 | validation: 0.28113760405496185]
	TIME [epoch: 2.6 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27545427500820274		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.27545427500820274 | validation: 0.3691899815109516]
	TIME [epoch: 2.6 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2986790136920947		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.2986790136920947 | validation: 0.3126064145726503]
	TIME [epoch: 2.6 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2727565598689542		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.2727565598689542 | validation: 0.2578494654509608]
	TIME [epoch: 2.6 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3197139204272267		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.3197139204272267 | validation: 0.28472520502264226]
	TIME [epoch: 2.59 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.270904211501129		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.270904211501129 | validation: 0.3030251431309968]
	TIME [epoch: 2.59 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.277222885585446		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.277222885585446 | validation: 0.286028607062336]
	TIME [epoch: 2.6 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27133085660636114		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.27133085660636114 | validation: 0.26723465786715794]
	TIME [epoch: 2.59 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2706041974493443		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.2706041974493443 | validation: 0.27152104387255127]
	TIME [epoch: 2.6 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2680008578004456		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.2680008578004456 | validation: 0.28409133110007806]
	TIME [epoch: 2.6 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2725914512623388		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.2725914512623388 | validation: 0.31627565262512713]
	TIME [epoch: 2.6 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27348574662916325		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.27348574662916325 | validation: 0.28208417671498076]
	TIME [epoch: 2.61 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2688784916097701		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.2688784916097701 | validation: 0.26750732160333524]
	TIME [epoch: 2.61 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2692577358136682		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.2692577358136682 | validation: 0.32191932503549225]
	TIME [epoch: 2.6 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2798378722231272		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.2798378722231272 | validation: 0.2967138380591488]
	TIME [epoch: 2.6 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27112255202925367		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.27112255202925367 | validation: 0.27073935922991127]
	TIME [epoch: 2.6 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27493686105647364		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.27493686105647364 | validation: 0.2624398003175734]
	TIME [epoch: 2.59 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27110682104617034		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.27110682104617034 | validation: 0.27001718235203837]
	TIME [epoch: 2.6 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26784688480503954		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.26784688480503954 | validation: 0.2945255727250485]
	TIME [epoch: 2.59 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2687654720036046		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.2687654720036046 | validation: 0.27584697996614144]
	TIME [epoch: 2.6 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2714466418141542		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.2714466418141542 | validation: 0.27787465205465844]
	TIME [epoch: 2.6 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.266254367498408		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.266254367498408 | validation: 0.2644824129825493]
	TIME [epoch: 2.6 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26769964061417584		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.26769964061417584 | validation: 0.29182030904845274]
	TIME [epoch: 2.6 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2682982780252272		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.2682982780252272 | validation: 0.27618576811014023]
	TIME [epoch: 2.61 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2652277365815037		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.2652277365815037 | validation: 0.2860758023587381]
	TIME [epoch: 2.59 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26531428087625775		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.26531428087625775 | validation: 0.3017253703451257]
	TIME [epoch: 2.61 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2732086608584025		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.2732086608584025 | validation: 0.26894248487837363]
	TIME [epoch: 2.59 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2696142640751745		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.2696142640751745 | validation: 0.2816677797054015]
	TIME [epoch: 2.6 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26342329357469385		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.26342329357469385 | validation: 0.289339306762293]
	TIME [epoch: 2.6 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2694602850532337		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.2694602850532337 | validation: 0.2725361165404339]
	TIME [epoch: 2.61 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2705460467712523		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.2705460467712523 | validation: 0.2696923940835991]
	TIME [epoch: 2.6 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2678956154004097		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.2678956154004097 | validation: 0.271449646579962]
	TIME [epoch: 2.61 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26560105733109046		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.26560105733109046 | validation: 0.29272449633427844]
	TIME [epoch: 2.6 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26882336687305713		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.26882336687305713 | validation: 0.33904839006352466]
	TIME [epoch: 2.6 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28108871116110384		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.28108871116110384 | validation: 0.27020690051181634]
	TIME [epoch: 2.59 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27120324251481565		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.27120324251481565 | validation: 0.27863089708332456]
	TIME [epoch: 2.6 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2677000039025386		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.2677000039025386 | validation: 0.275615046598027]
	TIME [epoch: 2.59 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2722223006741213		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.2722223006741213 | validation: 0.31034524177475026]
	TIME [epoch: 2.61 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26937967537253366		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.26937967537253366 | validation: 0.2675830649365605]
	TIME [epoch: 2.59 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27295677556772		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.27295677556772 | validation: 0.2597143082040176]
	TIME [epoch: 2.61 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2658584014020121		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.2658584014020121 | validation: 0.3038052864937652]
	TIME [epoch: 2.6 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.266908295800026		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.266908295800026 | validation: 0.25576112794002814]
	TIME [epoch: 2.61 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27269479505925626		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.27269479505925626 | validation: 0.29109654592313305]
	TIME [epoch: 2.6 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2713002423074468		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.2713002423074468 | validation: 0.2813495794665003]
	TIME [epoch: 2.61 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26786284431041524		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.26786284431041524 | validation: 0.25285286925907363]
	TIME [epoch: 2.59 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2784428316163816		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.2784428316163816 | validation: 0.32547384388340783]
	TIME [epoch: 2.61 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27944418066588683		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.27944418066588683 | validation: 0.3052602739429444]
	TIME [epoch: 2.59 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2665327969452143		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.2665327969452143 | validation: 0.26366815890463785]
	TIME [epoch: 2.61 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27714377132861706		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.27714377132861706 | validation: 0.29855385487240826]
	TIME [epoch: 2.59 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26728016608115435		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.26728016608115435 | validation: 0.28807977178896005]
	TIME [epoch: 2.6 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2659438738642779		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.2659438738642779 | validation: 0.26456857857859134]
	TIME [epoch: 2.59 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2659254840037225		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.2659254840037225 | validation: 0.28742241521709094]
	TIME [epoch: 2.6 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26633916693466986		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.26633916693466986 | validation: 0.2821440043229556]
	TIME [epoch: 2.59 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2679759805385825		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.2679759805385825 | validation: 0.28108248386997176]
	TIME [epoch: 2.6 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2669959440460709		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.2669959440460709 | validation: 0.25111133674000036]
	TIME [epoch: 2.59 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26855886772515464		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.26855886772515464 | validation: 0.28920514359617483]
	TIME [epoch: 2.6 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2665800582216106		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.2665800582216106 | validation: 0.3009482610099271]
	TIME [epoch: 2.59 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26603558875499905		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.26603558875499905 | validation: 0.2651423552552602]
	TIME [epoch: 2.6 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26668059336077693		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.26668059336077693 | validation: 0.3453352662887748]
	TIME [epoch: 2.59 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2808614052044375		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.2808614052044375 | validation: 0.27380868585940055]
	TIME [epoch: 2.6 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26743686123951527		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.26743686123951527 | validation: 0.26070950953606187]
	TIME [epoch: 2.6 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2669427545850446		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.2669427545850446 | validation: 0.30175578624457644]
	TIME [epoch: 2.6 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26657974584602856		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.26657974584602856 | validation: 0.2755340116497257]
	TIME [epoch: 2.59 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26440492532381776		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.26440492532381776 | validation: 0.2552286665022879]
	TIME [epoch: 2.6 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.264867892201286		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.264867892201286 | validation: 0.2825468370416351]
	TIME [epoch: 2.59 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2672783114508763		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.2672783114508763 | validation: 0.29022620688014894]
	TIME [epoch: 2.6 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26362394360853375		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.26362394360853375 | validation: 0.2530678974205129]
	TIME [epoch: 2.59 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2693189425845173		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.2693189425845173 | validation: 0.26847184995431667]
	TIME [epoch: 2.6 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2650680248665346		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.2650680248665346 | validation: 0.29300827261795653]
	TIME [epoch: 2.59 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2617525155776622		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.2617525155776622 | validation: 0.2847818764025619]
	TIME [epoch: 2.6 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26935355898850816		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.26935355898850816 | validation: 0.2779985076428377]
	TIME [epoch: 2.59 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2690653805585572		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.2690653805585572 | validation: 0.2690086736518943]
	TIME [epoch: 2.61 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2647529008621957		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.2647529008621957 | validation: 0.30084179495149604]
	TIME [epoch: 2.59 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26866510067569455		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.26866510067569455 | validation: 0.2488916867189801]
	TIME [epoch: 2.6 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28784336331230953		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.28784336331230953 | validation: 0.3064929446391272]
	TIME [epoch: 2.59 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26741969107809704		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.26741969107809704 | validation: 0.3037683777124494]
	TIME [epoch: 2.6 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26523991361335325		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.26523991361335325 | validation: 0.2843288376510397]
	TIME [epoch: 2.6 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2655572149399766		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.2655572149399766 | validation: 0.24742872292163512]
	TIME [epoch: 2.6 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2696614896994587		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.2696614896994587 | validation: 0.29371546866555953]
	TIME [epoch: 2.6 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2670491636273836		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.2670491636273836 | validation: 0.27683592962089226]
	TIME [epoch: 2.6 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26807235868410323		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.26807235868410323 | validation: 0.27114051100845143]
	TIME [epoch: 2.59 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2638645077139639		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.2638645077139639 | validation: 0.2715009203882399]
	TIME [epoch: 2.6 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26229575872503097		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.26229575872503097 | validation: 0.27861984071721413]
	TIME [epoch: 2.59 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26519190757572014		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.26519190757572014 | validation: 0.27060389193604373]
	TIME [epoch: 2.6 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26856448955882856		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.26856448955882856 | validation: 0.2603864490483523]
	TIME [epoch: 2.59 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2663391234306632		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.2663391234306632 | validation: 0.28022832664895647]
	TIME [epoch: 2.6 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2669320981167309		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.2669320981167309 | validation: 0.2737749143283496]
	TIME [epoch: 2.59 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2639195427710665		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.2639195427710665 | validation: 0.27190971061961283]
	TIME [epoch: 2.6 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26858053879979477		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.26858053879979477 | validation: 0.2725689181250456]
	TIME [epoch: 2.6 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2672326851072594		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.2672326851072594 | validation: 0.2549698896570091]
	TIME [epoch: 2.6 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2620421165586652		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.2620421165586652 | validation: 0.28059069562775485]
	TIME [epoch: 2.6 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2604449598554305		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.2604449598554305 | validation: 0.2680572465080119]
	TIME [epoch: 2.6 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26342895267738947		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.26342895267738947 | validation: 0.26587594046824714]
	TIME [epoch: 2.6 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26256528602734447		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.26256528602734447 | validation: 0.29400384466223156]
	TIME [epoch: 2.6 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.266686664835072		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.266686664835072 | validation: 0.2668013041231444]
	TIME [epoch: 163 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26306619682256005		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.26306619682256005 | validation: 0.2667839813505079]
	TIME [epoch: 5.64 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26094759071665413		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.26094759071665413 | validation: 0.26411983469689615]
	TIME [epoch: 5.6 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2611973776016693		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.2611973776016693 | validation: 0.29008570451571114]
	TIME [epoch: 5.62 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26008697337998693		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.26008697337998693 | validation: 0.26589341318606335]
	TIME [epoch: 5.58 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_4_v_mmd1_1005.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 2565.118 seconds.
