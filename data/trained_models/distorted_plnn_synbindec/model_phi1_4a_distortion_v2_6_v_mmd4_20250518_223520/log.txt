Args:
Namespace(name='model_phi1_4a_distortion_v2_6_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_6/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_6/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.025136888027191162, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3261024407

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.552788517450469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.552788517450469 | validation: 6.867185274948288]
	TIME [epoch: 162 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.26576267379433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.26576267379433 | validation: 7.082143509832668]
	TIME [epoch: 0.768 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.268774019363629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.268774019363629 | validation: 6.793596042169369]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.050465341650868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.050465341650868 | validation: 6.755270227038408]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.697761215390411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.697761215390411 | validation: 6.656439083575563]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.508618150396221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.508618150396221 | validation: 6.571370835632364]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.318699062374687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.318699062374687 | validation: 6.407368403589418]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.481335389004195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.481335389004195 | validation: 6.270574445362325]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.2259630000364385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.2259630000364385 | validation: 6.372024988521777]
	TIME [epoch: 0.694 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.066669041382589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.066669041382589 | validation: 6.129968761968752]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.935698285231184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.935698285231184 | validation: 6.137907808737399]
	TIME [epoch: 0.695 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.79987978639525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.79987978639525 | validation: 6.150698553550608]
	TIME [epoch: 0.694 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.743042952548101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.743042952548101 | validation: 5.98259326069301]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.7239671709752145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7239671709752145 | validation: 5.99719301502143]
	TIME [epoch: 0.696 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.5529672170374855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5529672170374855 | validation: 5.866049024379557]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.453130448502998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.453130448502998 | validation: 5.919668563241241]
	TIME [epoch: 0.693 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.425267071024264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.425267071024264 | validation: 5.890153896549279]
	TIME [epoch: 0.693 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.575595185313598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.575595185313598 | validation: 5.7883804562545365]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.272126763416499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.272126763416499 | validation: 5.67462193083862]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.122062431516371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.122062431516371 | validation: 5.6302598801364505]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.04581734000436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.04581734000436 | validation: 5.668237961020448]
	TIME [epoch: 0.693 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.071857977356225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.071857977356225 | validation: 5.774348999164919]
	TIME [epoch: 0.692 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.3575119727269955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3575119727269955 | validation: 5.528960211063268]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.868972108853346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.868972108853346 | validation: 5.652881271807272]
	TIME [epoch: 0.692 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.112286953962197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.112286953962197 | validation: 5.584787765680509]
	TIME [epoch: 0.689 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.952218828548784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.952218828548784 | validation: 5.478075468887008]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.859329148291565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.859329148291565 | validation: 5.402807853276627]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.709723205375612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.709723205375612 | validation: 5.445552768402137]
	TIME [epoch: 0.695 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.716097811287399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.716097811287399 | validation: 5.365567988749933]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.69371683968541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.69371683968541 | validation: 5.348448294254419]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6168052423868473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6168052423868473 | validation: 5.361172532625298]
	TIME [epoch: 0.693 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.620827147353084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.620827147353084 | validation: 5.32233225633545]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5936605974475144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5936605974475144 | validation: 5.304993660821973]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5581388188863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5581388188863 | validation: 5.288730906358283]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5426286833306793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5426286833306793 | validation: 5.273082756766269]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5221806129259257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5221806129259257 | validation: 5.26543728177889]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5027160364157033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5027160364157033 | validation: 5.2340798977695195]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.491456966322579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.491456966322579 | validation: 5.2277343746621945]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4759406693909485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4759406693909485 | validation: 5.209845316124249]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4704060669741126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4704060669741126 | validation: 5.1930958757704575]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4581576115132426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4581576115132426 | validation: 5.183827112365356]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4469454607969845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4469454607969845 | validation: 5.168401399570101]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.43952487779971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.43952487779971 | validation: 5.133783576355562]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4336519719772163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4336519719772163 | validation: 5.129194157103912]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4298705134927823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4298705134927823 | validation: 5.122248580162851]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.445130752036018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.445130752036018 | validation: 5.085628881485947]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5113787114983923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5113787114983923 | validation: 5.0471336176084005]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.366798203709275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.366798203709275 | validation: 5.063266206326592]
	TIME [epoch: 0.694 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.397110184288761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.397110184288761 | validation: 5.002234182016493]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3749882152312543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3749882152312543 | validation: 4.969037687061674]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3252789492422394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3252789492422394 | validation: 4.963938846675802]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3158387318961493		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.3158387318961493 | validation: 4.907141851859748]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2907454471567736		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 3.2907454471567736 | validation: 4.872928639681029]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.269941918062945		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 3.269941918062945 | validation: 4.864860570642328]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2543394842293787		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 3.2543394842293787 | validation: 4.803790439064054]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2299847944116165		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 3.2299847944116165 | validation: 4.747072716948753]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2025358441672775		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 3.2025358441672775 | validation: 4.648555995246343]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.160287808737197		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 3.160287808737197 | validation: 4.427480076247796]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0502368713538783		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 3.0502368713538783 | validation: 3.9728419682495684]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7191212142361962		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 2.7191212142361962 | validation: 3.8167407897686147]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6656016878179494		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 2.6656016878179494 | validation: 3.9165896841339602]
	TIME [epoch: 0.695 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8843449613602026		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 2.8843449613602026 | validation: 3.640629087451858]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4795530134563983		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 2.4795530134563983 | validation: 3.5783527821116183]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4622939699621056		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 2.4622939699621056 | validation: 3.4527257893520615]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3302180494125184		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 2.3302180494125184 | validation: 3.361692858799927]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.319154871238026		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 2.319154871238026 | validation: 3.1518387403417023]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2574678298114437		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 2.2574678298114437 | validation: 3.0497008119529045]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.228422275070026		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 2.228422275070026 | validation: 2.9906585828527863]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.202261172165117		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 2.202261172165117 | validation: 2.870822468941387]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1806505507113276		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 2.1806505507113276 | validation: 2.77967410143194]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.16193592859191		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 2.16193592859191 | validation: 2.719411318565066]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1484953100558806		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 2.1484953100558806 | validation: 2.581077940301631]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1247057861576404		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 2.1247057861576404 | validation: 2.5725307296279016]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1124348629937724		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 2.1124348629937724 | validation: 2.4105416436806837]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.170962169962916		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 2.170962169962916 | validation: 2.8759031932050574]
	TIME [epoch: 0.691 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2642620224514833		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 2.2642620224514833 | validation: 2.3545332757316237]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0884238580044627		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 2.0884238580044627 | validation: 2.2361085965110252]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.049793208604652		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 2.049793208604652 | validation: 2.354832090470145]
	TIME [epoch: 0.692 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.054935748441847		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 2.054935748441847 | validation: 2.1439372378908077]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0193700121404885		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 2.0193700121404885 | validation: 2.1893335373333835]
	TIME [epoch: 0.695 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.985015884788495		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.985015884788495 | validation: 1.8663076193606434]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.986103332271843		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.986103332271843 | validation: 2.970511159294613]
	TIME [epoch: 0.696 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.343829461612887		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 2.343829461612887 | validation: 2.056678665453037]
	TIME [epoch: 0.695 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9992583015475884		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.9992583015475884 | validation: 1.8152797008057118]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9369458912239037		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.9369458912239037 | validation: 2.2198009632895292]
	TIME [epoch: 0.697 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9995868143152455		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.9995868143152455 | validation: 1.8754962634658086]
	TIME [epoch: 0.694 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9053397717872695		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.9053397717872695 | validation: 1.9875612136855558]
	TIME [epoch: 0.694 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8649806772878128		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.8649806772878128 | validation: 1.6471484806164636]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8185264762349442		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.8185264762349442 | validation: 2.1756757376854714]
	TIME [epoch: 0.697 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9274505377633577		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.9274505377633577 | validation: 2.2510028341486286]
	TIME [epoch: 0.694 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.52628581495771		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 2.52628581495771 | validation: 2.355592339761976]
	TIME [epoch: 0.694 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0060234999394346		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 2.0060234999394346 | validation: 2.12816694597505]
	TIME [epoch: 0.696 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8913499178914572		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.8913499178914572 | validation: 1.7267927242210577]
	TIME [epoch: 0.693 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8050369877743961		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.8050369877743961 | validation: 1.5903758708111764]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7261533623309913		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.7261533623309913 | validation: 1.6511572686642688]
	TIME [epoch: 0.695 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7128351507094164		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.7128351507094164 | validation: 1.4433054541732855]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6906483069227778		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.6906483069227778 | validation: 1.9326192558082476]
	TIME [epoch: 0.696 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7730594266507154		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.7730594266507154 | validation: 1.4600976769536598]
	TIME [epoch: 0.694 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8410132714209904		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.8410132714209904 | validation: 2.2384794514073985]
	TIME [epoch: 0.694 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8965702042425696		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.8965702042425696 | validation: 1.4255373627197574]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.564103667965075		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.564103667965075 | validation: 1.30527498999514]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6205512682625665		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.6205512682625665 | validation: 2.1080001297053834]
	TIME [epoch: 0.695 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.851243998055983		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.851243998055983 | validation: 1.1857674210702727]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5497788309565241		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.5497788309565241 | validation: 1.215037405353138]
	TIME [epoch: 0.695 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4661117239675288		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.4661117239675288 | validation: 1.3442667822409675]
	TIME [epoch: 0.693 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.460844172423575		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.460844172423575 | validation: 1.2022514370714132]
	TIME [epoch: 0.694 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.568371047345318		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.568371047345318 | validation: 2.3986853741775276]
	TIME [epoch: 0.694 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9329707876152293		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.9329707876152293 | validation: 1.3991454493101974]
	TIME [epoch: 0.694 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4834690430592559		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.4834690430592559 | validation: 1.495703513681768]
	TIME [epoch: 0.693 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8139574859282075		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.8139574859282075 | validation: 1.959437441155793]
	TIME [epoch: 0.694 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7418913775471658		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.7418913775471658 | validation: 1.2323421021904208]
	TIME [epoch: 0.693 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4229143924094563		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.4229143924094563 | validation: 1.1700343972740346]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5702128484985343		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.5702128484985343 | validation: 1.6006079141652343]
	TIME [epoch: 0.695 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5307513402215043		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.5307513402215043 | validation: 1.1303143196322107]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3243182654456547		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.3243182654456547 | validation: 1.0256587037789717]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.393796600282393		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.393796600282393 | validation: 1.5394827198360992]
	TIME [epoch: 0.697 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4731712514504958		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.4731712514504958 | validation: 0.9870355495661958]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3450823586780403		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.3450823586780403 | validation: 1.180293746056605]
	TIME [epoch: 0.698 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2995091946135437		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.2995091946135437 | validation: 0.9841729638817137]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3013412324755722		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.3013412324755722 | validation: 1.4362268185008946]
	TIME [epoch: 0.696 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4026954785599872		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.4026954785599872 | validation: 1.0874885555199718]
	TIME [epoch: 0.694 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4582215269407168		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.4582215269407168 | validation: 1.8115606999189326]
	TIME [epoch: 0.695 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5839301369321594		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.5839301369321594 | validation: 0.9483654539276348]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2766982743413278		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.2766982743413278 | validation: 0.994798413118938]
	TIME [epoch: 0.695 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2973643946123954		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.2973643946123954 | validation: 1.6165350823836897]
	TIME [epoch: 0.694 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.456446359409832		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.456446359409832 | validation: 0.9380157862210492]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3374989816522191		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.3374989816522191 | validation: 1.1650319571654797]
	TIME [epoch: 0.695 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2527908468854214		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.2527908468854214 | validation: 0.9293468657651573]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2564113952805118		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.2564113952805118 | validation: 1.4646127815079604]
	TIME [epoch: 0.696 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3771539860088593		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.3771539860088593 | validation: 0.9775589181960709]
	TIME [epoch: 0.695 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3174768296627553		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.3174768296627553 | validation: 1.4313086107036028]
	TIME [epoch: 0.696 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3653272925348732		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.3653272925348732 | validation: 0.9011882837094861]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2662309740712243		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.2662309740712243 | validation: 1.3198619519625763]
	TIME [epoch: 0.695 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2975114433214703		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.2975114433214703 | validation: 0.9188165415991123]
	TIME [epoch: 0.695 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2424177212413343		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.2424177212413343 | validation: 1.3232862100482172]
	TIME [epoch: 0.698 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2688874903087612		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.2688874903087612 | validation: 0.9038430860465397]
	TIME [epoch: 0.694 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2382598597327539		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.2382598597327539 | validation: 1.3725390929259338]
	TIME [epoch: 0.699 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3064270371550284		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.3064270371550284 | validation: 0.8780787840085268]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.224790650512968		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.224790650512968 | validation: 1.2455173219129758]
	TIME [epoch: 0.698 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2212234199526628		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.2212234199526628 | validation: 0.8592968417919584]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2141426903753354		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.2141426903753354 | validation: 1.309202958991499]
	TIME [epoch: 0.696 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2435511703348032		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.2435511703348032 | validation: 0.9114844757573395]
	TIME [epoch: 0.694 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.207654970514605		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.207654970514605 | validation: 1.3632076064409802]
	TIME [epoch: 0.694 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.283328520398768		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.283328520398768 | validation: 0.9120095169849162]
	TIME [epoch: 0.692 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1849932927965943		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.1849932927965943 | validation: 1.126331775360241]
	TIME [epoch: 0.695 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1492263759216106		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.1492263759216106 | validation: 0.8625365123784943]
	TIME [epoch: 0.695 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2045577629652235		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.2045577629652235 | validation: 1.5124862493511182]
	TIME [epoch: 0.694 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3368940836279393		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.3368940836279393 | validation: 0.8176260659483785]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1693338617377225		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.1693338617377225 | validation: 1.0108933422937298]
	TIME [epoch: 0.696 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0992664760393536		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.0992664760393536 | validation: 0.7979872132357291]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_150.pth
	Model improved!!!
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1135652080311251		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.1135652080311251 | validation: 1.3293914806198834]
	TIME [epoch: 0.693 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2305472188685842		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.2305472188685842 | validation: 0.9447571649612305]
	TIME [epoch: 0.693 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.234880262314299		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.234880262314299 | validation: 1.4168483474638311]
	TIME [epoch: 0.691 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.276779016184152		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.276779016184152 | validation: 0.7911066454041675]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.092242708248118		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.092242708248118 | validation: 1.066455917304172]
	TIME [epoch: 0.693 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.096692930182459		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.096692930182459 | validation: 0.816644866718638]
	TIME [epoch: 0.691 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1487606729280426		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.1487606729280426 | validation: 1.4203405788159926]
	TIME [epoch: 0.691 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2526255865249918		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.2526255865249918 | validation: 0.8216443990345639]
	TIME [epoch: 0.693 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.111694428939986		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.111694428939986 | validation: 1.17700797681976]
	TIME [epoch: 0.691 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1264689988912446		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.1264689988912446 | validation: 0.8636609170716888]
	TIME [epoch: 0.691 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1342561766513288		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.1342561766513288 | validation: 1.34676287118703]
	TIME [epoch: 0.69 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1977077351705385		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.1977077351705385 | validation: 0.8364777952836744]
	TIME [epoch: 0.693 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0893257335099065		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.0893257335099065 | validation: 1.1059261868678523]
	TIME [epoch: 0.692 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0901713333854637		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.0901713333854637 | validation: 0.8304682244190847]
	TIME [epoch: 0.692 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1163493194752696		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.1163493194752696 | validation: 1.3884597180925593]
	TIME [epoch: 0.69 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2165505305431785		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.2165505305431785 | validation: 0.7789697336841958]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.084014156403585		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.084014156403585 | validation: 1.1555668466853282]
	TIME [epoch: 0.692 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1296956297043026		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.1296956297043026 | validation: 0.7729926420052217]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1764428347123863		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.1764428347123863 | validation: 1.2166746720944133]
	TIME [epoch: 0.693 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.126078454698011		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.126078454698011 | validation: 0.8564259983889418]
	TIME [epoch: 0.692 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1035843072055378		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.1035843072055378 | validation: 1.171761855682591]
	TIME [epoch: 0.69 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1063958508431324		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.1063958508431324 | validation: 0.8098117091002465]
	TIME [epoch: 0.692 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0800523858663125		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.0800523858663125 | validation: 1.2240548012169876]
	TIME [epoch: 0.691 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.12791794343316		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.12791794343316 | validation: 0.8063449063087221]
	TIME [epoch: 0.696 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0876099788856126		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.0876099788856126 | validation: 1.1688459584557647]
	TIME [epoch: 0.691 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0968327526514423		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.0968327526514423 | validation: 0.8110813219918195]
	TIME [epoch: 0.692 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0548044893977804		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.0548044893977804 | validation: 1.1985471234867722]
	TIME [epoch: 0.693 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0927459788118379		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.0927459788118379 | validation: 0.7996929868062882]
	TIME [epoch: 0.691 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0754894260450203		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.0754894260450203 | validation: 1.2308930013545702]
	TIME [epoch: 0.691 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.112788352563068		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.112788352563068 | validation: 0.7925811065803898]
	TIME [epoch: 0.692 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0502374166421873		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.0502374166421873 | validation: 1.1283841785058268]
	TIME [epoch: 0.693 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0508196605096098		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.0508196605096098 | validation: 0.7876371664724799]
	TIME [epoch: 0.693 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0588179941322835		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.0588179941322835 | validation: 1.2734709551267513]
	TIME [epoch: 0.69 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1218776683208487		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.1218776683208487 | validation: 0.8086158043077865]
	TIME [epoch: 0.694 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0221195050625989		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.0221195050625989 | validation: 1.105400531525163]
	TIME [epoch: 0.693 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0354195999949392		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.0354195999949392 | validation: 0.7685405137783264]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0340621765092561		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.0340621765092561 | validation: 1.247348550586098]
	TIME [epoch: 0.692 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1084767055957223		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.1084767055957223 | validation: 0.7892624570240835]
	TIME [epoch: 0.694 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.033179550262314		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.033179550262314 | validation: 1.0982608561208436]
	TIME [epoch: 0.691 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0344644070187596		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.0344644070187596 | validation: 0.7553421251907614]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_190.pth
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.056023427480114		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.056023427480114 | validation: 1.081914606318399]
	TIME [epoch: 0.693 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0122764649589535		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.0122764649589535 | validation: 0.7962951519556056]
	TIME [epoch: 0.691 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0113459597805678		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.0113459597805678 | validation: 1.3039057598224622]
	TIME [epoch: 0.69 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1276627899579048		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.1276627899579048 | validation: 0.7871493463806519]
	TIME [epoch: 0.692 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.009653111776753		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.009653111776753 | validation: 1.1114968039663704]
	TIME [epoch: 0.69 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0180910048481349		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.0180910048481349 | validation: 0.762283256940284]
	TIME [epoch: 0.692 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0242098335067733		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.0242098335067733 | validation: 1.2083942572140114]
	TIME [epoch: 0.69 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0647234880550747		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.0647234880550747 | validation: 0.7959557954311162]
	TIME [epoch: 0.692 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.022156863074639		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.022156863074639 | validation: 1.146896380177967]
	TIME [epoch: 0.692 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0300961908884252		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.0300961908884252 | validation: 0.7872693504443176]
	TIME [epoch: 0.691 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.019837652804731		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.019837652804731 | validation: 1.191143898558156]
	TIME [epoch: 171 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0379282806757963		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.0379282806757963 | validation: 0.7787037607965707]
	TIME [epoch: 1.37 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9862991230922745		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.9862991230922745 | validation: 1.116449124009369]
	TIME [epoch: 1.36 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0011173158098177		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.0011173158098177 | validation: 0.7493171584991499]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_204.pth
	Model improved!!!
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0062392514405563		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.0062392514405563 | validation: 1.2148045740970792]
	TIME [epoch: 1.36 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0550668699113128		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.0550668699113128 | validation: 0.7453735070573277]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_206.pth
	Model improved!!!
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9928065587921581		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.9928065587921581 | validation: 1.0762320684807796]
	TIME [epoch: 1.36 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9844019717795969		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.9844019717795969 | validation: 0.7740088404645852]
	TIME [epoch: 1.36 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9886866692410378		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.9886866692410378 | validation: 1.1947097047838444]
	TIME [epoch: 1.36 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0459705351566952		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.0459705351566952 | validation: 0.8241705332473022]
	TIME [epoch: 1.36 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0090654135742485		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.0090654135742485 | validation: 1.0609964753215935]
	TIME [epoch: 1.36 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9915029659578258		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.9915029659578258 | validation: 0.7591760034735819]
	TIME [epoch: 1.36 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.96034809125854		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.96034809125854 | validation: 1.1095099336195693]
	TIME [epoch: 1.36 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9970499536514269		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.9970499536514269 | validation: 0.7515073049996438]
	TIME [epoch: 1.36 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9968004109684969		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.9968004109684969 | validation: 1.1528957060932967]
	TIME [epoch: 1.35 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0237717484421964		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.0237717484421964 | validation: 0.7254751903904592]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.975337300637562		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.975337300637562 | validation: 1.0590439930496593]
	TIME [epoch: 1.35 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9588076270608079		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.9588076270608079 | validation: 0.7627843240998194]
	TIME [epoch: 1.36 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9727549822027162		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.9727549822027162 | validation: 1.1626211263748953]
	TIME [epoch: 1.36 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0241621754385548		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.0241621754385548 | validation: 0.7774616632669628]
	TIME [epoch: 1.35 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9771653908978999		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.9771653908978999 | validation: 1.05851674167793]
	TIME [epoch: 1.36 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9557745307768002		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.9557745307768002 | validation: 0.7458878662562572]
	TIME [epoch: 1.36 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9480528176744386		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.9480528176744386 | validation: 1.1270852269563585]
	TIME [epoch: 1.35 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9874443252940215		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.9874443252940215 | validation: 0.7627229253858101]
	TIME [epoch: 1.35 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9863832550106427		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.9863832550106427 | validation: 1.139464872395172]
	TIME [epoch: 1.35 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9959417680850158		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.9959417680850158 | validation: 0.7462512776724157]
	TIME [epoch: 1.36 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9322619861564454		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.9322619861564454 | validation: 1.0731443159329463]
	TIME [epoch: 1.35 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9500140932264918		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.9500140932264918 | validation: 0.7493464954729769]
	TIME [epoch: 1.36 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9693752365740812		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.9693752365740812 | validation: 1.1595781800635958]
	TIME [epoch: 1.35 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9951080497862961		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.9951080497862961 | validation: 0.7826972032847292]
	TIME [epoch: 1.36 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9556120817781624		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.9556120817781624 | validation: 1.0823977714565067]
	TIME [epoch: 1.35 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9619367345125761		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.9619367345125761 | validation: 0.7822858328684357]
	TIME [epoch: 1.36 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9430168213446841		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.9430168213446841 | validation: 1.072236834325623]
	TIME [epoch: 1.35 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9483993707731881		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.9483993707731881 | validation: 0.719611687067221]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_234.pth
	Model improved!!!
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9655393930027273		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.9655393930027273 | validation: 1.1539509302584292]
	TIME [epoch: 1.36 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9970844864536047		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.9970844864536047 | validation: 0.7280422588858355]
	TIME [epoch: 1.36 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9347815555822586		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.9347815555822586 | validation: 1.0461282158297842]
	TIME [epoch: 1.36 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9322226732636517		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.9322226732636517 | validation: 0.7593897764398776]
	TIME [epoch: 1.36 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9235562189165896		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.9235562189165896 | validation: 1.1400361340233207]
	TIME [epoch: 1.36 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9763884510129682		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.9763884510129682 | validation: 0.7651373016052436]
	TIME [epoch: 1.36 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9392162514790718		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.9392162514790718 | validation: 1.0819580111124179]
	TIME [epoch: 1.36 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9569733888002421		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.9569733888002421 | validation: 0.757659065657194]
	TIME [epoch: 1.36 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9293424863590434		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.9293424863590434 | validation: 1.069076621587209]
	TIME [epoch: 1.35 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9461054924041028		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.9461054924041028 | validation: 0.745286088424276]
	TIME [epoch: 1.36 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9313298220315056		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.9313298220315056 | validation: 1.0742866708537417]
	TIME [epoch: 1.36 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9314864121014221		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.9314864121014221 | validation: 0.7245880989239035]
	TIME [epoch: 1.36 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9472448936728008		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.9472448936728008 | validation: 1.12466669734562]
	TIME [epoch: 1.35 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9619894107701862		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.9619894107701862 | validation: 0.7262316718648938]
	TIME [epoch: 1.35 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9191963010713982		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.9191963010713982 | validation: 1.0621749916193417]
	TIME [epoch: 1.35 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9278357426277526		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.9278357426277526 | validation: 0.7571779912536579]
	TIME [epoch: 1.35 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9350531972159445		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.9350531972159445 | validation: 1.1217393486614806]
	TIME [epoch: 1.35 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.962914376928577		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.962914376928577 | validation: 0.8090627517290465]
	TIME [epoch: 1.36 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.928957415462148		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.928957415462148 | validation: 0.9894955570911711]
	TIME [epoch: 1.35 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8998926998707331		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.8998926998707331 | validation: 0.71684705477251]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_254.pth
	Model improved!!!
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.896759265759228		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.896759265759228 | validation: 1.1243075972449026]
	TIME [epoch: 1.36 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9521479637885122		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.9521479637885122 | validation: 0.7272766329611078]
	TIME [epoch: 1.36 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9613858038272209		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.9613858038272209 | validation: 1.0695851064330921]
	TIME [epoch: 1.36 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9273331841911469		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.9273331841911469 | validation: 0.7489350568960198]
	TIME [epoch: 1.36 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8945143740003855		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.8945143740003855 | validation: 0.9801594779120615]
	TIME [epoch: 1.36 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8843749228861715		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.8843749228861715 | validation: 0.7665337048275256]
	TIME [epoch: 1.36 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9056904992717638		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.9056904992717638 | validation: 1.125996521881651]
	TIME [epoch: 1.36 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9616646478722595		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.9616646478722595 | validation: 0.7494637378351244]
	TIME [epoch: 1.36 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9304675107190338		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.9304675107190338 | validation: 1.0843017235021077]
	TIME [epoch: 1.36 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9243017867512429		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.9243017867512429 | validation: 0.7209705945934975]
	TIME [epoch: 1.36 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9028456925168544		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.9028456925168544 | validation: 1.071732521554879]
	TIME [epoch: 1.36 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9177020496350593		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.9177020496350593 | validation: 0.7380017885499882]
	TIME [epoch: 1.36 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9046295485259603		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.9046295485259603 | validation: 1.0225508908747185]
	TIME [epoch: 1.36 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.901621413488296		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.901621413488296 | validation: 0.7472147421636919]
	TIME [epoch: 1.36 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9139026060794391		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.9139026060794391 | validation: 1.0209721502176656]
	TIME [epoch: 1.36 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9042232504307717		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.9042232504307717 | validation: 0.7762440137192912]
	TIME [epoch: 1.36 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8931754444778165		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.8931754444778165 | validation: 1.0145267756662844]
	TIME [epoch: 1.36 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.897853934649458		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.897853934649458 | validation: 0.7605507563376421]
	TIME [epoch: 1.36 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8828981392465898		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.8828981392465898 | validation: 1.0240873503242287]
	TIME [epoch: 1.36 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8819685842584144		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.8819685842584144 | validation: 0.7321932804667002]
	TIME [epoch: 1.36 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8888895688480141		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.8888895688480141 | validation: 1.0704266555534196]
	TIME [epoch: 1.36 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9099596455387883		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.9099596455387883 | validation: 0.7069665932425543]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_276.pth
	Model improved!!!
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9258275309569106		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.9258275309569106 | validation: 1.1282362096493037]
	TIME [epoch: 1.35 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9377380155658448		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.9377380155658448 | validation: 0.7421734555513507]
	TIME [epoch: 1.36 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8865387509416499		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.8865387509416499 | validation: 0.9864070544987795]
	TIME [epoch: 1.35 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8678147659491431		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.8678147659491431 | validation: 0.7651916389796689]
	TIME [epoch: 1.35 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8699633384992675		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.8699633384992675 | validation: 1.055183812425595]
	TIME [epoch: 1.35 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9265119571648562		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.9265119571648562 | validation: 0.8189205177056071]
	TIME [epoch: 1.36 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9086970269754386		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.9086970269754386 | validation: 1.0033806524603042]
	TIME [epoch: 1.35 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8805977371784388		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.8805977371784388 | validation: 0.7497466327511065]
	TIME [epoch: 1.36 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8392895300199195		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.8392895300199195 | validation: 1.0198546536819364]
	TIME [epoch: 1.36 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8732417304613048		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.8732417304613048 | validation: 0.7197717889331643]
	TIME [epoch: 1.36 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8917447549170029		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.8917447549170029 | validation: 1.1392204034970888]
	TIME [epoch: 1.36 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9309568971872163		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.9309568971872163 | validation: 0.7400375003122359]
	TIME [epoch: 1.36 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8716207164458387		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.8716207164458387 | validation: 0.9705914250850931]
	TIME [epoch: 1.36 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8588243757948243		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.8588243757948243 | validation: 0.7450662021379666]
	TIME [epoch: 1.36 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8824478692301022		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.8824478692301022 | validation: 1.0135368477143742]
	TIME [epoch: 1.35 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9014454676969347		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.9014454676969347 | validation: 0.7692465380184528]
	TIME [epoch: 1.35 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8847104490219717		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.8847104490219717 | validation: 0.9897305420841861]
	TIME [epoch: 1.35 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8800899877425942		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.8800899877425942 | validation: 0.743441397572865]
	TIME [epoch: 1.36 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8620659253664097		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.8620659253664097 | validation: 1.0461401241488466]
	TIME [epoch: 1.35 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.887648596990483		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.887648596990483 | validation: 0.7457990774138579]
	TIME [epoch: 1.36 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8784017042015154		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.8784017042015154 | validation: 0.9811915703950174]
	TIME [epoch: 1.35 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8490182814021844		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.8490182814021844 | validation: 0.7268833541051668]
	TIME [epoch: 1.36 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8319451147474998		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.8319451147474998 | validation: 1.0344116570080002]
	TIME [epoch: 1.36 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8685410328146594		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.8685410328146594 | validation: 0.7209883180510737]
	TIME [epoch: 1.36 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9100300759735472		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.9100300759735472 | validation: 1.0826708700943402]
	TIME [epoch: 1.36 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8923595327782263		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.8923595327782263 | validation: 0.6961952112523968]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_302.pth
	Model improved!!!
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8715204487745889		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.8715204487745889 | validation: 0.9669263602881344]
	TIME [epoch: 1.36 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.835138807105231		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.835138807105231 | validation: 0.7510271102125792]
	TIME [epoch: 1.36 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8325034716381379		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.8325034716381379 | validation: 1.0253406675777295]
	TIME [epoch: 1.36 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8931713695130299		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.8931713695130299 | validation: 0.8217422139819544]
	TIME [epoch: 1.35 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9204944581322017		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.9204944581322017 | validation: 0.9954292952041076]
	TIME [epoch: 1.36 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8726452585074722		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.8726452585074722 | validation: 0.7579256982340954]
	TIME [epoch: 1.36 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8367911697359719		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.8367911697359719 | validation: 0.950657960450688]
	TIME [epoch: 1.36 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8323833200915413		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.8323833200915413 | validation: 0.7067959294881404]
	TIME [epoch: 1.35 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.865435479477014		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.865435479477014 | validation: 1.0723043484365982]
	TIME [epoch: 1.35 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8855797989495264		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.8855797989495264 | validation: 0.7139057790086343]
	TIME [epoch: 1.35 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8680186038300696		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.8680186038300696 | validation: 1.0139346891109546]
	TIME [epoch: 1.35 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8447963257503218		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.8447963257503218 | validation: 0.7428428981856373]
	TIME [epoch: 1.35 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8246888330131261		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.8246888330131261 | validation: 0.9679888578332527]
	TIME [epoch: 1.35 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8433254116531643		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.8433254116531643 | validation: 0.8092470104539139]
	TIME [epoch: 1.35 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9030836941213183		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.9030836941213183 | validation: 0.9786261646724042]
	TIME [epoch: 1.35 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8886159831731429		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.8886159831731429 | validation: 0.7747221479745676]
	TIME [epoch: 1.35 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8359070323958453		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.8359070323958453 | validation: 0.9468971020663622]
	TIME [epoch: 1.35 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8145278776297898		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.8145278776297898 | validation: 0.7589203040586515]
	TIME [epoch: 1.35 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8253761217184145		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.8253761217184145 | validation: 1.0214418275250663]
	TIME [epoch: 1.35 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8574935383302043		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.8574935383302043 | validation: 0.7485250577882644]
	TIME [epoch: 1.35 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8518033591120076		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.8518033591120076 | validation: 1.020648683594169]
	TIME [epoch: 1.35 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8652423821662418		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.8652423821662418 | validation: 0.7596617233787253]
	TIME [epoch: 1.35 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8552563121266243		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.8552563121266243 | validation: 0.9747961567771001]
	TIME [epoch: 1.35 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8316070417598691		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.8316070417598691 | validation: 0.729677748214148]
	TIME [epoch: 1.35 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8354293833102661		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.8354293833102661 | validation: 1.0236371540803646]
	TIME [epoch: 1.35 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8380044852192691		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.8380044852192691 | validation: 0.6952910940500205]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_328.pth
	Model improved!!!
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8602274245446608		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.8602274245446608 | validation: 1.030103963682112]
	TIME [epoch: 1.36 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8473633478534146		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.8473633478534146 | validation: 0.6973175658630041]
	TIME [epoch: 1.36 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8058025206165089		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.8058025206165089 | validation: 0.9575277488380156]
	TIME [epoch: 1.36 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8107044528750373		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.8107044528750373 | validation: 0.7598329845249089]
	TIME [epoch: 1.36 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8356944910289328		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.8356944910289328 | validation: 1.0414875273065405]
	TIME [epoch: 1.35 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8902850164071451		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.8902850164071451 | validation: 0.8172148639950643]
	TIME [epoch: 1.36 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8659759780694753		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.8659759780694753 | validation: 0.9451626223443403]
	TIME [epoch: 1.36 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8295318070963131		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.8295318070963131 | validation: 0.7442904622807154]
	TIME [epoch: 1.35 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8032651360496974		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.8032651360496974 | validation: 0.9680554500603116]
	TIME [epoch: 1.35 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8018122765363167		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.8018122765363167 | validation: 0.746888063898997]
	TIME [epoch: 1.35 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8082931757312783		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.8082931757312783 | validation: 1.0016454887248503]
	TIME [epoch: 1.35 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8299771967339693		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.8299771967339693 | validation: 0.7394996861820466]
	TIME [epoch: 1.35 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8386152175677484		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.8386152175677484 | validation: 1.02974188357649]
	TIME [epoch: 1.35 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8581730299545162		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.8581730299545162 | validation: 0.7517050781932865]
	TIME [epoch: 1.35 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.824856151981686		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.824856151981686 | validation: 0.9221263414629783]
	TIME [epoch: 1.35 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8170672473303269		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.8170672473303269 | validation: 0.7865585975751662]
	TIME [epoch: 1.35 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8478845848166688		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.8478845848166688 | validation: 0.9386362990957928]
	TIME [epoch: 1.35 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8265764811749204		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.8265764811749204 | validation: 0.8149922399096481]
	TIME [epoch: 1.35 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.81152184183052		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.81152184183052 | validation: 0.9001221439157405]
	TIME [epoch: 1.35 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8032200906476175		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.8032200906476175 | validation: 0.800026126504238]
	TIME [epoch: 1.35 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8291523490142367		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.8291523490142367 | validation: 0.9406520386214468]
	TIME [epoch: 1.35 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8324778674505727		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.8324778674505727 | validation: 0.7839476989375623]
	TIME [epoch: 1.35 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7941588806624106		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.7941588806624106 | validation: 0.9042095623138201]
	TIME [epoch: 1.35 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7958034050295533		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.7958034050295533 | validation: 0.7326342687366175]
	TIME [epoch: 1.35 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7894579647637785		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.7894579647637785 | validation: 1.1170587769865838]
	TIME [epoch: 1.35 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8608776277024566		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.8608776277024566 | validation: 0.6969730250545549]
	TIME [epoch: 1.35 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9063839427429349		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.9063839427429349 | validation: 1.0179988709326415]
	TIME [epoch: 1.35 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8260149520519096		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.8260149520519096 | validation: 0.7347713634435069]
	TIME [epoch: 1.35 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7650010963359877		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.7650010963359877 | validation: 0.8439859803795517]
	TIME [epoch: 1.35 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7562939853135157		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.7562939853135157 | validation: 0.7564949943004721]
	TIME [epoch: 1.35 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.760921844067299		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.760921844067299 | validation: 0.9013463474226576]
	TIME [epoch: 1.35 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7940954846122464		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.7940954846122464 | validation: 0.8078821617694377]
	TIME [epoch: 1.35 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9013564358104362		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.9013564358104362 | validation: 1.0357959122898885]
	TIME [epoch: 1.35 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8742755126488119		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.8742755126488119 | validation: 0.7849695314309313]
	TIME [epoch: 1.35 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7891365620073063		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.7891365620073063 | validation: 0.8676528106138592]
	TIME [epoch: 1.35 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7479492418461955		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.7479492418461955 | validation: 0.7471783063758612]
	TIME [epoch: 1.35 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7663063266518471		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.7663063266518471 | validation: 0.9756373544785713]
	TIME [epoch: 1.35 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8097197021877998		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.8097197021877998 | validation: 0.7306512702773157]
	TIME [epoch: 1.35 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7952125961790697		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.7952125961790697 | validation: 0.9932804896524102]
	TIME [epoch: 1.35 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.80412748063344		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.80412748063344 | validation: 0.7060942301291402]
	TIME [epoch: 1.36 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8500792179804709		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.8500792179804709 | validation: 1.010404954810389]
	TIME [epoch: 1.36 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8071217606743639		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.8071217606743639 | validation: 0.7212982035781764]
	TIME [epoch: 1.35 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.790008608222972		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.790008608222972 | validation: 0.9331923285551691]
	TIME [epoch: 1.36 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7834433206947458		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.7834433206947458 | validation: 0.7950992033163367]
	TIME [epoch: 1.36 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.809409355037281		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.809409355037281 | validation: 0.9516406610967059]
	TIME [epoch: 1.36 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8642079163461215		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.8642079163461215 | validation: 0.7854150596537096]
	TIME [epoch: 1.36 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7995973289601855		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.7995973289601855 | validation: 0.8711306026053326]
	TIME [epoch: 1.36 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.77308310022782		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.77308310022782 | validation: 0.7478020779231899]
	TIME [epoch: 1.35 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7707684699484467		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.7707684699484467 | validation: 0.960934090013732]
	TIME [epoch: 1.36 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7868962399728809		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.7868962399728809 | validation: 0.718196818692952]
	TIME [epoch: 1.36 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8015164390527219		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.8015164390527219 | validation: 0.973522453322349]
	TIME [epoch: 1.36 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7805721032319954		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.7805721032319954 | validation: 0.7105455405623275]
	TIME [epoch: 1.35 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7749434561838799		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.7749434561838799 | validation: 0.96473811606296]
	TIME [epoch: 1.36 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7828754159704837		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.7828754159704837 | validation: 0.6735355847457659]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_382.pth
	Model improved!!!
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8042749942187462		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.8042749942187462 | validation: 0.9767322978718131]
	TIME [epoch: 1.35 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7973836956267951		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.7973836956267951 | validation: 0.708193062132512]
	TIME [epoch: 1.35 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7702829103841806		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.7702829103841806 | validation: 0.8853445467360199]
	TIME [epoch: 1.35 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7505533980472154		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.7505533980472154 | validation: 0.7846916399990745]
	TIME [epoch: 1.35 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7758143638738784		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.7758143638738784 | validation: 0.9219884753524001]
	TIME [epoch: 1.36 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8298846455794825		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.8298846455794825 | validation: 0.8223067861846247]
	TIME [epoch: 1.35 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8666405374434568		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.8666405374434568 | validation: 0.9273798031668954]
	TIME [epoch: 1.35 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7834360138744576		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.7834360138744576 | validation: 0.7382557345739559]
	TIME [epoch: 1.35 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7580117984505639		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.7580117984505639 | validation: 0.911138919840057]
	TIME [epoch: 1.35 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7604812142799492		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.7604812142799492 | validation: 0.729065814592677]
	TIME [epoch: 1.35 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7468912541099465		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.7468912541099465 | validation: 0.9053132568176516]
	TIME [epoch: 1.35 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7446002829926406		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.7446002829926406 | validation: 0.7092632318566182]
	TIME [epoch: 1.35 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7463471665906315		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.7463471665906315 | validation: 0.9933230755199042]
	TIME [epoch: 1.35 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7845661160479251		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.7845661160479251 | validation: 0.6961686750559403]
	TIME [epoch: 1.35 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8206975121436162		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.8206975121436162 | validation: 1.0035650289668445]
	TIME [epoch: 1.35 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8061160455261662		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.8061160455261662 | validation: 0.741734454342439]
	TIME [epoch: 1.36 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7766028160192615		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.7766028160192615 | validation: 0.8616383146429953]
	TIME [epoch: 1.36 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7681902493551458		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.7681902493551458 | validation: 0.862229496856897]
	TIME [epoch: 1.35 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7957678459834898		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.7957678459834898 | validation: 0.811759914788162]
	TIME [epoch: 1.36 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7804133017469798		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.7804133017469798 | validation: 0.7917386929891118]
	TIME [epoch: 1.36 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7364763843956861		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.7364763843956861 | validation: 0.8249419956086612]
	TIME [epoch: 1.35 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7268328671494851		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.7268328671494851 | validation: 0.7448987126527196]
	TIME [epoch: 1.36 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7378467304907389		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.7378467304907389 | validation: 0.970105869118751]
	TIME [epoch: 1.35 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7650780247572315		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.7650780247572315 | validation: 0.7332110221719401]
	TIME [epoch: 1.35 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8087301974295855		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.8087301974295855 | validation: 1.0025909487804403]
	TIME [epoch: 1.35 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.792162823962659		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.792162823962659 | validation: 0.7560513103650109]
	TIME [epoch: 1.36 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7532693097662313		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.7532693097662313 | validation: 0.8535257124783883]
	TIME [epoch: 1.35 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7469049036821741		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.7469049036821741 | validation: 0.8315893900097309]
	TIME [epoch: 1.35 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7687580150236687		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.7687580150236687 | validation: 0.8011957521752188]
	TIME [epoch: 1.36 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7707838711165738		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.7707838711165738 | validation: 0.8249288418859984]
	TIME [epoch: 1.36 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7510634420837016		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.7510634420837016 | validation: 0.7339486739311165]
	TIME [epoch: 1.36 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7261764538920127		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.7261764538920127 | validation: 0.8515733001533099]
	TIME [epoch: 1.36 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7349656752565011		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.7349656752565011 | validation: 0.6931673414624535]
	TIME [epoch: 1.36 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7587116319724754		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.7587116319724754 | validation: 1.021043663674521]
	TIME [epoch: 1.35 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.811204517969833		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.811204517969833 | validation: 0.7200542907803094]
	TIME [epoch: 1.36 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8338913654790464		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.8338913654790464 | validation: 0.952794651474833]
	TIME [epoch: 1.36 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.767625500489024		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.767625500489024 | validation: 0.7591742247094038]
	TIME [epoch: 1.36 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7550466107281238		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.7550466107281238 | validation: 0.8632910088780229]
	TIME [epoch: 1.36 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7441733942472292		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.7441733942472292 | validation: 0.7844819811251517]
	TIME [epoch: 1.36 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7266093011668489		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.7266093011668489 | validation: 0.8021046612352015]
	TIME [epoch: 1.35 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7231534187420853		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.7231534187420853 | validation: 0.7371708863920298]
	TIME [epoch: 1.35 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7216056902027382		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.7216056902027382 | validation: 0.9480711587888394]
	TIME [epoch: 1.35 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7606399387852969		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.7606399387852969 | validation: 0.7480372750659234]
	TIME [epoch: 1.35 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7802344550553737		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.7802344550553737 | validation: 0.921800936737604]
	TIME [epoch: 1.35 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7541978962217768		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.7541978962217768 | validation: 0.752684373576654]
	TIME [epoch: 1.35 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7368150285765245		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.7368150285765245 | validation: 0.811098237971804]
	TIME [epoch: 1.35 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7224490290690395		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.7224490290690395 | validation: 0.8277488108981107]
	TIME [epoch: 1.35 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7547412475973078		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.7547412475973078 | validation: 0.7780973734677438]
	TIME [epoch: 1.35 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7538828595617239		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.7538828595617239 | validation: 0.8252341886308958]
	TIME [epoch: 1.35 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7328363943085705		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.7328363943085705 | validation: 0.7588080159187504]
	TIME [epoch: 1.35 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7093094617748653		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.7093094617748653 | validation: 0.7936959795018903]
	TIME [epoch: 1.35 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7021700019855288		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.7021700019855288 | validation: 0.761559966624628]
	TIME [epoch: 1.35 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6921339646400659		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.6921339646400659 | validation: 0.793253252523194]
	TIME [epoch: 1.35 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7079655608188011		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.7079655608188011 | validation: 0.7279982153029924]
	TIME [epoch: 1.35 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7459543333343944		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.7459543333343944 | validation: 0.9110841060802571]
	TIME [epoch: 1.35 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7645829408034667		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.7645829408034667 | validation: 0.6579531207641112]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_438.pth
	Model improved!!!
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.740706533983429		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.740706533983429 | validation: 1.1516937196361663]
	TIME [epoch: 1.35 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8346715270432281		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.8346715270432281 | validation: 0.7208780779227036]
	TIME [epoch: 1.35 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7903516976448789		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.7903516976448789 | validation: 0.8791007077355083]
	TIME [epoch: 1.35 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7354145568829623		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.7354145568829623 | validation: 0.817496411201216]
	TIME [epoch: 1.35 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7231843363531075		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.7231843363531075 | validation: 0.7727181802353922]
	TIME [epoch: 1.35 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7214827900642214		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.7214827900642214 | validation: 0.7886285541304349]
	TIME [epoch: 1.35 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7169113566240581		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.7169113566240581 | validation: 0.7907647363836235]
	TIME [epoch: 1.35 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7056491503472762		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.7056491503472762 | validation: 0.7459067748020884]
	TIME [epoch: 1.35 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7167879994222566		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.7167879994222566 | validation: 0.8845576253184032]
	TIME [epoch: 1.35 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7277164627175582		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.7277164627175582 | validation: 0.731700925980576]
	TIME [epoch: 1.35 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.771276361480717		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.771276361480717 | validation: 0.9670386755887453]
	TIME [epoch: 1.35 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7509424725844412		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.7509424725844412 | validation: 0.7037079335865073]
	TIME [epoch: 1.35 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7077802731306452		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.7077802731306452 | validation: 0.8790620482890334]
	TIME [epoch: 1.35 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6969064334633234		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.6969064334633234 | validation: 0.6900993312725434]
	TIME [epoch: 1.35 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7083451987864956		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.7083451987864956 | validation: 0.9561087499019756]
	TIME [epoch: 1.35 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7351035026944237		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.7351035026944237 | validation: 0.6501842235516182]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_454.pth
	Model improved!!!
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7756892759090271		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.7756892759090271 | validation: 0.9205620763062315]
	TIME [epoch: 1.35 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7344686285039339		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.7344686285039339 | validation: 0.7201904630713234]
	TIME [epoch: 1.35 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6931280250626024		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.6931280250626024 | validation: 0.7634396494775085]
	TIME [epoch: 1.35 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6829321396591262		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.6829321396591262 | validation: 0.7626394779331411]
	TIME [epoch: 1.35 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6804220235728097		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.6804220235728097 | validation: 0.7523663721790025]
	TIME [epoch: 1.35 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.684530166091636		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.684530166091636 | validation: 0.814141851892554]
	TIME [epoch: 1.35 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7038487056899253		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.7038487056899253 | validation: 0.7628803906994579]
	TIME [epoch: 1.35 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7334899592414275		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.7334899592414275 | validation: 0.7903492248261274]
	TIME [epoch: 1.35 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7638174701029361		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.7638174701029361 | validation: 0.9300284603905742]
	TIME [epoch: 1.35 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7528586746256802		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.7528586746256802 | validation: 0.6778827306766781]
	TIME [epoch: 1.35 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7708106837189		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.7708106837189 | validation: 0.9149578342372534]
	TIME [epoch: 1.35 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7026949554989659		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.7026949554989659 | validation: 0.7462290905639838]
	TIME [epoch: 1.35 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6785560064430888		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.6785560064430888 | validation: 0.7832766321992846]
	TIME [epoch: 1.35 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6782536059091766		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.6782536059091766 | validation: 0.7460085841474928]
	TIME [epoch: 1.35 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6847560018840549		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.6847560018840549 | validation: 0.8064587742794267]
	TIME [epoch: 1.35 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7054171203554185		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.7054171203554185 | validation: 0.685038374011084]
	TIME [epoch: 1.35 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7032978513467305		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.7032978513467305 | validation: 0.8203427798906348]
	TIME [epoch: 1.35 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6883954531713525		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.6883954531713525 | validation: 0.6655761858230737]
	TIME [epoch: 1.35 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7115509319767267		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.7115509319767267 | validation: 0.9234449766331392]
	TIME [epoch: 1.35 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7265788391152889		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.7265788391152889 | validation: 0.6669780023031706]
	TIME [epoch: 1.35 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7021195026710955		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.7021195026710955 | validation: 0.8831271879808876]
	TIME [epoch: 1.35 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6894728355636522		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.6894728355636522 | validation: 0.6834889317711728]
	TIME [epoch: 1.35 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7087309062648509		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.7087309062648509 | validation: 0.9329558738274333]
	TIME [epoch: 1.35 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7520902568360343		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.7520902568360343 | validation: 0.7314162869346705]
	TIME [epoch: 1.35 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7699431292552791		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.7699431292552791 | validation: 0.8247931572933045]
	TIME [epoch: 1.35 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6883323654285985		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.6883323654285985 | validation: 0.7404014167769402]
	TIME [epoch: 1.35 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6774998007088854		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.6774998007088854 | validation: 0.7464671172383617]
	TIME [epoch: 1.35 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6720918961992094		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.6720918961992094 | validation: 0.8078293939138201]
	TIME [epoch: 1.35 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.676808254979324		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.676808254979324 | validation: 0.6881901542889595]
	TIME [epoch: 1.35 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6781352947831145		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.6781352947831145 | validation: 0.7943048912312433]
	TIME [epoch: 1.35 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6777120330984316		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.6777120330984316 | validation: 0.6852167187903969]
	TIME [epoch: 1.35 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6739003665538523		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.6739003665538523 | validation: 0.8245922023200842]
	TIME [epoch: 1.35 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6795537134696968		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.6795537134696968 | validation: 0.6714060411308329]
	TIME [epoch: 1.35 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6743351939611435		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.6743351939611435 | validation: 0.907321066675489]
	TIME [epoch: 1.35 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7069905244846902		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.7069905244846902 | validation: 0.6366528764473656]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_489.pth
	Model improved!!!
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7390457089030819		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.7390457089030819 | validation: 0.9692062932662066]
	TIME [epoch: 1.35 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7174999058594174		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.7174999058594174 | validation: 0.7205554317668154]
	TIME [epoch: 1.35 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.69298078181953		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.69298078181953 | validation: 0.8008442664070565]
	TIME [epoch: 1.35 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7229022708328683		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.7229022708328683 | validation: 0.7877316096893999]
	TIME [epoch: 1.35 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7109377798748789		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.7109377798748789 | validation: 0.7663438441491914]
	TIME [epoch: 1.35 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6728259803112804		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.6728259803112804 | validation: 0.7615347726580726]
	TIME [epoch: 1.36 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6608801983881385		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.6608801983881385 | validation: 0.7202692844897678]
	TIME [epoch: 1.35 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6645875307726675		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.6645875307726675 | validation: 0.8151657322215003]
	TIME [epoch: 1.35 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6772440750614933		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.6772440750614933 | validation: 0.6524572333005288]
	TIME [epoch: 1.35 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6798230068501192		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.6798230068501192 | validation: 0.889940678889559]
	TIME [epoch: 1.35 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.680428442597181		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.680428442597181 | validation: 0.6619059077217013]
	TIME [epoch: 1.35 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7082427518040609		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.7082427518040609 | validation: 0.9015180610845296]
	TIME [epoch: 174 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6861417803048897		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.6861417803048897 | validation: 0.7463304408555302]
	TIME [epoch: 2.68 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6573458411552896		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.6573458411552896 | validation: 0.7280851091245]
	TIME [epoch: 2.67 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6561598590086695		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.6561598590086695 | validation: 0.7824925033839858]
	TIME [epoch: 2.67 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6748887736721703		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.6748887736721703 | validation: 0.7013228109707538]
	TIME [epoch: 2.67 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6976886420311386		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.6976886420311386 | validation: 0.7941360538487631]
	TIME [epoch: 2.67 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6768325971174218		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.6768325971174218 | validation: 0.7826712929991345]
	TIME [epoch: 2.67 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6722783213825645		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.6722783213825645 | validation: 0.6955033394354664]
	TIME [epoch: 2.67 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6736426788703516		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.6736426788703516 | validation: 0.8547981474563944]
	TIME [epoch: 2.67 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6830318334463209		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.6830318334463209 | validation: 0.646820786297607]
	TIME [epoch: 2.67 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6876452379717256		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.6876452379717256 | validation: 0.8748658915839558]
	TIME [epoch: 2.67 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6703371456237698		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.6703371456237698 | validation: 0.6575409484243617]
	TIME [epoch: 2.67 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6565554967831212		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.6565554967831212 | validation: 0.8453700938033244]
	TIME [epoch: 2.67 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6614507668480153		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.6614507668480153 | validation: 0.6451220304272699]
	TIME [epoch: 2.67 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6729731657265607		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.6729731657265607 | validation: 0.7908108402660937]
	TIME [epoch: 2.67 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6634747538712524		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.6634747538712524 | validation: 0.6778540010132335]
	TIME [epoch: 2.67 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6434310858090543		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.6434310858090543 | validation: 0.737182008300711]
	TIME [epoch: 2.67 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6362848918416192		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.6362848918416192 | validation: 0.7857747490155315]
	TIME [epoch: 2.67 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.654274147361014		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.654274147361014 | validation: 0.6848505440352699]
	TIME [epoch: 2.67 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6806775591325078		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.6806775591325078 | validation: 0.8725489353368717]
	TIME [epoch: 2.67 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6878235162816637		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.6878235162816637 | validation: 0.6584861803333723]
	TIME [epoch: 2.67 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6713117328236093		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.6713117328236093 | validation: 0.7807497088667524]
	TIME [epoch: 2.67 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6340876921618933		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.6340876921618933 | validation: 0.6936389085363142]
	TIME [epoch: 2.67 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6293552805470055		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.6293552805470055 | validation: 0.8010045299673232]
	TIME [epoch: 2.67 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6424335812943431		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.6424335812943431 | validation: 0.6628803892893911]
	TIME [epoch: 2.67 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6788030571162035		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.6788030571162035 | validation: 0.8180379607313187]
	TIME [epoch: 2.67 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6567557361824986		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.6567557361824986 | validation: 0.6356681697468733]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_527.pth
	Model improved!!!
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6357030281392106		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.6357030281392106 | validation: 0.8217688990766971]
	TIME [epoch: 2.67 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6361887215175799		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.6361887215175799 | validation: 0.6361716964214321]
	TIME [epoch: 2.67 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6732645991517499		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.6732645991517499 | validation: 0.8960087913601186]
	TIME [epoch: 2.66 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6740483461853757		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.6740483461853757 | validation: 0.6779672509207263]
	TIME [epoch: 2.66 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.654040308418722		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.654040308418722 | validation: 0.7375690565278444]
	TIME [epoch: 2.66 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6273356577808626		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.6273356577808626 | validation: 0.7506322649889883]
	TIME [epoch: 2.66 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6241504996962165		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.6241504996962165 | validation: 0.6649886623779832]
	TIME [epoch: 2.67 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.627071922849355		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.627071922849355 | validation: 0.8014191160762281]
	TIME [epoch: 2.66 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6451879518640774		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.6451879518640774 | validation: 0.6324962319710454]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_536.pth
	Model improved!!!
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.649581995353162		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.649581995353162 | validation: 0.7619938072530106]
	TIME [epoch: 2.66 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6271373873952795		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.6271373873952795 | validation: 0.6796737704730863]
	TIME [epoch: 2.66 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6169980968711817		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.6169980968711817 | validation: 0.7370489726729053]
	TIME [epoch: 2.67 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.605297212926465		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.605297212926465 | validation: 0.6649796848690799]
	TIME [epoch: 2.67 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6082431978858315		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.6082431978858315 | validation: 0.751862120427188]
	TIME [epoch: 2.67 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6269286207138363		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.6269286207138363 | validation: 0.6233546671992419]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_542.pth
	Model improved!!!
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6828183468985788		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.6828183468985788 | validation: 0.8346012145355369]
	TIME [epoch: 2.67 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6376322236398224		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.6376322236398224 | validation: 0.578445324848312]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_544.pth
	Model improved!!!
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6689789094625391		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.6689789094625391 | validation: 0.9498992183581287]
	TIME [epoch: 2.67 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6834804459377591		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.6834804459377591 | validation: 0.6298369292974113]
	TIME [epoch: 2.67 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.62643489606047		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.62643489606047 | validation: 0.692761508896029]
	TIME [epoch: 2.67 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6148982174881917		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.6148982174881917 | validation: 0.7787962216477254]
	TIME [epoch: 2.67 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6275424502525351		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.6275424502525351 | validation: 0.6355281896309055]
	TIME [epoch: 2.67 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6178629218243069		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.6178629218243069 | validation: 0.7079735026254022]
	TIME [epoch: 2.66 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6014139079863158		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.6014139079863158 | validation: 0.666564101732998]
	TIME [epoch: 2.67 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5975748110150548		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.5975748110150548 | validation: 0.6905168195671414]
	TIME [epoch: 2.67 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6102084747537135		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.6102084747537135 | validation: 0.6537174946908836]
	TIME [epoch: 2.67 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6208624145198122		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.6208624145198122 | validation: 0.7401131133138732]
	TIME [epoch: 2.68 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6132316113878918		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.6132316113878918 | validation: 0.5998663013054172]
	TIME [epoch: 2.67 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6173273218297054		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.6173273218297054 | validation: 0.8503525541588269]
	TIME [epoch: 2.66 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6419889606503395		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.6419889606503395 | validation: 0.5795460323175323]
	TIME [epoch: 2.66 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6695258279063007		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.6695258279063007 | validation: 0.8552773706246475]
	TIME [epoch: 2.66 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.620023271024791		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.620023271024791 | validation: 0.6647295767620715]
	TIME [epoch: 2.66 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5938058662478255		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.5938058662478255 | validation: 0.6576274439371744]
	TIME [epoch: 2.67 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5814992501147359		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.5814992501147359 | validation: 0.7202921717182019]
	TIME [epoch: 2.67 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5930137083610677		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.5930137083610677 | validation: 0.651137942650823]
	TIME [epoch: 2.67 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5934354076983186		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.5934354076983186 | validation: 0.7152266035991057]
	TIME [epoch: 2.67 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5899109244536884		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.5899109244536884 | validation: 0.6668689886277286]
	TIME [epoch: 2.67 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6094445701797124		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.6094445701797124 | validation: 0.7181365496573584]
	TIME [epoch: 2.67 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.666009842777946		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.666009842777946 | validation: 0.7336319439592262]
	TIME [epoch: 2.67 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6163675249377029		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.6163675249377029 | validation: 0.6304818299151953]
	TIME [epoch: 2.66 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5898311890450073		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.5898311890450073 | validation: 0.6912945195607794]
	TIME [epoch: 2.66 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5862498029451123		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.5862498029451123 | validation: 0.6879919565075547]
	TIME [epoch: 2.66 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5981533299245648		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.5981533299245648 | validation: 0.6001073338800912]
	TIME [epoch: 2.67 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6108451805056527		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.6108451805056527 | validation: 0.8587172719689994]
	TIME [epoch: 2.67 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6351402119745092		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.6351402119745092 | validation: 0.56008055130933]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_572.pth
	Model improved!!!
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6504382543893013		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.6504382543893013 | validation: 0.8088715689028811]
	TIME [epoch: 2.67 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5886277657190668		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.5886277657190668 | validation: 0.6224203360768794]
	TIME [epoch: 2.66 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5789495634254418		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.5789495634254418 | validation: 0.6911548648179763]
	TIME [epoch: 2.67 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5801076347196442		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.5801076347196442 | validation: 0.6634418883610198]
	TIME [epoch: 2.66 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5909190382969637		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.5909190382969637 | validation: 0.6613483854631923]
	TIME [epoch: 2.67 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5651037467117807		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.5651037467117807 | validation: 0.6475380808814384]
	TIME [epoch: 2.66 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5666524691507594		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.5666524691507594 | validation: 0.6437087857927009]
	TIME [epoch: 2.66 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5711282456612491		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.5711282456612491 | validation: 0.6862677430922934]
	TIME [epoch: 2.66 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5845138948658807		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.5845138948658807 | validation: 0.6461375476875997]
	TIME [epoch: 2.66 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6047970270534212		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.6047970270534212 | validation: 0.7327014790619297]
	TIME [epoch: 2.66 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5824513645298329		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.5824513645298329 | validation: 0.5311409488557064]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_583.pth
	Model improved!!!
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5797114180545816		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.5797114180545816 | validation: 0.8507967640681184]
	TIME [epoch: 2.66 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6156522418944966		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.6156522418944966 | validation: 0.5279142483691303]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_585.pth
	Model improved!!!
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6031075045769105		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.6031075045769105 | validation: 0.7834704409597757]
	TIME [epoch: 2.66 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5741998021712241		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.5741998021712241 | validation: 0.5774059904319898]
	TIME [epoch: 2.67 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5715350471201738		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.5715350471201738 | validation: 0.6977877031988906]
	TIME [epoch: 2.67 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5654629716814382		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.5654629716814382 | validation: 0.5992857702660673]
	TIME [epoch: 2.66 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5639752147855343		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.5639752147855343 | validation: 0.6774583655754746]
	TIME [epoch: 2.67 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5517991521981407		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.5517991521981407 | validation: 0.607040545169625]
	TIME [epoch: 2.66 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5500719762612917		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.5500719762612917 | validation: 0.6554694761220401]
	TIME [epoch: 2.67 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5526335909731237		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.5526335909731237 | validation: 0.6040014291070755]
	TIME [epoch: 2.66 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5529922516553919		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.5529922516553919 | validation: 0.6956164215020716]
	TIME [epoch: 2.67 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5682798901727358		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.5682798901727358 | validation: 0.6095236470324961]
	TIME [epoch: 2.67 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5789070342518067		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.5789070342518067 | validation: 0.7212276242338712]
	TIME [epoch: 2.66 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.562006261670349		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.562006261670349 | validation: 0.6147982359117986]
	TIME [epoch: 2.66 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5344830770047128		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.5344830770047128 | validation: 0.657282414967288]
	TIME [epoch: 2.66 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.542379417100913		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.542379417100913 | validation: 0.6302052339711594]
	TIME [epoch: 2.67 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5411355565674644		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.5411355565674644 | validation: 0.5625621391204145]
	TIME [epoch: 2.66 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5735369550299262		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.5735369550299262 | validation: 0.7874769933799008]
	TIME [epoch: 2.68 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5868500930488868		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.5868500930488868 | validation: 0.45431675531714816]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_602.pth
	Model improved!!!
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6399394840201571		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.6399394840201571 | validation: 0.8613756672780912]
	TIME [epoch: 2.68 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5972307567618539		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.5972307567618539 | validation: 0.6204849461076776]
	TIME [epoch: 2.66 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5443703276619726		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.5443703276619726 | validation: 0.582507775235602]
	TIME [epoch: 2.67 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5411658518009028		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.5411658518009028 | validation: 0.6938849326515815]
	TIME [epoch: 2.66 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5406932969536689		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.5406932969536689 | validation: 0.546413235111069]
	TIME [epoch: 2.66 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5454320578361722		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.5454320578361722 | validation: 0.7035016636204827]
	TIME [epoch: 2.66 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5352890139352569		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.5352890139352569 | validation: 0.5601757993285457]
	TIME [epoch: 2.67 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5511515160636852		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.5511515160636852 | validation: 0.7020477495596023]
	TIME [epoch: 2.67 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5368660749992747		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.5368660749992747 | validation: 0.5444836509365228]
	TIME [epoch: 2.67 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5308887661394399		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.5308887661394399 | validation: 0.6801791502342553]
	TIME [epoch: 2.67 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5258509168438696		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.5258509168438696 | validation: 0.5674411503336029]
	TIME [epoch: 2.67 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5394313248881949		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.5394313248881949 | validation: 0.678800437213047]
	TIME [epoch: 2.67 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5320385495292834		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.5320385495292834 | validation: 0.5787774142030621]
	TIME [epoch: 2.67 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5253546468485168		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.5253546468485168 | validation: 0.6543468064593982]
	TIME [epoch: 2.66 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5252240053553022		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.5252240053553022 | validation: 0.5845023732585005]
	TIME [epoch: 2.67 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5261891119741913		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.5261891119741913 | validation: 0.617705476584961]
	TIME [epoch: 2.66 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5089263734149457		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.5089263734149457 | validation: 0.6387500416535604]
	TIME [epoch: 2.67 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5112983753220915		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.5112983753220915 | validation: 0.5440794816189283]
	TIME [epoch: 2.66 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5194190001716702		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.5194190001716702 | validation: 0.7453166553046032]
	TIME [epoch: 2.67 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5520842254999353		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.5520842254999353 | validation: 0.47598036385334697]
	TIME [epoch: 2.66 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5635303537296199		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.5635303537296199 | validation: 0.7869521504841895]
	TIME [epoch: 2.66 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5612690506183737		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.5612690506183737 | validation: 0.5115065684769071]
	TIME [epoch: 2.66 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5580547773447597		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.5580547773447597 | validation: 0.6559343353154976]
	TIME [epoch: 2.66 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4994717269408714		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.4994717269408714 | validation: 0.5634561183085028]
	TIME [epoch: 2.66 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4970387677425657		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.4970387677425657 | validation: 0.6043167117887924]
	TIME [epoch: 2.66 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4914029207459308		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.4914029207459308 | validation: 0.5630775057211748]
	TIME [epoch: 2.67 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5000434119648032		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.5000434119648032 | validation: 0.6523886131919103]
	TIME [epoch: 2.67 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5075600173098606		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.5075600173098606 | validation: 0.5055744779681847]
	TIME [epoch: 2.66 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5338726745846508		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.5338726745846508 | validation: 0.7620260033619377]
	TIME [epoch: 2.67 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5421869339064281		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.5421869339064281 | validation: 0.4602454389727698]
	TIME [epoch: 2.67 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5491702112151441		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.5491702112151441 | validation: 0.7247098905952098]
	TIME [epoch: 2.67 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5166836730907461		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.5166836730907461 | validation: 0.5098530159687215]
	TIME [epoch: 2.67 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5112762104644558		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.5112762104644558 | validation: 0.6393371745255649]
	TIME [epoch: 2.67 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4998288505453277		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.4998288505453277 | validation: 0.539512896576493]
	TIME [epoch: 2.67 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.485526175734519		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.485526175734519 | validation: 0.6141205956207187]
	TIME [epoch: 2.67 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47784306516991876		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.47784306516991876 | validation: 0.5321992303811465]
	TIME [epoch: 2.67 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48185089179211416		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.48185089179211416 | validation: 0.6445147466990273]
	TIME [epoch: 2.67 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48214727469426744		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.48214727469426744 | validation: 0.482687093367449]
	TIME [epoch: 2.67 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5104518207240866		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.5104518207240866 | validation: 0.787034118056877]
	TIME [epoch: 2.67 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5755057993505668		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.5755057993505668 | validation: 0.4699596626060865]
	TIME [epoch: 2.67 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5322151546802418		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.5322151546802418 | validation: 0.63240070792529]
	TIME [epoch: 2.67 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47237842946672914		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.47237842946672914 | validation: 0.5554997679235862]
	TIME [epoch: 2.67 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4697801720470015		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.4697801720470015 | validation: 0.5652123104072572]
	TIME [epoch: 2.66 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4809268067471815		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.4809268067471815 | validation: 0.5344472453524249]
	TIME [epoch: 2.66 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4779335539449953		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.4779335539449953 | validation: 0.6043962896998403]
	TIME [epoch: 2.68 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.482015845358108		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.482015845358108 | validation: 0.5207458373447826]
	TIME [epoch: 2.67 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.484352369261029		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.484352369261029 | validation: 0.6070651167832246]
	TIME [epoch: 2.67 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47615801694932913		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.47615801694932913 | validation: 0.5257596198928611]
	TIME [epoch: 2.68 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4689645061408778		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.4689645061408778 | validation: 0.6327185038989666]
	TIME [epoch: 2.68 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4690309211160324		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.4690309211160324 | validation: 0.4224906943361382]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_652.pth
	Model improved!!!
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5020384409130261		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.5020384409130261 | validation: 0.8112248899611493]
	TIME [epoch: 2.67 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5578860364776361		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.5578860364776361 | validation: 0.4785918527795132]
	TIME [epoch: 2.68 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5217199299080048		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.5217199299080048 | validation: 0.5921767801981906]
	TIME [epoch: 2.67 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4581797203499045		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.4581797203499045 | validation: 0.5455118828577223]
	TIME [epoch: 2.67 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4493114200021047		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.4493114200021047 | validation: 0.5211254004813055]
	TIME [epoch: 2.67 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4496651123929669		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.4496651123929669 | validation: 0.5993774786142124]
	TIME [epoch: 2.68 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4667319423191536		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.4667319423191536 | validation: 0.4677892296044819]
	TIME [epoch: 2.67 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4812140928219305		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.4812140928219305 | validation: 0.6457272932060841]
	TIME [epoch: 2.67 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4803440461931339		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.4803440461931339 | validation: 0.4385773327903173]
	TIME [epoch: 2.67 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47356209967313817		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.47356209967313817 | validation: 0.6756409799347747]
	TIME [epoch: 2.67 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47611226346375374		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.47611226346375374 | validation: 0.45366604104531066]
	TIME [epoch: 2.67 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46677966670808013		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.46677966670808013 | validation: 0.624658476040202]
	TIME [epoch: 2.67 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4569189105049626		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.4569189105049626 | validation: 0.46228977330543436]
	TIME [epoch: 2.67 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44026076019863025		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.44026076019863025 | validation: 0.5970836295435793]
	TIME [epoch: 2.67 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4519416618129355		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.4519416618129355 | validation: 0.4409272315391041]
	TIME [epoch: 2.67 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46985341459425206		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.46985341459425206 | validation: 0.6242619027341322]
	TIME [epoch: 2.67 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.471796223220139		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.471796223220139 | validation: 0.4604209887728814]
	TIME [epoch: 2.67 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4745139554882324		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.4745139554882324 | validation: 0.5777285662565664]
	TIME [epoch: 2.67 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4384955631693256		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.4384955631693256 | validation: 0.4870012448707464]
	TIME [epoch: 2.67 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42979197340528447		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.42979197340528447 | validation: 0.5548239261033064]
	TIME [epoch: 2.67 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42917440711346244		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.42917440711346244 | validation: 0.4882250189043946]
	TIME [epoch: 2.67 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43584804186108095		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.43584804186108095 | validation: 0.6131857534787183]
	TIME [epoch: 2.67 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4512533283680634		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.4512533283680634 | validation: 0.3969819297178405]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_675.pth
	Model improved!!!
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4504210794100659		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.4504210794100659 | validation: 0.6873344604689051]
	TIME [epoch: 2.68 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4741815593301311		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.4741815593301311 | validation: 0.4411383414097445]
	TIME [epoch: 2.68 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4902227557096764		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.4902227557096764 | validation: 0.5516885834790665]
	TIME [epoch: 2.68 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4264712790873291		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.4264712790873291 | validation: 0.4900100779019143]
	TIME [epoch: 2.68 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42149031119978836		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.42149031119978836 | validation: 0.5583228115832022]
	TIME [epoch: 2.68 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4214503814601099		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.4214503814601099 | validation: 0.4469058495250224]
	TIME [epoch: 2.68 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4206867976504271		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.4206867976504271 | validation: 0.5907730837466679]
	TIME [epoch: 2.67 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44390846655103827		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.44390846655103827 | validation: 0.3882709953753459]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_683.pth
	Model improved!!!
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4574941047569911		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.4574941047569911 | validation: 0.641997462750593]
	TIME [epoch: 2.67 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43781825577297434		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.43781825577297434 | validation: 0.42240406427027044]
	TIME [epoch: 2.67 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42741681176656393		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.42741681176656393 | validation: 0.5543184612452259]
	TIME [epoch: 2.68 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4040841335611921		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.4040841335611921 | validation: 0.4364094107312778]
	TIME [epoch: 2.68 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4114464191799918		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.4114464191799918 | validation: 0.5580439845309665]
	TIME [epoch: 2.68 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41388639366857943		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.41388639366857943 | validation: 0.4284389842872456]
	TIME [epoch: 2.68 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42096836995676135		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.42096836995676135 | validation: 0.5970680782664936]
	TIME [epoch: 2.68 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4364769532303497		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.4364769532303497 | validation: 0.40562984384415873]
	TIME [epoch: 2.68 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4429346197951085		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.4429346197951085 | validation: 0.5637549405607518]
	TIME [epoch: 2.68 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41772431013489214		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.41772431013489214 | validation: 0.4641870156875794]
	TIME [epoch: 2.68 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40065275102399045		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.40065275102399045 | validation: 0.4962619100292216]
	TIME [epoch: 2.68 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3997520768985595		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.3997520768985595 | validation: 0.4790357619883855]
	TIME [epoch: 2.68 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3887336630205165		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.3887336630205165 | validation: 0.4751724921067467]
	TIME [epoch: 2.68 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3840302770620495		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.3840302770620495 | validation: 0.4758993228427696]
	TIME [epoch: 2.68 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.393141436925391		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.393141436925391 | validation: 0.5224489168492576]
	TIME [epoch: 2.68 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40488414593861083		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.40488414593861083 | validation: 0.4104976417804322]
	TIME [epoch: 2.68 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.441439756183118		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.441439756183118 | validation: 0.6177060419938457]
	TIME [epoch: 2.68 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4251436965433224		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.4251436965433224 | validation: 0.3502177610056039]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_701.pth
	Model improved!!!
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43575289768446		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.43575289768446 | validation: 0.609557885993206]
	TIME [epoch: 2.67 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4224303474398697		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.4224303474398697 | validation: 0.42103081659441854]
	TIME [epoch: 2.67 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38934666900776566		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.38934666900776566 | validation: 0.47649848485256685]
	TIME [epoch: 2.67 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37713292906406737		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.37713292906406737 | validation: 0.4897477759715694]
	TIME [epoch: 2.67 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36732047739993334		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.36732047739993334 | validation: 0.42915795805439744]
	TIME [epoch: 2.67 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3812463052991265		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.3812463052991265 | validation: 0.5371620689705586]
	TIME [epoch: 2.67 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3779226029405026		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.3779226029405026 | validation: 0.38353395870447643]
	TIME [epoch: 2.67 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3796570169710811		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.3796570169710811 | validation: 0.5414530537296076]
	TIME [epoch: 2.68 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3801205292149702		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.3801205292149702 | validation: 0.3730435835099035]
	TIME [epoch: 2.67 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38644492425523197		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.38644492425523197 | validation: 0.5914002411124057]
	TIME [epoch: 2.67 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42926092723633913		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.42926092723633913 | validation: 0.36063504286824677]
	TIME [epoch: 2.67 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.447978933978367		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.447978933978367 | validation: 0.5119880588650839]
	TIME [epoch: 2.67 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36828807089374294		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.36828807089374294 | validation: 0.4551941577779716]
	TIME [epoch: 2.67 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3632477751254781		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.3632477751254781 | validation: 0.42108311360916884]
	TIME [epoch: 2.67 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3672832740605834		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.3672832740605834 | validation: 0.4976028218829261]
	TIME [epoch: 2.68 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3809986961304499		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.3809986961304499 | validation: 0.3848900741509546]
	TIME [epoch: 2.67 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37864256770166793		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.37864256770166793 | validation: 0.5157055109612396]
	TIME [epoch: 2.67 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37386510822888186		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.37386510822888186 | validation: 0.36029704543823354]
	TIME [epoch: 2.67 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3767848175150727		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.3767848175150727 | validation: 0.5824220397287734]
	TIME [epoch: 2.68 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3922428512681019		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.3922428512681019 | validation: 0.363350235790618]
	TIME [epoch: 2.67 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37213599255765817		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.37213599255765817 | validation: 0.5009066712773027]
	TIME [epoch: 2.67 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36120153108981895		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.36120153108981895 | validation: 0.4207581590595795]
	TIME [epoch: 2.67 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36108810122160406		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.36108810122160406 | validation: 0.5000736462854104]
	TIME [epoch: 2.68 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3797064215347463		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.3797064215347463 | validation: 0.3700171940128869]
	TIME [epoch: 2.68 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3793282932263625		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.3793282932263625 | validation: 0.5194573017946674]
	TIME [epoch: 2.68 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36054986098845626		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.36054986098845626 | validation: 0.36979704035009314]
	TIME [epoch: 2.69 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35061303628236107		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.35061303628236107 | validation: 0.5279245010151651]
	TIME [epoch: 2.68 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3529476507362088		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.3529476507362088 | validation: 0.36294942423659426]
	TIME [epoch: 2.69 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3542601944181913		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.3542601944181913 | validation: 0.49781560710374406]
	TIME [epoch: 2.68 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3508929145063839		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.3508929145063839 | validation: 0.35352262297169196]
	TIME [epoch: 2.69 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35866377826583973		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.35866377826583973 | validation: 0.5041773963766486]
	TIME [epoch: 2.68 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35303152591070686		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.35303152591070686 | validation: 0.37660514667611833]
	TIME [epoch: 2.69 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35356352996834434		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.35356352996834434 | validation: 0.49860118613549764]
	TIME [epoch: 2.68 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35081701414367794		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.35081701414367794 | validation: 0.35069638305084505]
	TIME [epoch: 2.69 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3593102823011266		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.3593102823011266 | validation: 0.5153503934188043]
	TIME [epoch: 2.68 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3446488149293137		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.3446488149293137 | validation: 0.34560486008602365]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_737.pth
	Model improved!!!
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34260673918635776		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.34260673918635776 | validation: 0.4981232887442625]
	TIME [epoch: 2.67 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.331218028323332		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.331218028323332 | validation: 0.38032809342051555]
	TIME [epoch: 2.69 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32950862714610635		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.32950862714610635 | validation: 0.47179264163570384]
	TIME [epoch: 2.66 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33002229983063835		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.33002229983063835 | validation: 0.3608306366593461]
	TIME [epoch: 2.67 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3437061283107849		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.3437061283107849 | validation: 0.5024575965341356]
	TIME [epoch: 2.67 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3569138068409438		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.3569138068409438 | validation: 0.3465860539246323]
	TIME [epoch: 2.66 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36126126073491177		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.36126126073491177 | validation: 0.4346534795079487]
	TIME [epoch: 2.67 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3101044803551908		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.3101044803551908 | validation: 0.40394702635810253]
	TIME [epoch: 2.67 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31461260118263507		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.31461260118263507 | validation: 0.4242032926457848]
	TIME [epoch: 2.66 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3121541307163296		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.3121541307163296 | validation: 0.40540467227691296]
	TIME [epoch: 2.67 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31236104066185705		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.31236104066185705 | validation: 0.3846361206956982]
	TIME [epoch: 2.66 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31916276664648935		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.31916276664648935 | validation: 0.44550486226873587]
	TIME [epoch: 2.67 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32672195784951563		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.32672195784951563 | validation: 0.3345313994762551]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_750.pth
	Model improved!!!
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3375837561102488		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.3375837561102488 | validation: 0.5865842792626305]
	TIME [epoch: 2.69 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37771678947258397		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.37771678947258397 | validation: 0.3151524866637095]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_752.pth
	Model improved!!!
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33648735586361755		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.33648735586361755 | validation: 0.4536065454880149]
	TIME [epoch: 2.68 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3038916681721543		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.3038916681721543 | validation: 0.3788936257928621]
	TIME [epoch: 2.67 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3053553018031343		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.3053553018031343 | validation: 0.4365278224708453]
	TIME [epoch: 2.67 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3051293707203488		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.3051293707203488 | validation: 0.35561661511718357]
	TIME [epoch: 2.67 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30959222588799207		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.30959222588799207 | validation: 0.46561343157365587]
	TIME [epoch: 2.67 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32147425781019073		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.32147425781019073 | validation: 0.3166837660750045]
	TIME [epoch: 2.67 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3339291972354018		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.3339291972354018 | validation: 0.4771110353690324]
	TIME [epoch: 2.67 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31975840221659074		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.31975840221659074 | validation: 0.3491654190991537]
	TIME [epoch: 2.67 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30000902426816106		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.30000902426816106 | validation: 0.436822916073157]
	TIME [epoch: 3.47 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2962041571562593		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.2962041571562593 | validation: 0.32252989165016854]
	TIME [epoch: 2.68 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.315119819827395		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.315119819827395 | validation: 0.4606026069495676]
	TIME [epoch: 2.67 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30747501153691614		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.30747501153691614 | validation: 0.33502286097597356]
	TIME [epoch: 2.67 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31277314822609664		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.31277314822609664 | validation: 0.44530309042848765]
	TIME [epoch: 2.66 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30200209546094725		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.30200209546094725 | validation: 0.3674858347492464]
	TIME [epoch: 2.67 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29593632564361566		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.29593632564361566 | validation: 0.41483064961726196]
	TIME [epoch: 2.67 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28903624405637696		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.28903624405637696 | validation: 0.3453727867931208]
	TIME [epoch: 2.67 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2973568747106849		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.2973568747106849 | validation: 0.44911573980693853]
	TIME [epoch: 2.67 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2904963669195051		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.2904963669195051 | validation: 0.33163783640100697]
	TIME [epoch: 2.67 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29454998436520485		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.29454998436520485 | validation: 0.4905752543361899]
	TIME [epoch: 2.66 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30612028352444126		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.30612028352444126 | validation: 0.2991243842342152]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_772.pth
	Model improved!!!
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.303465199924824		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.303465199924824 | validation: 0.4449242979830699]
	TIME [epoch: 2.67 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2846165488057398		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.2846165488057398 | validation: 0.3318575053635111]
	TIME [epoch: 2.67 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28080725263765066		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.28080725263765066 | validation: 0.4081763661509228]
	TIME [epoch: 2.67 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2766067160177669		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.2766067160177669 | validation: 0.34878127166565137]
	TIME [epoch: 2.67 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28695217860809896		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.28695217860809896 | validation: 0.4230529475324987]
	TIME [epoch: 2.66 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28710113111409835		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.28710113111409835 | validation: 0.3295754809368281]
	TIME [epoch: 2.67 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2936689639259098		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.2936689639259098 | validation: 0.45299097966662266]
	TIME [epoch: 2.67 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2870205831749117		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.2870205831749117 | validation: 0.3142364815039491]
	TIME [epoch: 2.66 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2855029860756504		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.2855029860756504 | validation: 0.44309003989196916]
	TIME [epoch: 2.66 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28438955993614323		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.28438955993614323 | validation: 0.30608451607858445]
	TIME [epoch: 2.67 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28585763643127365		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.28585763643127365 | validation: 0.43636175575150243]
	TIME [epoch: 2.67 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28279107936830356		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.28279107936830356 | validation: 0.31977879233300427]
	TIME [epoch: 2.67 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27410452651281586		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.27410452651281586 | validation: 0.38870675328058074]
	TIME [epoch: 2.67 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2731057258505807		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.2731057258505807 | validation: 0.36510600861705184]
	TIME [epoch: 2.67 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2645390154917184		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.2645390154917184 | validation: 0.383166951998326]
	TIME [epoch: 2.67 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2634686255878372		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.2634686255878372 | validation: 0.35578629235418113]
	TIME [epoch: 2.66 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25935376414379446		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.25935376414379446 | validation: 0.44248671466898726]
	TIME [epoch: 2.67 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2859626580782736		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.2859626580782736 | validation: 0.2587764236710996]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_790.pth
	Model improved!!!
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3262118741275404		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.3262118741275404 | validation: 0.4215354604588722]
	TIME [epoch: 2.67 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2649559096728946		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.2649559096728946 | validation: 0.35726856311877797]
	TIME [epoch: 2.67 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2549799153797245		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.2549799153797245 | validation: 0.35679087305350743]
	TIME [epoch: 2.67 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25609737316550757		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.25609737316550757 | validation: 0.3774381134472841]
	TIME [epoch: 2.67 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2504169264395562		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.2504169264395562 | validation: 0.3067299241788062]
	TIME [epoch: 2.67 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24703330517036326		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.24703330517036326 | validation: 0.4196359576971158]
	TIME [epoch: 2.66 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2593348769965756		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.2593348769965756 | validation: 0.2709833079345965]
	TIME [epoch: 2.67 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2863009006036267		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.2863009006036267 | validation: 0.46755427406664674]
	TIME [epoch: 2.66 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2921586101842394		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.2921586101842394 | validation: 0.28965514860065367]
	TIME [epoch: 2.67 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2591082562906083		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.2591082562906083 | validation: 0.3880177515818603]
	TIME [epoch: 2.66 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2548102545292227		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.2548102545292227 | validation: 0.3249002700233974]
	TIME [epoch: 2.67 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2530330259281498		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.2530330259281498 | validation: 0.3710141499019071]
	TIME [epoch: 2.67 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25837032455867986		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.25837032455867986 | validation: 0.3197852620684562]
	TIME [epoch: 2.67 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2580843100768076		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.2580843100768076 | validation: 0.38726389022435753]
	TIME [epoch: 2.67 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24001044903570254		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.24001044903570254 | validation: 0.2986864829154758]
	TIME [epoch: 2.67 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25572741575704144		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.25572741575704144 | validation: 0.4219009212241968]
	TIME [epoch: 2.67 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25767976621229954		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.25767976621229954 | validation: 0.28325167486596875]
	TIME [epoch: 2.67 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25364944512241233		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.25364944512241233 | validation: 0.4245188239984297]
	TIME [epoch: 2.67 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25745901636720964		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.25745901636720964 | validation: 0.27545891170341935]
	TIME [epoch: 2.67 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2670380657409512		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.2670380657409512 | validation: 0.39072971928553873]
	TIME [epoch: 2.67 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24103382963274483		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.24103382963274483 | validation: 0.3136638817189996]
	TIME [epoch: 2.67 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23908382683644028		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.23908382683644028 | validation: 0.3623482556552977]
	TIME [epoch: 2.67 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23490490264299205		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.23490490264299205 | validation: 0.32235250774802515]
	TIME [epoch: 2.67 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23475951173095		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.23475951173095 | validation: 0.36837459026482733]
	TIME [epoch: 2.67 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23119730339586128		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.23119730339586128 | validation: 0.3222781222039571]
	TIME [epoch: 2.67 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23077961651192005		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.23077961651192005 | validation: 0.3803561466800924]
	TIME [epoch: 2.66 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23043645616742603		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.23043645616742603 | validation: 0.29940207776005795]
	TIME [epoch: 2.67 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24876805510619115		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.24876805510619115 | validation: 0.4644068002210837]
	TIME [epoch: 2.67 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28649484860189034		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.28649484860189034 | validation: 0.26126708094083073]
	TIME [epoch: 2.67 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2605876044056174		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.2605876044056174 | validation: 0.38201990866044583]
	TIME [epoch: 2.67 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22982344769250393		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.22982344769250393 | validation: 0.3356470959656954]
	TIME [epoch: 2.67 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22231367860498089		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.22231367860498089 | validation: 0.32530047284503394]
	TIME [epoch: 2.67 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22742203180966747		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.22742203180966747 | validation: 0.350167697651141]
	TIME [epoch: 2.67 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22570620025469226		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.22570620025469226 | validation: 0.31193818387456873]
	TIME [epoch: 2.67 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23031907759205575		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.23031907759205575 | validation: 0.3678747455289578]
	TIME [epoch: 2.67 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23060641385951136		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.23060641385951136 | validation: 0.28255396547951095]
	TIME [epoch: 2.66 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22575102820859871		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.22575102820859871 | validation: 0.37500157056530714]
	TIME [epoch: 2.67 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23097158460971337		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.23097158460971337 | validation: 0.2588086868021912]
	TIME [epoch: 2.66 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24152737290633655		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.24152737290633655 | validation: 0.41512375839295235]
	TIME [epoch: 2.66 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24954179530409637		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.24954179530409637 | validation: 0.23643711348328417]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_830.pth
	Model improved!!!
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2527670476984109		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.2527670476984109 | validation: 0.3855581083827356]
	TIME [epoch: 2.67 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22637987902758966		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.22637987902758966 | validation: 0.29587899424471237]
	TIME [epoch: 2.67 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21379604255719567		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.21379604255719567 | validation: 0.3414379981635827]
	TIME [epoch: 2.67 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21177821119411197		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.21177821119411197 | validation: 0.34424648926195867]
	TIME [epoch: 2.67 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21244858745971584		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.21244858745971584 | validation: 0.3081132196796144]
	TIME [epoch: 2.67 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20852715784746476		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.20852715784746476 | validation: 0.3430534363943549]
	TIME [epoch: 2.66 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20871267591697404		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.20871267591697404 | validation: 0.32736235848516215]
	TIME [epoch: 2.67 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21675231144473864		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.21675231144473864 | validation: 0.30408325805984737]
	TIME [epoch: 2.66 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.233827038430056		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.233827038430056 | validation: 0.3492971538427436]
	TIME [epoch: 2.67 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2203410157396186		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.2203410157396186 | validation: 0.27178314456118685]
	TIME [epoch: 2.66 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22560872272270274		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.22560872272270274 | validation: 0.3981709138710431]
	TIME [epoch: 2.67 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23202596442151646		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.23202596442151646 | validation: 0.22216652900839945]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_842.pth
	Model improved!!!
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24025003308259144		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.24025003308259144 | validation: 0.38063543918066356]
	TIME [epoch: 2.67 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21850081953535824		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.21850081953535824 | validation: 0.29286887034941184]
	TIME [epoch: 2.67 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21018724237700886		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.21018724237700886 | validation: 0.3369846502693249]
	TIME [epoch: 2.66 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2065852096668452		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.2065852096668452 | validation: 0.30557775606630716]
	TIME [epoch: 2.66 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20156047873729652		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.20156047873729652 | validation: 0.3238298051257436]
	TIME [epoch: 2.67 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1987710745802912		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.1987710745802912 | validation: 0.36149852807447624]
	TIME [epoch: 2.67 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20307715110422528		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.20307715110422528 | validation: 0.26380711567197235]
	TIME [epoch: 2.67 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20956479578789164		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.20956479578789164 | validation: 0.37940415498985786]
	TIME [epoch: 2.66 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22834837893974275		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.22834837893974275 | validation: 0.2532644691069453]
	TIME [epoch: 2.66 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23077884443725097		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.23077884443725097 | validation: 0.35575162042644]
	TIME [epoch: 2.67 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2122715283978136		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.2122715283978136 | validation: 0.28024564834470883]
	TIME [epoch: 2.66 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21528005286004154		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.21528005286004154 | validation: 0.3335569890482295]
	TIME [epoch: 2.66 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19840789182304314		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.19840789182304314 | validation: 0.29652066096848745]
	TIME [epoch: 2.66 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19297665115771934		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.19297665115771934 | validation: 0.33960877091327984]
	TIME [epoch: 2.66 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19717996761076761		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.19717996761076761 | validation: 0.2660240541023781]
	TIME [epoch: 2.66 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19815473375398704		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.19815473375398704 | validation: 0.33755646831233665]
	TIME [epoch: 2.66 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19687729106666887		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.19687729106666887 | validation: 0.26566151925175907]
	TIME [epoch: 2.66 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19943505213580273		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.19943505213580273 | validation: 0.359123080205296]
	TIME [epoch: 2.67 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20305297914804427		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.20305297914804427 | validation: 0.24831711552294725]
	TIME [epoch: 2.67 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21406073248530155		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.21406073248530155 | validation: 0.3472399023832504]
	TIME [epoch: 2.66 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20924462345852612		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.20924462345852612 | validation: 0.2771218288604886]
	TIME [epoch: 2.67 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20478328337259286		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.20478328337259286 | validation: 0.3198120326937224]
	TIME [epoch: 2.67 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19727529041505634		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.19727529041505634 | validation: 0.25527982621851114]
	TIME [epoch: 2.67 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1990242624220457		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.1990242624220457 | validation: 0.3410362122358825]
	TIME [epoch: 2.66 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19362972782840693		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.19362972782840693 | validation: 0.24541144408191828]
	TIME [epoch: 2.67 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19197346927672676		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.19197346927672676 | validation: 0.3368218379426007]
	TIME [epoch: 2.67 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19345029917178516		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.19345029917178516 | validation: 0.24952183433005795]
	TIME [epoch: 2.67 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19280820816562788		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.19280820816562788 | validation: 0.3595585388441387]
	TIME [epoch: 2.67 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21014157178814027		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.21014157178814027 | validation: 0.2135991492055628]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_871.pth
	Model improved!!!
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21308800543288095		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.21308800543288095 | validation: 0.33622031652029244]
	TIME [epoch: 2.68 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19484772951851903		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.19484772951851903 | validation: 0.27223548399923586]
	TIME [epoch: 2.67 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18385138484960473		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.18385138484960473 | validation: 0.3092502116872205]
	TIME [epoch: 2.68 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17985663622838316		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.17985663622838316 | validation: 0.2849153964290417]
	TIME [epoch: 2.67 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18137312075967246		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.18137312075967246 | validation: 0.3103202299587963]
	TIME [epoch: 2.67 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19001567891327406		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.19001567891327406 | validation: 0.2573947950472917]
	TIME [epoch: 2.67 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19741241549340052		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.19741241549340052 | validation: 0.3475208810521819]
	TIME [epoch: 2.67 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20026196773082		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.20026196773082 | validation: 0.2094963835686702]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_879.pth
	Model improved!!!
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21440223086332308		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.21440223086332308 | validation: 0.33122474550430864]
	TIME [epoch: 2.67 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18927453869351193		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.18927453869351193 | validation: 0.2685802946191624]
	TIME [epoch: 2.67 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17577522835284554		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.17577522835284554 | validation: 0.2836945537319327]
	TIME [epoch: 2.66 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17310867524429516		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.17310867524429516 | validation: 0.28482075784304334]
	TIME [epoch: 2.67 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17104086412768166		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.17104086412768166 | validation: 0.26585941557086346]
	TIME [epoch: 2.66 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17644124311840756		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.17644124311840756 | validation: 0.3268660910516868]
	TIME [epoch: 2.67 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18813134905567488		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.18813134905567488 | validation: 0.21538503660965633]
	TIME [epoch: 2.66 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1921384298534359		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.1921384298534359 | validation: 0.3411808285557504]
	TIME [epoch: 2.67 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18701140382466772		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.18701140382466772 | validation: 0.22003403182483672]
	TIME [epoch: 2.66 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18836511817544957		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.18836511817544957 | validation: 0.3580271097876206]
	TIME [epoch: 2.67 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18744476150690872		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.18744476150690872 | validation: 0.23895893730847653]
	TIME [epoch: 2.66 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1751869014849679		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.1751869014849679 | validation: 0.2861273568850713]
	TIME [epoch: 2.66 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17320543396555857		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.17320543396555857 | validation: 0.3035574589932857]
	TIME [epoch: 2.66 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.181609603244851		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.181609603244851 | validation: 0.24268925906330516]
	TIME [epoch: 2.66 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1773412473639522		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.1773412473639522 | validation: 0.31854024570769796]
	TIME [epoch: 2.66 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17149605686316302		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.17149605686316302 | validation: 0.23525754631014026]
	TIME [epoch: 2.67 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17786785054732585		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.17786785054732585 | validation: 0.3167807209070819]
	TIME [epoch: 2.67 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18683142976681716		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.18683142976681716 | validation: 0.24110420796257676]
	TIME [epoch: 2.66 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1836035521204999		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.1836035521204999 | validation: 0.2819322557277599]
	TIME [epoch: 2.67 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16923732266741176		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.16923732266741176 | validation: 0.2593648980212538]
	TIME [epoch: 2.67 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16349886625928167		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.16349886625928167 | validation: 0.27213808618780794]
	TIME [epoch: 2.67 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16354771063063733		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.16354771063063733 | validation: 0.2913759810159613]
	TIME [epoch: 2.68 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16585229963926287		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.16585229963926287 | validation: 0.2516592097962154]
	TIME [epoch: 2.68 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16442560774457732		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.16442560774457732 | validation: 0.29939352891678644]
	TIME [epoch: 2.68 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16828817670917978		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.16828817670917978 | validation: 0.21659958534908452]
	TIME [epoch: 2.68 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1777905913436202		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.1777905913436202 | validation: 0.3526046564376346]
	TIME [epoch: 2.68 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20287518017777498		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.20287518017777498 | validation: 0.21400809133388174]
	TIME [epoch: 2.68 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1926468979792042		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.1926468979792042 | validation: 0.27517863199273485]
	TIME [epoch: 2.69 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16136921631241663		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.16136921631241663 | validation: 0.29413239566223465]
	TIME [epoch: 2.68 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1599605770148046		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.1599605770148046 | validation: 0.23522346306958475]
	TIME [epoch: 2.68 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15907979997057312		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.15907979997057312 | validation: 0.31457392220730296]
	TIME [epoch: 2.68 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17014619409379594		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.17014619409379594 | validation: 0.20156621720963433]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_911.pth
	Model improved!!!
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18574439786242966		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.18574439786242966 | validation: 0.2919491594453115]
	TIME [epoch: 2.67 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16593867305088392		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.16593867305088392 | validation: 0.2579257404340342]
	TIME [epoch: 2.67 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15419356229941317		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.15419356229941317 | validation: 0.2426508201467435]
	TIME [epoch: 2.67 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15848035314882952		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.15848035314882952 | validation: 0.303962045081391]
	TIME [epoch: 2.67 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16141791413138165		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.16141791413138165 | validation: 0.1906980669341354]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_916.pth
	Model improved!!!
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17951478907000618		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.17951478907000618 | validation: 0.323515151673055]
	TIME [epoch: 2.68 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1755606787778996		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.1755606787778996 | validation: 0.24715861007684065]
	TIME [epoch: 2.68 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1620024870785911		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.1620024870785911 | validation: 0.2780956231045337]
	TIME [epoch: 2.68 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15202628144821778		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.15202628144821778 | validation: 0.2395587361216236]
	TIME [epoch: 2.68 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15238629270427928		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.15238629270427928 | validation: 0.26887620668070167]
	TIME [epoch: 2.68 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15391542253535656		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.15391542253535656 | validation: 0.2381755764069684]
	TIME [epoch: 2.68 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15237600294070675		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.15237600294070675 | validation: 0.24740540174515005]
	TIME [epoch: 2.68 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1516972071289576		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.1516972071289576 | validation: 0.2428804981307091]
	TIME [epoch: 2.67 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1492863925025188		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.1492863925025188 | validation: 0.279485719206941]
	TIME [epoch: 2.67 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15660114680139806		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.15660114680139806 | validation: 0.18816322314066344]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_926.pth
	Model improved!!!
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1997441746361449		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.1997441746361449 | validation: 0.3160779242161801]
	TIME [epoch: 2.67 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1743763432623417		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.1743763432623417 | validation: 0.20010264419537827]
	TIME [epoch: 2.67 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1622305760318058		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.1622305760318058 | validation: 0.29098147567490373]
	TIME [epoch: 2.66 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15535031629228224		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.15535031629228224 | validation: 0.20918961524637034]
	TIME [epoch: 2.67 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15299744842908303		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.15299744842908303 | validation: 0.2657053636102788]
	TIME [epoch: 2.67 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1528616125893323		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.1528616125893323 | validation: 0.22307751759011243]
	TIME [epoch: 2.67 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1533867785351236		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.1533867785351236 | validation: 0.2811366581929745]
	TIME [epoch: 2.67 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16304597035711546		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.16304597035711546 | validation: 0.2236647352071234]
	TIME [epoch: 2.67 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15583606154389626		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.15583606154389626 | validation: 0.2631814982285114]
	TIME [epoch: 2.67 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1521694964949424		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.1521694964949424 | validation: 0.21004124391447562]
	TIME [epoch: 2.68 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15733672636462148		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.15733672636462148 | validation: 0.30941333197413584]
	TIME [epoch: 2.68 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1596534142665597		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.1596534142665597 | validation: 0.20825715951419888]
	TIME [epoch: 2.69 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1494487335754924		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.1494487335754924 | validation: 0.25762773776806525]
	TIME [epoch: 2.67 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1470884337966992		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.1470884337966992 | validation: 0.23252709191525478]
	TIME [epoch: 2.68 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14254947947579508		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.14254947947579508 | validation: 0.2631516468742851]
	TIME [epoch: 2.67 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1452026911124494		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.1452026911124494 | validation: 0.23142305099228588]
	TIME [epoch: 2.68 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1435954238222966		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.1435954238222966 | validation: 0.2505222129477554]
	TIME [epoch: 2.67 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15090888243839282		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.15090888243839282 | validation: 0.21174692444760482]
	TIME [epoch: 2.68 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1588427508034483		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.1588427508034483 | validation: 0.2677784946506358]
	TIME [epoch: 2.67 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16133001844252262		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.16133001844252262 | validation: 0.1923703331759865]
	TIME [epoch: 2.68 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1554415068063475		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.1554415068063475 | validation: 0.30216334424615027]
	TIME [epoch: 2.67 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16205887600480523		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.16205887600480523 | validation: 0.19761044390676896]
	TIME [epoch: 2.67 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15701593116517734		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.15701593116517734 | validation: 0.2749635232106478]
	TIME [epoch: 2.67 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1471590650677845		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.1471590650677845 | validation: 0.22533204522320532]
	TIME [epoch: 2.68 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14272930929560598		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.14272930929560598 | validation: 0.23108707334284737]
	TIME [epoch: 2.68 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14275732084649648		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.14275732084649648 | validation: 0.21477879339654377]
	TIME [epoch: 2.68 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14773180062266364		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.14773180062266364 | validation: 0.26318004596028455]
	TIME [epoch: 2.68 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.145490529944692		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.145490529944692 | validation: 0.20304260522358664]
	TIME [epoch: 2.68 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14284370371272906		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.14284370371272906 | validation: 0.26243507946245986]
	TIME [epoch: 2.68 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14302379065612014		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.14302379065612014 | validation: 0.19078410425267503]
	TIME [epoch: 2.68 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14484262538537382		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.14484262538537382 | validation: 0.26392271557000296]
	TIME [epoch: 2.68 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1401830532271673		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.1401830532271673 | validation: 0.20554115765594227]
	TIME [epoch: 2.68 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14131685430850507		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.14131685430850507 | validation: 0.2689769793690835]
	TIME [epoch: 2.68 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14513436133870442		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.14513436133870442 | validation: 0.17472551404130798]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_960.pth
	Model improved!!!
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15215270738063916		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.15215270738063916 | validation: 0.27972269156281987]
	TIME [epoch: 2.67 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1494442477801498		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.1494442477801498 | validation: 0.18496551945027856]
	TIME [epoch: 2.66 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14726212974065372		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.14726212974065372 | validation: 0.2585284891237786]
	TIME [epoch: 2.66 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1436552209340712		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.1436552209340712 | validation: 0.22324074111342052]
	TIME [epoch: 2.67 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13793182183631292		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.13793182183631292 | validation: 0.230251781759001]
	TIME [epoch: 2.67 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.138479477232009		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.138479477232009 | validation: 0.22785135003770096]
	TIME [epoch: 2.67 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13345280607676394		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.13345280607676394 | validation: 0.22579539827004527]
	TIME [epoch: 2.67 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13351145846731657		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.13351145846731657 | validation: 0.216462871474125]
	TIME [epoch: 2.67 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13608807094157196		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.13608807094157196 | validation: 0.23914196653496173]
	TIME [epoch: 2.67 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13771586687318418		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.13771586687318418 | validation: 0.22447045438283741]
	TIME [epoch: 2.67 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13497553218678068		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.13497553218678068 | validation: 0.2447348697678586]
	TIME [epoch: 2.67 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13840747257868943		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.13840747257868943 | validation: 0.17714802735555316]
	TIME [epoch: 2.67 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1444178977846166		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.1444178977846166 | validation: 0.26294504042737454]
	TIME [epoch: 2.67 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14542960978561886		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.14542960978561886 | validation: 0.16934083451499718]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_974.pth
	Model improved!!!
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14769742816751155		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.14769742816751155 | validation: 0.27290469871978934]
	TIME [epoch: 2.68 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14324985496121123		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.14324985496121123 | validation: 0.20863717528074274]
	TIME [epoch: 2.68 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13079940507468746		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.13079940507468746 | validation: 0.1900521229190219]
	TIME [epoch: 2.68 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14016230810477992		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.14016230810477992 | validation: 0.25674086451575895]
	TIME [epoch: 2.67 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1342392161434281		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.1342392161434281 | validation: 0.18304603625521393]
	TIME [epoch: 2.67 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1374551285578726		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.1374551285578726 | validation: 0.2523248367797825]
	TIME [epoch: 2.66 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1423307517588315		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.1423307517588315 | validation: 0.19415576126571074]
	TIME [epoch: 2.67 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1338298174525364		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.1338298174525364 | validation: 0.2256029402622596]
	TIME [epoch: 2.67 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13027814266023174		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.13027814266023174 | validation: 0.18697369418075546]
	TIME [epoch: 2.67 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13130184804346542		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.13130184804346542 | validation: 0.2525366203284598]
	TIME [epoch: 2.67 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13165837919142037		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.13165837919142037 | validation: 0.1861403468701783]
	TIME [epoch: 2.67 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13137348821028805		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.13137348821028805 | validation: 0.22311172991180195]
	TIME [epoch: 2.67 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13173865314381586		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.13173865314381586 | validation: 0.19274656622946837]
	TIME [epoch: 2.67 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13696023204896232		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.13696023204896232 | validation: 0.24874838786920367]
	TIME [epoch: 2.67 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13580177870692972		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.13580177870692972 | validation: 0.1769439168860732]
	TIME [epoch: 2.67 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13715173417786133		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.13715173417786133 | validation: 0.2247780474513777]
	TIME [epoch: 2.67 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13004682662989656		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.13004682662989656 | validation: 0.17827034485971677]
	TIME [epoch: 2.67 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13252711022134767		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.13252711022134767 | validation: 0.24892243811811252]
	TIME [epoch: 2.67 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13001904139613768		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.13001904139613768 | validation: 0.17336951190589167]
	TIME [epoch: 2.67 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13291905134545748		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.13291905134545748 | validation: 0.2629178321850172]
	TIME [epoch: 2.67 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1401902509297529		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.1401902509297529 | validation: 0.16700427975459994]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_995.pth
	Model improved!!!
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1385051304742577		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.1385051304742577 | validation: 0.23040911316927765]
	TIME [epoch: 2.68 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12602071277754706		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.12602071277754706 | validation: 0.19029209580816542]
	TIME [epoch: 2.68 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12419512150962407		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.12419512150962407 | validation: 0.2098218925277384]
	TIME [epoch: 2.67 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12387014266502698		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.12387014266502698 | validation: 0.209683838208932]
	TIME [epoch: 2.67 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12580486937963925		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.12580486937963925 | validation: 0.19705260421620283]
	TIME [epoch: 2.67 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11805298566066279		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.11805298566066279 | validation: 0.18312522448638038]
	TIME [epoch: 178 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11969698013953047		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.11969698013953047 | validation: 0.2442387947244364]
	TIME [epoch: 5.72 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13358832934162324		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.13358832934162324 | validation: 0.17337490947855164]
	TIME [epoch: 5.71 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13252589018336342		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.13252589018336342 | validation: 0.23470600314597984]
	TIME [epoch: 5.71 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13279637952062626		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.13279637952062626 | validation: 0.16593312113402314]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1005.pth
	Model improved!!!
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13622097855565848		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.13622097855565848 | validation: 0.20455529965041627]
	TIME [epoch: 5.7 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12021399144477496		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.12021399144477496 | validation: 0.19092017968673536]
	TIME [epoch: 5.71 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12298516830713356		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.12298516830713356 | validation: 0.1988663605930294]
	TIME [epoch: 5.71 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12230954326199016		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.12230954326199016 | validation: 0.19945650780961577]
	TIME [epoch: 5.71 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12372250988743509		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.12372250988743509 | validation: 0.20615941611262092]
	TIME [epoch: 5.71 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1188643706232242		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.1188643706232242 | validation: 0.16939071973870584]
	TIME [epoch: 5.71 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1296332647459967		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.1296332647459967 | validation: 0.24777303524802574]
	TIME [epoch: 5.71 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13314823779322318		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.13314823779322318 | validation: 0.16127602320990392]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1013.pth
	Model improved!!!
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1292822193544559		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.1292822193544559 | validation: 0.1982178064255496]
	TIME [epoch: 5.71 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12076985364968526		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.12076985364968526 | validation: 0.2090685699858666]
	TIME [epoch: 5.7 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1162952840145201		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.1162952840145201 | validation: 0.1776779455816582]
	TIME [epoch: 5.71 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11919970826066502		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.11919970826066502 | validation: 0.23777569601875037]
	TIME [epoch: 5.7 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12446946722188511		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.12446946722188511 | validation: 0.16127235458866176]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1018.pth
	Model improved!!!
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12954655291029143		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.12954655291029143 | validation: 0.22943249883116368]
	TIME [epoch: 5.71 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12884797217140487		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.12884797217140487 | validation: 0.1726679680421422]
	TIME [epoch: 5.71 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1211629233886994		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.1211629233886994 | validation: 0.21245425077403632]
	TIME [epoch: 5.71 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11766607668955832		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.11766607668955832 | validation: 0.19751185712203778]
	TIME [epoch: 5.71 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11857755653084445		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.11857755653084445 | validation: 0.1706171061920032]
	TIME [epoch: 5.71 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11858121714830773		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.11858121714830773 | validation: 0.2521048456994489]
	TIME [epoch: 5.71 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13280373173936463		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.13280373173936463 | validation: 0.16176943231118493]
	TIME [epoch: 5.71 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13394506126322753		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.13394506126322753 | validation: 0.19353903803608785]
	TIME [epoch: 5.71 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11718555087700898		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.11718555087700898 | validation: 0.21293033724397012]
	TIME [epoch: 5.71 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11808050210131982		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.11808050210131982 | validation: 0.18560417697941456]
	TIME [epoch: 5.71 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11301252850522514		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.11301252850522514 | validation: 0.20159388055859148]
	TIME [epoch: 5.71 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11394720043168503		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.11394720043168503 | validation: 0.19326382408954124]
	TIME [epoch: 5.72 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11424678176444314		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.11424678176444314 | validation: 0.2104881788445165]
	TIME [epoch: 5.71 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11919966329548473		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.11919966329548473 | validation: 0.17827028466852546]
	TIME [epoch: 5.71 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11928894063558193		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.11928894063558193 | validation: 0.19257650336316282]
	TIME [epoch: 5.71 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12031975911373965		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.12031975911373965 | validation: 0.14883469819225825]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1034.pth
	Model improved!!!
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1246002038036842		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.1246002038036842 | validation: 0.25219933848926185]
	TIME [epoch: 5.71 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1380152171851139		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.1380152171851139 | validation: 0.16549212391880544]
	TIME [epoch: 5.71 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11655023261180336		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.11655023261180336 | validation: 0.19337597789498942]
	TIME [epoch: 5.71 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11125245692468141		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.11125245692468141 | validation: 0.1854408940267528]
	TIME [epoch: 5.71 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11269757669360812		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.11269757669360812 | validation: 0.18803575943333062]
	TIME [epoch: 5.71 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11500838046149411		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.11500838046149411 | validation: 0.18895202856330845]
	TIME [epoch: 5.71 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11062466955131944		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.11062466955131944 | validation: 0.1504727866946073]
	TIME [epoch: 5.71 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11706490884749247		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.11706490884749247 | validation: 0.22496899657349242]
	TIME [epoch: 5.71 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12148617053133563		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.12148617053133563 | validation: 0.15868452629935487]
	TIME [epoch: 5.71 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12130306500269585		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.12130306500269585 | validation: 0.1907329764347195]
	TIME [epoch: 5.7 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1180769687185267		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.1180769687185267 | validation: 0.16013521494299054]
	TIME [epoch: 5.71 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11795971008772767		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.11795971008772767 | validation: 0.1928443827077127]
	TIME [epoch: 5.7 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11508156513260794		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.11508156513260794 | validation: 0.1895646154909152]
	TIME [epoch: 5.71 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10985659463486179		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.10985659463486179 | validation: 0.16058177397794113]
	TIME [epoch: 5.71 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11711933558499354		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.11711933558499354 | validation: 0.22433411097539163]
	TIME [epoch: 5.71 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11827300879160067		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.11827300879160067 | validation: 0.13817919216781094]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1050.pth
	Model improved!!!
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11454816304370423		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.11454816304370423 | validation: 0.20155955281525137]
	TIME [epoch: 5.7 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1110615898100596		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.1110615898100596 | validation: 0.1456346040093414]
	TIME [epoch: 5.71 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11362821964274339		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.11362821964274339 | validation: 0.2145215277157199]
	TIME [epoch: 5.71 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11641473469594509		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.11641473469594509 | validation: 0.15300405297427733]
	TIME [epoch: 5.71 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1123174776068946		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.1123174776068946 | validation: 0.198499944648021]
	TIME [epoch: 5.71 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11292968417452182		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.11292968417452182 | validation: 0.1537474205319779]
	TIME [epoch: 5.71 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10863543564038086		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.10863543564038086 | validation: 0.19723580038903357]
	TIME [epoch: 5.71 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1085857106201458		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.1085857106201458 | validation: 0.17157427611644058]
	TIME [epoch: 5.71 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10761504842451043		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.10761504842451043 | validation: 0.17712778228625725]
	TIME [epoch: 5.7 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10810845684108883		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.10810845684108883 | validation: 0.1809773494929396]
	TIME [epoch: 5.7 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11519075761459842		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.11519075761459842 | validation: 0.17278434264120032]
	TIME [epoch: 5.71 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11131928310181124		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.11131928310181124 | validation: 0.16675413283134102]
	TIME [epoch: 5.7 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10822060862206939		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.10822060862206939 | validation: 0.2037601764048075]
	TIME [epoch: 5.71 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11072661531921582		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.11072661531921582 | validation: 0.16917159486044298]
	TIME [epoch: 5.7 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10602414080027511		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.10602414080027511 | validation: 0.19101861083927185]
	TIME [epoch: 5.71 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10861612600166712		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.10861612600166712 | validation: 0.12921702690085018]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1066.pth
	Model improved!!!
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12475427018608792		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.12475427018608792 | validation: 0.2255700241947372]
	TIME [epoch: 5.71 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12530256799463588		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.12530256799463588 | validation: 0.16234129357763838]
	TIME [epoch: 5.71 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10874475966861046		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.10874475966861046 | validation: 0.14535880553424793]
	TIME [epoch: 5.7 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11398841835486327		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.11398841835486327 | validation: 0.20791941779344736]
	TIME [epoch: 5.7 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11757793545043654		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.11757793545043654 | validation: 0.1618669740179732]
	TIME [epoch: 5.72 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10934457255963825		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.10934457255963825 | validation: 0.1484366067479046]
	TIME [epoch: 5.71 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1073515674713363		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.1073515674713363 | validation: 0.18924648714081516]
	TIME [epoch: 5.71 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11107953365758637		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.11107953365758637 | validation: 0.15239699963910708]
	TIME [epoch: 5.7 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11071133293930267		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.11071133293930267 | validation: 0.18214263406754969]
	TIME [epoch: 5.71 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10833141302555284		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.10833141302555284 | validation: 0.18065200428640538]
	TIME [epoch: 5.7 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10545827749432535		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.10545827749432535 | validation: 0.1655119792272742]
	TIME [epoch: 5.71 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10335884658047981		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.10335884658047981 | validation: 0.17974492280377888]
	TIME [epoch: 5.71 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10269259123727334		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.10269259123727334 | validation: 0.15544697114605902]
	TIME [epoch: 5.71 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10551710592644017		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.10551710592644017 | validation: 0.16976773868756675]
	TIME [epoch: 5.71 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10556324118859099		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.10556324118859099 | validation: 0.15320063567681558]
	TIME [epoch: 5.71 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10221638992794865		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.10221638992794865 | validation: 0.15262275558164382]
	TIME [epoch: 5.71 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10226534349488482		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.10226534349488482 | validation: 0.15655434996634118]
	TIME [epoch: 5.71 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0992615609129816		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.0992615609129816 | validation: 0.1702204775811449]
	TIME [epoch: 5.71 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10511662649308048		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.10511662649308048 | validation: 0.18175565647249353]
	TIME [epoch: 5.71 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10859257482300877		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.10859257482300877 | validation: 0.11919946923025432]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1086.pth
	Model improved!!!
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12804073481582331		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.12804073481582331 | validation: 0.20148788078395707]
	TIME [epoch: 5.72 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11478696571914014		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.11478696571914014 | validation: 0.14012207257960305]
	TIME [epoch: 5.71 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10383772795307103		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.10383772795307103 | validation: 0.15585347037064898]
	TIME [epoch: 5.71 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10149161593195363		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.10149161593195363 | validation: 0.15954052556368148]
	TIME [epoch: 5.71 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10138298214147483		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.10138298214147483 | validation: 0.1703982485491312]
	TIME [epoch: 5.7 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10202925607419756		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.10202925607419756 | validation: 0.16927285430588324]
	TIME [epoch: 5.71 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10188457659967023		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.10188457659967023 | validation: 0.1577467001493695]
	TIME [epoch: 5.7 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10042177547349626		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.10042177547349626 | validation: 0.1616009249297521]
	TIME [epoch: 5.71 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10092640822233172		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.10092640822233172 | validation: 0.1616822945670424]
	TIME [epoch: 5.71 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10005205040439367		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.10005205040439367 | validation: 0.14496544047530519]
	TIME [epoch: 5.7 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10480238330443496		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.10480238330443496 | validation: 0.1785580881828703]
	TIME [epoch: 5.71 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1076354122819283		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.1076354122819283 | validation: 0.12799281745013805]
	TIME [epoch: 5.7 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11074145056690018		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.11074145056690018 | validation: 0.18812882111135232]
	TIME [epoch: 5.7 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10472740556776071		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.10472740556776071 | validation: 0.13183458845710028]
	TIME [epoch: 5.7 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10370789754350529		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.10370789754350529 | validation: 0.1614089253258263]
	TIME [epoch: 5.7 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10059614052309261		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.10059614052309261 | validation: 0.15594127161093604]
	TIME [epoch: 5.71 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09769184999164462		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.09769184999164462 | validation: 0.16295899054554946]
	TIME [epoch: 5.71 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10172514497100651		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.10172514497100651 | validation: 0.1561641588552659]
	TIME [epoch: 5.7 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1006247580679986		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.1006247580679986 | validation: 0.1417154909526341]
	TIME [epoch: 5.7 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10522546429769147		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.10522546429769147 | validation: 0.18726634099077807]
	TIME [epoch: 5.7 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10585923669621827		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.10585923669621827 | validation: 0.12513642758230895]
	TIME [epoch: 5.7 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1032724268052463		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.1032724268052463 | validation: 0.1718953443883556]
	TIME [epoch: 5.71 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10071364714798037		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.10071364714798037 | validation: 0.1500149302912006]
	TIME [epoch: 5.71 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09682223213931149		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.09682223213931149 | validation: 0.14376829424509616]
	TIME [epoch: 5.7 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09847532057847243		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.09847532057847243 | validation: 0.13890770606467437]
	TIME [epoch: 5.7 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09734388911500098		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.09734388911500098 | validation: 0.15901849755760356]
	TIME [epoch: 5.7 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09990030448281272		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.09990030448281272 | validation: 0.15452898086755532]
	TIME [epoch: 5.71 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09721525127487929		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.09721525127487929 | validation: 0.15990193713094314]
	TIME [epoch: 5.7 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09760690636258453		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.09760690636258453 | validation: 0.11830361507550032]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1115.pth
	Model improved!!!
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10494171628435456		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.10494171628435456 | validation: 0.2075916580680453]
	TIME [epoch: 5.74 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11220183547726716		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.11220183547726716 | validation: 0.13667030700448485]
	TIME [epoch: 5.74 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1030398965484534		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.1030398965484534 | validation: 0.1463980100454437]
	TIME [epoch: 5.74 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09957247886324044		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.09957247886324044 | validation: 0.1820824851552043]
	TIME [epoch: 5.74 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1021906584981915		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.1021906584981915 | validation: 0.14206080087325326]
	TIME [epoch: 5.74 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10066803323457651		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.10066803323457651 | validation: 0.15281127017445226]
	TIME [epoch: 5.74 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09455137683298993		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.09455137683298993 | validation: 0.14550711428763027]
	TIME [epoch: 5.73 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09756292473830261		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.09756292473830261 | validation: 0.15406513852925127]
	TIME [epoch: 5.74 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10008495393815078		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.10008495393815078 | validation: 0.13540166041747012]
	TIME [epoch: 5.73 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0970755117056532		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.0970755117056532 | validation: 0.15459142126637038]
	TIME [epoch: 5.73 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09818872619668842		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.09818872619668842 | validation: 0.1518585740377868]
	TIME [epoch: 5.73 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0962290615418165		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.0962290615418165 | validation: 0.1616079997029597]
	TIME [epoch: 5.73 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09388340880204625		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.09388340880204625 | validation: 0.12133618114859686]
	TIME [epoch: 5.74 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1052213999965014		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.1052213999965014 | validation: 0.18819415712239518]
	TIME [epoch: 5.73 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09922876359613154		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.09922876359613154 | validation: 0.1313622273204549]
	TIME [epoch: 5.73 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09762265502847531		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.09762265502847531 | validation: 0.15890390224679643]
	TIME [epoch: 5.73 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09616835059446772		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.09616835059446772 | validation: 0.16780424382584158]
	TIME [epoch: 5.73 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09859724249428758		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.09859724249428758 | validation: 0.13881860828432918]
	TIME [epoch: 5.73 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09595939488888221		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.09595939488888221 | validation: 0.1507439990886976]
	TIME [epoch: 5.73 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09722739678762579		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.09722739678762579 | validation: 0.1476596577244056]
	TIME [epoch: 5.73 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09365803024382122		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.09365803024382122 | validation: 0.14522647012906573]
	TIME [epoch: 5.73 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09417947554344579		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.09417947554344579 | validation: 0.1442772006304232]
	TIME [epoch: 5.76 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09535532837058248		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.09535532837058248 | validation: 0.17369132052217284]
	TIME [epoch: 5.73 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09408527037304171		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.09408527037304171 | validation: 0.11570637449028101]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1139.pth
	Model improved!!!
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10524133707024845		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.10524133707024845 | validation: 0.1816239457862889]
	TIME [epoch: 5.71 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10931501201171365		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.10931501201171365 | validation: 0.13245277639729316]
	TIME [epoch: 5.71 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09511397954425262		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.09511397954425262 | validation: 0.13555583816908756]
	TIME [epoch: 5.72 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09628021540306887		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.09628021540306887 | validation: 0.1435091336671593]
	TIME [epoch: 5.71 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09180548136051744		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.09180548136051744 | validation: 0.1428411767285331]
	TIME [epoch: 5.73 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09224663579385439		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.09224663579385439 | validation: 0.14412297587035616]
	TIME [epoch: 5.71 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09518995113367176		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.09518995113367176 | validation: 0.14779174686614652]
	TIME [epoch: 5.71 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0911147164546094		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.0911147164546094 | validation: 0.14164126086850684]
	TIME [epoch: 5.71 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09330430489989575		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.09330430489989575 | validation: 0.13976452187627844]
	TIME [epoch: 5.71 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08993423223558095		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.08993423223558095 | validation: 0.14284232362801635]
	TIME [epoch: 5.72 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09220099957768195		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.09220099957768195 | validation: 0.1389104426196093]
	TIME [epoch: 5.71 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09308351331994137		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.09308351331994137 | validation: 0.1445657365883516]
	TIME [epoch: 5.72 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0938471704659056		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.0938471704659056 | validation: 0.13987913518709436]
	TIME [epoch: 5.71 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09514431805156216		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.09514431805156216 | validation: 0.1115951882712444]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1153.pth
	Model improved!!!
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10503184324510727		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.10503184324510727 | validation: 0.15399265710217824]
	TIME [epoch: 5.73 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09557156264234085		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.09557156264234085 | validation: 0.12750838644051996]
	TIME [epoch: 5.72 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09045301518100395		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.09045301518100395 | validation: 0.14634048853448087]
	TIME [epoch: 5.74 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09358726567572898		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.09358726567572898 | validation: 0.14921632778675276]
	TIME [epoch: 5.72 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09099910048187163		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.09099910048187163 | validation: 0.12032295123977706]
	TIME [epoch: 5.73 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0967614314703453		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.0967614314703453 | validation: 0.15646929775327073]
	TIME [epoch: 5.73 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09223459577413409		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.09223459577413409 | validation: 0.12619807171842232]
	TIME [epoch: 5.72 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09234309750800661		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.09234309750800661 | validation: 0.14064675099080154]
	TIME [epoch: 5.72 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08792907251609997		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.08792907251609997 | validation: 0.1345898029647075]
	TIME [epoch: 5.73 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08744032494597324		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.08744032494597324 | validation: 0.13403333476507706]
	TIME [epoch: 5.7 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08921554237121941		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.08921554237121941 | validation: 0.1301511870112789]
	TIME [epoch: 5.71 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09315820393717492		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.09315820393717492 | validation: 0.14300740732610304]
	TIME [epoch: 5.71 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08855847006530311		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.08855847006530311 | validation: 0.11804020903373101]
	TIME [epoch: 5.71 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0915052786293392		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.0915052786293392 | validation: 0.1425557754973581]
	TIME [epoch: 5.7 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09384527277355123		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.09384527277355123 | validation: 0.12240863986236872]
	TIME [epoch: 5.71 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09391485711024418		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.09391485711024418 | validation: 0.13538220931416606]
	TIME [epoch: 5.71 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.086409424269538		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.086409424269538 | validation: 0.141042015825293]
	TIME [epoch: 5.72 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08811109770160744		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.08811109770160744 | validation: 0.11838717526914273]
	TIME [epoch: 5.72 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09292082281890711		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.09292082281890711 | validation: 0.1649387355134746]
	TIME [epoch: 5.71 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09677726168225548		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.09677726168225548 | validation: 0.11775620324951164]
	TIME [epoch: 5.71 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08682100267368516		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.08682100267368516 | validation: 0.12942492678335835]
	TIME [epoch: 5.7 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0898158099841103		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.0898158099841103 | validation: 0.13388529578616346]
	TIME [epoch: 5.71 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08902810354298511		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.08902810354298511 | validation: 0.13395470857181163]
	TIME [epoch: 5.7 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08692919083253184		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.08692919083253184 | validation: 0.13272183845236638]
	TIME [epoch: 5.71 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08958486931862675		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.08958486931862675 | validation: 0.15704489304692867]
	TIME [epoch: 5.7 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09123863009540539		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.09123863009540539 | validation: 0.1213235397561109]
	TIME [epoch: 5.71 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0921656006623535		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.0921656006623535 | validation: 0.14373455572103078]
	TIME [epoch: 5.71 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09260709820171543		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.09260709820171543 | validation: 0.11839365545817829]
	TIME [epoch: 5.71 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09373402662913581		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.09373402662913581 | validation: 0.13547378152177703]
	TIME [epoch: 5.71 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08861896407241218		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.08861896407241218 | validation: 0.12672128382252068]
	TIME [epoch: 5.71 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08527451309712715		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.08527451309712715 | validation: 0.14074457091808387]
	TIME [epoch: 5.71 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08661104791435138		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.08661104791435138 | validation: 0.12629990421460685]
	TIME [epoch: 5.71 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08897143908415409		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.08897143908415409 | validation: 0.13934037414593772]
	TIME [epoch: 5.71 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08864480765039398		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.08864480765039398 | validation: 0.12520802358136676]
	TIME [epoch: 5.72 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09222126076850674		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.09222126076850674 | validation: 0.15841897493512364]
	TIME [epoch: 5.7 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09254338574293958		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.09254338574293958 | validation: 0.11092087240307154]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1189.pth
	Model improved!!!
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0905587020762202		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.0905587020762202 | validation: 0.12786695214453447]
	TIME [epoch: 5.73 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08542977541348214		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.08542977541348214 | validation: 0.13632304052794975]
	TIME [epoch: 5.73 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08534742872028633		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.08534742872028633 | validation: 0.11667037728679186]
	TIME [epoch: 5.73 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0862470467228155		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.0862470467228155 | validation: 0.12509186990967308]
	TIME [epoch: 5.73 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08511190482436075		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.08511190482436075 | validation: 0.13445147553866046]
	TIME [epoch: 5.73 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08761955923914083		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.08761955923914083 | validation: 0.12419147832357354]
	TIME [epoch: 5.73 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0843187880363875		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.0843187880363875 | validation: 0.12299470791806576]
	TIME [epoch: 5.73 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08585411214042309		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.08585411214042309 | validation: 0.1407414015488462]
	TIME [epoch: 5.73 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08510310874647235		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.08510310874647235 | validation: 0.11133603114958875]
	TIME [epoch: 5.72 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08993524225202176		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.08993524225202176 | validation: 0.15157534034356926]
	TIME [epoch: 5.72 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08977905318897321		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.08977905318897321 | validation: 0.11551143667921182]
	TIME [epoch: 5.73 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08732199593822508		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.08732199593822508 | validation: 0.1296722580263236]
	TIME [epoch: 5.72 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08720364411444192		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.08720364411444192 | validation: 0.12269781417433459]
	TIME [epoch: 5.71 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08604784838580055		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.08604784838580055 | validation: 0.12840067135913266]
	TIME [epoch: 5.72 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0843365421622925		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.0843365421622925 | validation: 0.13595981436576215]
	TIME [epoch: 5.71 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08627860766933416		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.08627860766933416 | validation: 0.1220102480568106]
	TIME [epoch: 5.7 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08757288128169044		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.08757288128169044 | validation: 0.1368036741520105]
	TIME [epoch: 5.73 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08312608450226616		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.08312608450226616 | validation: 0.133505396264067]
	TIME [epoch: 5.72 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08365375419531092		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.08365375419531092 | validation: 0.11519899903077507]
	TIME [epoch: 5.72 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08484255065543087		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.08484255065543087 | validation: 0.14292181277565247]
	TIME [epoch: 5.72 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08516632820861116		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.08516632820861116 | validation: 0.11733997600403666]
	TIME [epoch: 5.72 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08550673974341168		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.08550673974341168 | validation: 0.1332756163366892]
	TIME [epoch: 5.72 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0845223717497942		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.0845223717497942 | validation: 0.10840129444687117]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1212.pth
	Model improved!!!
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08625455830417306		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.08625455830417306 | validation: 0.14026271551670522]
	TIME [epoch: 5.72 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08440168476099384		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.08440168476099384 | validation: 0.13099658380612886]
	TIME [epoch: 5.71 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08587315121626993		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.08587315121626993 | validation: 0.10750889233079244]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1215.pth
	Model improved!!!
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08678524775343206		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.08678524775343206 | validation: 0.15178436340270735]
	TIME [epoch: 5.72 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08864944037575245		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.08864944037575245 | validation: 0.1196615916608918]
	TIME [epoch: 5.73 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08295269687065876		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.08295269687065876 | validation: 0.10975554669823384]
	TIME [epoch: 5.73 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08259214827261538		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.08259214827261538 | validation: 0.11826153711473585]
	TIME [epoch: 5.73 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0855209619042051		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.0855209619042051 | validation: 0.1407174067418636]
	TIME [epoch: 5.73 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0845115924780788		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.0845115924780788 | validation: 0.11326586366088241]
	TIME [epoch: 5.73 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08725945833707269		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.08725945833707269 | validation: 0.12759862552566686]
	TIME [epoch: 5.74 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08702288415568035		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.08702288415568035 | validation: 0.1191793584542344]
	TIME [epoch: 5.71 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08481204296511503		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.08481204296511503 | validation: 0.12558904331855442]
	TIME [epoch: 5.72 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08066631009073154		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.08066631009073154 | validation: 0.11031027266473133]
	TIME [epoch: 5.72 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08174648650213627		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.08174648650213627 | validation: 0.1312993047940952]
	TIME [epoch: 5.72 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.083075451602272		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.083075451602272 | validation: 0.12415561899812806]
	TIME [epoch: 5.72 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08211395276819124		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.08211395276819124 | validation: 0.11585669495813256]
	TIME [epoch: 5.72 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0816935748124668		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.0816935748124668 | validation: 0.11388253915548263]
	TIME [epoch: 5.72 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08436793638720026		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.08436793638720026 | validation: 0.13673133769403714]
	TIME [epoch: 5.72 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08068027997168799		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.08068027997168799 | validation: 0.1164593481772177]
	TIME [epoch: 5.72 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08415775552513843		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.08415775552513843 | validation: 0.12239828566269605]
	TIME [epoch: 5.72 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08031894436044082		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.08031894436044082 | validation: 0.1168096383406001]
	TIME [epoch: 5.72 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08382375760614964		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.08382375760614964 | validation: 0.13475971844494553]
	TIME [epoch: 5.72 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08553252449257068		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.08553252449257068 | validation: 0.10493709622894959]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1235.pth
	Model improved!!!
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08425515506358135		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.08425515506358135 | validation: 0.12941796891280993]
	TIME [epoch: 5.72 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08094666430374123		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.08094666430374123 | validation: 0.12512051701645927]
	TIME [epoch: 5.73 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08125493566675605		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.08125493566675605 | validation: 0.10622178716529061]
	TIME [epoch: 5.72 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08285870938686862		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.08285870938686862 | validation: 0.11402396299828284]
	TIME [epoch: 5.72 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07992428806092025		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.07992428806092025 | validation: 0.11888654710462708]
	TIME [epoch: 5.72 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08121271249211169		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.08121271249211169 | validation: 0.12994521848637006]
	TIME [epoch: 5.72 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08213438098177839		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.08213438098177839 | validation: 0.10044579465851289]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1242.pth
	Model improved!!!
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08049554729616354		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.08049554729616354 | validation: 0.12372567573424507]
	TIME [epoch: 5.7 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0817946673373474		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.0817946673373474 | validation: 0.1168774638406573]
	TIME [epoch: 5.71 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07963069511688506		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.07963069511688506 | validation: 0.11471633659443432]
	TIME [epoch: 5.7 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0807191469942546		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.0807191469942546 | validation: 0.12286761449169935]
	TIME [epoch: 5.71 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08159606195003413		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.08159606195003413 | validation: 0.11836014497869995]
	TIME [epoch: 5.71 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08297856746100678		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.08297856746100678 | validation: 0.12142270183573099]
	TIME [epoch: 5.71 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08144958744553701		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.08144958744553701 | validation: 0.1176384911205143]
	TIME [epoch: 5.71 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08041366613788417		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.08041366613788417 | validation: 0.11519804086266526]
	TIME [epoch: 5.71 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08014735710452282		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.08014735710452282 | validation: 0.11335666670916984]
	TIME [epoch: 5.7 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0817573219777493		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.0817573219777493 | validation: 0.1368230618052933]
	TIME [epoch: 5.71 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07897093981101652		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.07897093981101652 | validation: 0.11001161373547355]
	TIME [epoch: 6.68 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07870175113490539		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.07870175113490539 | validation: 0.13013440963562423]
	TIME [epoch: 5.72 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0821631259826592		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.0821631259826592 | validation: 0.10453096159192532]
	TIME [epoch: 5.7 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08112545971684859		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.08112545971684859 | validation: 0.12054366473611977]
	TIME [epoch: 5.7 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08112274558695205		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.08112274558695205 | validation: 0.10784493064508305]
	TIME [epoch: 5.7 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07742002894520934		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.07742002894520934 | validation: 0.10838061061870748]
	TIME [epoch: 5.71 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08121137713498822		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.08121137713498822 | validation: 0.13144493215374595]
	TIME [epoch: 5.7 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0806957174396656		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.0806957174396656 | validation: 0.0953934062450128]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1260.pth
	Model improved!!!
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08113900085743479		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.08113900085743479 | validation: 0.11588907202710558]
	TIME [epoch: 5.7 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08101679298508081		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.08101679298508081 | validation: 0.1497674727754095]
	TIME [epoch: 5.7 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08304841545692894		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.08304841545692894 | validation: 0.10148716555268017]
	TIME [epoch: 5.7 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08328336629421473		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.08328336629421473 | validation: 0.12232311781669347]
	TIME [epoch: 5.71 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08043489850780171		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.08043489850780171 | validation: 0.10405643148711252]
	TIME [epoch: 5.7 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07811777209560698		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.07811777209560698 | validation: 0.11169389435431985]
	TIME [epoch: 5.7 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07853221789738266		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.07853221789738266 | validation: 0.12845473209721617]
	TIME [epoch: 5.7 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07770513441320312		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.07770513441320312 | validation: 0.11072398533625752]
	TIME [epoch: 5.71 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07768450076316931		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.07768450076316931 | validation: 0.10987146594625082]
	TIME [epoch: 5.73 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07881701662832627		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.07881701662832627 | validation: 0.10533401013816744]
	TIME [epoch: 5.71 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07644107532361816		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.07644107532361816 | validation: 0.11148898160657034]
	TIME [epoch: 5.7 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07871231611668893		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.07871231611668893 | validation: 0.10984886164794552]
	TIME [epoch: 5.71 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07955282931981326		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.07955282931981326 | validation: 0.10774006464572071]
	TIME [epoch: 5.7 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0753998533190627		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.0753998533190627 | validation: 0.0983997651326748]
	TIME [epoch: 5.7 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08159425492322905		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.08159425492322905 | validation: 0.12255821610461684]
	TIME [epoch: 5.7 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0805541757501237		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.0805541757501237 | validation: 0.10975159053560885]
	TIME [epoch: 5.7 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07957165284438814		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.07957165284438814 | validation: 0.11357999347977561]
	TIME [epoch: 5.7 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07959494907266483		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.07959494907266483 | validation: 0.10298539834759302]
	TIME [epoch: 5.7 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07783925376392875		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.07783925376392875 | validation: 0.11395690737907244]
	TIME [epoch: 5.71 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08081228827053077		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.08081228827053077 | validation: 0.11123239826142142]
	TIME [epoch: 5.71 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.076717517371119		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.076717517371119 | validation: 0.11869370309511124]
	TIME [epoch: 5.7 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07765503007250721		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.07765503007250721 | validation: 0.10769875805418527]
	TIME [epoch: 5.7 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07768121399226599		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.07768121399226599 | validation: 0.10345781572672524]
	TIME [epoch: 5.7 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07733579315381396		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.07733579315381396 | validation: 0.11533906581645433]
	TIME [epoch: 5.71 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07880778808860335		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.07880778808860335 | validation: 0.09919581329893011]
	TIME [epoch: 5.7 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08012450348396548		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.08012450348396548 | validation: 0.12124196984999394]
	TIME [epoch: 5.71 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08210918788356217		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.08210918788356217 | validation: 0.10222865223870539]
	TIME [epoch: 5.72 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08089772571152064		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.08089772571152064 | validation: 0.1146627847195691]
	TIME [epoch: 5.7 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0745600569251752		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.0745600569251752 | validation: 0.09884342840169671]
	TIME [epoch: 5.7 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07958035201465942		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.07958035201465942 | validation: 0.11322105470609034]
	TIME [epoch: 5.71 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08024539268387695		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.08024539268387695 | validation: 0.09572043485288276]
	TIME [epoch: 5.69 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08141598465207292		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.08141598465207292 | validation: 0.10978521750895523]
	TIME [epoch: 5.71 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07529379037588725		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.07529379037588725 | validation: 0.10462118668377243]
	TIME [epoch: 5.7 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07534869008648075		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.07534869008648075 | validation: 0.09713827960611802]
	TIME [epoch: 5.7 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07668638675125566		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.07668638675125566 | validation: 0.11546189230693538]
	TIME [epoch: 5.7 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07503093964625307		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.07503093964625307 | validation: 0.10979720190244278]
	TIME [epoch: 5.7 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07652112502056746		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.07652112502056746 | validation: 0.10393439265174087]
	TIME [epoch: 5.7 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07498581706608466		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.07498581706608466 | validation: 0.11154368525002485]
	TIME [epoch: 5.7 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07731803991520654		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.07731803991520654 | validation: 0.10379270117293188]
	TIME [epoch: 5.71 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07449805708735738		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.07449805708735738 | validation: 0.10904132607617566]
	TIME [epoch: 5.7 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07467242120193561		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.07467242120193561 | validation: 0.10559752259612372]
	TIME [epoch: 5.71 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07337741715619216		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.07337741715619216 | validation: 0.10824472494557283]
	TIME [epoch: 5.7 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07609008837383595		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.07609008837383595 | validation: 0.1153015998386735]
	TIME [epoch: 5.71 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07477112965662384		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.07477112965662384 | validation: 0.09938053958836585]
	TIME [epoch: 5.7 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07777930200540428		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.07777930200540428 | validation: 0.11290928990114663]
	TIME [epoch: 5.7 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07557477020888193		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.07557477020888193 | validation: 0.09826787566118972]
	TIME [epoch: 5.7 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07764633378636945		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.07764633378636945 | validation: 0.11431451145640814]
	TIME [epoch: 5.7 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0755248667089915		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.0755248667089915 | validation: 0.11603858118291302]
	TIME [epoch: 5.7 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07972904952453781		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.07972904952453781 | validation: 0.09322270951674617]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1309.pth
	Model improved!!!
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07510606709034943		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.07510606709034943 | validation: 0.11549343790838101]
	TIME [epoch: 5.7 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07570522827202489		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.07570522827202489 | validation: 0.10463247619701242]
	TIME [epoch: 5.69 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07387875990030005		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.07387875990030005 | validation: 0.11132675707581163]
	TIME [epoch: 5.7 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07337586939738856		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.07337586939738856 | validation: 0.10804576259949988]
	TIME [epoch: 5.69 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0763699227876741		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.0763699227876741 | validation: 0.11424697729048568]
	TIME [epoch: 5.7 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07615759938358059		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.07615759938358059 | validation: 0.09811031475671717]
	TIME [epoch: 5.7 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07446811737263435		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.07446811737263435 | validation: 0.10686501103455948]
	TIME [epoch: 5.7 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0738604050234027		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.0738604050234027 | validation: 0.09542701419127876]
	TIME [epoch: 5.69 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0771729787124553		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.0771729787124553 | validation: 0.1137620833744184]
	TIME [epoch: 5.7 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07599070035610485		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.07599070035610485 | validation: 0.10088638321618759]
	TIME [epoch: 5.69 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07358633250002843		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.07358633250002843 | validation: 0.09798528868470352]
	TIME [epoch: 5.7 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07315675599301567		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.07315675599301567 | validation: 0.10181874848506718]
	TIME [epoch: 5.7 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07488840822591084		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.07488840822591084 | validation: 0.10115136904541494]
	TIME [epoch: 5.7 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07143946109344854		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.07143946109344854 | validation: 0.10893345276652373]
	TIME [epoch: 5.7 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07748003482207907		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.07748003482207907 | validation: 0.09714968333233481]
	TIME [epoch: 5.7 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07471420358614099		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.07471420358614099 | validation: 0.10920350409065334]
	TIME [epoch: 5.7 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0718164730171836		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.0718164730171836 | validation: 0.10250403436957659]
	TIME [epoch: 5.7 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07479163877228234		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.07479163877228234 | validation: 0.11567872224133047]
	TIME [epoch: 5.7 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07440870008959463		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.07440870008959463 | validation: 0.09435052552223847]
	TIME [epoch: 5.7 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07447505699889365		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.07447505699889365 | validation: 0.10458882424555692]
	TIME [epoch: 5.69 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07459757870358122		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.07459757870358122 | validation: 0.10411724556088822]
	TIME [epoch: 5.69 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07522276690324191		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.07522276690324191 | validation: 0.09853285402418258]
	TIME [epoch: 5.7 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0753646617084057		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.0753646617084057 | validation: 0.09651419943182657]
	TIME [epoch: 5.7 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07555238450830468		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.07555238450830468 | validation: 0.096309516925066]
	TIME [epoch: 5.69 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07460546794843609		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.07460546794843609 | validation: 0.10499624897437483]
	TIME [epoch: 5.7 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07383965101456698		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.07383965101456698 | validation: 0.0963691460082746]
	TIME [epoch: 5.69 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0770917591601552		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.0770917591601552 | validation: 0.11308620794584534]
	TIME [epoch: 5.7 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07612820477440811		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.07612820477440811 | validation: 0.09178830429623057]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1337.pth
	Model improved!!!
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0736237467149295		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.0736237467149295 | validation: 0.10048085859894383]
	TIME [epoch: 5.7 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07152947480932019		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.07152947480932019 | validation: 0.10750518353720225]
	TIME [epoch: 5.69 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07204909279374741		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.07204909279374741 | validation: 0.09637296849673645]
	TIME [epoch: 5.7 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07282778801609352		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.07282778801609352 | validation: 0.08975218526633336]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1341.pth
	Model improved!!!
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07114397015555579		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.07114397015555579 | validation: 0.10124270920817424]
	TIME [epoch: 5.7 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07268395136708108		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.07268395136708108 | validation: 0.0987340520764147]
	TIME [epoch: 5.69 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07190761367699745		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.07190761367699745 | validation: 0.10068933965811672]
	TIME [epoch: 5.7 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0714417067450594		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.0714417067450594 | validation: 0.09597174457333113]
	TIME [epoch: 5.69 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0745523937647546		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.0745523937647546 | validation: 0.09908209661509237]
	TIME [epoch: 5.7 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07221990345935839		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.07221990345935839 | validation: 0.10536899866222266]
	TIME [epoch: 5.69 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07412904752349393		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.07412904752349393 | validation: 0.09839078286716775]
	TIME [epoch: 5.7 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07308868232041812		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.07308868232041812 | validation: 0.11121440697055616]
	TIME [epoch: 5.7 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07259478024798197		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.07259478024798197 | validation: 0.09606266757127309]
	TIME [epoch: 5.7 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07256950257569894		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.07256950257569894 | validation: 0.09622062907635849]
	TIME [epoch: 5.7 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07288272252476084		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.07288272252476084 | validation: 0.1029105279924114]
	TIME [epoch: 5.7 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07198199030268482		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.07198199030268482 | validation: 0.09704057201359056]
	TIME [epoch: 5.7 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06998246638884612		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.06998246638884612 | validation: 0.09977204912846575]
	TIME [epoch: 5.69 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07398033559271966		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.07398033559271966 | validation: 0.0946078037936034]
	TIME [epoch: 5.69 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07291380255445368		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.07291380255445368 | validation: 0.10546446266924406]
	TIME [epoch: 5.7 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07374132215418602		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.07374132215418602 | validation: 0.08674857694832555]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1357.pth
	Model improved!!!
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0726075820981526		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.0726075820981526 | validation: 0.10512411274733835]
	TIME [epoch: 5.71 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07152241447432538		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.07152241447432538 | validation: 0.1003465637973088]
	TIME [epoch: 5.71 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07074108466761858		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.07074108466761858 | validation: 0.09460188324651404]
	TIME [epoch: 5.71 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07233163394919882		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.07233163394919882 | validation: 0.1259132233994202]
	TIME [epoch: 5.7 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08099401955922644		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.08099401955922644 | validation: 0.0928454295492242]
	TIME [epoch: 5.7 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07228310464006872		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.07228310464006872 | validation: 0.08975830989792523]
	TIME [epoch: 5.7 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07310152962307924		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.07310152962307924 | validation: 0.09931505507688518]
	TIME [epoch: 5.7 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06946424622135954		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.06946424622135954 | validation: 0.09848109967255285]
	TIME [epoch: 5.7 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0733244797487218		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.0733244797487218 | validation: 0.09435725201017289]
	TIME [epoch: 5.69 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0715218868942189		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.0715218868942189 | validation: 0.10267661452704574]
	TIME [epoch: 5.7 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07211586273382845		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.07211586273382845 | validation: 0.10357933090290568]
	TIME [epoch: 5.7 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06958155738078359		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.06958155738078359 | validation: 0.09920617721652535]
	TIME [epoch: 5.7 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06934900491555411		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.06934900491555411 | validation: 0.10043610511477818]
	TIME [epoch: 5.7 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07092774125671357		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.07092774125671357 | validation: 0.10562266083459171]
	TIME [epoch: 5.69 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07042670337028438		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.07042670337028438 | validation: 0.09587313402622955]
	TIME [epoch: 5.7 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07136325393641388		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.07136325393641388 | validation: 0.09975451681408808]
	TIME [epoch: 5.69 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07213194838996854		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.07213194838996854 | validation: 0.09716992756222241]
	TIME [epoch: 5.7 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06976616597209394		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.06976616597209394 | validation: 0.0957359474821222]
	TIME [epoch: 5.69 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07077865901395419		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.07077865901395419 | validation: 0.09382778023165839]
	TIME [epoch: 5.7 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07054909924582518		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.07054909924582518 | validation: 0.08881108258621057]
	TIME [epoch: 5.7 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07181964493774143		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.07181964493774143 | validation: 0.08718549104602975]
	TIME [epoch: 5.7 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07162832750592994		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.07162832750592994 | validation: 0.09972064642937778]
	TIME [epoch: 5.71 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07116952257373355		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.07116952257373355 | validation: 0.1064463684200685]
	TIME [epoch: 5.7 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07061219596892578		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.07061219596892578 | validation: 0.08889426422411789]
	TIME [epoch: 5.7 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07296594016598829		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.07296594016598829 | validation: 0.09364996705677653]
	TIME [epoch: 5.7 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0687786707473576		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.0687786707473576 | validation: 0.0954484736466113]
	TIME [epoch: 5.71 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06997209633385883		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.06997209633385883 | validation: 0.0914220970354185]
	TIME [epoch: 5.7 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07066025083046476		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.07066025083046476 | validation: 0.09719792185777508]
	TIME [epoch: 5.7 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06805211275237598		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.06805211275237598 | validation: 0.09129507148489886]
	TIME [epoch: 5.7 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06904079589045853		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.06904079589045853 | validation: 0.09853084169941755]
	TIME [epoch: 5.69 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06944060699430861		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.06944060699430861 | validation: 0.0972992452982851]
	TIME [epoch: 5.7 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0699493212235756		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.0699493212235756 | validation: 0.10142406689774153]
	TIME [epoch: 5.7 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06856677284634496		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.06856677284634496 | validation: 0.10008614524487976]
	TIME [epoch: 5.7 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07062700511527167		[learning rate: 8.6788e-05]
	Learning Rate: 8.67884e-05
	LOSS [training: 0.07062700511527167 | validation: 0.09854860681903135]
	TIME [epoch: 5.7 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07250271214768399		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.07250271214768399 | validation: 0.09060168838719157]
	TIME [epoch: 5.69 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06787134753555993		[learning rate: 8.6176e-05]
	Learning Rate: 8.61757e-05
	LOSS [training: 0.06787134753555993 | validation: 0.10315653404846375]
	TIME [epoch: 5.7 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0696674973528374		[learning rate: 8.5871e-05]
	Learning Rate: 8.5871e-05
	LOSS [training: 0.0696674973528374 | validation: 0.0936184141933164]
	TIME [epoch: 5.7 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07070624317267264		[learning rate: 8.5567e-05]
	Learning Rate: 8.55673e-05
	LOSS [training: 0.07070624317267264 | validation: 0.09371011047603092]
	TIME [epoch: 5.7 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06857096668897249		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.06857096668897249 | validation: 0.09800947167550843]
	TIME [epoch: 5.7 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06988475975407728		[learning rate: 8.4963e-05]
	Learning Rate: 8.49632e-05
	LOSS [training: 0.06988475975407728 | validation: 0.09401083689670803]
	TIME [epoch: 5.7 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06902600257899047		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.06902600257899047 | validation: 0.09577360408700664]
	TIME [epoch: 5.7 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06906710380218105		[learning rate: 8.4363e-05]
	Learning Rate: 8.43634e-05
	LOSS [training: 0.06906710380218105 | validation: 0.09805980113515679]
	TIME [epoch: 5.7 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06776581351144406		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.06776581351144406 | validation: 0.0885263818180073]
	TIME [epoch: 5.7 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06942950584348571		[learning rate: 8.3768e-05]
	Learning Rate: 8.37678e-05
	LOSS [training: 0.06942950584348571 | validation: 0.09866160462809176]
	TIME [epoch: 5.7 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06975422453200539		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.06975422453200539 | validation: 0.09339784581987401]
	TIME [epoch: 5.7 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07052059300110126		[learning rate: 8.3176e-05]
	Learning Rate: 8.31764e-05
	LOSS [training: 0.07052059300110126 | validation: 0.08728869378628698]
	TIME [epoch: 5.71 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06806893084812042		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.06806893084812042 | validation: 0.08754204360868123]
	TIME [epoch: 5.7 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07005756480787531		[learning rate: 8.2589e-05]
	Learning Rate: 8.25892e-05
	LOSS [training: 0.07005756480787531 | validation: 0.09071252657108737]
	TIME [epoch: 5.7 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06976541009502786		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.06976541009502786 | validation: 0.10799542646609153]
	TIME [epoch: 5.7 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07085846179618842		[learning rate: 8.2006e-05]
	Learning Rate: 8.20061e-05
	LOSS [training: 0.07085846179618842 | validation: 0.083394170832174]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1407.pth
	Model improved!!!
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0684045338949293		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.0684045338949293 | validation: 0.09129398986055491]
	TIME [epoch: 5.7 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07208160224249584		[learning rate: 8.1427e-05]
	Learning Rate: 8.14272e-05
	LOSS [training: 0.07208160224249584 | validation: 0.09197721666345848]
	TIME [epoch: 5.69 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07048543489321993		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.07048543489321993 | validation: 0.09577438247880382]
	TIME [epoch: 5.7 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07119364198417474		[learning rate: 8.0852e-05]
	Learning Rate: 8.08523e-05
	LOSS [training: 0.07119364198417474 | validation: 0.09115807809257481]
	TIME [epoch: 5.7 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06892898717179809		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.06892898717179809 | validation: 0.10089142028408966]
	TIME [epoch: 5.69 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06838419329556923		[learning rate: 8.0281e-05]
	Learning Rate: 8.02815e-05
	LOSS [training: 0.06838419329556923 | validation: 0.0935394582356495]
	TIME [epoch: 5.69 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06699940607609786		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.06699940607609786 | validation: 0.08724800751813822]
	TIME [epoch: 5.7 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06667784312531012		[learning rate: 7.9715e-05]
	Learning Rate: 7.97147e-05
	LOSS [training: 0.06667784312531012 | validation: 0.09978481130085474]
	TIME [epoch: 5.7 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06813863976805813		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.06813863976805813 | validation: 0.088989206434352]
	TIME [epoch: 5.69 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06767723653893995		[learning rate: 7.9152e-05]
	Learning Rate: 7.9152e-05
	LOSS [training: 0.06767723653893995 | validation: 0.09183586920805435]
	TIME [epoch: 5.69 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0683674873775096		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.0683674873775096 | validation: 0.10916695393286564]
	TIME [epoch: 5.7 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07008523672175403		[learning rate: 7.8593e-05]
	Learning Rate: 7.85931e-05
	LOSS [training: 0.07008523672175403 | validation: 0.08918317435774281]
	TIME [epoch: 5.71 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0682093337568002		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.0682093337568002 | validation: 0.09743573365792303]
	TIME [epoch: 5.7 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07078969643255549		[learning rate: 7.8038e-05]
	Learning Rate: 7.80383e-05
	LOSS [training: 0.07078969643255549 | validation: 0.09285130842683059]
	TIME [epoch: 5.7 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06698880242090799		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.06698880242090799 | validation: 0.09059140063002691]
	TIME [epoch: 5.7 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06792934959453735		[learning rate: 7.7487e-05]
	Learning Rate: 7.74873e-05
	LOSS [training: 0.06792934959453735 | validation: 0.09251022850917706]
	TIME [epoch: 5.7 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06698106177626474		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.06698106177626474 | validation: 0.08398105873320204]
	TIME [epoch: 5.7 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0672346750735545		[learning rate: 7.694e-05]
	Learning Rate: 7.69403e-05
	LOSS [training: 0.0672346750735545 | validation: 0.08998446185117669]
	TIME [epoch: 5.7 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0687075254886357		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.0687075254886357 | validation: 0.08532752577907388]
	TIME [epoch: 5.7 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06890251838085845		[learning rate: 7.6397e-05]
	Learning Rate: 7.63971e-05
	LOSS [training: 0.06890251838085845 | validation: 0.0951949780658568]
	TIME [epoch: 5.69 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0685470850388331		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.0685470850388331 | validation: 0.08945636338518244]
	TIME [epoch: 5.7 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06624451413241075		[learning rate: 7.5858e-05]
	Learning Rate: 7.58578e-05
	LOSS [training: 0.06624451413241075 | validation: 0.0852357378698851]
	TIME [epoch: 5.7 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06860016355185318		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.06860016355185318 | validation: 0.09500602429457605]
	TIME [epoch: 5.7 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06820636312650571		[learning rate: 7.5322e-05]
	Learning Rate: 7.53222e-05
	LOSS [training: 0.06820636312650571 | validation: 0.09295071175239902]
	TIME [epoch: 5.71 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06897455418224867		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.06897455418224867 | validation: 0.0946476888990258]
	TIME [epoch: 5.7 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06841560908856269		[learning rate: 7.479e-05]
	Learning Rate: 7.47905e-05
	LOSS [training: 0.06841560908856269 | validation: 0.09757752299589569]
	TIME [epoch: 5.7 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06679692973665685		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.06679692973665685 | validation: 0.10565374904793684]
	TIME [epoch: 5.7 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06899719242829887		[learning rate: 7.4262e-05]
	Learning Rate: 7.42625e-05
	LOSS [training: 0.06899719242829887 | validation: 0.08111770806648487]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1435.pth
	Model improved!!!
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06790057123865355		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.06790057123865355 | validation: 0.09190102716346117]
	TIME [epoch: 5.7 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06637917534546457		[learning rate: 7.3738e-05]
	Learning Rate: 7.37382e-05
	LOSS [training: 0.06637917534546457 | validation: 0.09097630998235869]
	TIME [epoch: 5.7 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07239147459211064		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.07239147459211064 | validation: 0.08777636927853548]
	TIME [epoch: 5.7 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06668706929351656		[learning rate: 7.3218e-05]
	Learning Rate: 7.32176e-05
	LOSS [training: 0.06668706929351656 | validation: 0.08923194790219748]
	TIME [epoch: 5.7 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06689583331151194		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.06689583331151194 | validation: 0.09527021536527423]
	TIME [epoch: 5.7 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06914908664363471		[learning rate: 7.2701e-05]
	Learning Rate: 7.27007e-05
	LOSS [training: 0.06914908664363471 | validation: 0.08891982719394702]
	TIME [epoch: 5.7 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06576402601212353		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.06576402601212353 | validation: 0.08713203591945368]
	TIME [epoch: 5.7 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06783112257501685		[learning rate: 7.2187e-05]
	Learning Rate: 7.21874e-05
	LOSS [training: 0.06783112257501685 | validation: 0.08944471668203716]
	TIME [epoch: 5.7 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06797939624379314		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.06797939624379314 | validation: 0.10149691505299535]
	TIME [epoch: 5.7 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06638116519779588		[learning rate: 7.1678e-05]
	Learning Rate: 7.16778e-05
	LOSS [training: 0.06638116519779588 | validation: 0.09953003151771583]
	TIME [epoch: 5.7 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06537455475674024		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.06537455475674024 | validation: 0.09725139880335675]
	TIME [epoch: 5.69 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06388306946500116		[learning rate: 7.1172e-05]
	Learning Rate: 7.11718e-05
	LOSS [training: 0.06388306946500116 | validation: 0.10025620555433395]
	TIME [epoch: 5.7 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0677160497885258		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.0677160497885258 | validation: 0.08523698869776258]
	TIME [epoch: 5.7 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06897666389040623		[learning rate: 7.0669e-05]
	Learning Rate: 7.06693e-05
	LOSS [training: 0.06897666389040623 | validation: 0.09310648851050926]
	TIME [epoch: 5.7 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06722685062378402		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.06722685062378402 | validation: 0.08035013003749786]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1450.pth
	Model improved!!!
EPOCH 1451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06795432176665987		[learning rate: 7.017e-05]
	Learning Rate: 7.01704e-05
	LOSS [training: 0.06795432176665987 | validation: 0.08952629172542781]
	TIME [epoch: 5.7 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06815540046202814		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.06815540046202814 | validation: 0.09173421280577913]
	TIME [epoch: 5.7 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06700202496021414		[learning rate: 6.9675e-05]
	Learning Rate: 6.9675e-05
	LOSS [training: 0.06700202496021414 | validation: 0.08877730157787549]
	TIME [epoch: 5.71 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06578706714225695		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.06578706714225695 | validation: 0.1002263176184451]
	TIME [epoch: 5.7 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06486146327367291		[learning rate: 6.9183e-05]
	Learning Rate: 6.91831e-05
	LOSS [training: 0.06486146327367291 | validation: 0.09092948268209704]
	TIME [epoch: 5.7 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06469668112406692		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.06469668112406692 | validation: 0.08571149451201208]
	TIME [epoch: 5.7 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0655436211670781		[learning rate: 6.8695e-05]
	Learning Rate: 6.86947e-05
	LOSS [training: 0.0655436211670781 | validation: 0.09559272195967222]
	TIME [epoch: 5.7 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06838983800699655		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.06838983800699655 | validation: 0.08491179068610805]
	TIME [epoch: 5.7 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06900880469298558		[learning rate: 6.821e-05]
	Learning Rate: 6.82097e-05
	LOSS [training: 0.06900880469298558 | validation: 0.09096065160992685]
	TIME [epoch: 5.7 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.066371257332087		[learning rate: 6.7969e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.066371257332087 | validation: 0.09303063146587823]
	TIME [epoch: 5.7 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06783427625406946		[learning rate: 6.7728e-05]
	Learning Rate: 6.77282e-05
	LOSS [training: 0.06783427625406946 | validation: 0.09309028972355296]
	TIME [epoch: 5.7 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0668704986940374		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.0668704986940374 | validation: 0.08608668094592906]
	TIME [epoch: 5.7 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06570104175807327		[learning rate: 6.725e-05]
	Learning Rate: 6.725e-05
	LOSS [training: 0.06570104175807327 | validation: 0.08400691580365571]
	TIME [epoch: 5.7 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06715073376227756		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.06715073376227756 | validation: 0.08839658538432056]
	TIME [epoch: 5.69 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06784185756999814		[learning rate: 6.6775e-05]
	Learning Rate: 6.67752e-05
	LOSS [training: 0.06784185756999814 | validation: 0.0810322732288073]
	TIME [epoch: 5.7 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06668321766695937		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.06668321766695937 | validation: 0.08668026749449709]
	TIME [epoch: 5.7 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06847660289200298		[learning rate: 6.6304e-05]
	Learning Rate: 6.63038e-05
	LOSS [training: 0.06847660289200298 | validation: 0.08011035761904445]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1467.pth
	Model improved!!!
EPOCH 1468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06497891501650309		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.06497891501650309 | validation: 0.09256370850924793]
	TIME [epoch: 5.73 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06391400845579309		[learning rate: 6.5836e-05]
	Learning Rate: 6.58357e-05
	LOSS [training: 0.06391400845579309 | validation: 0.08019568033662922]
	TIME [epoch: 5.73 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06466633493246877		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.06466633493246877 | validation: 0.08695459570149665]
	TIME [epoch: 5.73 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06753005914863458		[learning rate: 6.5371e-05]
	Learning Rate: 6.53709e-05
	LOSS [training: 0.06753005914863458 | validation: 0.08634422635959585]
	TIME [epoch: 5.74 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06941635518055024		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.06941635518055024 | validation: 0.09619979346167278]
	TIME [epoch: 5.73 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06807920294643807		[learning rate: 6.4909e-05]
	Learning Rate: 6.49094e-05
	LOSS [training: 0.06807920294643807 | validation: 0.08237295855063055]
	TIME [epoch: 5.73 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06685147779475059		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.06685147779475059 | validation: 0.08848956975409558]
	TIME [epoch: 5.72 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06594342128762884		[learning rate: 6.4451e-05]
	Learning Rate: 6.44512e-05
	LOSS [training: 0.06594342128762884 | validation: 0.08582161230769125]
	TIME [epoch: 5.73 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06338410480907526		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.06338410480907526 | validation: 0.10495966981046848]
	TIME [epoch: 5.73 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07499888045034707		[learning rate: 6.3996e-05]
	Learning Rate: 6.39962e-05
	LOSS [training: 0.07499888045034707 | validation: 0.09429026027957504]
	TIME [epoch: 5.73 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06999941856303657		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.06999941856303657 | validation: 0.07536585280534823]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1478.pth
	Model improved!!!
EPOCH 1479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06683657130613072		[learning rate: 6.3544e-05]
	Learning Rate: 6.35444e-05
	LOSS [training: 0.06683657130613072 | validation: 0.08544495078028963]
	TIME [epoch: 5.71 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0668355767360053		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.0668355767360053 | validation: 0.09146972781533476]
	TIME [epoch: 5.71 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06705833792779713		[learning rate: 6.3096e-05]
	Learning Rate: 6.30958e-05
	LOSS [training: 0.06705833792779713 | validation: 0.08577114208511116]
	TIME [epoch: 5.72 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06557654185404252		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.06557654185404252 | validation: 0.08324414739238645]
	TIME [epoch: 5.73 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06421011514786377		[learning rate: 6.265e-05]
	Learning Rate: 6.26503e-05
	LOSS [training: 0.06421011514786377 | validation: 0.0897310872042942]
	TIME [epoch: 5.73 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0663196804200887		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.0663196804200887 | validation: 0.09510775031109461]
	TIME [epoch: 5.73 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0662077832775546		[learning rate: 6.2208e-05]
	Learning Rate: 6.2208e-05
	LOSS [training: 0.0662077832775546 | validation: 0.0850622878704669]
	TIME [epoch: 5.73 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06416844168971793		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.06416844168971793 | validation: 0.08816311082702144]
	TIME [epoch: 5.74 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06528523269207312		[learning rate: 6.1769e-05]
	Learning Rate: 6.17688e-05
	LOSS [training: 0.06528523269207312 | validation: 0.08341801159651645]
	TIME [epoch: 5.73 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06682887192782723		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.06682887192782723 | validation: 0.0835852915453747]
	TIME [epoch: 5.73 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06626272393880217		[learning rate: 6.1333e-05]
	Learning Rate: 6.13327e-05
	LOSS [training: 0.06626272393880217 | validation: 0.08713626093874521]
	TIME [epoch: 5.73 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06625525094294556		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.06625525094294556 | validation: 0.08780351359222537]
	TIME [epoch: 5.73 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06497221437557435		[learning rate: 6.09e-05]
	Learning Rate: 6.08998e-05
	LOSS [training: 0.06497221437557435 | validation: 0.08704017249448245]
	TIME [epoch: 5.73 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06558707623614367		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.06558707623614367 | validation: 0.08453693708228492]
	TIME [epoch: 5.73 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06581500895220363		[learning rate: 6.047e-05]
	Learning Rate: 6.04698e-05
	LOSS [training: 0.06581500895220363 | validation: 0.09383628714433895]
	TIME [epoch: 5.72 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06490430098915338		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.06490430098915338 | validation: 0.09874178417741354]
	TIME [epoch: 5.73 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06703580523027507		[learning rate: 6.0043e-05]
	Learning Rate: 6.00429e-05
	LOSS [training: 0.06703580523027507 | validation: 0.07796053275582954]
	TIME [epoch: 5.73 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06724325065111511		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.06724325065111511 | validation: 0.0862559312568352]
	TIME [epoch: 5.73 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06462433310700767		[learning rate: 5.9619e-05]
	Learning Rate: 5.9619e-05
	LOSS [training: 0.06462433310700767 | validation: 0.08463331912128691]
	TIME [epoch: 5.74 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06434864215075636		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.06434864215075636 | validation: 0.08972341089893772]
	TIME [epoch: 5.73 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0674900894035735		[learning rate: 5.9198e-05]
	Learning Rate: 5.91981e-05
	LOSS [training: 0.0674900894035735 | validation: 0.09509421460828951]
	TIME [epoch: 5.74 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06445454427348982		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.06445454427348982 | validation: 0.08621699446619956]
	TIME [epoch: 5.73 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06695957600571634		[learning rate: 5.878e-05]
	Learning Rate: 5.87802e-05
	LOSS [training: 0.06695957600571634 | validation: 0.08903745924130069]
	TIME [epoch: 5.72 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06526428207819278		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.06526428207819278 | validation: 0.08290141582201122]
	TIME [epoch: 5.72 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06448171725968752		[learning rate: 5.8365e-05]
	Learning Rate: 5.83652e-05
	LOSS [training: 0.06448171725968752 | validation: 0.07877915114284957]
	TIME [epoch: 5.72 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06425489503924181		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.06425489503924181 | validation: 0.0907457493629868]
	TIME [epoch: 5.72 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06340349171890652		[learning rate: 5.7953e-05]
	Learning Rate: 5.79531e-05
	LOSS [training: 0.06340349171890652 | validation: 0.07919721092490922]
	TIME [epoch: 5.71 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061793336066583215		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.061793336066583215 | validation: 0.08308134599322935]
	TIME [epoch: 5.73 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06426899251074046		[learning rate: 5.7544e-05]
	Learning Rate: 5.7544e-05
	LOSS [training: 0.06426899251074046 | validation: 0.08291257444253086]
	TIME [epoch: 5.74 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06570109604917546		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.06570109604917546 | validation: 0.10448748547247844]
	TIME [epoch: 5.73 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0662349294137638		[learning rate: 5.7138e-05]
	Learning Rate: 5.71378e-05
	LOSS [training: 0.0662349294137638 | validation: 0.08501892568901953]
	TIME [epoch: 5.74 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06491192977410144		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.06491192977410144 | validation: 0.08654273029931409]
	TIME [epoch: 5.73 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06683934108334004		[learning rate: 5.6734e-05]
	Learning Rate: 5.67344e-05
	LOSS [training: 0.06683934108334004 | validation: 0.08414776649148628]
	TIME [epoch: 5.74 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06311256096983878		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.06311256096983878 | validation: 0.08761094710832805]
	TIME [epoch: 5.73 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.064761890171847		[learning rate: 5.6334e-05]
	Learning Rate: 5.63338e-05
	LOSS [training: 0.064761890171847 | validation: 0.07803921603458647]
	TIME [epoch: 5.74 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0629416807192796		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.0629416807192796 | validation: 0.08227917628774337]
	TIME [epoch: 5.74 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06639101428705783		[learning rate: 5.5936e-05]
	Learning Rate: 5.59361e-05
	LOSS [training: 0.06639101428705783 | validation: 0.09059547005911277]
	TIME [epoch: 5.74 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06507780950022352		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.06507780950022352 | validation: 0.09200585772728244]
	TIME [epoch: 5.72 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06387446275555797		[learning rate: 5.5541e-05]
	Learning Rate: 5.55412e-05
	LOSS [training: 0.06387446275555797 | validation: 0.08922308000760104]
	TIME [epoch: 5.71 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06475648801632444		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.06475648801632444 | validation: 0.07936572862823585]
	TIME [epoch: 5.71 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06424830800970764		[learning rate: 5.5149e-05]
	Learning Rate: 5.51491e-05
	LOSS [training: 0.06424830800970764 | validation: 0.08247700749486485]
	TIME [epoch: 5.7 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06428041509672998		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.06428041509672998 | validation: 0.07451064268382772]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1520.pth
	Model improved!!!
EPOCH 1521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06760409477020074		[learning rate: 5.476e-05]
	Learning Rate: 5.47598e-05
	LOSS [training: 0.06760409477020074 | validation: 0.08632160072142056]
	TIME [epoch: 5.74 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06434906445675664		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.06434906445675664 | validation: 0.08220112031652954]
	TIME [epoch: 5.75 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06609518538462353		[learning rate: 5.4373e-05]
	Learning Rate: 5.43732e-05
	LOSS [training: 0.06609518538462353 | validation: 0.07711164775642465]
	TIME [epoch: 5.74 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06562135130136022		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.06562135130136022 | validation: 0.08379554407300378]
	TIME [epoch: 5.74 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06725036406288715		[learning rate: 5.3989e-05]
	Learning Rate: 5.39893e-05
	LOSS [training: 0.06725036406288715 | validation: 0.08181066585939048]
	TIME [epoch: 5.74 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.063810598566977		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.063810598566977 | validation: 0.08414050653190155]
	TIME [epoch: 5.74 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06630928806100528		[learning rate: 5.3608e-05]
	Learning Rate: 5.36082e-05
	LOSS [training: 0.06630928806100528 | validation: 0.09074594253080713]
	TIME [epoch: 5.74 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06345424244355219		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.06345424244355219 | validation: 0.08317997127575315]
	TIME [epoch: 5.75 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06535661115779942		[learning rate: 5.323e-05]
	Learning Rate: 5.32297e-05
	LOSS [training: 0.06535661115779942 | validation: 0.08637447815340199]
	TIME [epoch: 5.71 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062358485028986874		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.062358485028986874 | validation: 0.08058897549075221]
	TIME [epoch: 5.71 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06225320273025225		[learning rate: 5.2854e-05]
	Learning Rate: 5.28539e-05
	LOSS [training: 0.06225320273025225 | validation: 0.07929734661833224]
	TIME [epoch: 5.71 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06684859371069073		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.06684859371069073 | validation: 0.08450197167031995]
	TIME [epoch: 5.72 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06364108969331143		[learning rate: 5.2481e-05]
	Learning Rate: 5.24807e-05
	LOSS [training: 0.06364108969331143 | validation: 0.08790610518295698]
	TIME [epoch: 5.72 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06469380890992665		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.06469380890992665 | validation: 0.08613866060754005]
	TIME [epoch: 5.72 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06522313034204873		[learning rate: 5.211e-05]
	Learning Rate: 5.21103e-05
	LOSS [training: 0.06522313034204873 | validation: 0.0933016195170072]
	TIME [epoch: 5.71 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06267852622532273		[learning rate: 5.1926e-05]
	Learning Rate: 5.1926e-05
	LOSS [training: 0.06267852622532273 | validation: 0.09259934386066665]
	TIME [epoch: 5.74 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06480425522579901		[learning rate: 5.1742e-05]
	Learning Rate: 5.17424e-05
	LOSS [training: 0.06480425522579901 | validation: 0.08841212005155423]
	TIME [epoch: 5.71 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06196457240332811		[learning rate: 5.1559e-05]
	Learning Rate: 5.15594e-05
	LOSS [training: 0.06196457240332811 | validation: 0.08398698538595918]
	TIME [epoch: 5.72 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06518698062351941		[learning rate: 5.1377e-05]
	Learning Rate: 5.13771e-05
	LOSS [training: 0.06518698062351941 | validation: 0.08116075699606484]
	TIME [epoch: 5.71 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06413493505300807		[learning rate: 5.1195e-05]
	Learning Rate: 5.11954e-05
	LOSS [training: 0.06413493505300807 | validation: 0.07978761281912247]
	TIME [epoch: 5.72 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06359123015213987		[learning rate: 5.1014e-05]
	Learning Rate: 5.10144e-05
	LOSS [training: 0.06359123015213987 | validation: 0.08663290844454918]
	TIME [epoch: 5.71 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06553209613349863		[learning rate: 5.0834e-05]
	Learning Rate: 5.0834e-05
	LOSS [training: 0.06553209613349863 | validation: 0.08047616053865757]
	TIME [epoch: 5.71 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06510676164815887		[learning rate: 5.0654e-05]
	Learning Rate: 5.06542e-05
	LOSS [training: 0.06510676164815887 | validation: 0.08596332214922765]
	TIME [epoch: 5.72 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06413348569229685		[learning rate: 5.0475e-05]
	Learning Rate: 5.04751e-05
	LOSS [training: 0.06413348569229685 | validation: 0.08947830700064255]
	TIME [epoch: 5.71 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06504505651251184		[learning rate: 5.0297e-05]
	Learning Rate: 5.02966e-05
	LOSS [training: 0.06504505651251184 | validation: 0.08322129510034104]
	TIME [epoch: 5.71 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06349326369997377		[learning rate: 5.0119e-05]
	Learning Rate: 5.01187e-05
	LOSS [training: 0.06349326369997377 | validation: 0.0856457220463359]
	TIME [epoch: 5.72 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0648818464938986		[learning rate: 4.9942e-05]
	Learning Rate: 4.99415e-05
	LOSS [training: 0.0648818464938986 | validation: 0.08629178266119093]
	TIME [epoch: 5.71 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06358277712067145		[learning rate: 4.9765e-05]
	Learning Rate: 4.97649e-05
	LOSS [training: 0.06358277712067145 | validation: 0.08082370768650171]
	TIME [epoch: 5.72 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06188612017463841		[learning rate: 4.9589e-05]
	Learning Rate: 4.95889e-05
	LOSS [training: 0.06188612017463841 | validation: 0.07735199583236291]
	TIME [epoch: 5.71 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06471492077175263		[learning rate: 4.9414e-05]
	Learning Rate: 4.94136e-05
	LOSS [training: 0.06471492077175263 | validation: 0.08525523328242751]
	TIME [epoch: 5.71 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06345176521178605		[learning rate: 4.9239e-05]
	Learning Rate: 4.92388e-05
	LOSS [training: 0.06345176521178605 | validation: 0.08829994121229609]
	TIME [epoch: 5.72 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06766309128060913		[learning rate: 4.9065e-05]
	Learning Rate: 4.90647e-05
	LOSS [training: 0.06766309128060913 | validation: 0.07386126025481365]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1552.pth
	Model improved!!!
EPOCH 1553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06320150689048734		[learning rate: 4.8891e-05]
	Learning Rate: 4.88912e-05
	LOSS [training: 0.06320150689048734 | validation: 0.07979168776693207]
	TIME [epoch: 5.73 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0633450721373589		[learning rate: 4.8718e-05]
	Learning Rate: 4.87183e-05
	LOSS [training: 0.0633450721373589 | validation: 0.08740088201236645]
	TIME [epoch: 5.73 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06481314944775175		[learning rate: 4.8546e-05]
	Learning Rate: 4.85461e-05
	LOSS [training: 0.06481314944775175 | validation: 0.07995046684874041]
	TIME [epoch: 5.72 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06329646145442265		[learning rate: 4.8374e-05]
	Learning Rate: 4.83744e-05
	LOSS [training: 0.06329646145442265 | validation: 0.08910006206276944]
	TIME [epoch: 5.73 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06292602955971593		[learning rate: 4.8203e-05]
	Learning Rate: 4.82033e-05
	LOSS [training: 0.06292602955971593 | validation: 0.08321270007756365]
	TIME [epoch: 5.73 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0661231194406733		[learning rate: 4.8033e-05]
	Learning Rate: 4.80329e-05
	LOSS [training: 0.0661231194406733 | validation: 0.08790491918022403]
	TIME [epoch: 5.73 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06325532954465524		[learning rate: 4.7863e-05]
	Learning Rate: 4.7863e-05
	LOSS [training: 0.06325532954465524 | validation: 0.08670734870053051]
	TIME [epoch: 5.73 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06261809908271918		[learning rate: 4.7694e-05]
	Learning Rate: 4.76938e-05
	LOSS [training: 0.06261809908271918 | validation: 0.08199894994277235]
	TIME [epoch: 5.73 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06335033032909393		[learning rate: 4.7525e-05]
	Learning Rate: 4.75251e-05
	LOSS [training: 0.06335033032909393 | validation: 0.08904196948672237]
	TIME [epoch: 5.73 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062338448914213876		[learning rate: 4.7357e-05]
	Learning Rate: 4.73571e-05
	LOSS [training: 0.062338448914213876 | validation: 0.0834700516923105]
	TIME [epoch: 5.73 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0644254250606214		[learning rate: 4.719e-05]
	Learning Rate: 4.71896e-05
	LOSS [training: 0.0644254250606214 | validation: 0.08482305342894708]
	TIME [epoch: 5.73 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06343132806919634		[learning rate: 4.7023e-05]
	Learning Rate: 4.70227e-05
	LOSS [training: 0.06343132806919634 | validation: 0.08380490272476826]
	TIME [epoch: 5.73 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0627137525978331		[learning rate: 4.6856e-05]
	Learning Rate: 4.68564e-05
	LOSS [training: 0.0627137525978331 | validation: 0.08332661228849897]
	TIME [epoch: 5.72 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06386514048392967		[learning rate: 4.6691e-05]
	Learning Rate: 4.66907e-05
	LOSS [training: 0.06386514048392967 | validation: 0.08433050153608651]
	TIME [epoch: 5.73 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0618210746970989		[learning rate: 4.6526e-05]
	Learning Rate: 4.65257e-05
	LOSS [training: 0.0618210746970989 | validation: 0.07706345377346295]
	TIME [epoch: 5.73 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06403200442407234		[learning rate: 4.6361e-05]
	Learning Rate: 4.63611e-05
	LOSS [training: 0.06403200442407234 | validation: 0.08374116103986151]
	TIME [epoch: 5.73 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06192578670536259		[learning rate: 4.6197e-05]
	Learning Rate: 4.61972e-05
	LOSS [training: 0.06192578670536259 | validation: 0.07729575951537065]
	TIME [epoch: 5.74 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06225202925241524		[learning rate: 4.6034e-05]
	Learning Rate: 4.60338e-05
	LOSS [training: 0.06225202925241524 | validation: 0.0873717391954326]
	TIME [epoch: 5.72 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06448499765059525		[learning rate: 4.5871e-05]
	Learning Rate: 4.5871e-05
	LOSS [training: 0.06448499765059525 | validation: 0.08151290747387396]
	TIME [epoch: 5.73 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06036119966677024		[learning rate: 4.5709e-05]
	Learning Rate: 4.57088e-05
	LOSS [training: 0.06036119966677024 | validation: 0.0869375487368943]
	TIME [epoch: 5.72 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06252227204440108		[learning rate: 4.5547e-05]
	Learning Rate: 4.55472e-05
	LOSS [training: 0.06252227204440108 | validation: 0.07976662582500094]
	TIME [epoch: 5.73 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061842940717014794		[learning rate: 4.5386e-05]
	Learning Rate: 4.53861e-05
	LOSS [training: 0.061842940717014794 | validation: 0.07881929725139138]
	TIME [epoch: 5.72 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06263659235989598		[learning rate: 4.5226e-05]
	Learning Rate: 4.52256e-05
	LOSS [training: 0.06263659235989598 | validation: 0.08047658148123714]
	TIME [epoch: 5.73 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06209684629451215		[learning rate: 4.5066e-05]
	Learning Rate: 4.50657e-05
	LOSS [training: 0.06209684629451215 | validation: 0.077692993103961]
	TIME [epoch: 5.72 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06248958717685106		[learning rate: 4.4906e-05]
	Learning Rate: 4.49064e-05
	LOSS [training: 0.06248958717685106 | validation: 0.07639470672304015]
	TIME [epoch: 5.72 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0618037453023093		[learning rate: 4.4748e-05]
	Learning Rate: 4.47476e-05
	LOSS [training: 0.0618037453023093 | validation: 0.08647778686318401]
	TIME [epoch: 5.73 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06373241054508064		[learning rate: 4.4589e-05]
	Learning Rate: 4.45893e-05
	LOSS [training: 0.06373241054508064 | validation: 0.08384386161514495]
	TIME [epoch: 5.72 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06414635714727247		[learning rate: 4.4432e-05]
	Learning Rate: 4.44316e-05
	LOSS [training: 0.06414635714727247 | validation: 0.07929319819494847]
	TIME [epoch: 5.72 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06311332324442509		[learning rate: 4.4275e-05]
	Learning Rate: 4.42745e-05
	LOSS [training: 0.06311332324442509 | validation: 0.08891081862510704]
	TIME [epoch: 5.72 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06372311825760306		[learning rate: 4.4118e-05]
	Learning Rate: 4.4118e-05
	LOSS [training: 0.06372311825760306 | validation: 0.07950553404357637]
	TIME [epoch: 5.72 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06413880854850909		[learning rate: 4.3962e-05]
	Learning Rate: 4.3962e-05
	LOSS [training: 0.06413880854850909 | validation: 0.07871041767602839]
	TIME [epoch: 5.72 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0634381039324223		[learning rate: 4.3807e-05]
	Learning Rate: 4.38065e-05
	LOSS [training: 0.0634381039324223 | validation: 0.08355144321509107]
	TIME [epoch: 5.72 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06304934532731603		[learning rate: 4.3652e-05]
	Learning Rate: 4.36516e-05
	LOSS [training: 0.06304934532731603 | validation: 0.07734207727034564]
	TIME [epoch: 5.73 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06280992110196781		[learning rate: 4.3497e-05]
	Learning Rate: 4.34972e-05
	LOSS [training: 0.06280992110196781 | validation: 0.07436774929098725]
	TIME [epoch: 5.72 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062133239383359226		[learning rate: 4.3343e-05]
	Learning Rate: 4.33434e-05
	LOSS [training: 0.062133239383359226 | validation: 0.0803598107625308]
	TIME [epoch: 5.73 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06276870155763892		[learning rate: 4.319e-05]
	Learning Rate: 4.31902e-05
	LOSS [training: 0.06276870155763892 | validation: 0.0798018415967125]
	TIME [epoch: 5.72 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06451808952485248		[learning rate: 4.3037e-05]
	Learning Rate: 4.30374e-05
	LOSS [training: 0.06451808952485248 | validation: 0.0785971660287406]
	TIME [epoch: 5.73 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06249319615076117		[learning rate: 4.2885e-05]
	Learning Rate: 4.28852e-05
	LOSS [training: 0.06249319615076117 | validation: 0.07690910022739877]
	TIME [epoch: 5.72 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06370745826384921		[learning rate: 4.2734e-05]
	Learning Rate: 4.27336e-05
	LOSS [training: 0.06370745826384921 | validation: 0.07310616859191398]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1591.pth
	Model improved!!!
EPOCH 1592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0626584898839617		[learning rate: 4.2582e-05]
	Learning Rate: 4.25825e-05
	LOSS [training: 0.0626584898839617 | validation: 0.07738185948692423]
	TIME [epoch: 5.7 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06264420487570846		[learning rate: 4.2432e-05]
	Learning Rate: 4.24319e-05
	LOSS [training: 0.06264420487570846 | validation: 0.07716271296303591]
	TIME [epoch: 5.7 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06348515471283657		[learning rate: 4.2282e-05]
	Learning Rate: 4.22819e-05
	LOSS [training: 0.06348515471283657 | validation: 0.08530254990173357]
	TIME [epoch: 5.71 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06294382272526043		[learning rate: 4.2132e-05]
	Learning Rate: 4.21323e-05
	LOSS [training: 0.06294382272526043 | validation: 0.08387510827798736]
	TIME [epoch: 5.7 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06279312319550057		[learning rate: 4.1983e-05]
	Learning Rate: 4.19833e-05
	LOSS [training: 0.06279312319550057 | validation: 0.08152633651565694]
	TIME [epoch: 5.7 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0616642172885766		[learning rate: 4.1835e-05]
	Learning Rate: 4.18349e-05
	LOSS [training: 0.0616642172885766 | validation: 0.07277163707972104]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1597.pth
	Model improved!!!
EPOCH 1598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06274848419925569		[learning rate: 4.1687e-05]
	Learning Rate: 4.1687e-05
	LOSS [training: 0.06274848419925569 | validation: 0.07809424079858429]
	TIME [epoch: 5.73 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061404396747196846		[learning rate: 4.154e-05]
	Learning Rate: 4.15395e-05
	LOSS [training: 0.061404396747196846 | validation: 0.07715278181285505]
	TIME [epoch: 5.73 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06409940102181186		[learning rate: 4.1393e-05]
	Learning Rate: 4.13926e-05
	LOSS [training: 0.06409940102181186 | validation: 0.07766206478091991]
	TIME [epoch: 5.71 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061754395897224326		[learning rate: 4.1246e-05]
	Learning Rate: 4.12463e-05
	LOSS [training: 0.061754395897224326 | validation: 0.08078852028801325]
	TIME [epoch: 5.73 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06511167767870095		[learning rate: 4.11e-05]
	Learning Rate: 4.11004e-05
	LOSS [training: 0.06511167767870095 | validation: 0.0786589144383266]
	TIME [epoch: 5.74 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.063195190002551		[learning rate: 4.0955e-05]
	Learning Rate: 4.09551e-05
	LOSS [training: 0.063195190002551 | validation: 0.08397495217279033]
	TIME [epoch: 5.74 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06164745828608844		[learning rate: 4.081e-05]
	Learning Rate: 4.08103e-05
	LOSS [training: 0.06164745828608844 | validation: 0.07793479482673404]
	TIME [epoch: 5.74 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06055192950419541		[learning rate: 4.0666e-05]
	Learning Rate: 4.06659e-05
	LOSS [training: 0.06055192950419541 | validation: 0.07855188368526622]
	TIME [epoch: 5.74 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061431388360512554		[learning rate: 4.0522e-05]
	Learning Rate: 4.05221e-05
	LOSS [training: 0.061431388360512554 | validation: 0.07162971926615791]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1606.pth
	Model improved!!!
EPOCH 1607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06178553585997459		[learning rate: 4.0379e-05]
	Learning Rate: 4.03788e-05
	LOSS [training: 0.06178553585997459 | validation: 0.07371883841069879]
	TIME [epoch: 5.73 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06235645060942968		[learning rate: 4.0236e-05]
	Learning Rate: 4.02361e-05
	LOSS [training: 0.06235645060942968 | validation: 0.08047448275462915]
	TIME [epoch: 5.74 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06229732325553295		[learning rate: 4.0094e-05]
	Learning Rate: 4.00938e-05
	LOSS [training: 0.06229732325553295 | validation: 0.08118224283093287]
	TIME [epoch: 5.73 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06217352943856572		[learning rate: 3.9952e-05]
	Learning Rate: 3.9952e-05
	LOSS [training: 0.06217352943856572 | validation: 0.08044505633168436]
	TIME [epoch: 5.73 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06227875506112944		[learning rate: 3.9811e-05]
	Learning Rate: 3.98107e-05
	LOSS [training: 0.06227875506112944 | validation: 0.07732815470822876]
	TIME [epoch: 5.73 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060479887349902554		[learning rate: 3.967e-05]
	Learning Rate: 3.967e-05
	LOSS [training: 0.060479887349902554 | validation: 0.07910122246396198]
	TIME [epoch: 5.73 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0630219953920736		[learning rate: 3.953e-05]
	Learning Rate: 3.95297e-05
	LOSS [training: 0.0630219953920736 | validation: 0.08699845188562433]
	TIME [epoch: 5.73 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061365356834479606		[learning rate: 3.939e-05]
	Learning Rate: 3.93899e-05
	LOSS [training: 0.061365356834479606 | validation: 0.08204048257324864]
	TIME [epoch: 5.73 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061173002602450045		[learning rate: 3.9251e-05]
	Learning Rate: 3.92506e-05
	LOSS [training: 0.061173002602450045 | validation: 0.09106266870633284]
	TIME [epoch: 5.73 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060056162256530515		[learning rate: 3.9112e-05]
	Learning Rate: 3.91118e-05
	LOSS [training: 0.060056162256530515 | validation: 0.07311771439689185]
	TIME [epoch: 5.74 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06253682185562223		[learning rate: 3.8973e-05]
	Learning Rate: 3.89735e-05
	LOSS [training: 0.06253682185562223 | validation: 0.07303033298227657]
	TIME [epoch: 5.73 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06345014703807757		[learning rate: 3.8836e-05]
	Learning Rate: 3.88357e-05
	LOSS [training: 0.06345014703807757 | validation: 0.07815795157579469]
	TIME [epoch: 5.73 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06010768762400958		[learning rate: 3.8698e-05]
	Learning Rate: 3.86983e-05
	LOSS [training: 0.06010768762400958 | validation: 0.08342434048997828]
	TIME [epoch: 5.73 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06034409826758363		[learning rate: 3.8561e-05]
	Learning Rate: 3.85615e-05
	LOSS [training: 0.06034409826758363 | validation: 0.07514601067095755]
	TIME [epoch: 5.73 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06160291553522679		[learning rate: 3.8425e-05]
	Learning Rate: 3.84251e-05
	LOSS [training: 0.06160291553522679 | validation: 0.08260548402654325]
	TIME [epoch: 5.73 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06041552891721035		[learning rate: 3.8289e-05]
	Learning Rate: 3.82893e-05
	LOSS [training: 0.06041552891721035 | validation: 0.07710249538516487]
	TIME [epoch: 5.73 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06017404726537232		[learning rate: 3.8154e-05]
	Learning Rate: 3.81539e-05
	LOSS [training: 0.06017404726537232 | validation: 0.0788554708686512]
	TIME [epoch: 5.73 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06233863691033289		[learning rate: 3.8019e-05]
	Learning Rate: 3.8019e-05
	LOSS [training: 0.06233863691033289 | validation: 0.08026234606895673]
	TIME [epoch: 5.73 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06135927183993577		[learning rate: 3.7885e-05]
	Learning Rate: 3.78845e-05
	LOSS [training: 0.06135927183993577 | validation: 0.0796185418281915]
	TIME [epoch: 5.73 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06063667585780573		[learning rate: 3.7751e-05]
	Learning Rate: 3.77505e-05
	LOSS [training: 0.06063667585780573 | validation: 0.07957033438053816]
	TIME [epoch: 5.74 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06251911530095597		[learning rate: 3.7617e-05]
	Learning Rate: 3.7617e-05
	LOSS [training: 0.06251911530095597 | validation: 0.07828684929808742]
	TIME [epoch: 5.73 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0614993897502667		[learning rate: 3.7484e-05]
	Learning Rate: 3.7484e-05
	LOSS [training: 0.0614993897502667 | validation: 0.07654350708638735]
	TIME [epoch: 5.73 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06319591210447896		[learning rate: 3.7351e-05]
	Learning Rate: 3.73515e-05
	LOSS [training: 0.06319591210447896 | validation: 0.07137143088711549]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1629.pth
	Model improved!!!
EPOCH 1630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06123056479327097		[learning rate: 3.7219e-05]
	Learning Rate: 3.72194e-05
	LOSS [training: 0.06123056479327097 | validation: 0.08094933145091682]
	TIME [epoch: 5.7 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061170278347380355		[learning rate: 3.7088e-05]
	Learning Rate: 3.70878e-05
	LOSS [training: 0.061170278347380355 | validation: 0.07474819436300635]
	TIME [epoch: 5.7 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0598272674683846		[learning rate: 3.6957e-05]
	Learning Rate: 3.69566e-05
	LOSS [training: 0.0598272674683846 | validation: 0.07677333656354857]
	TIME [epoch: 5.7 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061557687955299686		[learning rate: 3.6826e-05]
	Learning Rate: 3.68259e-05
	LOSS [training: 0.061557687955299686 | validation: 0.07719568374897526]
	TIME [epoch: 5.7 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06276775102879834		[learning rate: 3.6696e-05]
	Learning Rate: 3.66957e-05
	LOSS [training: 0.06276775102879834 | validation: 0.08074000364308179]
	TIME [epoch: 5.7 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06060158243569498		[learning rate: 3.6566e-05]
	Learning Rate: 3.6566e-05
	LOSS [training: 0.06060158243569498 | validation: 0.06889226151480782]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1635.pth
	Model improved!!!
EPOCH 1636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06009346269759107		[learning rate: 3.6437e-05]
	Learning Rate: 3.64367e-05
	LOSS [training: 0.06009346269759107 | validation: 0.08249872299292307]
	TIME [epoch: 5.73 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05984036928072399		[learning rate: 3.6308e-05]
	Learning Rate: 3.63078e-05
	LOSS [training: 0.05984036928072399 | validation: 0.07741587334996405]
	TIME [epoch: 5.72 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06228081108399215		[learning rate: 3.6179e-05]
	Learning Rate: 3.61794e-05
	LOSS [training: 0.06228081108399215 | validation: 0.07721896321942351]
	TIME [epoch: 5.73 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06046516844616888		[learning rate: 3.6051e-05]
	Learning Rate: 3.60515e-05
	LOSS [training: 0.06046516844616888 | validation: 0.08364316560606311]
	TIME [epoch: 5.72 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061136880392200645		[learning rate: 3.5924e-05]
	Learning Rate: 3.5924e-05
	LOSS [training: 0.061136880392200645 | validation: 0.08239435688397902]
	TIME [epoch: 5.72 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06169680801249741		[learning rate: 3.5797e-05]
	Learning Rate: 3.5797e-05
	LOSS [training: 0.06169680801249741 | validation: 0.07168394370882733]
	TIME [epoch: 5.73 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06112884613812867		[learning rate: 3.567e-05]
	Learning Rate: 3.56704e-05
	LOSS [training: 0.06112884613812867 | validation: 0.07742802307726715]
	TIME [epoch: 5.73 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05846785963658187		[learning rate: 3.5544e-05]
	Learning Rate: 3.55442e-05
	LOSS [training: 0.05846785963658187 | validation: 0.07892468642921938]
	TIME [epoch: 5.73 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06225526348531516		[learning rate: 3.5419e-05]
	Learning Rate: 3.54186e-05
	LOSS [training: 0.06225526348531516 | validation: 0.09060867299206357]
	TIME [epoch: 5.72 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06438584815690294		[learning rate: 3.5293e-05]
	Learning Rate: 3.52933e-05
	LOSS [training: 0.06438584815690294 | validation: 0.09249244341003598]
	TIME [epoch: 5.69 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06220024019056743		[learning rate: 3.5169e-05]
	Learning Rate: 3.51685e-05
	LOSS [training: 0.06220024019056743 | validation: 0.07258444472461427]
	TIME [epoch: 5.7 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06080024705072739		[learning rate: 3.5044e-05]
	Learning Rate: 3.50441e-05
	LOSS [training: 0.06080024705072739 | validation: 0.07977604983516423]
	TIME [epoch: 5.7 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05963376255573594		[learning rate: 3.492e-05]
	Learning Rate: 3.49202e-05
	LOSS [training: 0.05963376255573594 | validation: 0.0854390715703387]
	TIME [epoch: 5.69 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060050729056956305		[learning rate: 3.4797e-05]
	Learning Rate: 3.47967e-05
	LOSS [training: 0.060050729056956305 | validation: 0.07485217676421298]
	TIME [epoch: 5.7 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06052087965316961		[learning rate: 3.4674e-05]
	Learning Rate: 3.46737e-05
	LOSS [training: 0.06052087965316961 | validation: 0.07823943090700769]
	TIME [epoch: 5.7 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061924195790259734		[learning rate: 3.4551e-05]
	Learning Rate: 3.45511e-05
	LOSS [training: 0.061924195790259734 | validation: 0.07521155981953971]
	TIME [epoch: 5.7 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06212101912616198		[learning rate: 3.4429e-05]
	Learning Rate: 3.44289e-05
	LOSS [training: 0.06212101912616198 | validation: 0.07481148291470681]
	TIME [epoch: 5.7 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05998079951155787		[learning rate: 3.4307e-05]
	Learning Rate: 3.43072e-05
	LOSS [training: 0.05998079951155787 | validation: 0.07810096607273925]
	TIME [epoch: 5.69 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060877354957238075		[learning rate: 3.4186e-05]
	Learning Rate: 3.41858e-05
	LOSS [training: 0.060877354957238075 | validation: 0.08010053878137541]
	TIME [epoch: 5.7 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061612593115303156		[learning rate: 3.4065e-05]
	Learning Rate: 3.4065e-05
	LOSS [training: 0.061612593115303156 | validation: 0.07726147848446674]
	TIME [epoch: 5.69 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05986146729050461		[learning rate: 3.3944e-05]
	Learning Rate: 3.39445e-05
	LOSS [training: 0.05986146729050461 | validation: 0.07452427598861498]
	TIME [epoch: 5.71 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060360634392341066		[learning rate: 3.3824e-05]
	Learning Rate: 3.38245e-05
	LOSS [training: 0.060360634392341066 | validation: 0.07617559347858127]
	TIME [epoch: 5.7 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061351273027612355		[learning rate: 3.3705e-05]
	Learning Rate: 3.37049e-05
	LOSS [training: 0.061351273027612355 | validation: 0.07153138263920353]
	TIME [epoch: 5.7 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05956700719867113		[learning rate: 3.3586e-05]
	Learning Rate: 3.35857e-05
	LOSS [training: 0.05956700719867113 | validation: 0.07590029039819167]
	TIME [epoch: 5.71 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06041966609367208		[learning rate: 3.3467e-05]
	Learning Rate: 3.34669e-05
	LOSS [training: 0.06041966609367208 | validation: 0.07647589417057976]
	TIME [epoch: 5.7 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05911729475445972		[learning rate: 3.3349e-05]
	Learning Rate: 3.33486e-05
	LOSS [training: 0.05911729475445972 | validation: 0.07969067136650156]
	TIME [epoch: 5.7 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058262280654592685		[learning rate: 3.3231e-05]
	Learning Rate: 3.32306e-05
	LOSS [training: 0.058262280654592685 | validation: 0.07771228176451382]
	TIME [epoch: 5.7 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059083535659142504		[learning rate: 3.3113e-05]
	Learning Rate: 3.31131e-05
	LOSS [training: 0.059083535659142504 | validation: 0.08183148758554548]
	TIME [epoch: 5.7 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06170084252350743		[learning rate: 3.2996e-05]
	Learning Rate: 3.2996e-05
	LOSS [training: 0.06170084252350743 | validation: 0.07748734293129156]
	TIME [epoch: 5.71 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06055864992289669		[learning rate: 3.2879e-05]
	Learning Rate: 3.28794e-05
	LOSS [training: 0.06055864992289669 | validation: 0.07899566934252804]
	TIME [epoch: 5.71 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059284761024151963		[learning rate: 3.2763e-05]
	Learning Rate: 3.27631e-05
	LOSS [training: 0.059284761024151963 | validation: 0.08631609282126956]
	TIME [epoch: 5.71 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06139846169985888		[learning rate: 3.2647e-05]
	Learning Rate: 3.26472e-05
	LOSS [training: 0.06139846169985888 | validation: 0.07906521652752348]
	TIME [epoch: 5.71 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059445961118986576		[learning rate: 3.2532e-05]
	Learning Rate: 3.25318e-05
	LOSS [training: 0.059445961118986576 | validation: 0.07811375987800523]
	TIME [epoch: 5.7 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061200636011256054		[learning rate: 3.2417e-05]
	Learning Rate: 3.24167e-05
	LOSS [training: 0.061200636011256054 | validation: 0.08114622412753109]
	TIME [epoch: 5.71 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0608709468239313		[learning rate: 3.2302e-05]
	Learning Rate: 3.23021e-05
	LOSS [training: 0.0608709468239313 | validation: 0.08970669739399972]
	TIME [epoch: 5.7 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060035495931801575		[learning rate: 3.2188e-05]
	Learning Rate: 3.21879e-05
	LOSS [training: 0.060035495931801575 | validation: 0.07339321789584562]
	TIME [epoch: 5.7 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0595128532146709		[learning rate: 3.2074e-05]
	Learning Rate: 3.20741e-05
	LOSS [training: 0.0595128532146709 | validation: 0.07530215342343333]
	TIME [epoch: 5.7 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06126467910864602		[learning rate: 3.1961e-05]
	Learning Rate: 3.19606e-05
	LOSS [training: 0.06126467910864602 | validation: 0.07940255017316963]
	TIME [epoch: 5.7 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060349537453975095		[learning rate: 3.1848e-05]
	Learning Rate: 3.18476e-05
	LOSS [training: 0.060349537453975095 | validation: 0.06921813814207665]
	TIME [epoch: 5.7 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05927335171952455		[learning rate: 3.1735e-05]
	Learning Rate: 3.1735e-05
	LOSS [training: 0.05927335171952455 | validation: 0.07906185285057984]
	TIME [epoch: 5.7 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061065015084173724		[learning rate: 3.1623e-05]
	Learning Rate: 3.16228e-05
	LOSS [training: 0.061065015084173724 | validation: 0.07726332595493143]
	TIME [epoch: 5.7 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06031198420538054		[learning rate: 3.1511e-05]
	Learning Rate: 3.1511e-05
	LOSS [training: 0.06031198420538054 | validation: 0.07478152830656022]
	TIME [epoch: 5.69 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059926707952385236		[learning rate: 3.14e-05]
	Learning Rate: 3.13995e-05
	LOSS [training: 0.059926707952385236 | validation: 0.07913342234255841]
	TIME [epoch: 5.71 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05986100219285995		[learning rate: 3.1288e-05]
	Learning Rate: 3.12885e-05
	LOSS [training: 0.05986100219285995 | validation: 0.07991533434313874]
	TIME [epoch: 5.69 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058555390917644105		[learning rate: 3.1178e-05]
	Learning Rate: 3.11779e-05
	LOSS [training: 0.058555390917644105 | validation: 0.07908139813568568]
	TIME [epoch: 5.71 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06251886286096682		[learning rate: 3.1068e-05]
	Learning Rate: 3.10676e-05
	LOSS [training: 0.06251886286096682 | validation: 0.06663316073605388]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1681.pth
	Model improved!!!
EPOCH 1682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06089632151029997		[learning rate: 3.0958e-05]
	Learning Rate: 3.09577e-05
	LOSS [training: 0.06089632151029997 | validation: 0.07204148064619888]
	TIME [epoch: 5.7 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05859936292345443		[learning rate: 3.0848e-05]
	Learning Rate: 3.08483e-05
	LOSS [training: 0.05859936292345443 | validation: 0.0767951894722982]
	TIME [epoch: 5.7 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05901572695176954		[learning rate: 3.0739e-05]
	Learning Rate: 3.07392e-05
	LOSS [training: 0.05901572695176954 | validation: 0.06990004811020653]
	TIME [epoch: 5.71 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06029904950143339		[learning rate: 3.063e-05]
	Learning Rate: 3.06305e-05
	LOSS [training: 0.06029904950143339 | validation: 0.08283394112160594]
	TIME [epoch: 5.69 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06189008792545675		[learning rate: 3.0522e-05]
	Learning Rate: 3.05222e-05
	LOSS [training: 0.06189008792545675 | validation: 0.06953090028630053]
	TIME [epoch: 5.71 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0606572396753138		[learning rate: 3.0414e-05]
	Learning Rate: 3.04142e-05
	LOSS [training: 0.0606572396753138 | validation: 0.07450661168455035]
	TIME [epoch: 5.7 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06025063626433127		[learning rate: 3.0307e-05]
	Learning Rate: 3.03067e-05
	LOSS [training: 0.06025063626433127 | validation: 0.07728729283661886]
	TIME [epoch: 5.71 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06184979077538692		[learning rate: 3.02e-05]
	Learning Rate: 3.01995e-05
	LOSS [training: 0.06184979077538692 | validation: 0.07697124248408616]
	TIME [epoch: 5.69 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06272415703618629		[learning rate: 3.0093e-05]
	Learning Rate: 3.00927e-05
	LOSS [training: 0.06272415703618629 | validation: 0.0799050035658333]
	TIME [epoch: 5.71 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059965027336599114		[learning rate: 2.9986e-05]
	Learning Rate: 2.99863e-05
	LOSS [training: 0.059965027336599114 | validation: 0.07416004626757393]
	TIME [epoch: 5.7 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06097452734905219		[learning rate: 2.988e-05]
	Learning Rate: 2.98803e-05
	LOSS [training: 0.06097452734905219 | validation: 0.07065337377011367]
	TIME [epoch: 5.7 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06083691204477904		[learning rate: 2.9775e-05]
	Learning Rate: 2.97746e-05
	LOSS [training: 0.06083691204477904 | validation: 0.07308389520452753]
	TIME [epoch: 5.7 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059511744127833895		[learning rate: 2.9669e-05]
	Learning Rate: 2.96693e-05
	LOSS [training: 0.059511744127833895 | validation: 0.07064282301815887]
	TIME [epoch: 5.71 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05911062533531		[learning rate: 2.9564e-05]
	Learning Rate: 2.95644e-05
	LOSS [training: 0.05911062533531 | validation: 0.07372489927253564]
	TIME [epoch: 5.69 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059390159094580036		[learning rate: 2.946e-05]
	Learning Rate: 2.94599e-05
	LOSS [training: 0.059390159094580036 | validation: 0.08131127219597682]
	TIME [epoch: 5.71 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060305168569376545		[learning rate: 2.9356e-05]
	Learning Rate: 2.93557e-05
	LOSS [training: 0.060305168569376545 | validation: 0.07112236849791802]
	TIME [epoch: 5.7 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059375233021062776		[learning rate: 2.9252e-05]
	Learning Rate: 2.92519e-05
	LOSS [training: 0.059375233021062776 | validation: 0.08008321410124296]
	TIME [epoch: 5.71 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06290429561363445		[learning rate: 2.9148e-05]
	Learning Rate: 2.91485e-05
	LOSS [training: 0.06290429561363445 | validation: 0.0746443612245092]
	TIME [epoch: 5.69 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059619785745046894		[learning rate: 2.9045e-05]
	Learning Rate: 2.90454e-05
	LOSS [training: 0.059619785745046894 | validation: 0.07155342858764183]
	TIME [epoch: 5.7 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05925626879474947		[learning rate: 2.8943e-05]
	Learning Rate: 2.89427e-05
	LOSS [training: 0.05925626879474947 | validation: 0.08040277690808155]
	TIME [epoch: 5.69 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058261551111634134		[learning rate: 2.884e-05]
	Learning Rate: 2.88403e-05
	LOSS [training: 0.058261551111634134 | validation: 0.07696327769585964]
	TIME [epoch: 5.71 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06038162855337699		[learning rate: 2.8738e-05]
	Learning Rate: 2.87383e-05
	LOSS [training: 0.06038162855337699 | validation: 0.08150021713105675]
	TIME [epoch: 5.71 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058971452156658714		[learning rate: 2.8637e-05]
	Learning Rate: 2.86367e-05
	LOSS [training: 0.058971452156658714 | validation: 0.07679501513369892]
	TIME [epoch: 5.71 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06137404479944263		[learning rate: 2.8535e-05]
	Learning Rate: 2.85355e-05
	LOSS [training: 0.06137404479944263 | validation: 0.07223642126911382]
	TIME [epoch: 5.71 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058780663935687996		[learning rate: 2.8435e-05]
	Learning Rate: 2.84345e-05
	LOSS [training: 0.058780663935687996 | validation: 0.07642767181758539]
	TIME [epoch: 5.7 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06053353800761054		[learning rate: 2.8334e-05]
	Learning Rate: 2.8334e-05
	LOSS [training: 0.06053353800761054 | validation: 0.06972075879427914]
	TIME [epoch: 5.71 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057537211867628224		[learning rate: 2.8234e-05]
	Learning Rate: 2.82338e-05
	LOSS [training: 0.057537211867628224 | validation: 0.0715862021317009]
	TIME [epoch: 5.7 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060538702105225356		[learning rate: 2.8134e-05]
	Learning Rate: 2.8134e-05
	LOSS [training: 0.060538702105225356 | validation: 0.07388014667547568]
	TIME [epoch: 5.71 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05901198682847713		[learning rate: 2.8034e-05]
	Learning Rate: 2.80345e-05
	LOSS [training: 0.05901198682847713 | validation: 0.07195206981041477]
	TIME [epoch: 5.7 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05823625494589385		[learning rate: 2.7935e-05]
	Learning Rate: 2.79353e-05
	LOSS [training: 0.05823625494589385 | validation: 0.07452685528334932]
	TIME [epoch: 5.71 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05967986714176513		[learning rate: 2.7837e-05]
	Learning Rate: 2.78366e-05
	LOSS [training: 0.05967986714176513 | validation: 0.07457990853058842]
	TIME [epoch: 5.7 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060209052553132025		[learning rate: 2.7738e-05]
	Learning Rate: 2.77381e-05
	LOSS [training: 0.060209052553132025 | validation: 0.07635800631847883]
	TIME [epoch: 5.7 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05854178526252814		[learning rate: 2.764e-05]
	Learning Rate: 2.764e-05
	LOSS [training: 0.05854178526252814 | validation: 0.08371986041985656]
	TIME [epoch: 5.7 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06231626015971006		[learning rate: 2.7542e-05]
	Learning Rate: 2.75423e-05
	LOSS [training: 0.06231626015971006 | validation: 0.06851444917406492]
	TIME [epoch: 5.7 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0600095971598913		[learning rate: 2.7445e-05]
	Learning Rate: 2.74449e-05
	LOSS [training: 0.0600095971598913 | validation: 0.07157996018185267]
	TIME [epoch: 5.7 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06136274454312158		[learning rate: 2.7348e-05]
	Learning Rate: 2.73478e-05
	LOSS [training: 0.06136274454312158 | validation: 0.07874354770498682]
	TIME [epoch: 5.71 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05789359921486423		[learning rate: 2.7251e-05]
	Learning Rate: 2.72511e-05
	LOSS [training: 0.05789359921486423 | validation: 0.07950011218320449]
	TIME [epoch: 5.71 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06027217675225572		[learning rate: 2.7155e-05]
	Learning Rate: 2.71548e-05
	LOSS [training: 0.06027217675225572 | validation: 0.07378363078015612]
	TIME [epoch: 5.71 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062437734511755084		[learning rate: 2.7059e-05]
	Learning Rate: 2.70587e-05
	LOSS [training: 0.062437734511755084 | validation: 0.07217535124223168]
	TIME [epoch: 5.7 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0591822857174733		[learning rate: 2.6963e-05]
	Learning Rate: 2.69631e-05
	LOSS [training: 0.0591822857174733 | validation: 0.07415916317411918]
	TIME [epoch: 5.72 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05836661567762883		[learning rate: 2.6868e-05]
	Learning Rate: 2.68677e-05
	LOSS [training: 0.05836661567762883 | validation: 0.0806718809229112]
	TIME [epoch: 5.71 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05969424835570344		[learning rate: 2.6773e-05]
	Learning Rate: 2.67727e-05
	LOSS [training: 0.05969424835570344 | validation: 0.07241227222292583]
	TIME [epoch: 5.71 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05921690662362413		[learning rate: 2.6678e-05]
	Learning Rate: 2.6678e-05
	LOSS [training: 0.05921690662362413 | validation: 0.07415735666078303]
	TIME [epoch: 5.71 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06151326013705743		[learning rate: 2.6584e-05]
	Learning Rate: 2.65837e-05
	LOSS [training: 0.06151326013705743 | validation: 0.073722900392784]
	TIME [epoch: 5.71 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05798377707225089		[learning rate: 2.649e-05]
	Learning Rate: 2.64897e-05
	LOSS [training: 0.05798377707225089 | validation: 0.06806166062999666]
	TIME [epoch: 5.7 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06006841505799353		[learning rate: 2.6396e-05]
	Learning Rate: 2.6396e-05
	LOSS [training: 0.06006841505799353 | validation: 0.07741027872817732]
	TIME [epoch: 5.71 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05681780254272741		[learning rate: 2.6303e-05]
	Learning Rate: 2.63027e-05
	LOSS [training: 0.05681780254272741 | validation: 0.0748684037611024]
	TIME [epoch: 5.71 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0585581854978992		[learning rate: 2.621e-05]
	Learning Rate: 2.62097e-05
	LOSS [training: 0.0585581854978992 | validation: 0.07410951621199488]
	TIME [epoch: 5.71 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057747964707876076		[learning rate: 2.6117e-05]
	Learning Rate: 2.6117e-05
	LOSS [training: 0.057747964707876076 | validation: 0.07053710427289306]
	TIME [epoch: 5.71 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06137252691279947		[learning rate: 2.6025e-05]
	Learning Rate: 2.60246e-05
	LOSS [training: 0.06137252691279947 | validation: 0.07802913135567577]
	TIME [epoch: 5.71 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05916139350562299		[learning rate: 2.5933e-05]
	Learning Rate: 2.59326e-05
	LOSS [training: 0.05916139350562299 | validation: 0.07459856462940828]
	TIME [epoch: 5.72 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060477093372811666		[learning rate: 2.5841e-05]
	Learning Rate: 2.58409e-05
	LOSS [training: 0.060477093372811666 | validation: 0.07851435826063373]
	TIME [epoch: 5.72 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05949501246043031		[learning rate: 2.575e-05]
	Learning Rate: 2.57495e-05
	LOSS [training: 0.05949501246043031 | validation: 0.0762079336500387]
	TIME [epoch: 5.71 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06019390524622503		[learning rate: 2.5658e-05]
	Learning Rate: 2.56585e-05
	LOSS [training: 0.06019390524622503 | validation: 0.07312617125114614]
	TIME [epoch: 5.72 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059794478725595845		[learning rate: 2.5568e-05]
	Learning Rate: 2.55677e-05
	LOSS [training: 0.059794478725595845 | validation: 0.07197975538673533]
	TIME [epoch: 5.71 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057624745887285125		[learning rate: 2.5477e-05]
	Learning Rate: 2.54773e-05
	LOSS [training: 0.057624745887285125 | validation: 0.07339047868410185]
	TIME [epoch: 5.71 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0599845586570897		[learning rate: 2.5387e-05]
	Learning Rate: 2.53872e-05
	LOSS [training: 0.0599845586570897 | validation: 0.0693055431834874]
	TIME [epoch: 5.7 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058568418124416884		[learning rate: 2.5297e-05]
	Learning Rate: 2.52975e-05
	LOSS [training: 0.058568418124416884 | validation: 0.0732413742619753]
	TIME [epoch: 5.72 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05883369077070854		[learning rate: 2.5208e-05]
	Learning Rate: 2.5208e-05
	LOSS [training: 0.05883369077070854 | validation: 0.07076258314230756]
	TIME [epoch: 5.71 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05930667851979883		[learning rate: 2.5119e-05]
	Learning Rate: 2.51189e-05
	LOSS [training: 0.05930667851979883 | validation: 0.07175270532952978]
	TIME [epoch: 5.72 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05848309306771446		[learning rate: 2.503e-05]
	Learning Rate: 2.503e-05
	LOSS [training: 0.05848309306771446 | validation: 0.07645443999542063]
	TIME [epoch: 5.71 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06027086260745726		[learning rate: 2.4942e-05]
	Learning Rate: 2.49415e-05
	LOSS [training: 0.06027086260745726 | validation: 0.0712541733438205]
	TIME [epoch: 5.71 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058168838713934204		[learning rate: 2.4853e-05]
	Learning Rate: 2.48533e-05
	LOSS [training: 0.058168838713934204 | validation: 0.07567306231834268]
	TIME [epoch: 5.7 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058787478414033416		[learning rate: 2.4765e-05]
	Learning Rate: 2.47655e-05
	LOSS [training: 0.058787478414033416 | validation: 0.07178335326861054]
	TIME [epoch: 5.72 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061522206375827546		[learning rate: 2.4678e-05]
	Learning Rate: 2.46779e-05
	LOSS [training: 0.061522206375827546 | validation: 0.07347035869821285]
	TIME [epoch: 5.71 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058640466643567535		[learning rate: 2.4591e-05]
	Learning Rate: 2.45906e-05
	LOSS [training: 0.058640466643567535 | validation: 0.07729057560268111]
	TIME [epoch: 5.72 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059366165453405007		[learning rate: 2.4504e-05]
	Learning Rate: 2.45037e-05
	LOSS [training: 0.059366165453405007 | validation: 0.07825211163078637]
	TIME [epoch: 5.72 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05957374048945031		[learning rate: 2.4417e-05]
	Learning Rate: 2.4417e-05
	LOSS [training: 0.05957374048945031 | validation: 0.07215773252062296]
	TIME [epoch: 5.72 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05955870756253116		[learning rate: 2.4331e-05]
	Learning Rate: 2.43307e-05
	LOSS [training: 0.05955870756253116 | validation: 0.07177393079326637]
	TIME [epoch: 5.72 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05730095938806885		[learning rate: 2.4245e-05]
	Learning Rate: 2.42446e-05
	LOSS [training: 0.05730095938806885 | validation: 0.07232935844658404]
	TIME [epoch: 5.71 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0573823057370236		[learning rate: 2.4159e-05]
	Learning Rate: 2.41589e-05
	LOSS [training: 0.0573823057370236 | validation: 0.07210135391897006]
	TIME [epoch: 5.71 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05876195563453096		[learning rate: 2.4073e-05]
	Learning Rate: 2.40735e-05
	LOSS [training: 0.05876195563453096 | validation: 0.0758576368282787]
	TIME [epoch: 5.72 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06067864144754717		[learning rate: 2.3988e-05]
	Learning Rate: 2.39883e-05
	LOSS [training: 0.06067864144754717 | validation: 0.0665526663573864]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1754.pth
	Model improved!!!
EPOCH 1755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05856710368422229		[learning rate: 2.3904e-05]
	Learning Rate: 2.39035e-05
	LOSS [training: 0.05856710368422229 | validation: 0.0670395556553845]
	TIME [epoch: 5.71 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0588234886047053		[learning rate: 2.3819e-05]
	Learning Rate: 2.3819e-05
	LOSS [training: 0.0588234886047053 | validation: 0.07500888544161176]
	TIME [epoch: 5.71 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06026492374240844		[learning rate: 2.3735e-05]
	Learning Rate: 2.37347e-05
	LOSS [training: 0.06026492374240844 | validation: 0.07450261869896173]
	TIME [epoch: 5.7 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05834142511854456		[learning rate: 2.3651e-05]
	Learning Rate: 2.36508e-05
	LOSS [training: 0.05834142511854456 | validation: 0.07406683763737702]
	TIME [epoch: 5.71 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05700552555757748		[learning rate: 2.3567e-05]
	Learning Rate: 2.35672e-05
	LOSS [training: 0.05700552555757748 | validation: 0.06879024519984628]
	TIME [epoch: 5.7 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059924549255868785		[learning rate: 2.3484e-05]
	Learning Rate: 2.34838e-05
	LOSS [training: 0.059924549255868785 | validation: 0.06856570482449552]
	TIME [epoch: 5.71 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05807651372066006		[learning rate: 2.3401e-05]
	Learning Rate: 2.34008e-05
	LOSS [training: 0.05807651372066006 | validation: 0.07663945806293017]
	TIME [epoch: 5.7 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05656816033814197		[learning rate: 2.3318e-05]
	Learning Rate: 2.33181e-05
	LOSS [training: 0.05656816033814197 | validation: 0.06809387418461589]
	TIME [epoch: 5.71 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058091353737447574		[learning rate: 2.3236e-05]
	Learning Rate: 2.32356e-05
	LOSS [training: 0.058091353737447574 | validation: 0.07961471278746875]
	TIME [epoch: 5.72 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05929340043893141		[learning rate: 2.3153e-05]
	Learning Rate: 2.31534e-05
	LOSS [training: 0.05929340043893141 | validation: 0.07327129543972251]
	TIME [epoch: 5.71 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05994236768099862		[learning rate: 2.3072e-05]
	Learning Rate: 2.30716e-05
	LOSS [training: 0.05994236768099862 | validation: 0.0732297943753581]
	TIME [epoch: 5.7 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05983744706367245		[learning rate: 2.299e-05]
	Learning Rate: 2.299e-05
	LOSS [training: 0.05983744706367245 | validation: 0.07322140408027672]
	TIME [epoch: 5.71 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06069741725648996		[learning rate: 2.2909e-05]
	Learning Rate: 2.29087e-05
	LOSS [training: 0.06069741725648996 | validation: 0.07323550788696599]
	TIME [epoch: 5.7 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05880626093364371		[learning rate: 2.2828e-05]
	Learning Rate: 2.28277e-05
	LOSS [training: 0.05880626093364371 | validation: 0.07972765320542008]
	TIME [epoch: 5.71 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05988929074084611		[learning rate: 2.2747e-05]
	Learning Rate: 2.27469e-05
	LOSS [training: 0.05988929074084611 | validation: 0.07638627487555966]
	TIME [epoch: 5.71 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05972891608183026		[learning rate: 2.2667e-05]
	Learning Rate: 2.26665e-05
	LOSS [training: 0.05972891608183026 | validation: 0.07571035852839078]
	TIME [epoch: 5.71 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06088419873078648		[learning rate: 2.2586e-05]
	Learning Rate: 2.25864e-05
	LOSS [training: 0.06088419873078648 | validation: 0.07298730584417536]
	TIME [epoch: 5.7 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058223171637996715		[learning rate: 2.2506e-05]
	Learning Rate: 2.25065e-05
	LOSS [training: 0.058223171637996715 | validation: 0.0728397998266153]
	TIME [epoch: 5.71 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058826178840208375		[learning rate: 2.2427e-05]
	Learning Rate: 2.24269e-05
	LOSS [training: 0.058826178840208375 | validation: 0.07189131603597448]
	TIME [epoch: 5.71 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05940896146696645		[learning rate: 2.2348e-05]
	Learning Rate: 2.23476e-05
	LOSS [training: 0.05940896146696645 | validation: 0.06956740659563376]
	TIME [epoch: 5.71 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0592007463126974		[learning rate: 2.2269e-05]
	Learning Rate: 2.22686e-05
	LOSS [training: 0.0592007463126974 | validation: 0.06654024435287663]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1775.pth
	Model improved!!!
EPOCH 1776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05878137233777803		[learning rate: 2.219e-05]
	Learning Rate: 2.21898e-05
	LOSS [training: 0.05878137233777803 | validation: 0.07552247836668924]
	TIME [epoch: 5.71 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05990090396236752		[learning rate: 2.2111e-05]
	Learning Rate: 2.21114e-05
	LOSS [training: 0.05990090396236752 | validation: 0.07337016470689003]
	TIME [epoch: 5.7 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05834971708014511		[learning rate: 2.2033e-05]
	Learning Rate: 2.20332e-05
	LOSS [training: 0.05834971708014511 | validation: 0.06647395375002804]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1778.pth
	Model improved!!!
EPOCH 1779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05786821645050814		[learning rate: 2.1955e-05]
	Learning Rate: 2.19553e-05
	LOSS [training: 0.05786821645050814 | validation: 0.0655958025794011]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1779.pth
	Model improved!!!
EPOCH 1780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05883515038842118		[learning rate: 2.1878e-05]
	Learning Rate: 2.18776e-05
	LOSS [training: 0.05883515038842118 | validation: 0.0730667590106948]
	TIME [epoch: 5.7 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05980491149228554		[learning rate: 2.18e-05]
	Learning Rate: 2.18003e-05
	LOSS [training: 0.05980491149228554 | validation: 0.07932579965821038]
	TIME [epoch: 5.72 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05988329985932033		[learning rate: 2.1723e-05]
	Learning Rate: 2.17232e-05
	LOSS [training: 0.05988329985932033 | validation: 0.07097749209034701]
	TIME [epoch: 5.71 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05846297430717999		[learning rate: 2.1646e-05]
	Learning Rate: 2.16464e-05
	LOSS [training: 0.05846297430717999 | validation: 0.07293777679762868]
	TIME [epoch: 5.7 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057765016210086645		[learning rate: 2.157e-05]
	Learning Rate: 2.15698e-05
	LOSS [training: 0.057765016210086645 | validation: 0.06841737408639471]
	TIME [epoch: 5.71 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05887187082854947		[learning rate: 2.1494e-05]
	Learning Rate: 2.14935e-05
	LOSS [training: 0.05887187082854947 | validation: 0.07604109353744788]
	TIME [epoch: 5.7 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05927444760807883		[learning rate: 2.1418e-05]
	Learning Rate: 2.14175e-05
	LOSS [training: 0.05927444760807883 | validation: 0.07449605996387844]
	TIME [epoch: 5.71 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05833590305458964		[learning rate: 2.1342e-05]
	Learning Rate: 2.13418e-05
	LOSS [training: 0.05833590305458964 | validation: 0.07579125002759747]
	TIME [epoch: 5.71 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057129487487423226		[learning rate: 2.1266e-05]
	Learning Rate: 2.12663e-05
	LOSS [training: 0.057129487487423226 | validation: 0.07953697616451581]
	TIME [epoch: 5.7 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05960014447845912		[learning rate: 2.1191e-05]
	Learning Rate: 2.11911e-05
	LOSS [training: 0.05960014447845912 | validation: 0.06936579370499718]
	TIME [epoch: 5.7 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058056155371688054		[learning rate: 2.1116e-05]
	Learning Rate: 2.11162e-05
	LOSS [training: 0.058056155371688054 | validation: 0.06973919389441612]
	TIME [epoch: 5.71 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058078435151185125		[learning rate: 2.1042e-05]
	Learning Rate: 2.10415e-05
	LOSS [training: 0.058078435151185125 | validation: 0.08108479581239252]
	TIME [epoch: 5.7 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059296462995501356		[learning rate: 2.0967e-05]
	Learning Rate: 2.09671e-05
	LOSS [training: 0.059296462995501356 | validation: 0.07192528986156993]
	TIME [epoch: 5.71 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059023333613789676		[learning rate: 2.0893e-05]
	Learning Rate: 2.0893e-05
	LOSS [training: 0.059023333613789676 | validation: 0.07581672624217918]
	TIME [epoch: 5.7 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05704767459450457		[learning rate: 2.0819e-05]
	Learning Rate: 2.08191e-05
	LOSS [training: 0.05704767459450457 | validation: 0.07527203487660443]
	TIME [epoch: 5.71 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059000066380700975		[learning rate: 2.0745e-05]
	Learning Rate: 2.07455e-05
	LOSS [training: 0.059000066380700975 | validation: 0.07408237519265042]
	TIME [epoch: 5.7 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0581348605486407		[learning rate: 2.0672e-05]
	Learning Rate: 2.06721e-05
	LOSS [training: 0.0581348605486407 | validation: 0.07825141165296488]
	TIME [epoch: 5.71 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05837008614174126		[learning rate: 2.0599e-05]
	Learning Rate: 2.0599e-05
	LOSS [training: 0.05837008614174126 | validation: 0.06539551827086447]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1797.pth
	Model improved!!!
EPOCH 1798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057468110551135945		[learning rate: 2.0526e-05]
	Learning Rate: 2.05262e-05
	LOSS [training: 0.057468110551135945 | validation: 0.07180380241189448]
	TIME [epoch: 5.7 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0579714121714327		[learning rate: 2.0454e-05]
	Learning Rate: 2.04536e-05
	LOSS [training: 0.0579714121714327 | validation: 0.08012289932313053]
	TIME [epoch: 5.7 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059532131182714956		[learning rate: 2.0381e-05]
	Learning Rate: 2.03812e-05
	LOSS [training: 0.059532131182714956 | validation: 0.07623279022966921]
	TIME [epoch: 5.71 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05707103110101718		[learning rate: 2.0309e-05]
	Learning Rate: 2.03092e-05
	LOSS [training: 0.05707103110101718 | validation: 0.07418040830011395]
	TIME [epoch: 5.7 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05786554161207815		[learning rate: 2.0237e-05]
	Learning Rate: 2.02374e-05
	LOSS [training: 0.05786554161207815 | validation: 0.07441601572550685]
	TIME [epoch: 5.73 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059170076843281995		[learning rate: 2.0166e-05]
	Learning Rate: 2.01658e-05
	LOSS [training: 0.059170076843281995 | validation: 0.07350516177214404]
	TIME [epoch: 5.7 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05673500185965646		[learning rate: 2.0094e-05]
	Learning Rate: 2.00945e-05
	LOSS [training: 0.05673500185965646 | validation: 0.07631098534311113]
	TIME [epoch: 5.7 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05990256898487755		[learning rate: 2.0023e-05]
	Learning Rate: 2.00234e-05
	LOSS [training: 0.05990256898487755 | validation: 0.06999153518490096]
	TIME [epoch: 5.71 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05584136445083446		[learning rate: 1.9953e-05]
	Learning Rate: 1.99526e-05
	LOSS [training: 0.05584136445083446 | validation: 0.0684828965542834]
	TIME [epoch: 5.7 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05875307940901629		[learning rate: 1.9882e-05]
	Learning Rate: 1.98821e-05
	LOSS [training: 0.05875307940901629 | validation: 0.08036253122166642]
	TIME [epoch: 5.7 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05948155172099122		[learning rate: 1.9812e-05]
	Learning Rate: 1.98118e-05
	LOSS [training: 0.05948155172099122 | validation: 0.06702617357889334]
	TIME [epoch: 5.7 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060668908057146094		[learning rate: 1.9742e-05]
	Learning Rate: 1.97417e-05
	LOSS [training: 0.060668908057146094 | validation: 0.0752241103257676]
	TIME [epoch: 5.7 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057990005416109214		[learning rate: 1.9672e-05]
	Learning Rate: 1.96719e-05
	LOSS [training: 0.057990005416109214 | validation: 0.06668000140805139]
	TIME [epoch: 5.7 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05878827389239438		[learning rate: 1.9602e-05]
	Learning Rate: 1.96023e-05
	LOSS [training: 0.05878827389239438 | validation: 0.07090429842810714]
	TIME [epoch: 5.7 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057398405154455934		[learning rate: 1.9533e-05]
	Learning Rate: 1.9533e-05
	LOSS [training: 0.057398405154455934 | validation: 0.07096536375462727]
	TIME [epoch: 5.71 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05838935598949875		[learning rate: 1.9464e-05]
	Learning Rate: 1.94639e-05
	LOSS [training: 0.05838935598949875 | validation: 0.06466836160122667]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1813.pth
	Model improved!!!
EPOCH 1814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057029034321570915		[learning rate: 1.9395e-05]
	Learning Rate: 1.93951e-05
	LOSS [training: 0.057029034321570915 | validation: 0.07452374829059824]
	TIME [epoch: 5.71 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05601356565558145		[learning rate: 1.9327e-05]
	Learning Rate: 1.93265e-05
	LOSS [training: 0.05601356565558145 | validation: 0.06658890087500205]
	TIME [epoch: 5.7 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05814272179953454		[learning rate: 1.9258e-05]
	Learning Rate: 1.92582e-05
	LOSS [training: 0.05814272179953454 | validation: 0.07744473333511916]
	TIME [epoch: 5.7 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058846992125250165		[learning rate: 1.919e-05]
	Learning Rate: 1.91901e-05
	LOSS [training: 0.058846992125250165 | validation: 0.07057520324910352]
	TIME [epoch: 5.7 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056696601191311695		[learning rate: 1.9122e-05]
	Learning Rate: 1.91222e-05
	LOSS [training: 0.056696601191311695 | validation: 0.075829123644459]
	TIME [epoch: 5.71 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05754009082362103		[learning rate: 1.9055e-05]
	Learning Rate: 1.90546e-05
	LOSS [training: 0.05754009082362103 | validation: 0.07356902623198855]
	TIME [epoch: 5.7 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05767730808881051		[learning rate: 1.8987e-05]
	Learning Rate: 1.89872e-05
	LOSS [training: 0.05767730808881051 | validation: 0.07166373386669322]
	TIME [epoch: 5.7 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05775970407869426		[learning rate: 1.892e-05]
	Learning Rate: 1.89201e-05
	LOSS [training: 0.05775970407869426 | validation: 0.0729327082945555]
	TIME [epoch: 5.7 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05795487935782921		[learning rate: 1.8853e-05]
	Learning Rate: 1.88532e-05
	LOSS [training: 0.05795487935782921 | validation: 0.07209143915752496]
	TIME [epoch: 5.71 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05811080344950547		[learning rate: 1.8787e-05]
	Learning Rate: 1.87865e-05
	LOSS [training: 0.05811080344950547 | validation: 0.06814349438469325]
	TIME [epoch: 5.7 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05768363990907304		[learning rate: 1.872e-05]
	Learning Rate: 1.87201e-05
	LOSS [training: 0.05768363990907304 | validation: 0.07266324579083128]
	TIME [epoch: 5.7 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05658116991624545		[learning rate: 1.8654e-05]
	Learning Rate: 1.86539e-05
	LOSS [training: 0.05658116991624545 | validation: 0.0716522114924678]
	TIME [epoch: 5.7 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05970637515409608		[learning rate: 1.8588e-05]
	Learning Rate: 1.85879e-05
	LOSS [training: 0.05970637515409608 | validation: 0.06694349130759024]
	TIME [epoch: 5.71 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05768038841796083		[learning rate: 1.8522e-05]
	Learning Rate: 1.85222e-05
	LOSS [training: 0.05768038841796083 | validation: 0.07164594394972613]
	TIME [epoch: 5.7 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05927059348542676		[learning rate: 1.8457e-05]
	Learning Rate: 1.84567e-05
	LOSS [training: 0.05927059348542676 | validation: 0.07257694277615494]
	TIME [epoch: 5.71 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05847645812757917		[learning rate: 1.8391e-05]
	Learning Rate: 1.83914e-05
	LOSS [training: 0.05847645812757917 | validation: 0.06822642364584605]
	TIME [epoch: 5.69 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05934298306141357		[learning rate: 1.8326e-05]
	Learning Rate: 1.83264e-05
	LOSS [training: 0.05934298306141357 | validation: 0.07303659503820868]
	TIME [epoch: 5.7 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054750422404469694		[learning rate: 1.8262e-05]
	Learning Rate: 1.82616e-05
	LOSS [training: 0.054750422404469694 | validation: 0.07961930477615554]
	TIME [epoch: 5.7 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05873204681016729		[learning rate: 1.8197e-05]
	Learning Rate: 1.8197e-05
	LOSS [training: 0.05873204681016729 | validation: 0.07122419466021518]
	TIME [epoch: 5.71 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05751447116556617		[learning rate: 1.8133e-05]
	Learning Rate: 1.81327e-05
	LOSS [training: 0.05751447116556617 | validation: 0.07933346709867559]
	TIME [epoch: 5.7 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06073413494969955		[learning rate: 1.8069e-05]
	Learning Rate: 1.80685e-05
	LOSS [training: 0.06073413494969955 | validation: 0.06750219219941199]
	TIME [epoch: 5.7 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05913323065947686		[learning rate: 1.8005e-05]
	Learning Rate: 1.80047e-05
	LOSS [training: 0.05913323065947686 | validation: 0.06231854488026935]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1835.pth
	Model improved!!!
EPOCH 1836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05793112721602098		[learning rate: 1.7941e-05]
	Learning Rate: 1.7941e-05
	LOSS [training: 0.05793112721602098 | validation: 0.07766128974506958]
	TIME [epoch: 5.71 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05677529030077869		[learning rate: 1.7878e-05]
	Learning Rate: 1.78775e-05
	LOSS [training: 0.05677529030077869 | validation: 0.0727755293635765]
	TIME [epoch: 5.7 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05838798495043566		[learning rate: 1.7814e-05]
	Learning Rate: 1.78143e-05
	LOSS [training: 0.05838798495043566 | validation: 0.07072573087924927]
	TIME [epoch: 5.7 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0569171380521352		[learning rate: 1.7751e-05]
	Learning Rate: 1.77513e-05
	LOSS [training: 0.0569171380521352 | validation: 0.06921694717252008]
	TIME [epoch: 5.7 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05903480942310129		[learning rate: 1.7689e-05]
	Learning Rate: 1.76886e-05
	LOSS [training: 0.05903480942310129 | validation: 0.07807442958978929]
	TIME [epoch: 5.7 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05829643580319111		[learning rate: 1.7626e-05]
	Learning Rate: 1.7626e-05
	LOSS [training: 0.05829643580319111 | validation: 0.07504099834033534]
	TIME [epoch: 5.7 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059811513826825015		[learning rate: 1.7564e-05]
	Learning Rate: 1.75637e-05
	LOSS [training: 0.059811513826825015 | validation: 0.0708174725090604]
	TIME [epoch: 5.7 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05858456328276706		[learning rate: 1.7502e-05]
	Learning Rate: 1.75016e-05
	LOSS [training: 0.05858456328276706 | validation: 0.06980747478504391]
	TIME [epoch: 5.7 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05924185746028582		[learning rate: 1.744e-05]
	Learning Rate: 1.74397e-05
	LOSS [training: 0.05924185746028582 | validation: 0.07343488851064622]
	TIME [epoch: 5.7 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05866985869005086		[learning rate: 1.7378e-05]
	Learning Rate: 1.7378e-05
	LOSS [training: 0.05866985869005086 | validation: 0.07203669000625015]
	TIME [epoch: 5.7 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0583289739397082		[learning rate: 1.7317e-05]
	Learning Rate: 1.73166e-05
	LOSS [training: 0.0583289739397082 | validation: 0.07518772579085914]
	TIME [epoch: 5.7 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057603311066233794		[learning rate: 1.7255e-05]
	Learning Rate: 1.72553e-05
	LOSS [training: 0.057603311066233794 | validation: 0.07316132014401369]
	TIME [epoch: 5.7 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056777217902457836		[learning rate: 1.7194e-05]
	Learning Rate: 1.71943e-05
	LOSS [training: 0.056777217902457836 | validation: 0.06610894936834891]
	TIME [epoch: 5.7 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05759080452305246		[learning rate: 1.7134e-05]
	Learning Rate: 1.71335e-05
	LOSS [training: 0.05759080452305246 | validation: 0.07929547100739302]
	TIME [epoch: 5.71 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05622247752756462		[learning rate: 1.7073e-05]
	Learning Rate: 1.70729e-05
	LOSS [training: 0.05622247752756462 | validation: 0.07390743327962233]
	TIME [epoch: 5.7 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0575860790083723		[learning rate: 1.7013e-05]
	Learning Rate: 1.70125e-05
	LOSS [training: 0.0575860790083723 | validation: 0.07536448983577981]
	TIME [epoch: 5.7 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05690410986371654		[learning rate: 1.6952e-05]
	Learning Rate: 1.69524e-05
	LOSS [training: 0.05690410986371654 | validation: 0.07066439573313156]
	TIME [epoch: 5.7 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058669044357604845		[learning rate: 1.6892e-05]
	Learning Rate: 1.68924e-05
	LOSS [training: 0.058669044357604845 | validation: 0.0747346964851594]
	TIME [epoch: 5.7 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05777178861788208		[learning rate: 1.6833e-05]
	Learning Rate: 1.68327e-05
	LOSS [training: 0.05777178861788208 | validation: 0.07104676922026486]
	TIME [epoch: 5.7 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05750347635535098		[learning rate: 1.6773e-05]
	Learning Rate: 1.67732e-05
	LOSS [training: 0.05750347635535098 | validation: 0.0681186674565788]
	TIME [epoch: 5.7 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05705396503407193		[learning rate: 1.6714e-05]
	Learning Rate: 1.67139e-05
	LOSS [training: 0.05705396503407193 | validation: 0.06599613384709174]
	TIME [epoch: 5.7 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057303325365356754		[learning rate: 1.6655e-05]
	Learning Rate: 1.66548e-05
	LOSS [training: 0.057303325365356754 | validation: 0.06872373996355298]
	TIME [epoch: 5.71 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05640786740320991		[learning rate: 1.6596e-05]
	Learning Rate: 1.65959e-05
	LOSS [training: 0.05640786740320991 | validation: 0.06439147958269406]
	TIME [epoch: 5.7 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05789002217096465		[learning rate: 1.6537e-05]
	Learning Rate: 1.65372e-05
	LOSS [training: 0.05789002217096465 | validation: 0.07022850938483637]
	TIME [epoch: 5.71 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05649984978701699		[learning rate: 1.6479e-05]
	Learning Rate: 1.64787e-05
	LOSS [training: 0.05649984978701699 | validation: 0.06723912492963521]
	TIME [epoch: 5.7 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057013111626491586		[learning rate: 1.642e-05]
	Learning Rate: 1.64204e-05
	LOSS [training: 0.057013111626491586 | validation: 0.06958041383161746]
	TIME [epoch: 5.7 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05766809689686153		[learning rate: 1.6362e-05]
	Learning Rate: 1.63624e-05
	LOSS [training: 0.05766809689686153 | validation: 0.08024442001013249]
	TIME [epoch: 5.7 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05748430656762916		[learning rate: 1.6305e-05]
	Learning Rate: 1.63045e-05
	LOSS [training: 0.05748430656762916 | validation: 0.07429928488167241]
	TIME [epoch: 5.7 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05746575422829912		[learning rate: 1.6247e-05]
	Learning Rate: 1.62469e-05
	LOSS [training: 0.05746575422829912 | validation: 0.06806145674084708]
	TIME [epoch: 5.7 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057926802674207016		[learning rate: 1.6189e-05]
	Learning Rate: 1.61894e-05
	LOSS [training: 0.057926802674207016 | validation: 0.07224023076031975]
	TIME [epoch: 5.7 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05892091410085343		[learning rate: 1.6132e-05]
	Learning Rate: 1.61322e-05
	LOSS [training: 0.05892091410085343 | validation: 0.07462446639728462]
	TIME [epoch: 5.7 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057319568636358495		[learning rate: 1.6075e-05]
	Learning Rate: 1.60751e-05
	LOSS [training: 0.057319568636358495 | validation: 0.07246181409187767]
	TIME [epoch: 5.71 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05484647154211558		[learning rate: 1.6018e-05]
	Learning Rate: 1.60183e-05
	LOSS [training: 0.05484647154211558 | validation: 0.0785229557999455]
	TIME [epoch: 5.7 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05667690820718083		[learning rate: 1.5962e-05]
	Learning Rate: 1.59616e-05
	LOSS [training: 0.05667690820718083 | validation: 0.07112635629740789]
	TIME [epoch: 5.71 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05782492281959957		[learning rate: 1.5905e-05]
	Learning Rate: 1.59052e-05
	LOSS [training: 0.05782492281959957 | validation: 0.07803171241035842]
	TIME [epoch: 5.71 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05914908304144552		[learning rate: 1.5849e-05]
	Learning Rate: 1.58489e-05
	LOSS [training: 0.05914908304144552 | validation: 0.068420643404532]
	TIME [epoch: 5.71 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05461799027750397		[learning rate: 1.5793e-05]
	Learning Rate: 1.57929e-05
	LOSS [training: 0.05461799027750397 | validation: 0.07350062718037462]
	TIME [epoch: 5.71 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05721115368581067		[learning rate: 1.5737e-05]
	Learning Rate: 1.5737e-05
	LOSS [training: 0.05721115368581067 | validation: 0.06960669422857485]
	TIME [epoch: 5.71 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056175521163889054		[learning rate: 1.5681e-05]
	Learning Rate: 1.56814e-05
	LOSS [training: 0.056175521163889054 | validation: 0.07784788707122257]
	TIME [epoch: 5.7 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057040896564924906		[learning rate: 1.5626e-05]
	Learning Rate: 1.56259e-05
	LOSS [training: 0.057040896564924906 | validation: 0.0713715288153908]
	TIME [epoch: 5.7 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05884186571288012		[learning rate: 1.5571e-05]
	Learning Rate: 1.55707e-05
	LOSS [training: 0.05884186571288012 | validation: 0.07243617095205278]
	TIME [epoch: 5.7 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058384062355086516		[learning rate: 1.5516e-05]
	Learning Rate: 1.55156e-05
	LOSS [training: 0.058384062355086516 | validation: 0.07770751499033646]
	TIME [epoch: 5.7 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057972496767152076		[learning rate: 1.5461e-05]
	Learning Rate: 1.54608e-05
	LOSS [training: 0.057972496767152076 | validation: 0.06571272807802045]
	TIME [epoch: 5.7 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056883910339497926		[learning rate: 1.5406e-05]
	Learning Rate: 1.54061e-05
	LOSS [training: 0.056883910339497926 | validation: 0.07053316682739927]
	TIME [epoch: 5.7 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057669164286142625		[learning rate: 1.5352e-05]
	Learning Rate: 1.53516e-05
	LOSS [training: 0.057669164286142625 | validation: 0.06976725657006282]
	TIME [epoch: 5.7 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05683437284703151		[learning rate: 1.5297e-05]
	Learning Rate: 1.52973e-05
	LOSS [training: 0.05683437284703151 | validation: 0.07754190228321749]
	TIME [epoch: 5.7 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057961487864276656		[learning rate: 1.5243e-05]
	Learning Rate: 1.52432e-05
	LOSS [training: 0.057961487864276656 | validation: 0.07149071283517917]
	TIME [epoch: 5.7 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058324323514099		[learning rate: 1.5189e-05]
	Learning Rate: 1.51893e-05
	LOSS [training: 0.058324323514099 | validation: 0.06736316810996061]
	TIME [epoch: 5.7 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058520289491423724		[learning rate: 1.5136e-05]
	Learning Rate: 1.51356e-05
	LOSS [training: 0.058520289491423724 | validation: 0.0713423795470687]
	TIME [epoch: 5.7 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05878359159147584		[learning rate: 1.5082e-05]
	Learning Rate: 1.50821e-05
	LOSS [training: 0.05878359159147584 | validation: 0.07060322174911361]
	TIME [epoch: 5.71 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05725549898590037		[learning rate: 1.5029e-05]
	Learning Rate: 1.50288e-05
	LOSS [training: 0.05725549898590037 | validation: 0.06725580676307302]
	TIME [epoch: 5.7 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0580905645819778		[learning rate: 1.4976e-05]
	Learning Rate: 1.49756e-05
	LOSS [training: 0.0580905645819778 | validation: 0.07682363241961754]
	TIME [epoch: 5.7 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05853920737391004		[learning rate: 1.4923e-05]
	Learning Rate: 1.49227e-05
	LOSS [training: 0.05853920737391004 | validation: 0.07574779668259722]
	TIME [epoch: 5.71 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05741144420178822		[learning rate: 1.487e-05]
	Learning Rate: 1.48699e-05
	LOSS [training: 0.05741144420178822 | validation: 0.07316189607423776]
	TIME [epoch: 5.7 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05609545423730323		[learning rate: 1.4817e-05]
	Learning Rate: 1.48173e-05
	LOSS [training: 0.05609545423730323 | validation: 0.06323994276005544]
	TIME [epoch: 5.71 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057796989522631535		[learning rate: 1.4765e-05]
	Learning Rate: 1.47649e-05
	LOSS [training: 0.057796989522631535 | validation: 0.06957391250506288]
	TIME [epoch: 5.7 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05748276155739985		[learning rate: 1.4713e-05]
	Learning Rate: 1.47127e-05
	LOSS [training: 0.05748276155739985 | validation: 0.06968437149302172]
	TIME [epoch: 5.7 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058109161424975825		[learning rate: 1.4661e-05]
	Learning Rate: 1.46607e-05
	LOSS [training: 0.058109161424975825 | validation: 0.06484708673158678]
	TIME [epoch: 5.7 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05649683613243994		[learning rate: 1.4609e-05]
	Learning Rate: 1.46088e-05
	LOSS [training: 0.05649683613243994 | validation: 0.06964663921238316]
	TIME [epoch: 5.7 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05906009342751212		[learning rate: 1.4557e-05]
	Learning Rate: 1.45572e-05
	LOSS [training: 0.05906009342751212 | validation: 0.06979699953015142]
	TIME [epoch: 5.71 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05708353346829982		[learning rate: 1.4506e-05]
	Learning Rate: 1.45057e-05
	LOSS [training: 0.05708353346829982 | validation: 0.06445028018857647]
	TIME [epoch: 5.71 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05652153781781662		[learning rate: 1.4454e-05]
	Learning Rate: 1.44544e-05
	LOSS [training: 0.05652153781781662 | validation: 0.07398525964368804]
	TIME [epoch: 5.7 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05808237110924985		[learning rate: 1.4403e-05]
	Learning Rate: 1.44033e-05
	LOSS [training: 0.05808237110924985 | validation: 0.0708691659294612]
	TIME [epoch: 5.71 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05609783207602893		[learning rate: 1.4352e-05]
	Learning Rate: 1.43524e-05
	LOSS [training: 0.05609783207602893 | validation: 0.07346834343256095]
	TIME [epoch: 5.7 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05627035266294561		[learning rate: 1.4302e-05]
	Learning Rate: 1.43016e-05
	LOSS [training: 0.05627035266294561 | validation: 0.0693453984348302]
	TIME [epoch: 5.7 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05722738023088361		[learning rate: 1.4251e-05]
	Learning Rate: 1.4251e-05
	LOSS [training: 0.05722738023088361 | validation: 0.07264751879276077]
	TIME [epoch: 5.7 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05839861503539411		[learning rate: 1.4201e-05]
	Learning Rate: 1.42006e-05
	LOSS [training: 0.05839861503539411 | validation: 0.063399629214194]
	TIME [epoch: 5.7 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05709428013157153		[learning rate: 1.415e-05]
	Learning Rate: 1.41504e-05
	LOSS [training: 0.05709428013157153 | validation: 0.06580258471661797]
	TIME [epoch: 5.7 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057564895161912696		[learning rate: 1.41e-05]
	Learning Rate: 1.41004e-05
	LOSS [training: 0.057564895161912696 | validation: 0.0626707166680031]
	TIME [epoch: 5.7 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05838204649895478		[learning rate: 1.4051e-05]
	Learning Rate: 1.40505e-05
	LOSS [training: 0.05838204649895478 | validation: 0.0745685570104984]
	TIME [epoch: 5.7 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05731094435565592		[learning rate: 1.4001e-05]
	Learning Rate: 1.40008e-05
	LOSS [training: 0.05731094435565592 | validation: 0.07231382792708232]
	TIME [epoch: 5.7 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05788098353367774		[learning rate: 1.3951e-05]
	Learning Rate: 1.39513e-05
	LOSS [training: 0.05788098353367774 | validation: 0.07455732860325466]
	TIME [epoch: 5.7 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05663926673856617		[learning rate: 1.3902e-05]
	Learning Rate: 1.3902e-05
	LOSS [training: 0.05663926673856617 | validation: 0.06922549645471927]
	TIME [epoch: 5.7 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05733751116841807		[learning rate: 1.3853e-05]
	Learning Rate: 1.38528e-05
	LOSS [training: 0.05733751116841807 | validation: 0.07162705934663556]
	TIME [epoch: 5.71 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056972491631739466		[learning rate: 1.3804e-05]
	Learning Rate: 1.38038e-05
	LOSS [training: 0.056972491631739466 | validation: 0.07698861257591175]
	TIME [epoch: 5.7 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05799396046838771		[learning rate: 1.3755e-05]
	Learning Rate: 1.3755e-05
	LOSS [training: 0.05799396046838771 | validation: 0.07057343894317405]
	TIME [epoch: 5.7 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05669186056990179		[learning rate: 1.3706e-05]
	Learning Rate: 1.37064e-05
	LOSS [training: 0.05669186056990179 | validation: 0.0768016913755671]
	TIME [epoch: 5.7 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05630300445175845		[learning rate: 1.3658e-05]
	Learning Rate: 1.36579e-05
	LOSS [training: 0.05630300445175845 | validation: 0.06834699688415173]
	TIME [epoch: 5.7 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05714996710572296		[learning rate: 1.361e-05]
	Learning Rate: 1.36096e-05
	LOSS [training: 0.05714996710572296 | validation: 0.07394728402965615]
	TIME [epoch: 5.7 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05799777710063918		[learning rate: 1.3562e-05]
	Learning Rate: 1.35615e-05
	LOSS [training: 0.05799777710063918 | validation: 0.06346447997498532]
	TIME [epoch: 5.7 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05673547966904552		[learning rate: 1.3514e-05]
	Learning Rate: 1.35135e-05
	LOSS [training: 0.05673547966904552 | validation: 0.07206302937836379]
	TIME [epoch: 5.71 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05657974689760593		[learning rate: 1.3466e-05]
	Learning Rate: 1.34658e-05
	LOSS [training: 0.05657974689760593 | validation: 0.07229025705625045]
	TIME [epoch: 5.7 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05757737105560464		[learning rate: 1.3418e-05]
	Learning Rate: 1.34181e-05
	LOSS [training: 0.05757737105560464 | validation: 0.07951219041130604]
	TIME [epoch: 5.71 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05699608466451585		[learning rate: 1.3371e-05]
	Learning Rate: 1.33707e-05
	LOSS [training: 0.05699608466451585 | validation: 0.07928299730403268]
	TIME [epoch: 5.7 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05666095056740018		[learning rate: 1.3323e-05]
	Learning Rate: 1.33234e-05
	LOSS [training: 0.05666095056740018 | validation: 0.07090560401298653]
	TIME [epoch: 5.71 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05853153772657348		[learning rate: 1.3276e-05]
	Learning Rate: 1.32763e-05
	LOSS [training: 0.05853153772657348 | validation: 0.07570189596730775]
	TIME [epoch: 5.7 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05894123193644205		[learning rate: 1.3229e-05]
	Learning Rate: 1.32294e-05
	LOSS [training: 0.05894123193644205 | validation: 0.068331577849487]
	TIME [epoch: 5.7 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05703217499008188		[learning rate: 1.3183e-05]
	Learning Rate: 1.31826e-05
	LOSS [training: 0.05703217499008188 | validation: 0.07389555616528519]
	TIME [epoch: 5.7 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057459332317267416		[learning rate: 1.3136e-05]
	Learning Rate: 1.3136e-05
	LOSS [training: 0.057459332317267416 | validation: 0.07439999535112861]
	TIME [epoch: 5.7 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05622546599723393		[learning rate: 1.309e-05]
	Learning Rate: 1.30895e-05
	LOSS [training: 0.05622546599723393 | validation: 0.06943681167576256]
	TIME [epoch: 5.7 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055379805145677245		[learning rate: 1.3043e-05]
	Learning Rate: 1.30432e-05
	LOSS [training: 0.055379805145677245 | validation: 0.06941103740704307]
	TIME [epoch: 5.7 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057060747691586915		[learning rate: 1.2997e-05]
	Learning Rate: 1.29971e-05
	LOSS [training: 0.057060747691586915 | validation: 0.07295545134981586]
	TIME [epoch: 5.71 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057169863407655355		[learning rate: 1.2951e-05]
	Learning Rate: 1.29511e-05
	LOSS [training: 0.057169863407655355 | validation: 0.06658205754723256]
	TIME [epoch: 5.7 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0569480949674438		[learning rate: 1.2905e-05]
	Learning Rate: 1.29053e-05
	LOSS [training: 0.0569480949674438 | validation: 0.06656331945273236]
	TIME [epoch: 5.7 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0554240361785469		[learning rate: 1.286e-05]
	Learning Rate: 1.28597e-05
	LOSS [training: 0.0554240361785469 | validation: 0.0731907118868643]
	TIME [epoch: 5.71 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05662831136823805		[learning rate: 1.2814e-05]
	Learning Rate: 1.28142e-05
	LOSS [training: 0.05662831136823805 | validation: 0.06714754566050492]
	TIME [epoch: 5.71 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05677645916171253		[learning rate: 1.2769e-05]
	Learning Rate: 1.27689e-05
	LOSS [training: 0.05677645916171253 | validation: 0.06947769557413878]
	TIME [epoch: 5.71 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05657583728980422		[learning rate: 1.2724e-05]
	Learning Rate: 1.27238e-05
	LOSS [training: 0.05657583728980422 | validation: 0.06684777727098534]
	TIME [epoch: 5.7 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055849778069480584		[learning rate: 1.2679e-05]
	Learning Rate: 1.26788e-05
	LOSS [training: 0.055849778069480584 | validation: 0.065578137900893]
	TIME [epoch: 5.7 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05664583878761066		[learning rate: 1.2634e-05]
	Learning Rate: 1.26339e-05
	LOSS [training: 0.05664583878761066 | validation: 0.06922001774399948]
	TIME [epoch: 5.7 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057470427385898165		[learning rate: 1.2589e-05]
	Learning Rate: 1.25893e-05
	LOSS [training: 0.057470427385898165 | validation: 0.0658312731483556]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_6_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_6_v_mmd4_1936.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 8070.384 seconds.
