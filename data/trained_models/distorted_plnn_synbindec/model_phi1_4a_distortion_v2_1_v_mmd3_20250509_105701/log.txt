Args:
Namespace(name='model_phi1_4a_distortion_v2_1_v_mmd3', outdir='out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_1/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_1/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.021322314, 0.1, 1.0], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 88562011

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6457577863340784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6457577863340784 | validation: 2.804234738206704]
	TIME [epoch: 170 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5345789158466703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5345789158466703 | validation: 2.7573660762223904]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3182618822372216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3182618822372216 | validation: 2.6514291052030203]
	TIME [epoch: 0.661 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.100044471786576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.100044471786576 | validation: 2.657476523136925]
	TIME [epoch: 0.657 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.143998766128487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.143998766128487 | validation: 2.4617883042755784]
	TIME [epoch: 0.662 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9305764318740901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9305764318740901 | validation: 2.3898982323013236]
	TIME [epoch: 0.658 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8593433071032144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8593433071032144 | validation: 2.32334373175743]
	TIME [epoch: 0.662 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6767139760815257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6767139760815257 | validation: 2.3501402233265822]
	TIME [epoch: 0.654 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7153820221545522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7153820221545522 | validation: 2.2632226537273974]
	TIME [epoch: 0.659 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6640251008803073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6640251008803073 | validation: 2.1657828381335245]
	TIME [epoch: 0.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6084996214497196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6084996214497196 | validation: 2.106659631821671]
	TIME [epoch: 0.659 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.524817790125466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.524817790125466 | validation: 2.182587026778973]
	TIME [epoch: 0.66 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5491816920198827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5491816920198827 | validation: 1.95777649849029]
	TIME [epoch: 0.658 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5182021449395209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5182021449395209 | validation: 1.918308240139509]
	TIME [epoch: 0.661 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4889848188249004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4889848188249004 | validation: 1.9712393998971196]
	TIME [epoch: 0.657 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4895032527516008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4895032527516008 | validation: 1.8924509830220189]
	TIME [epoch: 0.659 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4699912833028754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4699912833028754 | validation: 1.8671768487214149]
	TIME [epoch: 0.661 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.467544296418575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.467544296418575 | validation: 1.9079321242483296]
	TIME [epoch: 0.658 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4715732498413432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4715732498413432 | validation: 1.873040409206341]
	TIME [epoch: 0.657 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.464277129877776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.464277129877776 | validation: 1.9141035085466265]
	TIME [epoch: 0.653 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.475425887534372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.475425887534372 | validation: 1.8924066322703519]
	TIME [epoch: 0.655 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4932491439740228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4932491439740228 | validation: 1.9058254235831371]
	TIME [epoch: 0.654 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4558189245458915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4558189245458915 | validation: 1.8490281701449611]
	TIME [epoch: 0.655 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4511983077826187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4511983077826187 | validation: 1.893615211266296]
	TIME [epoch: 0.658 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4438756104514476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4438756104514476 | validation: 1.8243849901665783]
	TIME [epoch: 0.657 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4604331919086848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4604331919086848 | validation: 1.9942826699403975]
	TIME [epoch: 0.658 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4929182775574616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4929182775574616 | validation: 1.7939538577466103]
	TIME [epoch: 0.659 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4917254045880868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4917254045880868 | validation: 1.797612589678024]
	TIME [epoch: 0.658 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4292853179459115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4292853179459115 | validation: 1.8890982828841105]
	TIME [epoch: 0.661 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4393360564839142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4393360564839142 | validation: 1.775728370294025]
	TIME [epoch: 0.655 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.428209061271897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.428209061271897 | validation: 1.7735985187793908]
	TIME [epoch: 0.659 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.424664000069301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.424664000069301 | validation: 1.798706843906718]
	TIME [epoch: 0.66 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4203836512686292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4203836512686292 | validation: 1.7464704649543241]
	TIME [epoch: 0.659 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3986073378418735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3986073378418735 | validation: 1.812233849888408]
	TIME [epoch: 0.661 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4022438346723027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4022438346723027 | validation: 1.7574930771795652]
	TIME [epoch: 0.657 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.529854402900719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.529854402900719 | validation: 1.9262872095458528]
	TIME [epoch: 0.658 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4628846195514515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4628846195514515 | validation: 1.700577726147185]
	TIME [epoch: 0.656 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4806918162982623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4806918162982623 | validation: 1.6576096650673116]
	TIME [epoch: 0.658 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4291366848387583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4291366848387583 | validation: 1.8233312208785053]
	TIME [epoch: 0.662 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4145262379960062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4145262379960062 | validation: 1.6263791975740127]
	TIME [epoch: 0.658 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3810885517638036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3810885517638036 | validation: 1.6456574277798564]
	TIME [epoch: 0.662 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3596377356403133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3596377356403133 | validation: 1.6905875509376305]
	TIME [epoch: 0.656 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.353005412085954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.353005412085954 | validation: 1.622544022330714]
	TIME [epoch: 0.658 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.401921931759141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.401921931759141 | validation: 1.961058051882396]
	TIME [epoch: 0.657 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5277897531418496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5277897531418496 | validation: 1.6475010852793623]
	TIME [epoch: 0.656 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5537248891148432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5537248891148432 | validation: 1.5687102558810035]
	TIME [epoch: 0.676 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4991449408686737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4991449408686737 | validation: 1.5410626522042055]
	TIME [epoch: 0.654 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.40760469803558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.40760469803558 | validation: 1.625935737359788]
	TIME [epoch: 0.662 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3958011605147216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3958011605147216 | validation: 1.5673542132670728]
	TIME [epoch: 0.649 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3516097255077728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3516097255077728 | validation: 1.5666786104102741]
	TIME [epoch: 0.65 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.328118515299841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.328118515299841 | validation: 1.5542941706888151]
	TIME [epoch: 0.651 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.318130253149584		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.318130253149584 | validation: 1.5668863843692133]
	TIME [epoch: 0.652 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3126395676013405		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.3126395676013405 | validation: 1.588215460371533]
	TIME [epoch: 0.651 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3233207216752738		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.3233207216752738 | validation: 1.5236736490808447]
	TIME [epoch: 0.649 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.401806808212098		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.401806808212098 | validation: 1.5769733826429504]
	TIME [epoch: 0.654 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3106966217042635		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.3106966217042635 | validation: 1.455616296003762]
	TIME [epoch: 0.654 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3307895232884557		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.3307895232884557 | validation: 1.5459666768035665]
	TIME [epoch: 0.654 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3017135067073793		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.3017135067073793 | validation: 1.4601525825863488]
	TIME [epoch: 0.652 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3879481954869086		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.3879481954869086 | validation: 1.4932245008060692]
	TIME [epoch: 0.649 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.274436678551416		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.274436678551416 | validation: 1.4136949374363468]
	TIME [epoch: 0.65 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2677577085027096		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.2677577085027096 | validation: 1.4979055712091816]
	TIME [epoch: 0.654 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.301073253566717		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.301073253566717 | validation: 1.5297170124723363]
	TIME [epoch: 0.651 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5284086995144364		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.5284086995144364 | validation: 1.4181182306360212]
	TIME [epoch: 0.652 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.429895165559854		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.429895165559854 | validation: 1.4422535573691753]
	TIME [epoch: 0.65 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.341526515907806		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.341526515907806 | validation: 1.6153812621668031]
	TIME [epoch: 0.653 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3718023351897342		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.3718023351897342 | validation: 1.4564602538659739]
	TIME [epoch: 0.653 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4732353493331356		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.4732353493331356 | validation: 1.4284663601410506]
	TIME [epoch: 0.652 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4529446918188973		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.4529446918188973 | validation: 1.3594280714879519]
	TIME [epoch: 0.651 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3261083377374037		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.3261083377374037 | validation: 1.7019519264269072]
	TIME [epoch: 0.654 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4335808961765464		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.4335808961765464 | validation: 1.4067018075955862]
	TIME [epoch: 0.654 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3866074482984374		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.3866074482984374 | validation: 1.4079813967135308]
	TIME [epoch: 0.651 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3990267240384395		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.3990267240384395 | validation: 1.3606309866891624]
	TIME [epoch: 0.652 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2911706331873907		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.2911706331873907 | validation: 1.5612696721384949]
	TIME [epoch: 0.653 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3772433062421754		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.3772433062421754 | validation: 1.3800875265343628]
	TIME [epoch: 0.648 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3342318747626443		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.3342318747626443 | validation: 1.3803942863934304]
	TIME [epoch: 0.651 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3041423253155642		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.3041423253155642 | validation: 1.4591716188383488]
	TIME [epoch: 0.648 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2872242459509562		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.2872242459509562 | validation: 1.362501713431752]
	TIME [epoch: 0.656 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.271384612301824		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.271384612301824 | validation: 1.3720719846411675]
	TIME [epoch: 0.65 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2419012575591675		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.2419012575591675 | validation: 1.3855151402092563]
	TIME [epoch: 0.651 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2373057627839403		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.2373057627839403 | validation: 1.327307522154471]
	TIME [epoch: 0.648 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2674360115930052		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.2674360115930052 | validation: 1.3968093939283037]
	TIME [epoch: 0.655 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2591585568301318		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.2591585568301318 | validation: 1.3221133296338181]
	TIME [epoch: 0.652 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3018355820516967		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.3018355820516967 | validation: 1.337767914410847]
	TIME [epoch: 0.656 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2050699614699278		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.2050699614699278 | validation: 1.3272596456318606]
	TIME [epoch: 0.649 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2010303207983615		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.2010303207983615 | validation: 1.2864091353951954]
	TIME [epoch: 0.654 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2426671903456628		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.2426671903456628 | validation: 1.5117237978742755]
	TIME [epoch: 0.648 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4039461882975348		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.4039461882975348 | validation: 1.3783498855889456]
	TIME [epoch: 0.651 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.384157267598692		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.384157267598692 | validation: 1.3065867178791302]
	TIME [epoch: 0.65 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2874073770813868		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.2874073770813868 | validation: 1.5153480031316935]
	TIME [epoch: 0.651 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3490377452280995		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.3490377452280995 | validation: 1.3281947749626326]
	TIME [epoch: 0.649 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2976498434549246		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.2976498434549246 | validation: 1.2990022670733101]
	TIME [epoch: 0.649 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.228009966531452		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.228009966531452 | validation: 1.443264286311835]
	TIME [epoch: 0.65 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3259500426789208		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.3259500426789208 | validation: 1.3054516914896703]
	TIME [epoch: 0.65 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2419976465189808		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.2419976465189808 | validation: 1.2847230731686592]
	TIME [epoch: 0.648 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.188667327920463		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.188667327920463 | validation: 1.3413753378547548]
	TIME [epoch: 0.656 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2191854969602716		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.2191854969602716 | validation: 1.2736731233278007]
	TIME [epoch: 0.651 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2579045224342131		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.2579045224342131 | validation: 1.3039856388304967]
	TIME [epoch: 0.66 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1866696502399416		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.1866696502399416 | validation: 1.2676845482598171]
	TIME [epoch: 0.657 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1995644684446956		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.1995644684446956 | validation: 1.2615306684727854]
	TIME [epoch: 0.657 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.173373932275241		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.173373932275241 | validation: 1.2305190869725449]
	TIME [epoch: 0.658 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1527683937741404		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.1527683937741404 | validation: 1.2656076197105062]
	TIME [epoch: 0.66 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1873662565491776		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.1873662565491776 | validation: 1.282049502907386]
	TIME [epoch: 0.656 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3422095557497558		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.3422095557497558 | validation: 1.3495226352991505]
	TIME [epoch: 0.654 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.242847758600058		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.242847758600058 | validation: 1.4313996468450645]
	TIME [epoch: 0.655 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3547822727706984		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.3547822727706984 | validation: 1.543262727016782]
	TIME [epoch: 0.655 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.526977977255038		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.526977977255038 | validation: 1.2470178665816518]
	TIME [epoch: 0.659 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.201440773581641		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.201440773581641 | validation: 1.6434099748786903]
	TIME [epoch: 0.653 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5032963474955148		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.5032963474955148 | validation: 1.2796649795310397]
	TIME [epoch: 0.659 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1514421271919726		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.1514421271919726 | validation: 1.2476466205491776]
	TIME [epoch: 0.654 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1926431953843508		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.1926431953843508 | validation: 1.2988449676161122]
	TIME [epoch: 0.655 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2336390429329467		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.2336390429329467 | validation: 1.213509855578062]
	TIME [epoch: 0.655 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1820782954890467		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.1820782954890467 | validation: 1.2435404863007962]
	TIME [epoch: 0.659 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.14373100802641		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.14373100802641 | validation: 1.2113325792271354]
	TIME [epoch: 0.652 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1416809983658798		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.1416809983658798 | validation: 1.2495019087387984]
	TIME [epoch: 0.66 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1588226975214473		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.1588226975214473 | validation: 1.2202347191031127]
	TIME [epoch: 0.657 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2249033413843267		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.2249033413843267 | validation: 1.2576684859078766]
	TIME [epoch: 0.655 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.151382683702068		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.151382683702068 | validation: 1.2076531921668086]
	TIME [epoch: 0.652 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1836600701382327		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.1836600701382327 | validation: 1.3071442203683632]
	TIME [epoch: 0.656 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2075179195403056		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.2075179195403056 | validation: 1.214942941637563]
	TIME [epoch: 0.657 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.225717544175139		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.225717544175139 | validation: 1.2104664844230206]
	TIME [epoch: 0.656 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1077777718965311		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.1077777718965311 | validation: 1.2052890243170455]
	TIME [epoch: 0.656 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1149577967808044		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.1149577967808044 | validation: 1.1928631733775035]
	TIME [epoch: 0.656 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.187871794788101		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.187871794788101 | validation: 1.31341810651828]
	TIME [epoch: 0.656 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.20058339546288		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.20058339546288 | validation: 1.2085679027509895]
	TIME [epoch: 0.657 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2211538157810282		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.2211538157810282 | validation: 1.2160149316817315]
	TIME [epoch: 0.655 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0945773049573964		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.0945773049573964 | validation: 1.1879156813051963]
	TIME [epoch: 0.655 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1010133347467486		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.1010133347467486 | validation: 1.1742439328558247]
	TIME [epoch: 0.653 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1729482533914786		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.1729482533914786 | validation: 1.2392288344210596]
	TIME [epoch: 0.656 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1600519271947818		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.1600519271947818 | validation: 1.1896412290527232]
	TIME [epoch: 0.658 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1718857367597582		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.1718857367597582 | validation: 1.1949630511504896]
	TIME [epoch: 0.654 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0996308770003367		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.0996308770003367 | validation: 1.1806040480345326]
	TIME [epoch: 0.65 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.115129347419746		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.115129347419746 | validation: 1.2725029432612847]
	TIME [epoch: 0.653 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1434221084313168		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.1434221084313168 | validation: 1.1858407920766196]
	TIME [epoch: 0.652 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2107239245091446		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.2107239245091446 | validation: 1.2058307952010254]
	TIME [epoch: 0.65 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1013348443364777		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.1013348443364777 | validation: 1.1609976824949328]
	TIME [epoch: 0.653 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1954932951275714		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.1954932951275714 | validation: 1.1973227627705127]
	TIME [epoch: 0.654 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0843720670160202		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.0843720670160202 | validation: 1.1536856057291627]
	TIME [epoch: 0.654 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0634190237721475		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.0634190237721475 | validation: 1.124477113553204]
	TIME [epoch: 0.653 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1134592059784216		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.1134592059784216 | validation: 1.312229100961476]
	TIME [epoch: 0.657 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.222853632735823		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.222853632735823 | validation: 1.1736610838041117]
	TIME [epoch: 0.656 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1807947932905423		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.1807947932905423 | validation: 1.1234883424275441]
	TIME [epoch: 0.656 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0469542626363306		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.0469542626363306 | validation: 1.1217478508172782]
	TIME [epoch: 0.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0504747820092117		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.0504747820092117 | validation: 1.1258328233679291]
	TIME [epoch: 0.661 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0752012650525193		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.0752012650525193 | validation: 1.1528745980569746]
	TIME [epoch: 0.655 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1446310964743478		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.1446310964743478 | validation: 1.1706615772684583]
	TIME [epoch: 0.656 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.126131574118868		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.126131574118868 | validation: 1.1497069951538232]
	TIME [epoch: 0.654 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1460790800972642		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.1460790800972642 | validation: 1.167105985062939]
	TIME [epoch: 0.655 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0526908120519347		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.0526908120519347 | validation: 1.0898547027986183]
	TIME [epoch: 0.655 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.073452252850729		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.073452252850729 | validation: 1.2375620721694565]
	TIME [epoch: 0.659 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.159262561860579		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.159262561860579 | validation: 1.123149837194671]
	TIME [epoch: 0.655 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1344508144966232		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.1344508144966232 | validation: 1.1895683945691553]
	TIME [epoch: 0.658 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0586933551667468		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.0586933551667468 | validation: 1.0766890985527702]
	TIME [epoch: 0.656 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_152.pth
	Model improved!!!
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0414965355793675		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.0414965355793675 | validation: 1.1147658474068687]
	TIME [epoch: 0.656 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.054519885737319		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.054519885737319 | validation: 1.1288615421325034]
	TIME [epoch: 0.656 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1253255208027486		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.1253255208027486 | validation: 1.1062941626647782]
	TIME [epoch: 0.656 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.060560772305291		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.060560772305291 | validation: 1.094541346420923]
	TIME [epoch: 0.653 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.07883792911544		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.07883792911544 | validation: 1.128580029563435]
	TIME [epoch: 0.654 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.087698197792811		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.087698197792811 | validation: 1.12549704839734]
	TIME [epoch: 0.659 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1191928308508445		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.1191928308508445 | validation: 1.113735089101387]
	TIME [epoch: 0.654 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.022524351194888		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.022524351194888 | validation: 1.0799564748669013]
	TIME [epoch: 0.651 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0786767936215405		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.0786767936215405 | validation: 1.2221664400846888]
	TIME [epoch: 0.654 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1399482368815814		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.1399482368815814 | validation: 1.0679255899567515]
	TIME [epoch: 0.654 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.063479438952142		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.063479438952142 | validation: 1.106402360130786]
	TIME [epoch: 0.655 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0298524375220064		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.0298524375220064 | validation: 1.0693236588843797]
	TIME [epoch: 0.658 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0556309473905459		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.0556309473905459 | validation: 1.0799538985853054]
	TIME [epoch: 0.655 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0502489550346428		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.0502489550346428 | validation: 1.0700973148360255]
	TIME [epoch: 0.654 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.064538405315636		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.064538405315636 | validation: 1.0539702089991765]
	TIME [epoch: 0.654 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_167.pth
	Model improved!!!
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0040948252257955		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.0040948252257955 | validation: 1.0551811179738975]
	TIME [epoch: 0.655 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.004147762178049		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.004147762178049 | validation: 1.0491285235783252]
	TIME [epoch: 0.654 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0176463445318313		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.0176463445318313 | validation: 1.050973368107611]
	TIME [epoch: 0.655 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0573444130433822		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.0573444130433822 | validation: 1.076704846751962]
	TIME [epoch: 0.655 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0250474376873135		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.0250474376873135 | validation: 1.0476376396989886]
	TIME [epoch: 0.659 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0402369029543241		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.0402369029543241 | validation: 1.2281193063298053]
	TIME [epoch: 0.659 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0808711862081881		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.0808711862081881 | validation: 1.0439134826409986]
	TIME [epoch: 0.656 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0725324728304524		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.0725324728304524 | validation: 1.0735305328832538]
	TIME [epoch: 0.658 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0180955493857726		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.0180955493857726 | validation: 0.9909608263606251]
	TIME [epoch: 0.657 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9861686383218657		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.9861686383218657 | validation: 1.0459083551280184]
	TIME [epoch: 0.665 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0026168783668827		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.0026168783668827 | validation: 1.029846451970414]
	TIME [epoch: 0.657 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.034183806942838		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.034183806942838 | validation: 1.040572823201904]
	TIME [epoch: 0.654 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.10820883679078		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.10820883679078 | validation: 1.0677511408220919]
	TIME [epoch: 0.654 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9869297746547665		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.9869297746547665 | validation: 1.0461175271669967]
	TIME [epoch: 0.655 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0107932874418277		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.0107932874418277 | validation: 1.0888101244559232]
	TIME [epoch: 0.653 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0045085996557042		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.0045085996557042 | validation: 0.9975112690861918]
	TIME [epoch: 0.656 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0185050588516413		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.0185050588516413 | validation: 1.021974653664807]
	TIME [epoch: 0.655 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9813270504677786		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.9813270504677786 | validation: 0.9941644307701243]
	TIME [epoch: 0.654 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9976236822396718		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.9976236822396718 | validation: 1.049520689777199]
	TIME [epoch: 0.654 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.050499642580929		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.050499642580929 | validation: 1.0200841102976121]
	TIME [epoch: 0.654 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0722957437653804		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.0722957437653804 | validation: 1.0082432777903518]
	TIME [epoch: 0.657 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9401630864461447		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.9401630864461447 | validation: 0.996142885876407]
	TIME [epoch: 0.655 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9748379013412052		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.9748379013412052 | validation: 1.0959930798526896]
	TIME [epoch: 0.656 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0050354444322709		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.0050354444322709 | validation: 0.9750447373497795]
	TIME [epoch: 0.656 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9773774153902242		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.9773774153902242 | validation: 0.9991889387281262]
	TIME [epoch: 0.658 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0505279162142387		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.0505279162142387 | validation: 1.007040425369757]
	TIME [epoch: 0.655 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9819095834876727		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.9819095834876727 | validation: 1.0326685806796867]
	TIME [epoch: 0.653 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.050270594981851		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.050270594981851 | validation: 1.1187123129853576]
	TIME [epoch: 0.652 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.035396186584764		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.035396186584764 | validation: 0.9875457820867002]
	TIME [epoch: 0.654 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9910565849131118		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.9910565849131118 | validation: 0.976253694167707]
	TIME [epoch: 0.652 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9668711379596823		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.9668711379596823 | validation: 0.9921706600471769]
	TIME [epoch: 0.652 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0056142670181476		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.0056142670181476 | validation: 0.9451999431482341]
	TIME [epoch: 0.651 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_199.pth
	Model improved!!!
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9557966920587783		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.9557966920587783 | validation: 0.990253722422099]
	TIME [epoch: 0.657 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9778406559414247		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.9778406559414247 | validation: 0.9786401500765548]
	TIME [epoch: 178 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0042835194733521		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.0042835194733521 | validation: 1.0305453810839893]
	TIME [epoch: 1.29 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0034436369970974		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.0034436369970974 | validation: 0.9355318827841537]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9373738223512404		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.9373738223512404 | validation: 0.9604039507936076]
	TIME [epoch: 1.28 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9388307681095889		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.9388307681095889 | validation: 0.9345468355505266]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9589053104614587		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.9589053104614587 | validation: 1.02819863103097]
	TIME [epoch: 1.28 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0166172572619399		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.0166172572619399 | validation: 0.9390979165017603]
	TIME [epoch: 1.27 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9348637051280809		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.9348637051280809 | validation: 0.9381193422900168]
	TIME [epoch: 1.27 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.962977792631705		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.962977792631705 | validation: 1.0487926956445524]
	TIME [epoch: 1.27 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0134222499944456		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.0134222499944456 | validation: 1.0845331307370245]
	TIME [epoch: 1.27 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0853664681126216		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.0853664681126216 | validation: 0.9922502418140742]
	TIME [epoch: 1.28 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9317866604366758		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.9317866604366758 | validation: 0.9268443900063098]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9373158301903365		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.9373158301903365 | validation: 1.0521941114419497]
	TIME [epoch: 1.28 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9796593601032462		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.9796593601032462 | validation: 0.9432383675971652]
	TIME [epoch: 1.27 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9948182495309439		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.9948182495309439 | validation: 1.0096637506328996]
	TIME [epoch: 1.27 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9581543694822611		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.9581543694822611 | validation: 0.9198133298595845]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9343522367631856		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.9343522367631856 | validation: 0.9849942953724938]
	TIME [epoch: 1.28 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9421513090229944		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.9421513090229944 | validation: 0.9316210423285828]
	TIME [epoch: 1.28 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.954827906602664		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.954827906602664 | validation: 0.9754488436321136]
	TIME [epoch: 1.28 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0144988357940963		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.0144988357940963 | validation: 0.9473724107778421]
	TIME [epoch: 1.28 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9260058230563616		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.9260058230563616 | validation: 0.9177535270361133]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9233009100655579		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.9233009100655579 | validation: 1.001810790513063]
	TIME [epoch: 1.28 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9643799709834341		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.9643799709834341 | validation: 0.9940241549316104]
	TIME [epoch: 1.27 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0221074793973641		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.0221074793973641 | validation: 0.9303367902112587]
	TIME [epoch: 1.27 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9225068417454881		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.9225068417454881 | validation: 0.89729834313416]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_225.pth
	Model improved!!!
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9114289408083218		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.9114289408083218 | validation: 0.9671712798629013]
	TIME [epoch: 1.28 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9239326280708318		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.9239326280708318 | validation: 0.9361215997044412]
	TIME [epoch: 1.27 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9561527373571447		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.9561527373571447 | validation: 0.9694356435542866]
	TIME [epoch: 1.27 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9440391903353847		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.9440391903353847 | validation: 0.9350462540696818]
	TIME [epoch: 1.27 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9715757028721188		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.9715757028721188 | validation: 0.8876300536786106]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_230.pth
	Model improved!!!
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9387397027895371		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.9387397027895371 | validation: 0.965362543558534]
	TIME [epoch: 1.28 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.970036135941026		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.970036135941026 | validation: 0.9583243376330034]
	TIME [epoch: 1.28 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9839716847911346		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.9839716847911346 | validation: 0.9948151881865631]
	TIME [epoch: 1.28 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9459638977121414		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.9459638977121414 | validation: 0.9105151704781556]
	TIME [epoch: 1.27 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9169243472968245		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.9169243472968245 | validation: 0.9339247206184217]
	TIME [epoch: 1.28 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9389872284121009		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.9389872284121009 | validation: 0.9238957372126901]
	TIME [epoch: 1.27 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9232552428026		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.9232552428026 | validation: 0.9191671522759434]
	TIME [epoch: 1.28 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9462080501934194		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.9462080501934194 | validation: 0.9716780833787322]
	TIME [epoch: 1.27 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9371200714179865		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.9371200714179865 | validation: 0.9313286939366967]
	TIME [epoch: 1.27 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9450882342442162		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.9450882342442162 | validation: 0.9349108181475639]
	TIME [epoch: 1.27 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9172543865234795		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.9172543865234795 | validation: 0.8988640783988585]
	TIME [epoch: 1.27 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9194107604089138		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.9194107604089138 | validation: 0.9209549389062559]
	TIME [epoch: 1.27 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9140426567260744		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.9140426567260744 | validation: 0.911944649018885]
	TIME [epoch: 1.27 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9380744354379533		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.9380744354379533 | validation: 0.9029320161716949]
	TIME [epoch: 1.28 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9190887369609914		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.9190887369609914 | validation: 0.8973013380346043]
	TIME [epoch: 1.27 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9269269990431763		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.9269269990431763 | validation: 0.8663973774218787]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9030564138509152		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.9030564138509152 | validation: 0.9108368674453065]
	TIME [epoch: 1.27 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9235336728235563		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.9235336728235563 | validation: 0.9725261966095005]
	TIME [epoch: 1.27 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9907137242736621		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.9907137242736621 | validation: 1.0114490222467372]
	TIME [epoch: 1.27 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9656717079184125		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.9656717079184125 | validation: 0.8911411633732998]
	TIME [epoch: 1.27 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.900281357192467		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.900281357192467 | validation: 0.91961849978794]
	TIME [epoch: 1.27 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.886549107321587		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.886549107321587 | validation: 0.8786685421713998]
	TIME [epoch: 1.27 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.90833533363789		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.90833533363789 | validation: 0.983075289178402]
	TIME [epoch: 1.27 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9354809772354833		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.9354809772354833 | validation: 0.8866587050151651]
	TIME [epoch: 1.28 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.916465137889469		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.916465137889469 | validation: 0.9149299084124792]
	TIME [epoch: 1.28 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9300777472376984		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.9300777472376984 | validation: 0.9073883432879886]
	TIME [epoch: 1.27 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.945188624349075		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.945188624349075 | validation: 0.8910430704972427]
	TIME [epoch: 1.27 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9475243701631898		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.9475243701631898 | validation: 0.9191840402610594]
	TIME [epoch: 1.27 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8956163572976277		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.8956163572976277 | validation: 0.9103296710455189]
	TIME [epoch: 1.27 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9223679018670674		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.9223679018670674 | validation: 0.9645829141474476]
	TIME [epoch: 1.27 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9103676528945684		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.9103676528945684 | validation: 0.8830974262037219]
	TIME [epoch: 1.27 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9027077503430957		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.9027077503430957 | validation: 0.9125396558737684]
	TIME [epoch: 1.27 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9129735519727566		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.9129735519727566 | validation: 0.9066145475112415]
	TIME [epoch: 1.27 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9350580112202989		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.9350580112202989 | validation: 0.8989639994577596]
	TIME [epoch: 1.27 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9126727225255624		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.9126727225255624 | validation: 0.8880720163433552]
	TIME [epoch: 1.27 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.909035437809379		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.909035437809379 | validation: 0.8604723521270996]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8965365658504938		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.8965365658504938 | validation: 0.8950356375482134]
	TIME [epoch: 1.27 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.912320476295422		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.912320476295422 | validation: 0.8484925945920042]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9022491315184147		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.9022491315184147 | validation: 0.949423904099751]
	TIME [epoch: 1.27 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9472608760747528		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.9472608760747528 | validation: 0.8984194295055205]
	TIME [epoch: 1.27 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9511604045943713		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.9511604045943713 | validation: 0.9301899493980095]
	TIME [epoch: 1.27 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.91343640052745		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.91343640052745 | validation: 0.8421977367234789]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_272.pth
	Model improved!!!
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8806204773980149		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.8806204773980149 | validation: 0.9097627723194052]
	TIME [epoch: 1.28 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8907899028487298		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.8907899028487298 | validation: 0.8568340527059469]
	TIME [epoch: 1.29 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9057555622678163		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.9057555622678163 | validation: 0.9223237760965981]
	TIME [epoch: 1.28 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9318573819344014		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.9318573819344014 | validation: 0.851491435344601]
	TIME [epoch: 1.28 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8866962654620505		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.8866962654620505 | validation: 0.8720486664210226]
	TIME [epoch: 1.28 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8878633098346517		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.8878633098346517 | validation: 0.848181821257562]
	TIME [epoch: 1.28 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8955142562131737		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.8955142562131737 | validation: 0.9383806576177305]
	TIME [epoch: 1.28 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9282208845212295		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.9282208845212295 | validation: 0.8534003859930547]
	TIME [epoch: 1.29 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8935586235565799		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.8935586235565799 | validation: 0.8936990966046086]
	TIME [epoch: 1.27 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8888082957057639		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.8888082957057639 | validation: 0.840156620519624]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_282.pth
	Model improved!!!
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8849088185616719		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.8849088185616719 | validation: 0.9010533654178192]
	TIME [epoch: 1.28 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9092272667672029		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.9092272667672029 | validation: 0.844146369731454]
	TIME [epoch: 1.28 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8935325212418249		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.8935325212418249 | validation: 0.8718320455057136]
	TIME [epoch: 1.27 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9152603919563322		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.9152603919563322 | validation: 0.8399234559271029]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8821064409522306		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.8821064409522306 | validation: 0.8768618671814965]
	TIME [epoch: 1.27 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8972172522551506		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.8972172522551506 | validation: 1.0303825321149003]
	TIME [epoch: 1.27 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9688535237751736		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.9688535237751736 | validation: 0.9342749389285392]
	TIME [epoch: 1.27 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9571270772160605		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.9571270772160605 | validation: 0.8383478062771959]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_290.pth
	Model improved!!!
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8576327088799998		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.8576327088799998 | validation: 0.8181495533634229]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8508411770704905		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.8508411770704905 | validation: 0.9065419088974718]
	TIME [epoch: 1.27 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8723237612172815		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.8723237612172815 | validation: 0.8632779225097246]
	TIME [epoch: 1.27 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8875103964766788		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.8875103964766788 | validation: 0.8966570714508277]
	TIME [epoch: 1.27 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8734181608253452		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.8734181608253452 | validation: 0.8264844374392505]
	TIME [epoch: 1.27 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8758007004848157		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.8758007004848157 | validation: 0.8966964245802889]
	TIME [epoch: 1.27 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9212761214887359		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.9212761214887359 | validation: 0.830753173331728]
	TIME [epoch: 1.27 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8950968010965903		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.8950968010965903 | validation: 0.8886078205542454]
	TIME [epoch: 1.28 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.914831967445783		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.914831967445783 | validation: 0.8308199559338831]
	TIME [epoch: 1.28 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8773705878411588		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.8773705878411588 | validation: 0.8621838694317377]
	TIME [epoch: 1.28 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8771388466128446		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.8771388466128446 | validation: 0.8333519401956044]
	TIME [epoch: 1.28 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.887074030372181		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.887074030372181 | validation: 0.8927605484092486]
	TIME [epoch: 1.28 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8973663378281662		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.8973663378281662 | validation: 0.8347373912826428]
	TIME [epoch: 1.28 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8851834072400303		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.8851834072400303 | validation: 0.8774141742206535]
	TIME [epoch: 1.27 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8903144602318781		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.8903144602318781 | validation: 0.8101961715782959]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8713759913133444		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.8713759913133444 | validation: 0.8605961039875472]
	TIME [epoch: 1.28 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8801654551233772		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.8801654551233772 | validation: 0.8218935926154984]
	TIME [epoch: 1.28 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8685723751366743		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.8685723751366743 | validation: 0.8695702511219171]
	TIME [epoch: 1.27 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8912493214939601		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.8912493214939601 | validation: 0.8100032712782271]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_309.pth
	Model improved!!!
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8697187190009575		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.8697187190009575 | validation: 0.883402209997319]
	TIME [epoch: 1.27 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8840046103417452		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.8840046103417452 | validation: 0.8231296183458845]
	TIME [epoch: 1.27 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.887213493679389		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.887213493679389 | validation: 0.8991187836997775]
	TIME [epoch: 1.27 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8864272302204066		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.8864272302204066 | validation: 0.8192921066178545]
	TIME [epoch: 1.27 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8659057987366096		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.8659057987366096 | validation: 0.8801590617576394]
	TIME [epoch: 1.27 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8649366622178367		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.8649366622178367 | validation: 0.824870188158689]
	TIME [epoch: 1.27 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8590071338320661		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.8590071338320661 | validation: 0.8730168305028464]
	TIME [epoch: 1.27 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8673820709501542		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.8673820709501542 | validation: 0.811510913463896]
	TIME [epoch: 1.27 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8687781151480379		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.8687781151480379 | validation: 0.8679296311599686]
	TIME [epoch: 1.27 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9078489024297929		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.9078489024297929 | validation: 0.8398317933089937]
	TIME [epoch: 1.27 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9081202700063414		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.9081202700063414 | validation: 0.9424178634250521]
	TIME [epoch: 1.28 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9593382311491773		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.9593382311491773 | validation: 0.921391087901041]
	TIME [epoch: 1.27 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8832894748817752		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.8832894748817752 | validation: 0.8306458118813552]
	TIME [epoch: 1.27 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8499901658053998		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.8499901658053998 | validation: 0.8511146998230237]
	TIME [epoch: 1.27 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8525629299534458		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.8525629299534458 | validation: 0.8565831617552799]
	TIME [epoch: 1.27 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8829588290051212		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.8829588290051212 | validation: 0.854383958871641]
	TIME [epoch: 1.27 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8632540847869886		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.8632540847869886 | validation: 0.8381407607615536]
	TIME [epoch: 1.27 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8686509012179624		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.8686509012179624 | validation: 0.8321515775968731]
	TIME [epoch: 1.27 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8606637995140521		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.8606637995140521 | validation: 0.8377648650264329]
	TIME [epoch: 1.27 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.864442532714894		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.864442532714894 | validation: 0.83644506016624]
	TIME [epoch: 1.27 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8541489849682543		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.8541489849682543 | validation: 0.8343728157672761]
	TIME [epoch: 1.27 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8649232053319105		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.8649232053319105 | validation: 0.8568683484957383]
	TIME [epoch: 1.28 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8558401606034554		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.8558401606034554 | validation: 0.8280398369245365]
	TIME [epoch: 1.27 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8653837299601035		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.8653837299601035 | validation: 0.8408039560831395]
	TIME [epoch: 1.27 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8565937992703451		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.8565937992703451 | validation: 0.8355691821140893]
	TIME [epoch: 1.27 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.86577878175177		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.86577878175177 | validation: 0.8517824884714358]
	TIME [epoch: 1.27 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8581108792215056		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.8581108792215056 | validation: 0.8346557048216567]
	TIME [epoch: 1.27 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8611824489565431		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.8611824489565431 | validation: 0.8511153534974127]
	TIME [epoch: 1.27 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8496619858098882		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.8496619858098882 | validation: 0.8229194839770199]
	TIME [epoch: 1.27 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8606611694790663		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.8606611694790663 | validation: 0.8773275536707843]
	TIME [epoch: 1.27 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8544881433208997		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.8544881433208997 | validation: 0.8189069879980203]
	TIME [epoch: 1.27 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8516182212656798		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.8516182212656798 | validation: 0.8756033768296494]
	TIME [epoch: 1.28 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.846545756119939		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.846545756119939 | validation: 0.8150480043139929]
	TIME [epoch: 1.27 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8563301741980195		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.8563301741980195 | validation: 0.8964201922964294]
	TIME [epoch: 1.28 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8842355442907033		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.8842355442907033 | validation: 0.8382284398106569]
	TIME [epoch: 1.27 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9256270179963556		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.9256270179963556 | validation: 0.8576270861191677]
	TIME [epoch: 1.27 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8953524805374915		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.8953524805374915 | validation: 0.8131597154977034]
	TIME [epoch: 1.27 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8490796620204604		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.8490796620204604 | validation: 0.8345208295011313]
	TIME [epoch: 1.27 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8452784282614727		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.8452784282614727 | validation: 0.8208094458583883]
	TIME [epoch: 1.27 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8614689270649823		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.8614689270649823 | validation: 0.8605302628963968]
	TIME [epoch: 1.27 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8733967720854006		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.8733967720854006 | validation: 0.81864342700728]
	TIME [epoch: 1.27 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.857159350117864		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.857159350117864 | validation: 0.8299621939943664]
	TIME [epoch: 1.27 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8590102906234183		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.8590102906234183 | validation: 0.8104383794474682]
	TIME [epoch: 1.27 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.855325482752458		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.855325482752458 | validation: 0.8484816684926]
	TIME [epoch: 1.27 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8628644515209087		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.8628644515209087 | validation: 0.8027332125688176]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_354.pth
	Model improved!!!
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8511715418341262		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.8511715418341262 | validation: 0.8379200627615577]
	TIME [epoch: 1.28 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8560097336076232		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.8560097336076232 | validation: 0.8056231944089529]
	TIME [epoch: 1.28 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8452664161514669		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.8452664161514669 | validation: 0.824883498232932]
	TIME [epoch: 1.28 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8499401153229812		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.8499401153229812 | validation: 0.8100753593843337]
	TIME [epoch: 1.27 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8565340977636435		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.8565340977636435 | validation: 0.8397048402129139]
	TIME [epoch: 1.28 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8628710682222586		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.8628710682222586 | validation: 0.8217418409444441]
	TIME [epoch: 1.27 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.850361888987565		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.850361888987565 | validation: 0.8393520958866945]
	TIME [epoch: 1.28 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8520766651369053		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.8520766651369053 | validation: 0.7996309195606166]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_362.pth
	Model improved!!!
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8426358032661199		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.8426358032661199 | validation: 0.8331028465519328]
	TIME [epoch: 1.28 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.848308114442597		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.848308114442597 | validation: 0.8014903763385086]
	TIME [epoch: 1.28 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8523618710267702		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.8523618710267702 | validation: 0.8208050602917555]
	TIME [epoch: 1.28 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8590363090241863		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.8590363090241863 | validation: 0.7976650622007049]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_366.pth
	Model improved!!!
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8495261888634804		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.8495261888634804 | validation: 0.8172223416069653]
	TIME [epoch: 1.27 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8484362359494284		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.8484362359494284 | validation: 0.8013715258698912]
	TIME [epoch: 1.27 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8403371210411994		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.8403371210411994 | validation: 0.8226396415122494]
	TIME [epoch: 1.28 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8520981029600241		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.8520981029600241 | validation: 0.8071189710040934]
	TIME [epoch: 1.27 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8465628447424802		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.8465628447424802 | validation: 0.8574436988917586]
	TIME [epoch: 1.28 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8550960480666714		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.8550960480666714 | validation: 0.814880924051209]
	TIME [epoch: 1.27 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8565368848203359		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.8565368848203359 | validation: 0.844453564801653]
	TIME [epoch: 1.28 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8447970842375863		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.8447970842375863 | validation: 0.7889230852337908]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_374.pth
	Model improved!!!
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8316875506552418		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.8316875506552418 | validation: 0.8175764340585954]
	TIME [epoch: 1.28 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8387051192330046		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.8387051192330046 | validation: 0.7995040884238132]
	TIME [epoch: 1.27 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8529803152658272		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.8529803152658272 | validation: 0.832386195858353]
	TIME [epoch: 1.28 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8694319126055922		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.8694319126055922 | validation: 0.7947118541830793]
	TIME [epoch: 1.27 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8359119893517648		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.8359119893517648 | validation: 0.8026418064069292]
	TIME [epoch: 1.27 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.82909401667526		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.82909401667526 | validation: 0.7864049375466543]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_380.pth
	Model improved!!!
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8348548498710342		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.8348548498710342 | validation: 0.8117930671845084]
	TIME [epoch: 1.28 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8525743368370087		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.8525743368370087 | validation: 0.8088171092961762]
	TIME [epoch: 1.28 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8516474312975353		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.8516474312975353 | validation: 0.8494410541611844]
	TIME [epoch: 1.28 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8507280883948033		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.8507280883948033 | validation: 0.7979193251812293]
	TIME [epoch: 1.27 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8357956284487466		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.8357956284487466 | validation: 0.8264091047424009]
	TIME [epoch: 1.27 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8270300776152106		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.8270300776152106 | validation: 0.7930070354071698]
	TIME [epoch: 1.28 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8256765768331421		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.8256765768331421 | validation: 0.8584640847760294]
	TIME [epoch: 1.28 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8397718848416613		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.8397718848416613 | validation: 0.8342223119026498]
	TIME [epoch: 1.27 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.873389495470477		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.873389495470477 | validation: 0.838952607060644]
	TIME [epoch: 1.28 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8712876320795948		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.8712876320795948 | validation: 0.8022000238887863]
	TIME [epoch: 1.27 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8514849507751139		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.8514849507751139 | validation: 0.8009870177789286]
	TIME [epoch: 1.27 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8202292459078274		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.8202292459078274 | validation: 0.791896275262977]
	TIME [epoch: 1.27 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8157211090275902		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.8157211090275902 | validation: 0.8551088841625243]
	TIME [epoch: 1.27 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.832540697982412		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.832540697982412 | validation: 0.8047889600440116]
	TIME [epoch: 1.27 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8313466358272994		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.8313466358272994 | validation: 0.8065482031921403]
	TIME [epoch: 1.27 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8170921524248983		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.8170921524248983 | validation: 0.7759257551521981]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_396.pth
	Model improved!!!
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8201819499212968		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.8201819499212968 | validation: 0.8138463649625077]
	TIME [epoch: 1.28 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8331650238860863		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.8331650238860863 | validation: 0.8155975106179719]
	TIME [epoch: 1.27 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8769473906886773		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.8769473906886773 | validation: 0.8039355150631322]
	TIME [epoch: 1.28 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8403861816029999		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.8403861816029999 | validation: 0.7835186589380513]
	TIME [epoch: 1.27 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8316425377339257		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.8316425377339257 | validation: 0.7921550213836127]
	TIME [epoch: 1.29 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8222724680287081		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.8222724680287081 | validation: 0.7858041475504525]
	TIME [epoch: 1.28 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8342810324115106		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.8342810324115106 | validation: 0.812362309157782]
	TIME [epoch: 1.28 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8338164570019921		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.8338164570019921 | validation: 0.7937030850126741]
	TIME [epoch: 1.28 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8349610099405564		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.8349610099405564 | validation: 0.8171639603063916]
	TIME [epoch: 1.28 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8231417326969045		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.8231417326969045 | validation: 0.7907024996233302]
	TIME [epoch: 1.28 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.820925974036779		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.820925974036779 | validation: 0.8107111124828267]
	TIME [epoch: 1.28 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8210057479611291		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.8210057479611291 | validation: 0.7881012599596322]
	TIME [epoch: 1.28 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8259323373211883		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.8259323373211883 | validation: 0.8462623707999305]
	TIME [epoch: 1.28 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8419344681218709		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.8419344681218709 | validation: 0.8016123625198812]
	TIME [epoch: 1.28 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8765281080376969		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.8765281080376969 | validation: 0.8325771695539855]
	TIME [epoch: 1.28 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8823314652386857		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.8823314652386857 | validation: 0.7663291231000235]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_412.pth
	Model improved!!!
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8110370111481126		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.8110370111481126 | validation: 0.799974579903341]
	TIME [epoch: 1.27 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8094049805101916		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.8094049805101916 | validation: 0.7810551643653839]
	TIME [epoch: 1.28 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8156101097266821		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.8156101097266821 | validation: 0.8168863667961848]
	TIME [epoch: 1.28 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8189606486150678		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.8189606486150678 | validation: 0.7897039292850296]
	TIME [epoch: 1.28 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8182855696113198		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.8182855696113198 | validation: 0.7975426088681332]
	TIME [epoch: 1.28 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8057423946048075		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.8057423946048075 | validation: 0.7821047109675853]
	TIME [epoch: 1.28 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8062882543756333		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.8062882543756333 | validation: 0.7978380531443956]
	TIME [epoch: 1.28 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8202476741395125		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.8202476741395125 | validation: 0.8060067592033757]
	TIME [epoch: 1.28 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8691540251496784		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.8691540251496784 | validation: 0.8038890400653795]
	TIME [epoch: 1.28 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8705080291278036		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.8705080291278036 | validation: 0.7862000471745577]
	TIME [epoch: 1.28 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8334131327083548		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.8334131327083548 | validation: 0.7611608608014081]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_423.pth
	Model improved!!!
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8064932303565379		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.8064932303565379 | validation: 0.7836994098337916]
	TIME [epoch: 1.27 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8114923313081067		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.8114923313081067 | validation: 0.8041585899584427]
	TIME [epoch: 1.27 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8552925141903777		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.8552925141903777 | validation: 0.8429470843389985]
	TIME [epoch: 1.27 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8628788693380715		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.8628788693380715 | validation: 0.7767364584509369]
	TIME [epoch: 1.26 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8246656113509897		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.8246656113509897 | validation: 0.7890312573610887]
	TIME [epoch: 1.26 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8208556989616689		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.8208556989616689 | validation: 0.7679306511615228]
	TIME [epoch: 1.26 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8217868885287913		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.8217868885287913 | validation: 0.8005895406781015]
	TIME [epoch: 1.27 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8298143884546976		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.8298143884546976 | validation: 0.7725529294451953]
	TIME [epoch: 1.26 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8294178571353115		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.8294178571353115 | validation: 0.8063173655146652]
	TIME [epoch: 1.27 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8235498885910707		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.8235498885910707 | validation: 0.7747874175388416]
	TIME [epoch: 1.26 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8126176146249833		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.8126176146249833 | validation: 0.7894927239282991]
	TIME [epoch: 1.27 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.810031242961513		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.810031242961513 | validation: 0.7669190256155217]
	TIME [epoch: 1.26 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8210424226121426		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.8210424226121426 | validation: 0.8002375057115328]
	TIME [epoch: 1.26 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8339739481909492		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.8339739481909492 | validation: 0.7666012507553195]
	TIME [epoch: 1.26 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8258275396111369		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.8258275396111369 | validation: 0.7722616743771669]
	TIME [epoch: 1.26 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.817740036283486		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.817740036283486 | validation: 0.7572051462868308]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_439.pth
	Model improved!!!
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8131218917872352		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.8131218917872352 | validation: 0.7838062824284443]
	TIME [epoch: 1.28 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8207261524998716		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.8207261524998716 | validation: 0.7797556392053482]
	TIME [epoch: 1.28 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8220710305464873		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.8220710305464873 | validation: 0.8022294594921191]
	TIME [epoch: 1.27 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8232866177504751		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.8232866177504751 | validation: 0.769091139272001]
	TIME [epoch: 1.27 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8208684775010111		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.8208684775010111 | validation: 0.7948910885484305]
	TIME [epoch: 1.28 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8211354101415154		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.8211354101415154 | validation: 0.7644060106528735]
	TIME [epoch: 1.27 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8218382331051578		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.8218382331051578 | validation: 0.7930663521480741]
	TIME [epoch: 1.28 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8220904508308475		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.8220904508308475 | validation: 0.7703206151386561]
	TIME [epoch: 1.26 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8181173147943008		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.8181173147943008 | validation: 0.783421495509619]
	TIME [epoch: 1.28 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8158855721137184		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.8158855721137184 | validation: 0.7528429694768738]
	TIME [epoch: 1.26 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_449.pth
	Model improved!!!
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8151341115191175		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.8151341115191175 | validation: 0.7782807315164872]
	TIME [epoch: 1.27 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8187909464657642		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.8187909464657642 | validation: 0.7616365181967607]
	TIME [epoch: 1.27 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8251116059554722		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.8251116059554722 | validation: 0.7964008416943646]
	TIME [epoch: 1.27 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8213438852434841		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.8213438852434841 | validation: 0.7584116869482109]
	TIME [epoch: 1.27 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8097851787985385		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.8097851787985385 | validation: 0.7706108752384433]
	TIME [epoch: 1.27 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8101452727450901		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.8101452727450901 | validation: 0.7641525470003422]
	TIME [epoch: 1.27 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8145941491707626		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.8145941491707626 | validation: 0.7965351854017361]
	TIME [epoch: 1.27 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8142833307061228		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.8142833307061228 | validation: 0.7650313940941923]
	TIME [epoch: 1.28 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8147898312652287		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.8147898312652287 | validation: 0.8038496185034977]
	TIME [epoch: 1.27 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.813919553083023		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.813919553083023 | validation: 0.7825450602203083]
	TIME [epoch: 1.28 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8121075639476388		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.8121075639476388 | validation: 0.7845829114435438]
	TIME [epoch: 1.27 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8188084104564949		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.8188084104564949 | validation: 0.7856756891058275]
	TIME [epoch: 1.28 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.850771871220851		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.850771871220851 | validation: 0.7624047448192539]
	TIME [epoch: 1.27 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8195826178057399		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.8195826178057399 | validation: 0.7565098284892859]
	TIME [epoch: 1.28 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8069308429100973		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.8069308429100973 | validation: 0.7690312777111417]
	TIME [epoch: 1.27 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8036568515718768		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.8036568515718768 | validation: 0.7605221286756672]
	TIME [epoch: 1.28 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8060038996211734		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.8060038996211734 | validation: 0.7791938283224742]
	TIME [epoch: 1.27 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.803050128033191		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.803050128033191 | validation: 0.7608674714714727]
	TIME [epoch: 1.28 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8046366279246228		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.8046366279246228 | validation: 0.7826314800655081]
	TIME [epoch: 1.27 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8057691265725552		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.8057691265725552 | validation: 0.755817364317042]
	TIME [epoch: 1.28 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8080819488550711		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.8080819488550711 | validation: 0.7676105772989953]
	TIME [epoch: 1.27 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8031892010552713		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.8031892010552713 | validation: 0.7455388551671721]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_471.pth
	Model improved!!!
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8037546244829209		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.8037546244829209 | validation: 0.7605226707112529]
	TIME [epoch: 1.28 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8157127963313673		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.8157127963313673 | validation: 0.7694258710576886]
	TIME [epoch: 1.27 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8271305299899063		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.8271305299899063 | validation: 0.7537064762480153]
	TIME [epoch: 1.27 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8059698406209118		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.8059698406209118 | validation: 0.7489888058401594]
	TIME [epoch: 1.28 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8011340758667345		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.8011340758667345 | validation: 0.7442262317091672]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_476.pth
	Model improved!!!
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7970117422599371		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.7970117422599371 | validation: 0.7541343265877465]
	TIME [epoch: 1.27 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7999946154345148		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.7999946154345148 | validation: 0.7872619906197857]
	TIME [epoch: 1.27 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8062526440759205		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.8062526440759205 | validation: 0.7744878337150162]
	TIME [epoch: 1.26 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8177807093464347		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.8177807093464347 | validation: 0.7986163478761944]
	TIME [epoch: 1.26 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8087258729394782		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.8087258729394782 | validation: 0.7489974075833903]
	TIME [epoch: 1.27 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8117973608432754		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.8117973608432754 | validation: 0.7935595259521571]
	TIME [epoch: 1.26 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8376311099787807		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.8376311099787807 | validation: 0.7516693058531886]
	TIME [epoch: 1.26 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8360718412028839		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.8360718412028839 | validation: 0.7752119951094466]
	TIME [epoch: 1.26 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8079314508954724		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.8079314508954724 | validation: 0.7474056010175532]
	TIME [epoch: 1.26 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7990522602261603		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.7990522602261603 | validation: 0.7675381812662455]
	TIME [epoch: 1.26 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.791472062634441		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.791472062634441 | validation: 0.7461418580283149]
	TIME [epoch: 1.26 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7926078218489986		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.7926078218489986 | validation: 0.7688945221816902]
	TIME [epoch: 1.27 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8009149070541444		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.8009149070541444 | validation: 0.742588419455773]
	TIME [epoch: 1.26 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_489.pth
	Model improved!!!
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8182279702859778		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.8182279702859778 | validation: 0.7886692600976706]
	TIME [epoch: 1.28 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8241502017828216		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.8241502017828216 | validation: 0.7451819648686678]
	TIME [epoch: 1.28 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8046483813076662		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.8046483813076662 | validation: 0.7549389967659257]
	TIME [epoch: 1.28 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7983396592630878		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.7983396592630878 | validation: 0.730071431884659]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_493.pth
	Model improved!!!
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7959666403582393		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.7959666403582393 | validation: 0.7706780851295437]
	TIME [epoch: 1.28 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7990898158118617		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.7990898158118617 | validation: 0.754156778069426]
	TIME [epoch: 1.28 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8028808892265586		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.8028808892265586 | validation: 0.7854862564063869]
	TIME [epoch: 1.27 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.80249528537577		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.80249528537577 | validation: 0.7425593788254917]
	TIME [epoch: 1.27 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8006836153997509		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.8006836153997509 | validation: 0.767323547792819]
	TIME [epoch: 1.27 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.800431729416195		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.800431729416195 | validation: 0.741875716269851]
	TIME [epoch: 1.26 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8104422742981856		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.8104422742981856 | validation: 0.7673421963474167]
	TIME [epoch: 1.26 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8160876183796458		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.8160876183796458 | validation: 0.7358641886744518]
	TIME [epoch: 181 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8000445616322222		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.8000445616322222 | validation: 0.755403300050633]
	TIME [epoch: 2.53 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7964148619519289		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.7964148619519289 | validation: 0.7522710228492235]
	TIME [epoch: 2.51 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8082769043633897		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.8082769043633897 | validation: 0.7777104519915624]
	TIME [epoch: 2.5 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8048180710264019		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.8048180710264019 | validation: 0.7428650735210176]
	TIME [epoch: 2.51 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8017401137571496		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.8017401137571496 | validation: 0.7457217638707216]
	TIME [epoch: 2.5 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7965739696309218		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.7965739696309218 | validation: 0.7368390044339521]
	TIME [epoch: 2.5 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8014261862619654		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.8014261862619654 | validation: 0.746422328930157]
	TIME [epoch: 2.5 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8056613767801035		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.8056613767801035 | validation: 0.7358174669323964]
	TIME [epoch: 2.5 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7930926347377958		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.7930926347377958 | validation: 0.7577827310828056]
	TIME [epoch: 2.5 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.792614984467086		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.792614984467086 | validation: 0.7561865202817067]
	TIME [epoch: 2.51 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8063329044551735		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.8063329044551735 | validation: 0.7720115179326609]
	TIME [epoch: 2.5 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8009420740285572		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.8009420740285572 | validation: 0.7418997553289414]
	TIME [epoch: 2.5 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7909568029720111		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.7909568029720111 | validation: 0.747625163272518]
	TIME [epoch: 2.5 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7899375075249755		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.7899375075249755 | validation: 0.7378026523954407]
	TIME [epoch: 2.5 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8024246569615467		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.8024246569615467 | validation: 0.7599906304284277]
	TIME [epoch: 2.51 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8168756943039034		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.8168756943039034 | validation: 0.743545835744684]
	TIME [epoch: 2.5 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8088381877623918		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.8088381877623918 | validation: 0.7386351897250312]
	TIME [epoch: 2.5 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7856796412325211		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.7856796412325211 | validation: 0.7283168185320741]
	TIME [epoch: 2.5 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_519.pth
	Model improved!!!
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7823685143062715		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.7823685143062715 | validation: 0.7506671324641605]
	TIME [epoch: 2.51 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7912441607658665		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.7912441607658665 | validation: 0.7491219770551416]
	TIME [epoch: 2.5 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7976951177006798		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.7976951177006798 | validation: 0.7652298659842668]
	TIME [epoch: 2.51 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7976623402289108		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.7976623402289108 | validation: 0.7266089347784722]
	TIME [epoch: 2.5 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_523.pth
	Model improved!!!
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7913241081927416		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.7913241081927416 | validation: 0.7537531423193375]
	TIME [epoch: 2.51 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.793341179560901		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.793341179560901 | validation: 0.7304912781423278]
	TIME [epoch: 2.51 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8081207105823811		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.8081207105823811 | validation: 0.7623957031025125]
	TIME [epoch: 2.51 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8240374127577255		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.8240374127577255 | validation: 0.729208971928832]
	TIME [epoch: 2.5 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8000456265235897		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.8000456265235897 | validation: 0.7382138554053043]
	TIME [epoch: 2.5 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7859335726940073		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.7859335726940073 | validation: 0.7262121703466409]
	TIME [epoch: 2.5 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_529.pth
	Model improved!!!
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7889469823953897		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.7889469823953897 | validation: 0.7525709951733428]
	TIME [epoch: 2.51 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7872607206020917		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.7872607206020917 | validation: 0.7288217615850802]
	TIME [epoch: 2.5 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7842154497242043		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.7842154497242043 | validation: 0.7420620588238169]
	TIME [epoch: 2.5 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7817407607518922		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.7817407607518922 | validation: 0.7342450081789722]
	TIME [epoch: 2.51 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7841358827144305		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.7841358827144305 | validation: 0.7453707349110626]
	TIME [epoch: 2.5 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.788850924038662		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.788850924038662 | validation: 0.7315994190904965]
	TIME [epoch: 2.5 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7929129533142728		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.7929129533142728 | validation: 0.7421108705346753]
	TIME [epoch: 2.5 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7950526183681891		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.7950526183681891 | validation: 0.7392933270552021]
	TIME [epoch: 2.5 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8081872603801192		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.8081872603801192 | validation: 0.7278563323560866]
	TIME [epoch: 2.5 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8031538195414129		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.8031538195414129 | validation: 0.7338288302983017]
	TIME [epoch: 2.51 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7938284304683131		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.7938284304683131 | validation: 0.7180262442045482]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_540.pth
	Model improved!!!
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7858891063683865		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.7858891063683865 | validation: 0.7593854810862652]
	TIME [epoch: 2.51 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7982467541991336		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.7982467541991336 | validation: 0.7405500489291442]
	TIME [epoch: 2.5 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8079484642668541		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.8079484642668541 | validation: 0.742941276483284]
	TIME [epoch: 2.51 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7960256035294765		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.7960256035294765 | validation: 0.7156149388467612]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_544.pth
	Model improved!!!
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7852043929462346		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.7852043929462346 | validation: 0.7288514338018934]
	TIME [epoch: 2.5 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7876513181619346		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.7876513181619346 | validation: 0.7243410401533267]
	TIME [epoch: 2.5 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7880084909361785		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.7880084909361785 | validation: 0.7506591518628603]
	TIME [epoch: 2.5 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7933669636375601		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.7933669636375601 | validation: 0.7263197643121613]
	TIME [epoch: 2.5 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.788366901630367		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.788366901630367 | validation: 0.7369071671608964]
	TIME [epoch: 2.5 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7870100004582146		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.7870100004582146 | validation: 0.7259962034202699]
	TIME [epoch: 2.5 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7848643493710636		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.7848643493710636 | validation: 0.7367987507044611]
	TIME [epoch: 2.5 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7888822380187119		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.7888822380187119 | validation: 0.7203914433578763]
	TIME [epoch: 2.5 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7943708345973113		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.7943708345973113 | validation: 0.7549781153224804]
	TIME [epoch: 2.51 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7942327430718383		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.7942327430718383 | validation: 0.7158974484867907]
	TIME [epoch: 2.5 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7891545255017038		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.7891545255017038 | validation: 0.73558864391428]
	TIME [epoch: 2.51 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7819448058048944		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.7819448058048944 | validation: 0.7148552273951106]
	TIME [epoch: 2.5 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_556.pth
	Model improved!!!
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7828233101203779		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.7828233101203779 | validation: 0.7362566421335789]
	TIME [epoch: 2.5 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7828439568154475		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.7828439568154475 | validation: 0.7126777699189795]
	TIME [epoch: 2.5 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_558.pth
	Model improved!!!
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7835619025885862		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.7835619025885862 | validation: 0.7293746037133454]
	TIME [epoch: 2.5 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7933990681781107		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.7933990681781107 | validation: 0.7072758675417347]
	TIME [epoch: 2.5 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_560.pth
	Model improved!!!
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7894049603755662		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.7894049603755662 | validation: 0.7428810421498508]
	TIME [epoch: 2.51 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7878770934778209		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.7878770934778209 | validation: 0.7198121034704239]
	TIME [epoch: 2.51 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7840154383392157		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.7840154383392157 | validation: 0.7390818217326406]
	TIME [epoch: 2.51 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7799656028503594		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.7799656028503594 | validation: 0.7185296496143109]
	TIME [epoch: 2.51 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7823040102509816		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.7823040102509816 | validation: 0.7434195389564208]
	TIME [epoch: 2.52 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7822408322289605		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.7822408322289605 | validation: 0.7262303680718821]
	TIME [epoch: 2.51 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7847826454921781		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.7847826454921781 | validation: 0.7328955307435729]
	TIME [epoch: 2.51 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7908573891480313		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.7908573891480313 | validation: 0.7313620991138007]
	TIME [epoch: 2.51 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7983674861432646		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.7983674861432646 | validation: 0.7164913348921629]
	TIME [epoch: 2.51 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7771338149217712		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.7771338149217712 | validation: 0.7226665424007593]
	TIME [epoch: 2.51 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7721019879558618		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.7721019879558618 | validation: 0.7053961142568064]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_571.pth
	Model improved!!!
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7730729901077942		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.7730729901077942 | validation: 0.7109502599854246]
	TIME [epoch: 2.52 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7751303810012512		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.7751303810012512 | validation: 0.732810533493264]
	TIME [epoch: 2.52 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7820504578454225		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.7820504578454225 | validation: 0.7312964421829021]
	TIME [epoch: 2.52 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7896777792405573		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.7896777792405573 | validation: 0.7322753047936035]
	TIME [epoch: 2.51 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7802815087186487		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.7802815087186487 | validation: 0.7246374452032529]
	TIME [epoch: 2.53 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7749683860206926		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.7749683860206926 | validation: 0.7203889176398497]
	TIME [epoch: 2.51 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7732940533288621		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.7732940533288621 | validation: 0.718759129929558]
	TIME [epoch: 2.51 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7760951195986886		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.7760951195986886 | validation: 0.7414904786397145]
	TIME [epoch: 2.51 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.783522732192088		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.783522732192088 | validation: 0.7099797118114073]
	TIME [epoch: 2.51 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7822688467670154		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.7822688467670154 | validation: 0.7345329987294988]
	TIME [epoch: 2.51 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8005541183072147		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.8005541183072147 | validation: 0.7217389372207521]
	TIME [epoch: 2.51 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8169567845167105		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.8169567845167105 | validation: 0.7278444528960568]
	TIME [epoch: 2.51 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7880019396443275		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.7880019396443275 | validation: 0.7038196838208617]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_584.pth
	Model improved!!!
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7712321367231061		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.7712321367231061 | validation: 0.7231884821817243]
	TIME [epoch: 2.52 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.775270538231934		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.775270538231934 | validation: 0.7124014808699117]
	TIME [epoch: 2.52 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7778860525760313		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.7778860525760313 | validation: 0.7293733678517967]
	TIME [epoch: 2.52 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7773329476400546		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.7773329476400546 | validation: 0.7125298188229469]
	TIME [epoch: 2.52 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7729179907291175		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.7729179907291175 | validation: 0.7074929244652604]
	TIME [epoch: 2.52 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7711833658200986		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.7711833658200986 | validation: 0.7106900037970529]
	TIME [epoch: 2.52 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7699829880129514		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.7699829880129514 | validation: 0.7065661055708212]
	TIME [epoch: 2.52 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7745626640943248		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.7745626640943248 | validation: 0.7183985475693975]
	TIME [epoch: 2.52 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7750568401960587		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.7750568401960587 | validation: 0.7220083151693001]
	TIME [epoch: 2.52 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.780535061723066		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.780535061723066 | validation: 0.7032782278821355]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_594.pth
	Model improved!!!
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7772971594569151		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.7772971594569151 | validation: 0.7008353890851078]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_595.pth
	Model improved!!!
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7783341476052307		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.7783341476052307 | validation: 0.7108517285309222]
	TIME [epoch: 2.51 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.781404423536259		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.781404423536259 | validation: 0.6997853177301304]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_597.pth
	Model improved!!!
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7863517570750369		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.7863517570750369 | validation: 0.7599914067437059]
	TIME [epoch: 2.51 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.802413615185749		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.802413615185749 | validation: 0.7128413934945703]
	TIME [epoch: 2.51 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7849718199980655		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.7849718199980655 | validation: 0.7115083374451739]
	TIME [epoch: 2.51 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7705727033292337		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.7705727033292337 | validation: 0.6892875879201181]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_601.pth
	Model improved!!!
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7653468707357688		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.7653468707357688 | validation: 0.7176187006411254]
	TIME [epoch: 2.52 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7686247785756496		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.7686247785756496 | validation: 0.7062742174132356]
	TIME [epoch: 2.52 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7740970507787083		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.7740970507787083 | validation: 0.7228618856759703]
	TIME [epoch: 2.51 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7740884237561162		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.7740884237561162 | validation: 0.7095006990372792]
	TIME [epoch: 2.52 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7790654092974493		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.7790654092974493 | validation: 0.718157648208241]
	TIME [epoch: 2.52 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7794534996537322		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.7794534996537322 | validation: 0.703756537212342]
	TIME [epoch: 2.51 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7812688369704488		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.7812688369704488 | validation: 0.7126662587859548]
	TIME [epoch: 2.52 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7770814603179386		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.7770814603179386 | validation: 0.6987881864109938]
	TIME [epoch: 2.52 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7736346140165759		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.7736346140165759 | validation: 0.7164422985782467]
	TIME [epoch: 2.52 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7755068407934332		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.7755068407934332 | validation: 0.694880783322123]
	TIME [epoch: 2.52 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7753789035387798		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.7753789035387798 | validation: 0.7204948885062761]
	TIME [epoch: 2.51 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7745844764421582		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.7745844764421582 | validation: 0.6988095319108405]
	TIME [epoch: 2.52 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7687099316173948		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.7687099316173948 | validation: 0.7060137061220112]
	TIME [epoch: 2.52 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7670805082928652		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.7670805082928652 | validation: 0.6975387966575943]
	TIME [epoch: 2.52 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7722708346070758		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.7722708346070758 | validation: 0.7053751932448371]
	TIME [epoch: 2.51 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7680093416506858		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.7680093416506858 | validation: 0.6961538482837778]
	TIME [epoch: 2.51 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7710336214441358		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.7710336214441358 | validation: 0.7116392441332279]
	TIME [epoch: 2.51 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.770276784984342		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.770276784984342 | validation: 0.6996726860488919]
	TIME [epoch: 2.52 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7728808993856129		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.7728808993856129 | validation: 0.7168984399601263]
	TIME [epoch: 2.51 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7794262483807916		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.7794262483807916 | validation: 0.7031365955896689]
	TIME [epoch: 2.52 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7911749379126668		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.7911749379126668 | validation: 0.7016783144015268]
	TIME [epoch: 2.51 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7789925475701511		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.7789925475701511 | validation: 0.6828125497437683]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_623.pth
	Model improved!!!
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7618158786120891		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.7618158786120891 | validation: 0.6986654267283166]
	TIME [epoch: 2.52 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7630764267040473		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.7630764267040473 | validation: 0.701244381794571]
	TIME [epoch: 2.51 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7704633402182745		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.7704633402182745 | validation: 0.7306394687247]
	TIME [epoch: 2.52 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7761875038133053		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.7761875038133053 | validation: 0.6940480826234301]
	TIME [epoch: 2.52 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7679112586988421		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.7679112586988421 | validation: 0.7025670297743075]
	TIME [epoch: 2.51 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7631097167154102		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.7631097167154102 | validation: 0.6922994116386456]
	TIME [epoch: 2.52 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7595564881703126		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.7595564881703126 | validation: 0.693468581710416]
	TIME [epoch: 2.52 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7622823678329258		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.7622823678329258 | validation: 0.7008855252932114]
	TIME [epoch: 2.52 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.766196009808427		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.766196009808427 | validation: 0.7214503164355388]
	TIME [epoch: 2.52 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7837281383780754		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.7837281383780754 | validation: 0.6989460729679686]
	TIME [epoch: 2.51 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7897482436328135		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.7897482436328135 | validation: 0.6973495851679683]
	TIME [epoch: 2.51 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7695738920628603		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.7695738920628603 | validation: 0.6853501603300206]
	TIME [epoch: 2.51 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7607699823159229		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.7607699823159229 | validation: 0.6927813489554139]
	TIME [epoch: 2.51 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7595776938473772		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.7595776938473772 | validation: 0.6886514387557112]
	TIME [epoch: 2.52 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7671531656255798		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.7671531656255798 | validation: 0.7141384492141057]
	TIME [epoch: 2.52 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7684971021710915		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.7684971021710915 | validation: 0.697599907927263]
	TIME [epoch: 2.51 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7683438144830026		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.7683438144830026 | validation: 0.6967872578111298]
	TIME [epoch: 2.51 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7623274207254189		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.7623274207254189 | validation: 0.6926245601097206]
	TIME [epoch: 2.52 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7621291624480867		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.7621291624480867 | validation: 0.6856981544128306]
	TIME [epoch: 2.52 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7672734848337089		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.7672734848337089 | validation: 0.6965992103404722]
	TIME [epoch: 2.51 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7752984369659268		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.7752984369659268 | validation: 0.6890429781920626]
	TIME [epoch: 2.52 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7667960756914145		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.7667960756914145 | validation: 0.6927298808523741]
	TIME [epoch: 2.51 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7615176842411446		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.7615176842411446 | validation: 0.6855452091569746]
	TIME [epoch: 2.51 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.759679804654562		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.759679804654562 | validation: 0.6967707696497143]
	TIME [epoch: 2.51 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7643589856849033		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.7643589856849033 | validation: 0.7098559292913365]
	TIME [epoch: 2.51 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7679348699643654		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.7679348699643654 | validation: 0.6842436166213312]
	TIME [epoch: 2.51 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7667295388678307		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.7667295388678307 | validation: 0.7004418744815923]
	TIME [epoch: 2.52 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7707394397371949		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.7707394397371949 | validation: 0.6811613966692402]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_651.pth
	Model improved!!!
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.777085459817458		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.777085459817458 | validation: 0.7029685034237895]
	TIME [epoch: 2.59 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7708369760956122		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.7708369760956122 | validation: 0.6828410776804201]
	TIME [epoch: 2.52 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7619426129882925		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.7619426129882925 | validation: 0.6944060623409022]
	TIME [epoch: 2.52 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7612906991465767		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.7612906991465767 | validation: 0.6811270926767469]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_655.pth
	Model improved!!!
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.759394702418388		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.759394702418388 | validation: 0.6889609874283101]
	TIME [epoch: 2.52 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7570121123018205		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.7570121123018205 | validation: 0.6922758535406628]
	TIME [epoch: 2.51 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7647463429124832		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.7647463429124832 | validation: 0.6968195488000096]
	TIME [epoch: 2.52 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7611262098241612		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.7611262098241612 | validation: 0.6812373164564037]
	TIME [epoch: 2.52 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7596355780366786		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.7596355780366786 | validation: 0.682620227869927]
	TIME [epoch: 2.51 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7556648724374694		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.7556648724374694 | validation: 0.6803451682618215]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_661.pth
	Model improved!!!
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7596473178728641		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.7596473178728641 | validation: 0.6997688851979114]
	TIME [epoch: 2.51 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7770071420666331		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.7770071420666331 | validation: 0.6801666313353665]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_663.pth
	Model improved!!!
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7731097941642645		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.7731097941642645 | validation: 0.693186540382392]
	TIME [epoch: 2.52 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7634911739454359		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.7634911739454359 | validation: 0.6765985813255155]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_665.pth
	Model improved!!!
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7576344586078284		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.7576344586078284 | validation: 0.6902248746081169]
	TIME [epoch: 2.56 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7547192364408863		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.7547192364408863 | validation: 0.6954420626225993]
	TIME [epoch: 2.52 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7595569677413748		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.7595569677413748 | validation: 0.6861426671939422]
	TIME [epoch: 2.52 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7602707217882838		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.7602707217882838 | validation: 0.6790465368222984]
	TIME [epoch: 2.52 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7557552494346266		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.7557552494346266 | validation: 0.6875165242236674]
	TIME [epoch: 2.52 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7526152187953353		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.7526152187953353 | validation: 0.6700869533208498]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_671.pth
	Model improved!!!
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7494191691719712		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.7494191691719712 | validation: 0.6850992591810527]
	TIME [epoch: 2.52 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7573064154362337		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.7573064154362337 | validation: 0.6737109440029873]
	TIME [epoch: 2.53 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7627758554951257		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.7627758554951257 | validation: 0.7004880049834764]
	TIME [epoch: 2.52 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.771854060712981		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.771854060712981 | validation: 0.6739657913048727]
	TIME [epoch: 2.52 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7656513840126918		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.7656513840126918 | validation: 0.687509617005202]
	TIME [epoch: 2.52 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7578250553667542		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.7578250553667542 | validation: 0.6746351941128168]
	TIME [epoch: 2.52 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7507082488862045		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.7507082488862045 | validation: 0.688308078695138]
	TIME [epoch: 2.52 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7565799970681818		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.7565799970681818 | validation: 0.6867014551259789]
	TIME [epoch: 2.52 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7534710011256642		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.7534710011256642 | validation: 0.6872714346427041]
	TIME [epoch: 2.52 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7550726379280989		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.7550726379280989 | validation: 0.6850846120437111]
	TIME [epoch: 2.52 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7576588994702246		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.7576588994702246 | validation: 0.6789881145290719]
	TIME [epoch: 2.52 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7535646987729722		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.7535646987729722 | validation: 0.6652976856131919]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_683.pth
	Model improved!!!
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7542620629784232		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.7542620629784232 | validation: 0.6756964707764503]
	TIME [epoch: 2.52 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7544075407522882		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.7544075407522882 | validation: 0.6769249104527201]
	TIME [epoch: 2.52 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7538646376575108		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.7538646376575108 | validation: 0.6692686413626587]
	TIME [epoch: 2.52 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.759258648537263		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.759258648537263 | validation: 0.6869412591739225]
	TIME [epoch: 2.51 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7596143432620792		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.7596143432620792 | validation: 0.6761267400752644]
	TIME [epoch: 2.52 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7519734231486658		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.7519734231486658 | validation: 0.686672921316153]
	TIME [epoch: 2.51 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7529518096288663		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.7529518096288663 | validation: 0.6909788290529385]
	TIME [epoch: 2.52 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7584455789519376		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.7584455789519376 | validation: 0.6835320066546117]
	TIME [epoch: 2.51 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7693485037249198		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.7693485037249198 | validation: 0.703386891163738]
	TIME [epoch: 2.52 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7648891821609314		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.7648891821609314 | validation: 0.6776110509719951]
	TIME [epoch: 2.51 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7590772723790207		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.7590772723790207 | validation: 0.6784303277704153]
	TIME [epoch: 2.52 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7531963715959301		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.7531963715959301 | validation: 0.6623567348804485]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_695.pth
	Model improved!!!
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7443758738367874		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.7443758738367874 | validation: 0.6747146640786639]
	TIME [epoch: 2.52 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7467479843664905		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.7467479843664905 | validation: 0.6670108137554496]
	TIME [epoch: 2.52 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7506586304938361		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.7506586304938361 | validation: 0.6772858474588747]
	TIME [epoch: 2.52 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7542661170748077		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.7542661170748077 | validation: 0.6752281609961287]
	TIME [epoch: 2.52 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7523182029714178		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.7523182029714178 | validation: 0.6812255584399367]
	TIME [epoch: 2.52 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7525878063133624		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.7525878063133624 | validation: 0.6759909168371481]
	TIME [epoch: 2.52 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7493172183015758		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.7493172183015758 | validation: 0.6871910363890237]
	TIME [epoch: 2.52 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7436601526125726		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.7436601526125726 | validation: 0.6607238605559984]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_703.pth
	Model improved!!!
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7467843313889521		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.7467843313889521 | validation: 0.680471391387786]
	TIME [epoch: 2.51 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7582383095794317		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.7582383095794317 | validation: 0.6744296475174374]
	TIME [epoch: 2.51 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.769878139817796		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.769878139817796 | validation: 0.6806660457990321]
	TIME [epoch: 2.52 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.759835780778912		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.759835780778912 | validation: 0.6691525848561566]
	TIME [epoch: 2.52 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7471222762514175		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.7471222762514175 | validation: 0.6644720527896564]
	TIME [epoch: 2.51 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7455864308667005		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.7455864308667005 | validation: 0.6766044661964522]
	TIME [epoch: 2.52 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7502033501069159		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.7502033501069159 | validation: 0.6789291090966144]
	TIME [epoch: 2.51 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7491907745393764		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.7491907745393764 | validation: 0.6647670689071561]
	TIME [epoch: 2.52 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7485914319615085		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.7485914319615085 | validation: 0.6606129106546815]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_712.pth
	Model improved!!!
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.74483307882729		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.74483307882729 | validation: 0.6598275826764489]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_713.pth
	Model improved!!!
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7458629497677711		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.7458629497677711 | validation: 0.6761580529861061]
	TIME [epoch: 2.51 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.748822519826676		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.748822519826676 | validation: 0.6678484167627521]
	TIME [epoch: 2.52 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7494149336926106		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.7494149336926106 | validation: 0.674393073613548]
	TIME [epoch: 2.51 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7538860466330608		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.7538860466330608 | validation: 0.6698383537305997]
	TIME [epoch: 2.53 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7570998136234082		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.7570998136234082 | validation: 0.6759126978657131]
	TIME [epoch: 2.51 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7526114543531608		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.7526114543531608 | validation: 0.6550999320406734]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_719.pth
	Model improved!!!
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7422854104555521		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.7422854104555521 | validation: 0.6546080398682399]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_720.pth
	Model improved!!!
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7427436346116568		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.7427436346116568 | validation: 0.6701683456858565]
	TIME [epoch: 2.52 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7477798059330387		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.7477798059330387 | validation: 0.6774653139970919]
	TIME [epoch: 2.52 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7463260335354588		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.7463260335354588 | validation: 0.6670627848771601]
	TIME [epoch: 2.52 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7443549370430921		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.7443549370430921 | validation: 0.66737773775264]
	TIME [epoch: 2.52 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7392715477960762		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.7392715477960762 | validation: 0.6545870022242939]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_725.pth
	Model improved!!!
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7419971875206882		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.7419971875206882 | validation: 0.6682571811089578]
	TIME [epoch: 2.52 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7415155673995172		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.7415155673995172 | validation: 0.6603595702073587]
	TIME [epoch: 2.52 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7433961459468748		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.7433961459468748 | validation: 0.6631010295153543]
	TIME [epoch: 2.52 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7440644097969158		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.7440644097969158 | validation: 0.6636652365553266]
	TIME [epoch: 2.52 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7459860012372934		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.7459860012372934 | validation: 0.6844307066399716]
	TIME [epoch: 2.51 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7489902932896909		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.7489902932896909 | validation: 0.6690157738461221]
	TIME [epoch: 2.52 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7610872375683186		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.7610872375683186 | validation: 0.675095588746157]
	TIME [epoch: 2.51 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.754280067473299		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.754280067473299 | validation: 0.6533489384796121]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_733.pth
	Model improved!!!
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7414916187937116		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.7414916187937116 | validation: 0.6625433824377641]
	TIME [epoch: 2.52 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7401318160688986		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.7401318160688986 | validation: 0.661465973568092]
	TIME [epoch: 2.52 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7403011637715562		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.7403011637715562 | validation: 0.6618877265202796]
	TIME [epoch: 2.52 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7405725318852782		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.7405725318852782 | validation: 0.6626057412660953]
	TIME [epoch: 2.51 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7452838620826177		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.7452838620826177 | validation: 0.6607374801611678]
	TIME [epoch: 2.52 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7412423877336528		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.7412423877336528 | validation: 0.6621562647012836]
	TIME [epoch: 2.51 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7440058624725702		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.7440058624725702 | validation: 0.6631195712974232]
	TIME [epoch: 2.51 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7424810086364206		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.7424810086364206 | validation: 0.6583499234051677]
	TIME [epoch: 2.51 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7435623534063057		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.7435623534063057 | validation: 0.6648176474181983]
	TIME [epoch: 2.51 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7400811000699419		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.7400811000699419 | validation: 0.6610370429578586]
	TIME [epoch: 2.51 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7425995763717188		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.7425995763717188 | validation: 0.6698745887362744]
	TIME [epoch: 2.52 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7413815954060465		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.7413815954060465 | validation: 0.6553558653320133]
	TIME [epoch: 2.52 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7522484508926366		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.7522484508926366 | validation: 0.6759902737558381]
	TIME [epoch: 2.52 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.756155894282526		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.756155894282526 | validation: 0.6569241192466528]
	TIME [epoch: 2.52 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7451506261901103		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.7451506261901103 | validation: 0.6619735790531839]
	TIME [epoch: 2.52 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7386733944742422		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.7386733944742422 | validation: 0.6512256539466806]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_749.pth
	Model improved!!!
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7365277839570873		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.7365277839570873 | validation: 0.6519397235307512]
	TIME [epoch: 2.52 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7385239712226825		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.7385239712226825 | validation: 0.6598437690407446]
	TIME [epoch: 2.52 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7358508895246236		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.7358508895246236 | validation: 0.6555247735459485]
	TIME [epoch: 2.52 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7344345956947589		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.7344345956947589 | validation: 0.6599757168083]
	TIME [epoch: 2.52 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7347364414379982		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.7347364414379982 | validation: 0.6531160983111866]
	TIME [epoch: 2.51 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7350473256260799		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.7350473256260799 | validation: 0.6656238165513306]
	TIME [epoch: 2.52 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7422117217204027		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.7422117217204027 | validation: 0.6603551725344388]
	TIME [epoch: 2.51 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7441281507963612		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.7441281507963612 | validation: 0.6549799740184699]
	TIME [epoch: 2.52 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7390391455867595		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.7390391455867595 | validation: 0.6583811105869198]
	TIME [epoch: 2.52 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7422181568629624		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.7422181568629624 | validation: 0.6564909324618032]
	TIME [epoch: 2.52 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.748533857481698		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.748533857481698 | validation: 0.6700718156852561]
	TIME [epoch: 2.52 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7520152869712347		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.7520152869712347 | validation: 0.6457414244733385]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_761.pth
	Model improved!!!
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.737668148158973		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.737668148158973 | validation: 0.6522778300275829]
	TIME [epoch: 2.52 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7340729872711118		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.7340729872711118 | validation: 0.655123169032947]
	TIME [epoch: 2.52 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7335351869329576		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.7335351869329576 | validation: 0.6492169051824054]
	TIME [epoch: 2.52 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7364851430729243		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.7364851430729243 | validation: 0.6550284810558815]
	TIME [epoch: 2.51 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7340136765814463		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.7340136765814463 | validation: 0.6595892727634277]
	TIME [epoch: 2.52 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7355035685804431		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.7355035685804431 | validation: 0.6476775110772133]
	TIME [epoch: 2.52 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.739204784808198		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.739204784808198 | validation: 0.6551688462857798]
	TIME [epoch: 2.52 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7412677333054433		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.7412677333054433 | validation: 0.65039500055927]
	TIME [epoch: 2.51 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7414016639830355		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.7414016639830355 | validation: 0.6472165094529603]
	TIME [epoch: 2.52 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7370107261411206		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.7370107261411206 | validation: 0.6506007779379069]
	TIME [epoch: 2.52 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7350097085911645		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.7350097085911645 | validation: 0.6558417724673558]
	TIME [epoch: 2.52 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7328303061333545		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.7328303061333545 | validation: 0.6501074559754736]
	TIME [epoch: 2.51 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7371738721642421		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.7371738721642421 | validation: 0.6509542047573181]
	TIME [epoch: 2.52 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7330063316906033		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.7330063316906033 | validation: 0.6575398982403992]
	TIME [epoch: 2.51 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7368609593327059		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.7368609593327059 | validation: 0.6526544067175152]
	TIME [epoch: 2.52 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7346950135921015		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.7346950135921015 | validation: 0.647178850815018]
	TIME [epoch: 2.51 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7388738505211238		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.7388738505211238 | validation: 0.6565155553366682]
	TIME [epoch: 2.52 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7336987355702342		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.7336987355702342 | validation: 0.6527657116217335]
	TIME [epoch: 2.51 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7316815449674737		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.7316815449674737 | validation: 0.6495557164427453]
	TIME [epoch: 2.52 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7351700648394376		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.7351700648394376 | validation: 0.6504461276637281]
	TIME [epoch: 2.52 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7390982612063295		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.7390982612063295 | validation: 0.6582710765031416]
	TIME [epoch: 2.52 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7414501738962659		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.7414501738962659 | validation: 0.6428706332459874]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_783.pth
	Model improved!!!
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7310477149967313		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.7310477149967313 | validation: 0.6419918551000374]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_784.pth
	Model improved!!!
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7288451402807988		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.7288451402807988 | validation: 0.6527107506181036]
	TIME [epoch: 2.52 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7305838033534164		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.7305838033534164 | validation: 0.6544929630362818]
	TIME [epoch: 2.52 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7331486097920157		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.7331486097920157 | validation: 0.6525199055953652]
	TIME [epoch: 2.52 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7323413033450933		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.7323413033450933 | validation: 0.6497444104370975]
	TIME [epoch: 2.52 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7348469991982822		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.7348469991982822 | validation: 0.6410857938858916]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_789.pth
	Model improved!!!
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7313693135006466		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.7313693135006466 | validation: 0.6527577540918288]
	TIME [epoch: 2.52 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7316165246502332		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.7316165246502332 | validation: 0.6456109975949145]
	TIME [epoch: 2.51 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7290434082035588		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.7290434082035588 | validation: 0.6539608438884823]
	TIME [epoch: 2.52 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7291592413900888		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.7291592413900888 | validation: 0.6420518572495932]
	TIME [epoch: 2.51 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7340655252913029		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.7340655252913029 | validation: 0.6501926140762744]
	TIME [epoch: 2.51 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7344181201021718		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.7344181201021718 | validation: 0.6460865714470704]
	TIME [epoch: 2.51 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7314529981339531		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.7314529981339531 | validation: 0.6543856039381646]
	TIME [epoch: 2.51 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7320901397972329		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.7320901397972329 | validation: 0.644278198465202]
	TIME [epoch: 2.51 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7299836333491129		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.7299836333491129 | validation: 0.6509156867809923]
	TIME [epoch: 2.51 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.725032818884979		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.725032818884979 | validation: 0.6456785766235124]
	TIME [epoch: 2.51 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7258729805535142		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.7258729805535142 | validation: 0.6401309665442471]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_800.pth
	Model improved!!!
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7247531641010235		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.7247531641010235 | validation: 0.6413592441971163]
	TIME [epoch: 2.52 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7301317767360689		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.7301317767360689 | validation: 0.6507341341593816]
	TIME [epoch: 2.52 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7260538330296606		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.7260538330296606 | validation: 0.6472828486081652]
	TIME [epoch: 2.52 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.729502842326315		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.729502842326315 | validation: 0.6435452942596482]
	TIME [epoch: 2.51 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7254339406047852		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.7254339406047852 | validation: 0.6399909219796387]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_805.pth
	Model improved!!!
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7263056707288439		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.7263056707288439 | validation: 0.6501645623805486]
	TIME [epoch: 2.52 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7256602728459479		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.7256602728459479 | validation: 0.6435087930618995]
	TIME [epoch: 2.52 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7320681970107102		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.7320681970107102 | validation: 0.6537274189421018]
	TIME [epoch: 2.52 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7373756167512656		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.7373756167512656 | validation: 0.6396236242435482]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_809.pth
	Model improved!!!
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7290189404954082		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.7290189404954082 | validation: 0.6388979305620626]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_810.pth
	Model improved!!!
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7246767530773315		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.7246767530773315 | validation: 0.6440603272242038]
	TIME [epoch: 2.52 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7248690324172924		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.7248690324172924 | validation: 0.64470318694919]
	TIME [epoch: 2.52 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7233657229084985		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.7233657229084985 | validation: 0.6462463871883286]
	TIME [epoch: 2.52 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7246056424585067		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.7246056424585067 | validation: 0.6493205555003757]
	TIME [epoch: 2.52 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7301757151071534		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.7301757151071534 | validation: 0.6628775181746397]
	TIME [epoch: 2.52 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7326206827896692		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.7326206827896692 | validation: 0.6347408476507868]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_816.pth
	Model improved!!!
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7319233804050893		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.7319233804050893 | validation: 0.6388829455996806]
	TIME [epoch: 2.52 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7243666341824249		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.7243666341824249 | validation: 0.6434541420454312]
	TIME [epoch: 2.52 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7216262620332582		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.7216262620332582 | validation: 0.6412147189779259]
	TIME [epoch: 2.52 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7241985485160995		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.7241985485160995 | validation: 0.6393239119962836]
	TIME [epoch: 2.52 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7266979538145969		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.7266979538145969 | validation: 0.6503775469932868]
	TIME [epoch: 2.52 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7245476059725745		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.7245476059725745 | validation: 0.6468042712753759]
	TIME [epoch: 2.52 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7291611024991187		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.7291611024991187 | validation: 0.6443362184171488]
	TIME [epoch: 2.51 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7220755690903095		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.7220755690903095 | validation: 0.6409800684941612]
	TIME [epoch: 2.52 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.722377114502263		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.722377114502263 | validation: 0.6424949803460663]
	TIME [epoch: 2.51 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7201252731171602		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.7201252731171602 | validation: 0.636028082935133]
	TIME [epoch: 2.51 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7206714053651503		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.7206714053651503 | validation: 0.6524687874546979]
	TIME [epoch: 2.51 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7250736266085932		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.7250736266085932 | validation: 0.6448156647351846]
	TIME [epoch: 2.51 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7345182991918898		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.7345182991918898 | validation: 0.6423591691005522]
	TIME [epoch: 2.53 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7239893080292771		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.7239893080292771 | validation: 0.6371649921613153]
	TIME [epoch: 2.51 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7186136697119945		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.7186136697119945 | validation: 0.636482550435124]
	TIME [epoch: 2.51 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.72031312773824		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.72031312773824 | validation: 0.6369383038540404]
	TIME [epoch: 2.51 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7229206193535079		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.7229206193535079 | validation: 0.6328865495336341]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_833.pth
	Model improved!!!
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7196959832608155		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.7196959832608155 | validation: 0.6332817663123754]
	TIME [epoch: 2.52 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7173184623692785		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.7173184623692785 | validation: 0.635019475820736]
	TIME [epoch: 2.52 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7184648534867515		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.7184648534867515 | validation: 0.6320973413618551]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_836.pth
	Model improved!!!
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7195170569355908		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.7195170569355908 | validation: 0.6371834816504185]
	TIME [epoch: 2.51 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7205686052440915		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.7205686052440915 | validation: 0.6477791567315102]
	TIME [epoch: 2.51 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7272090892456438		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.7272090892456438 | validation: 0.6593819382249615]
	TIME [epoch: 2.51 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7276291243743995		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.7276291243743995 | validation: 0.6429453380849315]
	TIME [epoch: 2.51 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7187450654124338		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.7187450654124338 | validation: 0.6351702926138504]
	TIME [epoch: 2.51 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.717766871395055		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.717766871395055 | validation: 0.623400049060237]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_842.pth
	Model improved!!!
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7162486308808318		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.7162486308808318 | validation: 0.6371629191730546]
	TIME [epoch: 2.52 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.71982087688189		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.71982087688189 | validation: 0.641797588997504]
	TIME [epoch: 2.51 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.720473218446322		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.720473218446322 | validation: 0.6397278376338491]
	TIME [epoch: 2.52 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7198656774738507		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.7198656774738507 | validation: 0.6332410955123]
	TIME [epoch: 2.52 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7148427017834662		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.7148427017834662 | validation: 0.6282020146545706]
	TIME [epoch: 2.51 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7174815692934448		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.7174815692934448 | validation: 0.6328813063216]
	TIME [epoch: 2.51 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7169566193971721		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.7169566193971721 | validation: 0.6267421394538694]
	TIME [epoch: 2.51 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.713505471295109		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.713505471295109 | validation: 0.6366037675018736]
	TIME [epoch: 2.51 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7189148782167234		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.7189148782167234 | validation: 0.6555305663932443]
	TIME [epoch: 2.51 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7254031571973684		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.7254031571973684 | validation: 0.642398223732306]
	TIME [epoch: 2.51 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7258324554191552		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.7258324554191552 | validation: 0.6294497334703549]
	TIME [epoch: 2.51 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7209406681721364		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.7209406681721364 | validation: 0.6282287274066002]
	TIME [epoch: 2.51 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7190339692983957		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.7190339692983957 | validation: 0.6368261713857156]
	TIME [epoch: 2.51 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.716933020831161		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.716933020831161 | validation: 0.6256440728985332]
	TIME [epoch: 2.51 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.711616367974228		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.711616367974228 | validation: 0.6260183040078042]
	TIME [epoch: 2.52 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7156435615772619		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.7156435615772619 | validation: 0.625016476215899]
	TIME [epoch: 2.51 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7133506856414552		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.7133506856414552 | validation: 0.6316672115111883]
	TIME [epoch: 2.51 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7144315285995267		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.7144315285995267 | validation: 0.6398759334459244]
	TIME [epoch: 2.51 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7186799389684128		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.7186799389684128 | validation: 0.6400026878405473]
	TIME [epoch: 2.51 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7189296713094635		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.7189296713094635 | validation: 0.6446573142635306]
	TIME [epoch: 2.51 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7180104851316065		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.7180104851316065 | validation: 0.6293901551634049]
	TIME [epoch: 2.51 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7163434438789317		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.7163434438789317 | validation: 0.6363183764493345]
	TIME [epoch: 2.51 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7148806195928742		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.7148806195928742 | validation: 0.6276598294910041]
	TIME [epoch: 2.51 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7108450958116899		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.7108450958116899 | validation: 0.6308307542589022]
	TIME [epoch: 2.51 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7135928639796778		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.7135928639796778 | validation: 0.6252642549001092]
	TIME [epoch: 2.51 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7112807260672128		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.7112807260672128 | validation: 0.6285770699586162]
	TIME [epoch: 2.51 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7142397888259577		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.7142397888259577 | validation: 0.6298743494503335]
	TIME [epoch: 2.52 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7136041601720807		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.7136041601720807 | validation: 0.6332443547379065]
	TIME [epoch: 2.51 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.716018531162201		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.716018531162201 | validation: 0.6401948296037832]
	TIME [epoch: 2.52 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7182020863664144		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.7182020863664144 | validation: 0.6376427602724578]
	TIME [epoch: 2.51 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7158235888842378		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.7158235888842378 | validation: 0.6199917454195356]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_873.pth
	Model improved!!!
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7156240817847194		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.7156240817847194 | validation: 0.6297114795480035]
	TIME [epoch: 2.52 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7148056129864687		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.7148056129864687 | validation: 0.6300680702016989]
	TIME [epoch: 2.51 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7184681813650815		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.7184681813650815 | validation: 0.6265954313171287]
	TIME [epoch: 2.51 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7110656955596525		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.7110656955596525 | validation: 0.6286244272272312]
	TIME [epoch: 2.51 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7103033453476261		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.7103033453476261 | validation: 0.6199552517360201]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_878.pth
	Model improved!!!
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7102383001715715		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.7102383001715715 | validation: 0.6211755229328255]
	TIME [epoch: 2.51 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7097860644343845		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.7097860644343845 | validation: 0.64020262496521]
	TIME [epoch: 2.51 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7133236284815984		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.7133236284815984 | validation: 0.633579554333877]
	TIME [epoch: 2.51 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7159652089093038		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.7159652089093038 | validation: 0.6204184470078533]
	TIME [epoch: 2.51 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7098254381576924		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.7098254381576924 | validation: 0.620244186416421]
	TIME [epoch: 2.51 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7088332098889116		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.7088332098889116 | validation: 0.6202408687607529]
	TIME [epoch: 2.51 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7086099211024127		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.7086099211024127 | validation: 0.6235771550529426]
	TIME [epoch: 2.51 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.706132681064185		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.706132681064185 | validation: 0.6198986298918174]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_886.pth
	Model improved!!!
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.709338168442865		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.709338168442865 | validation: 0.6203209141200084]
	TIME [epoch: 2.51 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7136320891936085		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.7136320891936085 | validation: 0.6368507032607]
	TIME [epoch: 2.51 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7183459770543924		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.7183459770543924 | validation: 0.6271256780635803]
	TIME [epoch: 2.51 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7180122351542922		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.7180122351542922 | validation: 0.6108268316318552]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_890.pth
	Model improved!!!
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7111545176908283		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.7111545176908283 | validation: 0.6226024563823322]
	TIME [epoch: 2.53 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7059574156645698		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.7059574156645698 | validation: 0.6215980976350646]
	TIME [epoch: 2.52 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7056753010538468		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.7056753010538468 | validation: 0.6255479071356487]
	TIME [epoch: 2.52 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7076331611777411		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.7076331611777411 | validation: 0.6227820645007007]
	TIME [epoch: 2.51 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7052374164842198		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.7052374164842198 | validation: 0.6246730049454545]
	TIME [epoch: 2.51 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7071675689957482		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.7071675689957482 | validation: 0.6247567995964558]
	TIME [epoch: 2.51 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7065854211939663		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.7065854211939663 | validation: 0.6295123560199348]
	TIME [epoch: 2.51 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7078519655225984		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.7078519655225984 | validation: 0.6365351822794327]
	TIME [epoch: 2.51 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7149557080973068		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.7149557080973068 | validation: 0.624184483640891]
	TIME [epoch: 2.51 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7131936225784782		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.7131936225784782 | validation: 0.6223269907188769]
	TIME [epoch: 2.52 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7091704826802547		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.7091704826802547 | validation: 0.6172974486041262]
	TIME [epoch: 2.53 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7075410810419281		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.7075410810419281 | validation: 0.6178430564809667]
	TIME [epoch: 2.52 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7064751671163856		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.7064751671163856 | validation: 0.6207233232834991]
	TIME [epoch: 2.52 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7050746759033903		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.7050746759033903 | validation: 0.6169406251022913]
	TIME [epoch: 2.52 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7049911647403438		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.7049911647403438 | validation: 0.6182094092394113]
	TIME [epoch: 2.52 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7066043075580386		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.7066043075580386 | validation: 0.6212561699432477]
	TIME [epoch: 2.52 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7079875464370196		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.7079875464370196 | validation: 0.6150709596675324]
	TIME [epoch: 2.52 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7072645415867413		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.7072645415867413 | validation: 0.6181733696901245]
	TIME [epoch: 2.52 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7038784198198966		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.7038784198198966 | validation: 0.6088622667272987]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_909.pth
	Model improved!!!
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7053478826053028		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.7053478826053028 | validation: 0.6250689723587545]
	TIME [epoch: 2.5 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7070781503504288		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.7070781503504288 | validation: 0.6150757603729939]
	TIME [epoch: 2.5 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7088417962148853		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.7088417962148853 | validation: 0.6204386273810845]
	TIME [epoch: 2.51 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7080354366771475		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.7080354366771475 | validation: 0.613741748102306]
	TIME [epoch: 2.53 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7065500687915742		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.7065500687915742 | validation: 0.6129918417414957]
	TIME [epoch: 2.51 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7023643340943222		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.7023643340943222 | validation: 0.615474768068258]
	TIME [epoch: 2.51 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7035740099567545		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.7035740099567545 | validation: 0.6175559353740457]
	TIME [epoch: 2.51 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7046674260081209		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.7046674260081209 | validation: 0.6116470352014824]
	TIME [epoch: 2.52 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7006849579891704		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.7006849579891704 | validation: 0.6213720611986262]
	TIME [epoch: 2.51 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7064259062940649		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.7064259062940649 | validation: 0.6246832704290102]
	TIME [epoch: 2.51 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7064800827600323		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.7064800827600323 | validation: 0.6139955422770684]
	TIME [epoch: 2.53 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7031857482435876		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.7031857482435876 | validation: 0.6136487935866752]
	TIME [epoch: 2.51 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7042390144847531		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.7042390144847531 | validation: 0.6144427020344162]
	TIME [epoch: 2.51 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7010433120599093		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.7010433120599093 | validation: 0.6231464015811027]
	TIME [epoch: 2.51 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.704355487450249		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.704355487450249 | validation: 0.6173390687411588]
	TIME [epoch: 2.52 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7016643776233419		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.7016643776233419 | validation: 0.6156977948568506]
	TIME [epoch: 2.51 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7058656070959282		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.7058656070959282 | validation: 0.6198492247121846]
	TIME [epoch: 2.51 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7072488656630719		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.7072488656630719 | validation: 0.6132223655859019]
	TIME [epoch: 2.51 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7059901432466416		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.7059901432466416 | validation: 0.6144545565077077]
	TIME [epoch: 2.51 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6997579658886162		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.6997579658886162 | validation: 0.6115599972766609]
	TIME [epoch: 2.51 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7018041097121229		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.7018041097121229 | validation: 0.6208888075975354]
	TIME [epoch: 2.51 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7034570006157697		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.7034570006157697 | validation: 0.6217180400086971]
	TIME [epoch: 2.51 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6997942606419204		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.6997942606419204 | validation: 0.6136290106769602]
	TIME [epoch: 2.51 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7010595976667143		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.7010595976667143 | validation: 0.6088007360772159]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_933.pth
	Model improved!!!
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6994772620952366		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.6994772620952366 | validation: 0.6121215240344022]
	TIME [epoch: 2.52 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6991501400077266		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.6991501400077266 | validation: 0.6107729409821891]
	TIME [epoch: 2.56 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7010941808782912		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.7010941808782912 | validation: 0.6060292546388898]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_936.pth
	Model improved!!!
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.703184619335151		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.703184619335151 | validation: 0.6225959543417068]
	TIME [epoch: 2.52 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7021301551184097		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.7021301551184097 | validation: 0.6132286903534363]
	TIME [epoch: 2.52 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7011118685558648		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.7011118685558648 | validation: 0.6209950746213949]
	TIME [epoch: 2.52 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6985871968991174		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.6985871968991174 | validation: 0.6161954673745783]
	TIME [epoch: 2.51 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6998746282879814		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.6998746282879814 | validation: 0.6042212223417276]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_941.pth
	Model improved!!!
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6968739420534545		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.6968739420534545 | validation: 0.6072962422441162]
	TIME [epoch: 2.51 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6976282733239659		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.6976282733239659 | validation: 0.608113447842836]
	TIME [epoch: 2.51 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.697929621940137		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.697929621940137 | validation: 0.614591006429767]
	TIME [epoch: 2.51 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6990324551670173		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.6990324551670173 | validation: 0.6116188302800328]
	TIME [epoch: 2.53 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6954057335459964		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.6954057335459964 | validation: 0.612559444201764]
	TIME [epoch: 2.52 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7015311760628217		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.7015311760628217 | validation: 0.6165002578606115]
	TIME [epoch: 2.52 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7005044603996718		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.7005044603996718 | validation: 0.6090103079615252]
	TIME [epoch: 2.51 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6995025670825317		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.6995025670825317 | validation: 0.6111642871595784]
	TIME [epoch: 2.52 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6988110266429419		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.6988110266429419 | validation: 0.6131609615643987]
	TIME [epoch: 2.52 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7004235795965607		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.7004235795965607 | validation: 0.6077641869666165]
	TIME [epoch: 2.51 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7016376595229593		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.7016376595229593 | validation: 0.6117431473877448]
	TIME [epoch: 2.52 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7022836635663834		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.7022836635663834 | validation: 0.6108983245767368]
	TIME [epoch: 2.51 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6975890178988918		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.6975890178988918 | validation: 0.6117528969984765]
	TIME [epoch: 2.51 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6947494217160902		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.6947494217160902 | validation: 0.6029200462837375]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_955.pth
	Model improved!!!
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7002751279004856		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.7002751279004856 | validation: 0.6123079613585681]
	TIME [epoch: 2.52 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.697515080741872		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.697515080741872 | validation: 0.6049204979554738]
	TIME [epoch: 2.52 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6985068160159048		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.6985068160159048 | validation: 0.6067428334066799]
	TIME [epoch: 2.52 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6998087374967621		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.6998087374967621 | validation: 0.6149657559424562]
	TIME [epoch: 2.52 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6978282026828413		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.6978282026828413 | validation: 0.6244798714460572]
	TIME [epoch: 2.51 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6973132215154033		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.6973132215154033 | validation: 0.6015740225343919]
	TIME [epoch: 2.55 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_961.pth
	Model improved!!!
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6959003957274641		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.6959003957274641 | validation: 0.6114191021230136]
	TIME [epoch: 2.52 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6940923399670764		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.6940923399670764 | validation: 0.605771440221222]
	TIME [epoch: 2.52 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7005931995316925		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.7005931995316925 | validation: 0.6146957570677565]
	TIME [epoch: 2.51 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.694993544295683		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.694993544295683 | validation: 0.6063160038185126]
	TIME [epoch: 2.51 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6946416493914594		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.6946416493914594 | validation: 0.6050147695191742]
	TIME [epoch: 2.52 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6951576774195646		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.6951576774195646 | validation: 0.6117380534556137]
	TIME [epoch: 2.51 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6957897452323453		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.6957897452323453 | validation: 0.6157297154851407]
	TIME [epoch: 2.52 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.698814607617942		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.698814607617942 | validation: 0.6050402220068598]
	TIME [epoch: 2.51 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6981925560927604		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.6981925560927604 | validation: 0.6038150191612393]
	TIME [epoch: 2.54 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6925867315091226		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.6925867315091226 | validation: 0.6024138226919469]
	TIME [epoch: 2.51 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6944664705065863		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.6944664705065863 | validation: 0.6069985361862132]
	TIME [epoch: 2.51 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6921418112275228		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.6921418112275228 | validation: 0.6038560893985526]
	TIME [epoch: 2.51 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6937939124179038		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.6937939124179038 | validation: 0.6030519816146895]
	TIME [epoch: 2.52 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6927925077596905		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.6927925077596905 | validation: 0.6039900199935492]
	TIME [epoch: 2.51 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.697141568439621		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.697141568439621 | validation: 0.6104885614405142]
	TIME [epoch: 2.51 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6956671365005005		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.6956671365005005 | validation: 0.613783774685599]
	TIME [epoch: 2.51 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6974073952275021		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.6974073952275021 | validation: 0.6059100965705726]
	TIME [epoch: 2.51 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6932513644450015		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.6932513644450015 | validation: 0.5984056756670183]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_979.pth
	Model improved!!!
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6901931255927493		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.6901931255927493 | validation: 0.6053676676384426]
	TIME [epoch: 2.51 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6934003160424558		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.6934003160424558 | validation: 0.6094322452003671]
	TIME [epoch: 2.51 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6908317589642128		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.6908317589642128 | validation: 0.60887248571832]
	TIME [epoch: 2.52 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.69039141657419		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.69039141657419 | validation: 0.6033029419548819]
	TIME [epoch: 2.51 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6933794685062283		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.6933794685062283 | validation: 0.614003242446977]
	TIME [epoch: 2.51 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6969458651731819		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.6969458651731819 | validation: 0.6078534824863399]
	TIME [epoch: 2.51 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6965337862295704		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.6965337862295704 | validation: 0.6059886108512939]
	TIME [epoch: 2.51 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.691377520580975		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.691377520580975 | validation: 0.6057222202569688]
	TIME [epoch: 2.51 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6923125901868675		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.6923125901868675 | validation: 0.6022809964286491]
	TIME [epoch: 2.52 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.694429491516985		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.694429491516985 | validation: 0.5991141752584741]
	TIME [epoch: 2.51 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6933239984529612		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.6933239984529612 | validation: 0.5932016113781161]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_990.pth
	Model improved!!!
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6896073456880545		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.6896073456880545 | validation: 0.6084032200145549]
	TIME [epoch: 2.56 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.690856200209229		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.690856200209229 | validation: 0.6095605922681162]
	TIME [epoch: 2.52 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6934554238325873		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.6934554238325873 | validation: 0.6085596474110134]
	TIME [epoch: 2.52 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6921439505971572		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.6921439505971572 | validation: 0.5961306987276292]
	TIME [epoch: 2.52 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.69276028889521		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.69276028889521 | validation: 0.6040827054573237]
	TIME [epoch: 2.52 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6922719286969314		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.6922719286969314 | validation: 0.6059915727673958]
	TIME [epoch: 2.52 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6918786472280666		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.6918786472280666 | validation: 0.596175220332752]
	TIME [epoch: 2.52 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6886111478830422		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.6886111478830422 | validation: 0.6034537110592703]
	TIME [epoch: 2.52 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6912956585979305		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.6912956585979305 | validation: 0.6000969543057431]
	TIME [epoch: 2.52 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6890413560690279		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.6890413560690279 | validation: 0.599218159638254]
	TIME [epoch: 2.52 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6888008557018356		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.6888008557018356 | validation: 0.5992516303710359]
	TIME [epoch: 186 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6880095818307151		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.6880095818307151 | validation: 0.5984434297950026]
	TIME [epoch: 5.44 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6886869955314455		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.6886869955314455 | validation: 0.6065466013593209]
	TIME [epoch: 5.42 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6899958882975982		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.6899958882975982 | validation: 0.603762051314277]
	TIME [epoch: 5.41 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6904445497733203		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.6904445497733203 | validation: 0.6116789552542516]
	TIME [epoch: 5.42 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6916773284598083		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.6916773284598083 | validation: 0.5967203620862237]
	TIME [epoch: 5.42 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.692660845384963		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.692660845384963 | validation: 0.6041921262041599]
	TIME [epoch: 5.41 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.691277303221157		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.691277303221157 | validation: 0.6029183762542183]
	TIME [epoch: 5.42 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6853890322535819		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.6853890322535819 | validation: 0.5981442194643997]
	TIME [epoch: 5.42 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6862659094103444		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.6862659094103444 | validation: 0.5923670511071265]
	TIME [epoch: 5.42 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1010.pth
	Model improved!!!
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6872286310908254		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.6872286310908254 | validation: 0.5977638808834335]
	TIME [epoch: 5.43 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6849459317171923		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.6849459317171923 | validation: 0.6034881716429941]
	TIME [epoch: 5.41 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6898210975436316		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.6898210975436316 | validation: 0.5965269499759318]
	TIME [epoch: 5.42 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6863875471255332		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.6863875471255332 | validation: 0.6022618593923261]
	TIME [epoch: 5.41 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6910978264038908		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.6910978264038908 | validation: 0.5905133172630465]
	TIME [epoch: 5.42 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1015.pth
	Model improved!!!
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6868863510123262		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.6868863510123262 | validation: 0.6021430071311156]
	TIME [epoch: 5.42 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6902434971760607		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.6902434971760607 | validation: 0.5968631969408847]
	TIME [epoch: 5.41 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6895077557433136		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.6895077557433136 | validation: 0.6068076323242155]
	TIME [epoch: 5.42 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6859754956480447		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.6859754956480447 | validation: 0.5925588378852124]
	TIME [epoch: 5.41 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6895437701998088		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.6895437701998088 | validation: 0.5946261109267136]
	TIME [epoch: 5.43 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6881405849121853		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.6881405849121853 | validation: 0.5929566691608568]
	TIME [epoch: 5.41 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6860381262975622		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.6860381262975622 | validation: 0.5980112046819137]
	TIME [epoch: 5.42 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6865399721950882		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.6865399721950882 | validation: 0.5945900763749653]
	TIME [epoch: 5.42 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6869252989846716		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.6869252989846716 | validation: 0.5981891236857388]
	TIME [epoch: 5.42 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6871912746017423		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.6871912746017423 | validation: 0.5901319662658091]
	TIME [epoch: 5.41 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1025.pth
	Model improved!!!
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6869439150622979		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.6869439150622979 | validation: 0.5866694738118189]
	TIME [epoch: 5.43 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1026.pth
	Model improved!!!
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.683781282146581		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.683781282146581 | validation: 0.5987086046749266]
	TIME [epoch: 5.41 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6872314083835005		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.6872314083835005 | validation: 0.6009054984612696]
	TIME [epoch: 5.42 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6835619206044509		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.6835619206044509 | validation: 0.5975021855102439]
	TIME [epoch: 5.41 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6858379341051313		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.6858379341051313 | validation: 0.5930041756053095]
	TIME [epoch: 5.42 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6843270617717414		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.6843270617717414 | validation: 0.6063742357829667]
	TIME [epoch: 5.43 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6855907142011014		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.6855907142011014 | validation: 0.5978940848532618]
	TIME [epoch: 5.43 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6848340085692332		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.6848340085692332 | validation: 0.5890288727435543]
	TIME [epoch: 5.43 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6847110324582784		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.6847110324582784 | validation: 0.5979185536689186]
	TIME [epoch: 5.42 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.685667442053105		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.685667442053105 | validation: 0.5947355634461324]
	TIME [epoch: 5.42 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6863218874904269		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.6863218874904269 | validation: 0.5980911847819183]
	TIME [epoch: 5.41 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6877372971328499		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.6877372971328499 | validation: 0.5906132799358218]
	TIME [epoch: 5.43 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6862778780266391		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.6862778780266391 | validation: 0.5932325696902865]
	TIME [epoch: 5.41 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6874968573301535		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.6874968573301535 | validation: 0.592548305115454]
	TIME [epoch: 5.42 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6876081841281554		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.6876081841281554 | validation: 0.5918257196258241]
	TIME [epoch: 5.41 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6867495478738315		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.6867495478738315 | validation: 0.5908631782545465]
	TIME [epoch: 5.42 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.682520225934815		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.682520225934815 | validation: 0.5915349906522697]
	TIME [epoch: 5.42 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6881540628934687		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.6881540628934687 | validation: 0.5936425398501712]
	TIME [epoch: 5.43 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6855048969361349		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.6855048969361349 | validation: 0.5914839493998556]
	TIME [epoch: 5.41 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.685209540462659		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.685209540462659 | validation: 0.5864373547321672]
	TIME [epoch: 5.42 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1045.pth
	Model improved!!!
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6837337599574275		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.6837337599574275 | validation: 0.600999154479462]
	TIME [epoch: 5.41 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6840918003895299		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.6840918003895299 | validation: 0.5880988951482067]
	TIME [epoch: 5.42 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.683586683505827		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.683586683505827 | validation: 0.5920235607991627]
	TIME [epoch: 5.42 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6824493738512125		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.6824493738512125 | validation: 0.5963816290797113]
	TIME [epoch: 5.42 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6831315100214188		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.6831315100214188 | validation: 0.5896534957183882]
	TIME [epoch: 5.42 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6854444912240345		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.6854444912240345 | validation: 0.5930585349180432]
	TIME [epoch: 5.42 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6815434181678407		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.6815434181678407 | validation: 0.5965043242713249]
	TIME [epoch: 5.42 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6846491494479413		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.6846491494479413 | validation: 0.6032610798867529]
	TIME [epoch: 5.42 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6824672923106064		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.6824672923106064 | validation: 0.5923106945533266]
	TIME [epoch: 5.41 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6810965741367255		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.6810965741367255 | validation: 0.5971210870162359]
	TIME [epoch: 5.42 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6819081012023491		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.6819081012023491 | validation: 0.5965574558770731]
	TIME [epoch: 5.41 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6822592839333024		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.6822592839333024 | validation: 0.5827676234490337]
	TIME [epoch: 5.42 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1057.pth
	Model improved!!!
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6832735798614834		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.6832735798614834 | validation: 0.591264690931618]
	TIME [epoch: 5.43 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.682202723531799		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.682202723531799 | validation: 0.5898796084270826]
	TIME [epoch: 5.42 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6800025178757502		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.6800025178757502 | validation: 0.5841997868631784]
	TIME [epoch: 5.41 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6803394483698147		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.6803394483698147 | validation: 0.5865922794963041]
	TIME [epoch: 5.41 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.681696047284027		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.681696047284027 | validation: 0.595319886515664]
	TIME [epoch: 5.41 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6828027814504003		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.6828027814504003 | validation: 0.5967977458493147]
	TIME [epoch: 5.41 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6816174940432131		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.6816174940432131 | validation: 0.5852828807174385]
	TIME [epoch: 5.41 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6828119521346406		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.6828119521346406 | validation: 0.5889581467343245]
	TIME [epoch: 5.41 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6819116009623402		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.6819116009623402 | validation: 0.5895064326296174]
	TIME [epoch: 5.41 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6790805149229504		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.6790805149229504 | validation: 0.5884375795736823]
	TIME [epoch: 5.4 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6808457029048812		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.6808457029048812 | validation: 0.593215546732384]
	TIME [epoch: 5.42 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6823027606170912		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.6823027606170912 | validation: 0.5972260565090702]
	TIME [epoch: 5.41 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6811419963059597		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.6811419963059597 | validation: 0.5861669739489898]
	TIME [epoch: 5.42 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6798872158840896		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.6798872158840896 | validation: 0.5864063976139015]
	TIME [epoch: 5.41 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.679334134108579		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.679334134108579 | validation: 0.5823036464434891]
	TIME [epoch: 5.41 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1072.pth
	Model improved!!!
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6797600018565126		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.6797600018565126 | validation: 0.5846725634116385]
	TIME [epoch: 5.41 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6783793000984472		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.6783793000984472 | validation: 0.5929153712873457]
	TIME [epoch: 5.42 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6814705833004737		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.6814705833004737 | validation: 0.5942318790346836]
	TIME [epoch: 5.42 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6779015996957055		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.6779015996957055 | validation: 0.5856542652511991]
	TIME [epoch: 5.41 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6803523395388436		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.6803523395388436 | validation: 0.5860272453745433]
	TIME [epoch: 5.41 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6825357465694744		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.6825357465694744 | validation: 0.5893894422162819]
	TIME [epoch: 5.41 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6800549612342068		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.6800549612342068 | validation: 0.5907129990197695]
	TIME [epoch: 5.41 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6814293898606872		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.6814293898606872 | validation: 0.5854221467582876]
	TIME [epoch: 5.42 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6785175723119915		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.6785175723119915 | validation: 0.5906256612207671]
	TIME [epoch: 5.41 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6784978424314562		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.6784978424314562 | validation: 0.5830685016722134]
	TIME [epoch: 5.41 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6783782506672646		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.6783782506672646 | validation: 0.5799615979124211]
	TIME [epoch: 5.41 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1083.pth
	Model improved!!!
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6772742823764086		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.6772742823764086 | validation: 0.5880569217856166]
	TIME [epoch: 5.42 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6784013169901805		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.6784013169901805 | validation: 0.57908338895787]
	TIME [epoch: 5.42 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1085.pth
	Model improved!!!
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6771583712035172		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.6771583712035172 | validation: 0.5784454536975393]
	TIME [epoch: 5.41 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1086.pth
	Model improved!!!
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6787858292923477		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.6787858292923477 | validation: 0.5844042496262365]
	TIME [epoch: 5.42 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6765139356590129		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.6765139356590129 | validation: 0.587391739799603]
	TIME [epoch: 5.42 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6809445694693651		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.6809445694693651 | validation: 0.5839067374032126]
	TIME [epoch: 5.41 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6788080168045381		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.6788080168045381 | validation: 0.5925533701809051]
	TIME [epoch: 5.42 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6778366045337745		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.6778366045337745 | validation: 0.590491322219757]
	TIME [epoch: 5.41 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6763278611333022		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.6763278611333022 | validation: 0.5851770672477727]
	TIME [epoch: 5.41 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6750308477995776		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.6750308477995776 | validation: 0.5819476547430007]
	TIME [epoch: 5.41 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6789095052154435		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.6789095052154435 | validation: 0.5898502942784276]
	TIME [epoch: 5.41 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6731794953837159		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.6731794953837159 | validation: 0.5798792482026771]
	TIME [epoch: 5.42 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6807808235776761		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.6807808235776761 | validation: 0.5852690821986135]
	TIME [epoch: 5.42 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6767048205515999		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.6767048205515999 | validation: 0.5890732332499529]
	TIME [epoch: 5.42 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6754573537134553		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.6754573537134553 | validation: 0.5809003218567427]
	TIME [epoch: 5.41 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6763784909338477		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.6763784909338477 | validation: 0.5773886844957329]
	TIME [epoch: 5.41 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1099.pth
	Model improved!!!
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6768241901920619		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.6768241901920619 | validation: 0.5904220455742702]
	TIME [epoch: 5.43 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6812456985796633		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.6812456985796633 | validation: 0.5859094023511511]
	TIME [epoch: 5.41 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6791596970442882		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.6791596970442882 | validation: 0.5759615162753212]
	TIME [epoch: 5.42 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1102.pth
	Model improved!!!
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6780101116521817		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.6780101116521817 | validation: 0.5794122902654824]
	TIME [epoch: 5.42 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6763760072068008		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.6763760072068008 | validation: 0.5813207572337705]
	TIME [epoch: 5.43 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6792262498234517		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.6792262498234517 | validation: 0.5809980443597599]
	TIME [epoch: 5.42 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6767447022847957		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.6767447022847957 | validation: 0.5791484975577842]
	TIME [epoch: 5.42 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6743621415622758		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.6743621415622758 | validation: 0.5888336534453175]
	TIME [epoch: 5.42 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6747800887661721		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.6747800887661721 | validation: 0.5774868719781623]
	TIME [epoch: 5.42 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6753972740941933		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.6753972740941933 | validation: 0.5743722134746546]
	TIME [epoch: 5.42 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1109.pth
	Model improved!!!
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6748709711549467		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.6748709711549467 | validation: 0.5780983182037085]
	TIME [epoch: 5.43 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6732990142593763		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.6732990142593763 | validation: 0.5841222458977539]
	TIME [epoch: 5.41 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6758833005653186		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.6758833005653186 | validation: 0.5753732314353632]
	TIME [epoch: 5.42 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6744967692780764		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.6744967692780764 | validation: 0.5727549179363]
	TIME [epoch: 5.42 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1113.pth
	Model improved!!!
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6752803683075268		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.6752803683075268 | validation: 0.5861817728475325]
	TIME [epoch: 5.41 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6756004901624059		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.6756004901624059 | validation: 0.57544752431919]
	TIME [epoch: 5.41 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6774446800353663		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.6774446800353663 | validation: 0.5848485724771609]
	TIME [epoch: 5.43 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6740706478023336		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.6740706478023336 | validation: 0.5720236089324576]
	TIME [epoch: 5.41 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1117.pth
	Model improved!!!
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6758397460663175		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.6758397460663175 | validation: 0.5858396533373038]
	TIME [epoch: 5.42 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6738530290363807		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.6738530290363807 | validation: 0.5731097343550005]
	TIME [epoch: 5.41 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6709248368518947		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.6709248368518947 | validation: 0.573212811367176]
	TIME [epoch: 5.41 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6717986445501368		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.6717986445501368 | validation: 0.5717886971935252]
	TIME [epoch: 5.42 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1121.pth
	Model improved!!!
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6743057690647646		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.6743057690647646 | validation: 0.5854590255760602]
	TIME [epoch: 5.41 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.672727645870652		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.672727645870652 | validation: 0.5751273032762229]
	TIME [epoch: 5.42 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6731536831441384		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.6731536831441384 | validation: 0.5767192753747631]
	TIME [epoch: 5.41 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6712864562281283		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.6712864562281283 | validation: 0.5731252341606322]
	TIME [epoch: 5.41 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6725673695565899		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.6725673695565899 | validation: 0.5757852826918214]
	TIME [epoch: 5.41 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6760963656648799		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.6760963656648799 | validation: 0.5678659327943533]
	TIME [epoch: 5.42 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1127.pth
	Model improved!!!
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6711322595268014		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.6711322595268014 | validation: 0.5834642448909363]
	TIME [epoch: 5.42 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6727698375548135		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.6727698375548135 | validation: 0.5782570354091724]
	TIME [epoch: 5.42 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6738877589101864		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.6738877589101864 | validation: 0.5784264243896966]
	TIME [epoch: 5.41 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6699492300280381		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.6699492300280381 | validation: 0.5762715203216675]
	TIME [epoch: 5.42 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6730452351480076		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.6730452351480076 | validation: 0.5769677474025343]
	TIME [epoch: 5.42 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6755323924665254		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.6755323924665254 | validation: 0.5715534712561205]
	TIME [epoch: 5.42 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6682982656869302		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.6682982656869302 | validation: 0.5782706914741517]
	TIME [epoch: 5.41 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6711435068258639		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.6711435068258639 | validation: 0.5758186062556263]
	TIME [epoch: 5.42 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6722896018134273		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.6722896018134273 | validation: 0.574443042903531]
	TIME [epoch: 5.41 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6738490923008524		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.6738490923008524 | validation: 0.5770016455123316]
	TIME [epoch: 5.42 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6747185686588553		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.6747185686588553 | validation: 0.5859657706196769]
	TIME [epoch: 5.41 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6736916109803852		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.6736916109803852 | validation: 0.5718573880018666]
	TIME [epoch: 5.42 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6745757842688193		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.6745757842688193 | validation: 0.57568045894339]
	TIME [epoch: 5.42 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6722521253311783		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.6722521253311783 | validation: 0.5702925861690575]
	TIME [epoch: 5.42 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6702665018708077		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.6702665018708077 | validation: 0.5797110195957954]
	TIME [epoch: 5.41 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6730617476322323		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.6730617476322323 | validation: 0.5689966342854557]
	TIME [epoch: 5.42 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.670342223094314		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.670342223094314 | validation: 0.5765830154974542]
	TIME [epoch: 5.41 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6691608068118698		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.6691608068118698 | validation: 0.5781155033172046]
	TIME [epoch: 5.41 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6681424898397569		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.6681424898397569 | validation: 0.5784973080642457]
	TIME [epoch: 5.41 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6718358596461802		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.6718358596461802 | validation: 0.5740889750910204]
	TIME [epoch: 5.42 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6716149400101725		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.6716149400101725 | validation: 0.5750525058231664]
	TIME [epoch: 5.41 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6676744994115336		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.6676744994115336 | validation: 0.57553153878531]
	TIME [epoch: 5.41 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6684112882961862		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.6684112882961862 | validation: 0.5713327387150875]
	TIME [epoch: 5.41 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6725389952905222		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.6725389952905222 | validation: 0.5777698489173153]
	TIME [epoch: 5.42 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6681287114546597		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.6681287114546597 | validation: 0.5820970701705898]
	TIME [epoch: 5.41 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6749022084788305		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.6749022084788305 | validation: 0.570073239611806]
	TIME [epoch: 5.42 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6681549310509232		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.6681549310509232 | validation: 0.5827339232854986]
	TIME [epoch: 5.41 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6709848521901421		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.6709848521901421 | validation: 0.5687935055855523]
	TIME [epoch: 5.41 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6716087265368106		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.6716087265368106 | validation: 0.5730666115383154]
	TIME [epoch: 5.41 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6703014851778554		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.6703014851778554 | validation: 0.5728368557434955]
	TIME [epoch: 5.42 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6676331896627002		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.6676331896627002 | validation: 0.5649884134761652]
	TIME [epoch: 5.42 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1158.pth
	Model improved!!!
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6671074902554317		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.6671074902554317 | validation: 0.5773111574746769]
	TIME [epoch: 5.41 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6683481845189075		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.6683481845189075 | validation: 0.5743708751922185]
	TIME [epoch: 5.42 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6676811897135168		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.6676811897135168 | validation: 0.5761203452762302]
	TIME [epoch: 5.42 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6672158586837567		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.6672158586837567 | validation: 0.5734705409663697]
	TIME [epoch: 5.41 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6695681132104021		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.6695681132104021 | validation: 0.5712496516301854]
	TIME [epoch: 5.42 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6704637659164291		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.6704637659164291 | validation: 0.5850732112085469]
	TIME [epoch: 5.42 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6696233585740653		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.6696233585740653 | validation: 0.5770509762313599]
	TIME [epoch: 5.42 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6677625344691721		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.6677625344691721 | validation: 0.5798708298835363]
	TIME [epoch: 5.42 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6692668430558709		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.6692668430558709 | validation: 0.5615800267417506]
	TIME [epoch: 5.42 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1167.pth
	Model improved!!!
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6684658088398117		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.6684658088398117 | validation: 0.5707390266358988]
	TIME [epoch: 5.46 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.667660306095417		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.667660306095417 | validation: 0.5716417098383764]
	TIME [epoch: 5.42 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6716514338733529		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.6716514338733529 | validation: 0.5703039143224488]
	TIME [epoch: 5.41 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6694958005686052		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.6694958005686052 | validation: 0.5673283665773463]
	TIME [epoch: 5.41 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6710034507286307		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.6710034507286307 | validation: 0.5751117497619417]
	TIME [epoch: 5.41 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6699094627734309		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.6699094627734309 | validation: 0.561816444033451]
	TIME [epoch: 5.41 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.667280478258603		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.667280478258603 | validation: 0.5643681909804756]
	TIME [epoch: 5.41 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6701320781021716		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.6701320781021716 | validation: 0.5817488109049566]
	TIME [epoch: 5.41 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6660863823095113		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.6660863823095113 | validation: 0.5667843134048168]
	TIME [epoch: 5.41 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6698149343038936		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.6698149343038936 | validation: 0.5682803281127301]
	TIME [epoch: 5.41 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6660388033531146		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.6660388033531146 | validation: 0.5619928118965245]
	TIME [epoch: 5.41 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6657251570758939		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.6657251570758939 | validation: 0.5744984866866469]
	TIME [epoch: 5.42 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6673046776545204		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.6673046776545204 | validation: 0.5699238926543254]
	TIME [epoch: 5.4 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6659221107969097		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.6659221107969097 | validation: 0.569192148602513]
	TIME [epoch: 5.42 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6663368343820986		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.6663368343820986 | validation: 0.5659557422619054]
	TIME [epoch: 5.41 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.667125552747491		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.667125552747491 | validation: 0.5711021463762185]
	TIME [epoch: 5.42 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6689011130268203		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.6689011130268203 | validation: 0.5608041044470911]
	TIME [epoch: 5.41 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1184.pth
	Model improved!!!
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6678020466388371		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.6678020466388371 | validation: 0.5648073280860991]
	TIME [epoch: 5.42 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.664763635484828		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.664763635484828 | validation: 0.5645225126878611]
	TIME [epoch: 5.42 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6666521694842765		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.6666521694842765 | validation: 0.5667282413152706]
	TIME [epoch: 5.41 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6654233643021721		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.6654233643021721 | validation: 0.562701682071649]
	TIME [epoch: 5.42 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6656730975224511		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.6656730975224511 | validation: 0.5718721638005718]
	TIME [epoch: 5.42 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6699467068603567		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.6699467068603567 | validation: 0.570639703387179]
	TIME [epoch: 5.42 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6654916431011304		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.6654916431011304 | validation: 0.5683491804916011]
	TIME [epoch: 5.41 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6650246801203581		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.6650246801203581 | validation: 0.5630778109586686]
	TIME [epoch: 5.41 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.668305804531008		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.668305804531008 | validation: 0.5712336210026424]
	TIME [epoch: 5.42 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6637512109895468		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.6637512109895468 | validation: 0.5640525546052472]
	TIME [epoch: 5.41 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6671811541173511		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.6671811541173511 | validation: 0.569506080005848]
	TIME [epoch: 5.42 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6681644796861653		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.6681644796861653 | validation: 0.5729569705461453]
	TIME [epoch: 5.41 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6672094243052695		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.6672094243052695 | validation: 0.5659262318315149]
	TIME [epoch: 5.41 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6669627988679182		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.6669627988679182 | validation: 0.5721259056205936]
	TIME [epoch: 5.41 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6635010907112229		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.6635010907112229 | validation: 0.5608456960417288]
	TIME [epoch: 5.41 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6642653265799626		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.6642653265799626 | validation: 0.5706395798932269]
	TIME [epoch: 5.43 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6643775956155422		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.6643775956155422 | validation: 0.5691729187196165]
	TIME [epoch: 5.41 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.664972208679173		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.664972208679173 | validation: 0.5590536005216293]
	TIME [epoch: 5.43 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1202.pth
	Model improved!!!
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6672916905942851		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.6672916905942851 | validation: 0.5732120238925463]
	TIME [epoch: 5.38 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6648488636786122		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.6648488636786122 | validation: 0.5711067798669268]
	TIME [epoch: 5.38 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6675896598316423		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.6675896598316423 | validation: 0.566675560218563]
	TIME [epoch: 5.38 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.663087875280486		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.663087875280486 | validation: 0.5566066179334864]
	TIME [epoch: 5.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1206.pth
	Model improved!!!
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6625224445794011		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.6625224445794011 | validation: 0.5530258722584807]
	TIME [epoch: 5.41 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1207.pth
	Model improved!!!
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6666100527835664		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.6666100527835664 | validation: 0.5658944548202384]
	TIME [epoch: 5.42 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.664546245052243		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.664546245052243 | validation: 0.5693348313121628]
	TIME [epoch: 5.42 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6616156929013955		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.6616156929013955 | validation: 0.5600151737042366]
	TIME [epoch: 5.41 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6622940375674929		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.6622940375674929 | validation: 0.5613278139729515]
	TIME [epoch: 5.41 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6589723378925444		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.6589723378925444 | validation: 0.5690015856719667]
	TIME [epoch: 5.41 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.663168252613809		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.663168252613809 | validation: 0.5701337154451029]
	TIME [epoch: 5.41 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6647681814042268		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.6647681814042268 | validation: 0.5695099110010124]
	TIME [epoch: 5.41 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6668341067664735		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.6668341067664735 | validation: 0.5511148556325586]
	TIME [epoch: 5.41 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1215.pth
	Model improved!!!
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6634722186879749		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.6634722186879749 | validation: 0.5633502402319895]
	TIME [epoch: 5.44 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6613240246038479		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.6613240246038479 | validation: 0.5622330337799679]
	TIME [epoch: 5.42 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6619930267393005		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.6619930267393005 | validation: 0.565047786884941]
	TIME [epoch: 5.41 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6632594128584981		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.6632594128584981 | validation: 0.5606659855094047]
	TIME [epoch: 5.41 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6648427360445283		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.6648427360445283 | validation: 0.5705899175263915]
	TIME [epoch: 5.43 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6650245553323993		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.6650245553323993 | validation: 0.562867268018701]
	TIME [epoch: 5.4 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6621723382352789		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.6621723382352789 | validation: 0.5624980114807252]
	TIME [epoch: 5.37 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6641082506151648		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.6641082506151648 | validation: 0.5559535316048366]
	TIME [epoch: 5.37 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6635174283778177		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.6635174283778177 | validation: 0.5639772534582685]
	TIME [epoch: 5.38 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6654081053142108		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.6654081053142108 | validation: 0.5680076803295788]
	TIME [epoch: 5.37 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6604896141267019		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.6604896141267019 | validation: 0.5533271216905081]
	TIME [epoch: 5.4 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.659216251252779		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.659216251252779 | validation: 0.5628031321346745]
	TIME [epoch: 5.4 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6662334267991729		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.6662334267991729 | validation: 0.565413462065559]
	TIME [epoch: 5.4 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.661871994738076		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.661871994738076 | validation: 0.5557904406912889]
	TIME [epoch: 5.4 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6672054171511604		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.6672054171511604 | validation: 0.5608633339555666]
	TIME [epoch: 5.4 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6648455708698185		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.6648455708698185 | validation: 0.5670509392226276]
	TIME [epoch: 5.4 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6613600899531038		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.6613600899531038 | validation: 0.5607056910080538]
	TIME [epoch: 5.41 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6612141264075213		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.6612141264075213 | validation: 0.5645750667644687]
	TIME [epoch: 5.4 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6612637521371414		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.6612637521371414 | validation: 0.559868201673385]
	TIME [epoch: 5.41 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6629249821644377		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.6629249821644377 | validation: 0.5703034899526694]
	TIME [epoch: 5.4 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6624528735761024		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.6624528735761024 | validation: 0.5577432972438418]
	TIME [epoch: 5.41 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6594117486845181		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.6594117486845181 | validation: 0.5713548325734656]
	TIME [epoch: 5.41 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6592819408219768		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.6592819408219768 | validation: 0.5657216870383763]
	TIME [epoch: 5.4 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6607673026324661		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.6607673026324661 | validation: 0.5634608894413278]
	TIME [epoch: 5.39 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6616490102934773		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.6616490102934773 | validation: 0.5622696588417048]
	TIME [epoch: 5.41 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6587761966285387		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.6587761966285387 | validation: 0.5604981639390444]
	TIME [epoch: 5.42 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6640547972055347		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.6640547972055347 | validation: 0.5606885099061863]
	TIME [epoch: 5.41 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6630202459115754		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.6630202459115754 | validation: 0.562320689676599]
	TIME [epoch: 5.4 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6631789475832622		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.6631789475832622 | validation: 0.5612892028712839]
	TIME [epoch: 5.4 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6577867066548287		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.6577867066548287 | validation: 0.5522355483396976]
	TIME [epoch: 5.4 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6613164589805911		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.6613164589805911 | validation: 0.5631526563595666]
	TIME [epoch: 5.4 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6603028434432873		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.6603028434432873 | validation: 0.5619907194476603]
	TIME [epoch: 5.4 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6601154049115908		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.6601154049115908 | validation: 0.5625290462677016]
	TIME [epoch: 5.4 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6601123401396214		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.6601123401396214 | validation: 0.5626562659034303]
	TIME [epoch: 5.39 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6594166489788645		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.6594166489788645 | validation: 0.5489999776485462]
	TIME [epoch: 5.4 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1250.pth
	Model improved!!!
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6588158296049531		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.6588158296049531 | validation: 0.5599006462054655]
	TIME [epoch: 5.4 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.656624743872468		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.656624743872468 | validation: 0.5521381245375533]
	TIME [epoch: 5.4 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6567210236787454		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.6567210236787454 | validation: 0.5521054325051187]
	TIME [epoch: 5.42 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6591773975691799		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.6591773975691799 | validation: 0.5618127017641886]
	TIME [epoch: 5.41 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6582324832070445		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.6582324832070445 | validation: 0.5609256270946189]
	TIME [epoch: 5.41 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6603609987371455		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.6603609987371455 | validation: 0.5542799737133388]
	TIME [epoch: 5.4 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.659223966245254		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.659223966245254 | validation: 0.5579396951449377]
	TIME [epoch: 5.4 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6609100715582449		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.6609100715582449 | validation: 0.5612399587891063]
	TIME [epoch: 5.4 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6591751041568981		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.6591751041568981 | validation: 0.5609224336786659]
	TIME [epoch: 5.4 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6587319855527791		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.6587319855527791 | validation: 0.5546581127024475]
	TIME [epoch: 5.4 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6601915808904057		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.6601915808904057 | validation: 0.5561402811457423]
	TIME [epoch: 5.4 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6568312856124702		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.6568312856124702 | validation: 0.5549355117470149]
	TIME [epoch: 5.4 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6578743567215355		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.6578743567215355 | validation: 0.5595521211759585]
	TIME [epoch: 5.4 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6603097282816448		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.6603097282816448 | validation: 0.5553304800124351]
	TIME [epoch: 5.4 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6574312307793188		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.6574312307793188 | validation: 0.5615495926284071]
	TIME [epoch: 5.4 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6554120860847646		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.6554120860847646 | validation: 0.5511686934321166]
	TIME [epoch: 5.4 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6592092002359626		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.6592092002359626 | validation: 0.5577446217412284]
	TIME [epoch: 5.44 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6586685493760005		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.6586685493760005 | validation: 0.5545758989859833]
	TIME [epoch: 5.4 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6592163273691614		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.6592163273691614 | validation: 0.5587786880435058]
	TIME [epoch: 5.42 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6515395517988641		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.6515395517988641 | validation: 0.5610653079757978]
	TIME [epoch: 5.4 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6579761308343806		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.6579761308343806 | validation: 0.5591953340951914]
	TIME [epoch: 5.41 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6584156120895165		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.6584156120895165 | validation: 0.5509013703865329]
	TIME [epoch: 5.4 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6568491091777605		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.6568491091777605 | validation: 0.5521908402602784]
	TIME [epoch: 5.43 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6583428639423795		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.6583428639423795 | validation: 0.5607272503181502]
	TIME [epoch: 5.39 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6576592153172699		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.6576592153172699 | validation: 0.5539622522244283]
	TIME [epoch: 5.43 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6576841564372642		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.6576841564372642 | validation: 0.5542014796928411]
	TIME [epoch: 5.39 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6535679247613024		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.6535679247613024 | validation: 0.556445279299747]
	TIME [epoch: 5.41 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6589413562668424		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.6589413562668424 | validation: 0.5585874341701527]
	TIME [epoch: 5.39 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6563314306175991		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.6563314306175991 | validation: 0.5623574309859543]
	TIME [epoch: 5.4 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6541630748473496		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.6541630748473496 | validation: 0.546087171527183]
	TIME [epoch: 5.41 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1280.pth
	Model improved!!!
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6569054421336512		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.6569054421336512 | validation: 0.5527726323975781]
	TIME [epoch: 5.4 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6563923889157911		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.6563923889157911 | validation: 0.5542388038901497]
	TIME [epoch: 5.39 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6568640661390889		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.6568640661390889 | validation: 0.5627790809095515]
	TIME [epoch: 5.39 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6534760913236872		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.6534760913236872 | validation: 0.554816964331013]
	TIME [epoch: 5.39 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6580763808089976		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.6580763808089976 | validation: 0.5578142117608457]
	TIME [epoch: 5.4 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6562006929667749		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.6562006929667749 | validation: 0.5582723964372233]
	TIME [epoch: 5.39 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6544937624910991		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.6544937624910991 | validation: 0.5554020459029435]
	TIME [epoch: 5.39 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6543634818377783		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.6543634818377783 | validation: 0.557534118335118]
	TIME [epoch: 5.4 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6553308117877004		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.6553308117877004 | validation: 0.5516691620515243]
	TIME [epoch: 5.39 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6569696588640835		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.6569696588640835 | validation: 0.5527735612354817]
	TIME [epoch: 5.39 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6536946115822607		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.6536946115822607 | validation: 0.5477633656033774]
	TIME [epoch: 5.39 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6563922464221645		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.6563922464221645 | validation: 0.5564576823742227]
	TIME [epoch: 5.4 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6548544044713487		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.6548544044713487 | validation: 0.5562070091084655]
	TIME [epoch: 5.39 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6572583738357943		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.6572583738357943 | validation: 0.5528440709235714]
	TIME [epoch: 5.39 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6557962636852704		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.6557962636852704 | validation: 0.5600685027238692]
	TIME [epoch: 5.4 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6581648852058826		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.6581648852058826 | validation: 0.5499969707024976]
	TIME [epoch: 5.39 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6560383371420102		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.6560383371420102 | validation: 0.5479237699202529]
	TIME [epoch: 5.4 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6529858300688133		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.6529858300688133 | validation: 0.5542975520524718]
	TIME [epoch: 5.39 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6559153083083488		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.6559153083083488 | validation: 0.5458342261898007]
	TIME [epoch: 5.4 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1299.pth
	Model improved!!!
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6537013886823791		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.6537013886823791 | validation: 0.549359662739817]
	TIME [epoch: 5.4 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6562683521648239		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.6562683521648239 | validation: 0.5555174634459678]
	TIME [epoch: 5.43 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6567457801813329		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.6567457801813329 | validation: 0.5538501767724477]
	TIME [epoch: 5.43 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6557997696805875		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.6557997696805875 | validation: 0.5518123444260455]
	TIME [epoch: 5.42 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6524257692686741		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.6524257692686741 | validation: 0.5518462650455332]
	TIME [epoch: 5.42 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6536368271168392		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.6536368271168392 | validation: 0.5452086167487815]
	TIME [epoch: 5.42 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1305.pth
	Model improved!!!
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.653349024495907		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.653349024495907 | validation: 0.5572544946071971]
	TIME [epoch: 5.42 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6521500976520792		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.6521500976520792 | validation: 0.5554329342782321]
	TIME [epoch: 5.4 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6538574601672197		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.6538574601672197 | validation: 0.5513438438818368]
	TIME [epoch: 5.4 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6504644984906404		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.6504644984906404 | validation: 0.5536446459615694]
	TIME [epoch: 5.4 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6543591081138351		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.6543591081138351 | validation: 0.5497070420920581]
	TIME [epoch: 5.4 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.654018877733668		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.654018877733668 | validation: 0.5573732230956244]
	TIME [epoch: 5.39 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6536380761488568		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.6536380761488568 | validation: 0.5412987193957592]
	TIME [epoch: 5.4 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1312.pth
	Model improved!!!
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6554985108142287		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.6554985108142287 | validation: 0.5557647215030518]
	TIME [epoch: 5.41 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.658122297057202		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.658122297057202 | validation: 0.5568375621954752]
	TIME [epoch: 5.4 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.650297411658666		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.650297411658666 | validation: 0.5588530754249399]
	TIME [epoch: 5.4 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6521654580019584		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.6521654580019584 | validation: 0.5425454568650085]
	TIME [epoch: 5.4 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6586902832419788		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.6586902832419788 | validation: 0.5490425221354173]
	TIME [epoch: 5.41 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6555760447670346		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.6555760447670346 | validation: 0.5461729691387639]
	TIME [epoch: 5.42 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6515570253664743		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.6515570253664743 | validation: 0.5555795911151482]
	TIME [epoch: 5.41 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6548662899621462		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.6548662899621462 | validation: 0.5519843020059597]
	TIME [epoch: 5.39 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.652467713936068		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.652467713936068 | validation: 0.5509205754326866]
	TIME [epoch: 5.4 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6524081803318122		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.6524081803318122 | validation: 0.547754226005765]
	TIME [epoch: 5.39 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.654299312496156		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.654299312496156 | validation: 0.5575299278116906]
	TIME [epoch: 5.41 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6515891320997315		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.6515891320997315 | validation: 0.5508304799112221]
	TIME [epoch: 5.39 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6522272609100622		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.6522272609100622 | validation: 0.5510714499842033]
	TIME [epoch: 5.39 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.655045535680321		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.655045535680321 | validation: 0.5509265138275403]
	TIME [epoch: 5.4 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6525989772087882		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.6525989772087882 | validation: 0.5514210605662647]
	TIME [epoch: 5.4 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6522070017213618		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.6522070017213618 | validation: 0.5414328496739642]
	TIME [epoch: 5.41 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6521461992015699		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.6521461992015699 | validation: 0.5503707551598522]
	TIME [epoch: 5.4 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6515457675938635		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.6515457675938635 | validation: 0.5505467534864972]
	TIME [epoch: 5.41 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6535780903133781		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.6535780903133781 | validation: 0.5527023936567258]
	TIME [epoch: 5.4 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6526138854354104		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.6526138854354104 | validation: 0.5467106737764594]
	TIME [epoch: 5.4 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6495292346876619		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.6495292346876619 | validation: 0.5572146742939053]
	TIME [epoch: 5.41 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6520419323971387		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.6520419323971387 | validation: 0.5394804103292049]
	TIME [epoch: 5.4 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1334.pth
	Model improved!!!
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6530057736989742		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.6530057736989742 | validation: 0.5525115306854785]
	TIME [epoch: 5.4 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6524645644120622		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.6524645644120622 | validation: 0.5485310112943446]
	TIME [epoch: 5.4 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6521349828979929		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.6521349828979929 | validation: 0.5546316717948353]
	TIME [epoch: 5.39 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6520539897065393		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.6520539897065393 | validation: 0.5463014503891664]
	TIME [epoch: 5.39 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6513154908744302		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.6513154908744302 | validation: 0.5565984357937839]
	TIME [epoch: 5.4 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6520004745520177		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.6520004745520177 | validation: 0.5520032232579587]
	TIME [epoch: 5.39 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6511837001649937		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.6511837001649937 | validation: 0.5444756440458084]
	TIME [epoch: 5.39 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6540145526771369		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.6540145526771369 | validation: 0.5500883977001424]
	TIME [epoch: 5.38 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6497887406128484		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.6497887406128484 | validation: 0.5534435505114579]
	TIME [epoch: 5.39 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6494280008867839		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.6494280008867839 | validation: 0.5486469114994176]
	TIME [epoch: 5.39 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6515479298789338		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.6515479298789338 | validation: 0.5499117551910152]
	TIME [epoch: 5.39 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6530186701191003		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.6530186701191003 | validation: 0.5365292290297277]
	TIME [epoch: 5.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1346.pth
	Model improved!!!
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6534168246952006		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.6534168246952006 | validation: 0.5540376031404456]
	TIME [epoch: 5.43 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6523995720622244		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.6523995720622244 | validation: 0.5509957626153187]
	TIME [epoch: 5.41 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6462892885640257		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.6462892885640257 | validation: 0.5451024971090085]
	TIME [epoch: 5.42 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.65315061496413		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.65315061496413 | validation: 0.5548812088568272]
	TIME [epoch: 5.41 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6499524043669967		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.6499524043669967 | validation: 0.5474000031218553]
	TIME [epoch: 5.41 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6471293328700963		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.6471293328700963 | validation: 0.5489596498741268]
	TIME [epoch: 5.41 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6520955551950275		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.6520955551950275 | validation: 0.5543602960092023]
	TIME [epoch: 5.41 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6505649136365554		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.6505649136365554 | validation: 0.5483738396835872]
	TIME [epoch: 5.41 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6520092225921268		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.6520092225921268 | validation: 0.5453816917269859]
	TIME [epoch: 5.42 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6488417246938256		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.6488417246938256 | validation: 0.5444872610378498]
	TIME [epoch: 5.41 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6529491377795517		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.6529491377795517 | validation: 0.548450377869142]
	TIME [epoch: 5.41 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6508163943300997		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.6508163943300997 | validation: 0.5473817388831075]
	TIME [epoch: 5.41 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6498366933077467		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.6498366933077467 | validation: 0.5557091926805624]
	TIME [epoch: 5.41 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6503892738223234		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.6503892738223234 | validation: 0.5435753180368235]
	TIME [epoch: 5.42 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6491171476409104		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.6491171476409104 | validation: 0.5452240731591806]
	TIME [epoch: 5.41 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6502828124005512		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.6502828124005512 | validation: 0.543669676925554]
	TIME [epoch: 5.41 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6528189570015839		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.6528189570015839 | validation: 0.5600168292387341]
	TIME [epoch: 5.41 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6521468226496119		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.6521468226496119 | validation: 0.5532771193761354]
	TIME [epoch: 5.41 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6516739662971395		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.6516739662971395 | validation: 0.5433694072490065]
	TIME [epoch: 5.41 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6484139480703818		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.6484139480703818 | validation: 0.5359614890828669]
	TIME [epoch: 5.41 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1366.pth
	Model improved!!!
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6499728641350049		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.6499728641350049 | validation: 0.5500206986832582]
	TIME [epoch: 5.42 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6485405498203687		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.6485405498203687 | validation: 0.5500589920684977]
	TIME [epoch: 5.41 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6524701508459115		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.6524701508459115 | validation: 0.5552453882410701]
	TIME [epoch: 5.41 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.650680697122115		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.650680697122115 | validation: 0.5396592967571768]
	TIME [epoch: 5.41 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6506798546815011		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.6506798546815011 | validation: 0.544807923423506]
	TIME [epoch: 5.42 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6549551441140976		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.6549551441140976 | validation: 0.5506094233174361]
	TIME [epoch: 5.41 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6512903695192033		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.6512903695192033 | validation: 0.5475890337514823]
	TIME [epoch: 5.41 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6499481306120097		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.6499481306120097 | validation: 0.5457815064067629]
	TIME [epoch: 5.4 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6508484482787443		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.6508484482787443 | validation: 0.545714231585185]
	TIME [epoch: 5.41 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6502387416585951		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.6502387416585951 | validation: 0.5469055402355026]
	TIME [epoch: 5.41 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.646781393853891		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.646781393853891 | validation: 0.5455729999177511]
	TIME [epoch: 5.4 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6463229372599102		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.6463229372599102 | validation: 0.5522389923602813]
	TIME [epoch: 5.41 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6490113323068641		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.6490113323068641 | validation: 0.5376585619631603]
	TIME [epoch: 5.4 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6488339964562729		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.6488339964562729 | validation: 0.5493410892003866]
	TIME [epoch: 5.41 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6485186364784571		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.6485186364784571 | validation: 0.5382025328065743]
	TIME [epoch: 5.41 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6498547765031819		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.6498547765031819 | validation: 0.5464519870621474]
	TIME [epoch: 5.41 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6510682320345802		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.6510682320345802 | validation: 0.5476307249504698]
	TIME [epoch: 5.4 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.647714318437277		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.647714318437277 | validation: 0.5403742309879238]
	TIME [epoch: 5.41 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.650027095054646		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.650027095054646 | validation: 0.551933289015528]
	TIME [epoch: 5.41 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6480312606135459		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.6480312606135459 | validation: 0.5456268266134243]
	TIME [epoch: 5.41 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6495756565567953		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.6495756565567953 | validation: 0.5453154903393391]
	TIME [epoch: 5.4 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6506579183929471		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.6506579183929471 | validation: 0.5506233843105145]
	TIME [epoch: 5.41 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.648399057815812		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.648399057815812 | validation: 0.5385510816306661]
	TIME [epoch: 5.4 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6465362382915458		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.6465362382915458 | validation: 0.5440464783455169]
	TIME [epoch: 5.41 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6513612384666682		[learning rate: 8.6788e-05]
	Learning Rate: 8.67884e-05
	LOSS [training: 0.6513612384666682 | validation: 0.5493424653174679]
	TIME [epoch: 5.42 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.648942495046297		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.648942495046297 | validation: 0.5356851166917528]
	TIME [epoch: 5.41 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1392.pth
	Model improved!!!
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6492156653904158		[learning rate: 8.6176e-05]
	Learning Rate: 8.61757e-05
	LOSS [training: 0.6492156653904158 | validation: 0.5368784349177809]
	TIME [epoch: 5.41 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6487982162462328		[learning rate: 8.5871e-05]
	Learning Rate: 8.5871e-05
	LOSS [training: 0.6487982162462328 | validation: 0.5406658164512591]
	TIME [epoch: 5.4 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6471026891847234		[learning rate: 8.5567e-05]
	Learning Rate: 8.55673e-05
	LOSS [training: 0.6471026891847234 | validation: 0.5558814102916717]
	TIME [epoch: 5.41 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6468739437338243		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.6468739437338243 | validation: 0.5494866800780507]
	TIME [epoch: 5.4 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6504922941904		[learning rate: 8.4963e-05]
	Learning Rate: 8.49632e-05
	LOSS [training: 0.6504922941904 | validation: 0.5409740988468623]
	TIME [epoch: 5.4 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6473648253815089		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.6473648253815089 | validation: 0.5444560993268492]
	TIME [epoch: 5.4 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6476318022574371		[learning rate: 8.4363e-05]
	Learning Rate: 8.43634e-05
	LOSS [training: 0.6476318022574371 | validation: 0.541103610736875]
	TIME [epoch: 5.4 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6464494949815628		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.6464494949815628 | validation: 0.544506117048872]
	TIME [epoch: 5.41 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6449147474574215		[learning rate: 8.3768e-05]
	Learning Rate: 8.37678e-05
	LOSS [training: 0.6449147474574215 | validation: 0.537725972821818]
	TIME [epoch: 5.41 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6517688714331753		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.6517688714331753 | validation: 0.5398725099508965]
	TIME [epoch: 5.4 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6449368662877081		[learning rate: 8.3176e-05]
	Learning Rate: 8.31764e-05
	LOSS [training: 0.6449368662877081 | validation: 0.5453778320147534]
	TIME [epoch: 5.42 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6490360127979169		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.6490360127979169 | validation: 0.5455837792212006]
	TIME [epoch: 5.4 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6492198634820168		[learning rate: 8.2589e-05]
	Learning Rate: 8.25892e-05
	LOSS [training: 0.6492198634820168 | validation: 0.545175552101104]
	TIME [epoch: 5.42 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6471731226211633		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.6471731226211633 | validation: 0.5377630477952672]
	TIME [epoch: 5.41 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.647813613013499		[learning rate: 8.2006e-05]
	Learning Rate: 8.20061e-05
	LOSS [training: 0.647813613013499 | validation: 0.5386340715600498]
	TIME [epoch: 5.41 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6473667327857947		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.6473667327857947 | validation: 0.5369172355629859]
	TIME [epoch: 5.4 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6465389307454473		[learning rate: 8.1427e-05]
	Learning Rate: 8.14272e-05
	LOSS [training: 0.6465389307454473 | validation: 0.5367478230489802]
	TIME [epoch: 5.4 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6476922625057436		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.6476922625057436 | validation: 0.5469623350511548]
	TIME [epoch: 5.41 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6473636011136809		[learning rate: 8.0852e-05]
	Learning Rate: 8.08523e-05
	LOSS [training: 0.6473636011136809 | validation: 0.5462288805827541]
	TIME [epoch: 5.4 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6446164533541654		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.6446164533541654 | validation: 0.539341059707135]
	TIME [epoch: 5.4 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6455779230543375		[learning rate: 8.0281e-05]
	Learning Rate: 8.02815e-05
	LOSS [training: 0.6455779230543375 | validation: 0.5483011373831036]
	TIME [epoch: 5.41 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6438293800302282		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.6438293800302282 | validation: 0.5383211879788492]
	TIME [epoch: 5.4 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6475806622622174		[learning rate: 7.9715e-05]
	Learning Rate: 7.97147e-05
	LOSS [training: 0.6475806622622174 | validation: 0.5420907459991663]
	TIME [epoch: 5.41 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6454669655656963		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.6454669655656963 | validation: 0.5454758563928401]
	TIME [epoch: 5.41 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6463533474976723		[learning rate: 7.9152e-05]
	Learning Rate: 7.9152e-05
	LOSS [training: 0.6463533474976723 | validation: 0.5374430650762795]
	TIME [epoch: 5.4 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6503180650350933		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.6503180650350933 | validation: 0.5455438370691632]
	TIME [epoch: 5.4 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6493872591510782		[learning rate: 7.8593e-05]
	Learning Rate: 7.85931e-05
	LOSS [training: 0.6493872591510782 | validation: 0.5479740777546679]
	TIME [epoch: 5.4 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.649468442893322		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.649468442893322 | validation: 0.5469385791532994]
	TIME [epoch: 5.4 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6454118741767091		[learning rate: 7.8038e-05]
	Learning Rate: 7.80383e-05
	LOSS [training: 0.6454118741767091 | validation: 0.5362721199066456]
	TIME [epoch: 5.41 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6454030032627819		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.6454030032627819 | validation: 0.5395954605371875]
	TIME [epoch: 5.41 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6466756730861328		[learning rate: 7.7487e-05]
	Learning Rate: 7.74873e-05
	LOSS [training: 0.6466756730861328 | validation: 0.5437834010799647]
	TIME [epoch: 5.4 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.644721389696075		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.644721389696075 | validation: 0.5396837945966269]
	TIME [epoch: 5.4 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6465292777784333		[learning rate: 7.694e-05]
	Learning Rate: 7.69403e-05
	LOSS [training: 0.6465292777784333 | validation: 0.54603566883321]
	TIME [epoch: 5.4 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.648960559569251		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.648960559569251 | validation: 0.5445816277506049]
	TIME [epoch: 5.4 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.641820468884936		[learning rate: 7.6397e-05]
	Learning Rate: 7.63971e-05
	LOSS [training: 0.641820468884936 | validation: 0.535506608768673]
	TIME [epoch: 5.41 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1427.pth
	Model improved!!!
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6411048108459528		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.6411048108459528 | validation: 0.5348161779589943]
	TIME [epoch: 5.4 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1428.pth
	Model improved!!!
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6462245705976427		[learning rate: 7.5858e-05]
	Learning Rate: 7.58578e-05
	LOSS [training: 0.6462245705976427 | validation: 0.5368788410546687]
	TIME [epoch: 5.41 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6434888455904995		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.6434888455904995 | validation: 0.5432714031351041]
	TIME [epoch: 5.4 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6451026937835671		[learning rate: 7.5322e-05]
	Learning Rate: 7.53222e-05
	LOSS [training: 0.6451026937835671 | validation: 0.5437635435928316]
	TIME [epoch: 5.4 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6439383455306366		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.6439383455306366 | validation: 0.5339777938939992]
	TIME [epoch: 5.41 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1432.pth
	Model improved!!!
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6454814755821403		[learning rate: 7.479e-05]
	Learning Rate: 7.47905e-05
	LOSS [training: 0.6454814755821403 | validation: 0.5295005633793956]
	TIME [epoch: 5.42 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1433.pth
	Model improved!!!
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6450918625315488		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.6450918625315488 | validation: 0.5396545385817911]
	TIME [epoch: 5.44 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6478920807457539		[learning rate: 7.4262e-05]
	Learning Rate: 7.42625e-05
	LOSS [training: 0.6478920807457539 | validation: 0.5413033145100885]
	TIME [epoch: 5.4 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6435996603140948		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.6435996603140948 | validation: 0.5386149566060408]
	TIME [epoch: 5.41 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6456211053243228		[learning rate: 7.3738e-05]
	Learning Rate: 7.37382e-05
	LOSS [training: 0.6456211053243228 | validation: 0.5426894028650917]
	TIME [epoch: 5.4 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6448616158916214		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.6448616158916214 | validation: 0.5449791758934136]
	TIME [epoch: 5.4 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6447321828884002		[learning rate: 7.3218e-05]
	Learning Rate: 7.32176e-05
	LOSS [training: 0.6447321828884002 | validation: 0.5405208615717358]
	TIME [epoch: 5.4 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6441942625502174		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.6441942625502174 | validation: 0.5372326072601821]
	TIME [epoch: 5.4 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6469802967127537		[learning rate: 7.2701e-05]
	Learning Rate: 7.27007e-05
	LOSS [training: 0.6469802967127537 | validation: 0.5403468877935425]
	TIME [epoch: 5.4 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6470334772890449		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.6470334772890449 | validation: 0.543471724226357]
	TIME [epoch: 5.41 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6482847573595288		[learning rate: 7.2187e-05]
	Learning Rate: 7.21874e-05
	LOSS [training: 0.6482847573595288 | validation: 0.5445168282364717]
	TIME [epoch: 5.4 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6421769538185409		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.6421769538185409 | validation: 0.5484917665834635]
	TIME [epoch: 5.4 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.644959441644392		[learning rate: 7.1678e-05]
	Learning Rate: 7.16778e-05
	LOSS [training: 0.644959441644392 | validation: 0.5390628180351894]
	TIME [epoch: 5.4 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.644563346906734		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.644563346906734 | validation: 0.5422740936741798]
	TIME [epoch: 5.4 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6449637291099104		[learning rate: 7.1172e-05]
	Learning Rate: 7.11718e-05
	LOSS [training: 0.6449637291099104 | validation: 0.5392485312813857]
	TIME [epoch: 5.4 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.64554789386566		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.64554789386566 | validation: 0.5405685594809427]
	TIME [epoch: 5.4 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6447541772629687		[learning rate: 7.0669e-05]
	Learning Rate: 7.06693e-05
	LOSS [training: 0.6447541772629687 | validation: 0.5413126920695978]
	TIME [epoch: 5.46 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.643133870619059		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.643133870619059 | validation: 0.5372245612761363]
	TIME [epoch: 5.4 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6459353807983496		[learning rate: 7.017e-05]
	Learning Rate: 7.01704e-05
	LOSS [training: 0.6459353807983496 | validation: 0.536735806917128]
	TIME [epoch: 5.4 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6469233214088957		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.6469233214088957 | validation: 0.5504534764118741]
	TIME [epoch: 5.4 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6463267090346819		[learning rate: 6.9675e-05]
	Learning Rate: 6.9675e-05
	LOSS [training: 0.6463267090346819 | validation: 0.5474140301696306]
	TIME [epoch: 5.4 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6443178913978109		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.6443178913978109 | validation: 0.538810680148616]
	TIME [epoch: 5.4 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6484855798460101		[learning rate: 6.9183e-05]
	Learning Rate: 6.91831e-05
	LOSS [training: 0.6484855798460101 | validation: 0.5284807510005953]
	TIME [epoch: 5.4 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1455.pth
	Model improved!!!
EPOCH 1456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6421127596729969		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.6421127596729969 | validation: 0.5428559157959885]
	TIME [epoch: 5.41 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6446418891653574		[learning rate: 6.8695e-05]
	Learning Rate: 6.86947e-05
	LOSS [training: 0.6446418891653574 | validation: 0.544993267295356]
	TIME [epoch: 5.42 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6438374403491878		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.6438374403491878 | validation: 0.5378659106023763]
	TIME [epoch: 5.46 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6441720647270114		[learning rate: 6.821e-05]
	Learning Rate: 6.82097e-05
	LOSS [training: 0.6441720647270114 | validation: 0.5389457985524275]
	TIME [epoch: 5.41 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6403350608944168		[learning rate: 6.7969e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.6403350608944168 | validation: 0.5387408456186394]
	TIME [epoch: 5.41 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6429629878130909		[learning rate: 6.7728e-05]
	Learning Rate: 6.77282e-05
	LOSS [training: 0.6429629878130909 | validation: 0.5415599663185988]
	TIME [epoch: 5.42 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6459234325474807		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.6459234325474807 | validation: 0.537322154026793]
	TIME [epoch: 5.4 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.644016363433279		[learning rate: 6.725e-05]
	Learning Rate: 6.725e-05
	LOSS [training: 0.644016363433279 | validation: 0.5510562056819847]
	TIME [epoch: 5.41 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6442884834316116		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.6442884834316116 | validation: 0.5438273825938985]
	TIME [epoch: 5.4 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6454621024242139		[learning rate: 6.6775e-05]
	Learning Rate: 6.67752e-05
	LOSS [training: 0.6454621024242139 | validation: 0.552612457362952]
	TIME [epoch: 5.41 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6405322920626406		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.6405322920626406 | validation: 0.541923519500877]
	TIME [epoch: 5.41 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6430482300127293		[learning rate: 6.6304e-05]
	Learning Rate: 6.63038e-05
	LOSS [training: 0.6430482300127293 | validation: 0.5393074579555007]
	TIME [epoch: 5.4 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6430422598122368		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.6430422598122368 | validation: 0.5486652976598461]
	TIME [epoch: 5.41 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.646656205629319		[learning rate: 6.5836e-05]
	Learning Rate: 6.58357e-05
	LOSS [training: 0.646656205629319 | validation: 0.5446437947516226]
	TIME [epoch: 5.41 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6471893318968944		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.6471893318968944 | validation: 0.5375720607007054]
	TIME [epoch: 5.4 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.641806764003991		[learning rate: 6.5371e-05]
	Learning Rate: 6.53709e-05
	LOSS [training: 0.641806764003991 | validation: 0.5268956843654194]
	TIME [epoch: 5.42 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1471.pth
	Model improved!!!
EPOCH 1472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6417031224377062		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.6417031224377062 | validation: 0.5317614955951381]
	TIME [epoch: 5.4 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6415348382716046		[learning rate: 6.4909e-05]
	Learning Rate: 6.49094e-05
	LOSS [training: 0.6415348382716046 | validation: 0.5334896548202682]
	TIME [epoch: 5.41 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6420706024810494		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.6420706024810494 | validation: 0.5396898872057599]
	TIME [epoch: 5.4 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6415365744597189		[learning rate: 6.4451e-05]
	Learning Rate: 6.44512e-05
	LOSS [training: 0.6415365744597189 | validation: 0.5281957142615895]
	TIME [epoch: 5.4 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6423611801235852		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.6423611801235852 | validation: 0.5374952467407845]
	TIME [epoch: 5.41 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6408565673362572		[learning rate: 6.3996e-05]
	Learning Rate: 6.39962e-05
	LOSS [training: 0.6408565673362572 | validation: 0.5247677233598944]
	TIME [epoch: 5.4 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1477.pth
	Model improved!!!
EPOCH 1478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6451277525623301		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.6451277525623301 | validation: 0.5297191612608889]
	TIME [epoch: 5.41 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6421474628192224		[learning rate: 6.3544e-05]
	Learning Rate: 6.35444e-05
	LOSS [training: 0.6421474628192224 | validation: 0.5410096420511916]
	TIME [epoch: 5.41 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.639660600244264		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.639660600244264 | validation: 0.5295961684034597]
	TIME [epoch: 5.4 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6420767198392479		[learning rate: 6.3096e-05]
	Learning Rate: 6.30958e-05
	LOSS [training: 0.6420767198392479 | validation: 0.5320063129851126]
	TIME [epoch: 5.41 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6421647757571636		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.6421647757571636 | validation: 0.5312916287544021]
	TIME [epoch: 5.4 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6432445749533272		[learning rate: 6.265e-05]
	Learning Rate: 6.26503e-05
	LOSS [training: 0.6432445749533272 | validation: 0.5408418224843272]
	TIME [epoch: 5.4 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6414081831607945		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.6414081831607945 | validation: 0.5386575418846217]
	TIME [epoch: 5.4 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6391568105842679		[learning rate: 6.2208e-05]
	Learning Rate: 6.2208e-05
	LOSS [training: 0.6391568105842679 | validation: 0.5221391747765555]
	TIME [epoch: 5.4 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1485.pth
	Model improved!!!
EPOCH 1486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6414003535392787		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.6414003535392787 | validation: 0.5391552617730363]
	TIME [epoch: 5.44 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6404019855131583		[learning rate: 6.1769e-05]
	Learning Rate: 6.17688e-05
	LOSS [training: 0.6404019855131583 | validation: 0.5331366440967558]
	TIME [epoch: 5.41 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6435715572470235		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.6435715572470235 | validation: 0.5340962995880972]
	TIME [epoch: 5.4 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.642505221755902		[learning rate: 6.1333e-05]
	Learning Rate: 6.13327e-05
	LOSS [training: 0.642505221755902 | validation: 0.5291192910086576]
	TIME [epoch: 5.41 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6421380906900178		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.6421380906900178 | validation: 0.54793907190191]
	TIME [epoch: 5.41 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.643366113501817		[learning rate: 6.09e-05]
	Learning Rate: 6.08998e-05
	LOSS [training: 0.643366113501817 | validation: 0.5377001670199919]
	TIME [epoch: 5.41 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6401581273816065		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.6401581273816065 | validation: 0.5431499565379313]
	TIME [epoch: 5.4 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6414616396291589		[learning rate: 6.047e-05]
	Learning Rate: 6.04698e-05
	LOSS [training: 0.6414616396291589 | validation: 0.5365423019068744]
	TIME [epoch: 5.4 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6436737976013172		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.6436737976013172 | validation: 0.5399885589828765]
	TIME [epoch: 5.39 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.640802952620902		[learning rate: 6.0043e-05]
	Learning Rate: 6.00429e-05
	LOSS [training: 0.640802952620902 | validation: 0.5384327065155464]
	TIME [epoch: 5.4 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6391012780334536		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.6391012780334536 | validation: 0.5323666995072075]
	TIME [epoch: 5.4 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6407215587382681		[learning rate: 5.9619e-05]
	Learning Rate: 5.9619e-05
	LOSS [training: 0.6407215587382681 | validation: 0.5359442904385431]
	TIME [epoch: 5.4 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.641665534592967		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.641665534592967 | validation: 0.5351583785459674]
	TIME [epoch: 5.41 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6407472520031476		[learning rate: 5.9198e-05]
	Learning Rate: 5.91981e-05
	LOSS [training: 0.6407472520031476 | validation: 0.532232063798107]
	TIME [epoch: 5.4 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6440665813717162		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.6440665813717162 | validation: 0.5480935426878965]
	TIME [epoch: 5.4 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6383778643563442		[learning rate: 5.878e-05]
	Learning Rate: 5.87802e-05
	LOSS [training: 0.6383778643563442 | validation: 0.5397729631981667]
	TIME [epoch: 5.44 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.640938074141986		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.640938074141986 | validation: 0.5259670827986993]
	TIME [epoch: 5.4 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6429949374410765		[learning rate: 5.8365e-05]
	Learning Rate: 5.83652e-05
	LOSS [training: 0.6429949374410765 | validation: 0.5393352131061413]
	TIME [epoch: 5.4 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6379845040575465		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.6379845040575465 | validation: 0.5317871621772242]
	TIME [epoch: 5.41 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6401436546748446		[learning rate: 5.7953e-05]
	Learning Rate: 5.79531e-05
	LOSS [training: 0.6401436546748446 | validation: 0.5356093972305119]
	TIME [epoch: 5.42 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6409929762051044		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.6409929762051044 | validation: 0.5391243460414188]
	TIME [epoch: 5.4 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6384533700966327		[learning rate: 5.7544e-05]
	Learning Rate: 5.7544e-05
	LOSS [training: 0.6384533700966327 | validation: 0.5304489108020299]
	TIME [epoch: 5.4 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6399910840731388		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.6399910840731388 | validation: 0.5333948028639967]
	TIME [epoch: 5.41 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6419291997768193		[learning rate: 5.7138e-05]
	Learning Rate: 5.71378e-05
	LOSS [training: 0.6419291997768193 | validation: 0.5327056308231155]
	TIME [epoch: 5.4 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6409133354115345		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.6409133354115345 | validation: 0.5363707281357896]
	TIME [epoch: 5.42 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6386007653682412		[learning rate: 5.6734e-05]
	Learning Rate: 5.67344e-05
	LOSS [training: 0.6386007653682412 | validation: 0.5247058438240421]
	TIME [epoch: 5.39 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6387977303640805		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.6387977303640805 | validation: 0.5284290557162807]
	TIME [epoch: 5.41 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6386157062393694		[learning rate: 5.6334e-05]
	Learning Rate: 5.63338e-05
	LOSS [training: 0.6386157062393694 | validation: 0.5310551747829765]
	TIME [epoch: 5.4 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6384188060495111		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.6384188060495111 | validation: 0.5330456026538551]
	TIME [epoch: 5.41 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6405672637291631		[learning rate: 5.5936e-05]
	Learning Rate: 5.59361e-05
	LOSS [training: 0.6405672637291631 | validation: 0.5351386550722521]
	TIME [epoch: 5.4 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.638990549693266		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.638990549693266 | validation: 0.536672121648433]
	TIME [epoch: 5.41 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6406698479048071		[learning rate: 5.5541e-05]
	Learning Rate: 5.55412e-05
	LOSS [training: 0.6406698479048071 | validation: 0.5378952932180902]
	TIME [epoch: 5.41 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6387586438238227		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.6387586438238227 | validation: 0.529078615190701]
	TIME [epoch: 5.41 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6413512989448146		[learning rate: 5.5149e-05]
	Learning Rate: 5.51491e-05
	LOSS [training: 0.6413512989448146 | validation: 0.5355380178385901]
	TIME [epoch: 5.41 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6410981383899342		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.6410981383899342 | validation: 0.5336726641282425]
	TIME [epoch: 5.41 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6396661290658431		[learning rate: 5.476e-05]
	Learning Rate: 5.47598e-05
	LOSS [training: 0.6396661290658431 | validation: 0.5335386073633319]
	TIME [epoch: 5.41 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6394125858066997		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.6394125858066997 | validation: 0.5413133741497725]
	TIME [epoch: 5.41 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6403200439319245		[learning rate: 5.4373e-05]
	Learning Rate: 5.43732e-05
	LOSS [training: 0.6403200439319245 | validation: 0.5321576244039464]
	TIME [epoch: 5.4 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6404903568022736		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.6404903568022736 | validation: 0.5295191476750221]
	TIME [epoch: 5.4 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6395065680940413		[learning rate: 5.3989e-05]
	Learning Rate: 5.39893e-05
	LOSS [training: 0.6395065680940413 | validation: 0.5325089246087551]
	TIME [epoch: 5.4 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6417934983251351		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.6417934983251351 | validation: 0.5239062553601579]
	TIME [epoch: 5.41 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6378857745111911		[learning rate: 5.3608e-05]
	Learning Rate: 5.36082e-05
	LOSS [training: 0.6378857745111911 | validation: 0.5339213192909438]
	TIME [epoch: 5.4 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6395867591908785		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.6395867591908785 | validation: 0.5404207026402117]
	TIME [epoch: 5.4 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6398620079234942		[learning rate: 5.323e-05]
	Learning Rate: 5.32297e-05
	LOSS [training: 0.6398620079234942 | validation: 0.5343707303159575]
	TIME [epoch: 5.4 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6376381354169901		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.6376381354169901 | validation: 0.5312569479390697]
	TIME [epoch: 5.4 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6404604305000728		[learning rate: 5.2854e-05]
	Learning Rate: 5.28539e-05
	LOSS [training: 0.6404604305000728 | validation: 0.5344461496008067]
	TIME [epoch: 5.42 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6373005225011096		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.6373005225011096 | validation: 0.5329097013654097]
	TIME [epoch: 5.41 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6349508059039526		[learning rate: 5.2481e-05]
	Learning Rate: 5.24807e-05
	LOSS [training: 0.6349508059039526 | validation: 0.540189039787621]
	TIME [epoch: 5.41 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6390795687933768		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.6390795687933768 | validation: 0.5282012120412881]
	TIME [epoch: 5.41 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6375730388173294		[learning rate: 5.211e-05]
	Learning Rate: 5.21103e-05
	LOSS [training: 0.6375730388173294 | validation: 0.5333163416374126]
	TIME [epoch: 5.41 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6403739252333484		[learning rate: 5.1926e-05]
	Learning Rate: 5.1926e-05
	LOSS [training: 0.6403739252333484 | validation: 0.5380870690921339]
	TIME [epoch: 5.42 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6373435650945228		[learning rate: 5.1742e-05]
	Learning Rate: 5.17424e-05
	LOSS [training: 0.6373435650945228 | validation: 0.5263737800825253]
	TIME [epoch: 5.41 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6416448515617819		[learning rate: 5.1559e-05]
	Learning Rate: 5.15594e-05
	LOSS [training: 0.6416448515617819 | validation: 0.5205724949370608]
	TIME [epoch: 5.4 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1538.pth
	Model improved!!!
EPOCH 1539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6366032838969627		[learning rate: 5.1377e-05]
	Learning Rate: 5.13771e-05
	LOSS [training: 0.6366032838969627 | validation: 0.5376796918904417]
	TIME [epoch: 5.41 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6360363330318077		[learning rate: 5.1195e-05]
	Learning Rate: 5.11954e-05
	LOSS [training: 0.6360363330318077 | validation: 0.5299242935650464]
	TIME [epoch: 5.41 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6395308061693848		[learning rate: 5.1014e-05]
	Learning Rate: 5.10144e-05
	LOSS [training: 0.6395308061693848 | validation: 0.5309255330458788]
	TIME [epoch: 5.41 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6378179307190529		[learning rate: 5.0834e-05]
	Learning Rate: 5.0834e-05
	LOSS [training: 0.6378179307190529 | validation: 0.5376300773469312]
	TIME [epoch: 5.41 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6395912049312128		[learning rate: 5.0654e-05]
	Learning Rate: 5.06542e-05
	LOSS [training: 0.6395912049312128 | validation: 0.5301964127381158]
	TIME [epoch: 5.41 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6383777751340408		[learning rate: 5.0475e-05]
	Learning Rate: 5.04751e-05
	LOSS [training: 0.6383777751340408 | validation: 0.5391313284021603]
	TIME [epoch: 5.41 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6406802168008442		[learning rate: 5.0297e-05]
	Learning Rate: 5.02966e-05
	LOSS [training: 0.6406802168008442 | validation: 0.5343144183156074]
	TIME [epoch: 5.4 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6410517651025979		[learning rate: 5.0119e-05]
	Learning Rate: 5.01187e-05
	LOSS [training: 0.6410517651025979 | validation: 0.5221402648242881]
	TIME [epoch: 5.41 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6404234579049092		[learning rate: 4.9942e-05]
	Learning Rate: 4.99415e-05
	LOSS [training: 0.6404234579049092 | validation: 0.5273428340260732]
	TIME [epoch: 5.41 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6374058638014722		[learning rate: 4.9765e-05]
	Learning Rate: 4.97649e-05
	LOSS [training: 0.6374058638014722 | validation: 0.5299527437504483]
	TIME [epoch: 5.41 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6389937147458367		[learning rate: 4.9589e-05]
	Learning Rate: 4.95889e-05
	LOSS [training: 0.6389937147458367 | validation: 0.5397348819165859]
	TIME [epoch: 5.4 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6385346932301106		[learning rate: 4.9414e-05]
	Learning Rate: 4.94136e-05
	LOSS [training: 0.6385346932301106 | validation: 0.5269193944458451]
	TIME [epoch: 5.45 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6377956381708095		[learning rate: 4.9239e-05]
	Learning Rate: 4.92388e-05
	LOSS [training: 0.6377956381708095 | validation: 0.5386571136917386]
	TIME [epoch: 5.45 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6405379592786247		[learning rate: 4.9065e-05]
	Learning Rate: 4.90647e-05
	LOSS [training: 0.6405379592786247 | validation: 0.5266511212941577]
	TIME [epoch: 5.41 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6382304379705191		[learning rate: 4.8891e-05]
	Learning Rate: 4.88912e-05
	LOSS [training: 0.6382304379705191 | validation: 0.5333041387987717]
	TIME [epoch: 5.41 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6398772049802841		[learning rate: 4.8718e-05]
	Learning Rate: 4.87183e-05
	LOSS [training: 0.6398772049802841 | validation: 0.5328771747623907]
	TIME [epoch: 5.41 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6365070251944268		[learning rate: 4.8546e-05]
	Learning Rate: 4.85461e-05
	LOSS [training: 0.6365070251944268 | validation: 0.5309876621796177]
	TIME [epoch: 5.41 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6353574091768427		[learning rate: 4.8374e-05]
	Learning Rate: 4.83744e-05
	LOSS [training: 0.6353574091768427 | validation: 0.5280599252740279]
	TIME [epoch: 5.41 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.634820050112356		[learning rate: 4.8203e-05]
	Learning Rate: 4.82033e-05
	LOSS [training: 0.634820050112356 | validation: 0.5239922095605051]
	TIME [epoch: 5.41 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6393762330768124		[learning rate: 4.8033e-05]
	Learning Rate: 4.80329e-05
	LOSS [training: 0.6393762330768124 | validation: 0.5280683234042371]
	TIME [epoch: 5.4 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.637378661826142		[learning rate: 4.7863e-05]
	Learning Rate: 4.7863e-05
	LOSS [training: 0.637378661826142 | validation: 0.5248738833017399]
	TIME [epoch: 5.41 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6354731512545305		[learning rate: 4.7694e-05]
	Learning Rate: 4.76938e-05
	LOSS [training: 0.6354731512545305 | validation: 0.5318293773571825]
	TIME [epoch: 5.4 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6371470767726739		[learning rate: 4.7525e-05]
	Learning Rate: 4.75251e-05
	LOSS [training: 0.6371470767726739 | validation: 0.5334944066871982]
	TIME [epoch: 5.41 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6365151701671369		[learning rate: 4.7357e-05]
	Learning Rate: 4.73571e-05
	LOSS [training: 0.6365151701671369 | validation: 0.5367878500563996]
	TIME [epoch: 5.41 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.639859569087354		[learning rate: 4.719e-05]
	Learning Rate: 4.71896e-05
	LOSS [training: 0.639859569087354 | validation: 0.5321536079429263]
	TIME [epoch: 5.41 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6371449066634637		[learning rate: 4.7023e-05]
	Learning Rate: 4.70227e-05
	LOSS [training: 0.6371449066634637 | validation: 0.5320324938798195]
	TIME [epoch: 5.4 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6420272838145868		[learning rate: 4.6856e-05]
	Learning Rate: 4.68564e-05
	LOSS [training: 0.6420272838145868 | validation: 0.5315088973226125]
	TIME [epoch: 5.41 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6348835134291088		[learning rate: 4.6691e-05]
	Learning Rate: 4.66907e-05
	LOSS [training: 0.6348835134291088 | validation: 0.5407624103290694]
	TIME [epoch: 5.41 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6377591575161317		[learning rate: 4.6526e-05]
	Learning Rate: 4.65257e-05
	LOSS [training: 0.6377591575161317 | validation: 0.5347975903003445]
	TIME [epoch: 5.41 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6366100385081371		[learning rate: 4.6361e-05]
	Learning Rate: 4.63611e-05
	LOSS [training: 0.6366100385081371 | validation: 0.5327222022855239]
	TIME [epoch: 5.4 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6391940808318095		[learning rate: 4.6197e-05]
	Learning Rate: 4.61972e-05
	LOSS [training: 0.6391940808318095 | validation: 0.519439365855782]
	TIME [epoch: 5.41 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1569.pth
	Model improved!!!
EPOCH 1570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6350350443254965		[learning rate: 4.6034e-05]
	Learning Rate: 4.60338e-05
	LOSS [training: 0.6350350443254965 | validation: 0.529678479540835]
	TIME [epoch: 5.41 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6401586614370544		[learning rate: 4.5871e-05]
	Learning Rate: 4.5871e-05
	LOSS [training: 0.6401586614370544 | validation: 0.5382019590369699]
	TIME [epoch: 5.41 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6382030717888724		[learning rate: 4.5709e-05]
	Learning Rate: 4.57088e-05
	LOSS [training: 0.6382030717888724 | validation: 0.5293212520765799]
	TIME [epoch: 5.4 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6346347111357917		[learning rate: 4.5547e-05]
	Learning Rate: 4.55472e-05
	LOSS [training: 0.6346347111357917 | validation: 0.5290578320317633]
	TIME [epoch: 5.4 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6388715396129421		[learning rate: 4.5386e-05]
	Learning Rate: 4.53861e-05
	LOSS [training: 0.6388715396129421 | validation: 0.534001534210135]
	TIME [epoch: 5.4 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6390876263059692		[learning rate: 4.5226e-05]
	Learning Rate: 4.52256e-05
	LOSS [training: 0.6390876263059692 | validation: 0.5285432163827523]
	TIME [epoch: 5.41 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6370199221181281		[learning rate: 4.5066e-05]
	Learning Rate: 4.50657e-05
	LOSS [training: 0.6370199221181281 | validation: 0.5273567226419024]
	TIME [epoch: 5.4 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6335139898600696		[learning rate: 4.4906e-05]
	Learning Rate: 4.49064e-05
	LOSS [training: 0.6335139898600696 | validation: 0.5309433420376248]
	TIME [epoch: 5.4 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6366235989090228		[learning rate: 4.4748e-05]
	Learning Rate: 4.47476e-05
	LOSS [training: 0.6366235989090228 | validation: 0.5326174767223378]
	TIME [epoch: 5.41 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6380332879194829		[learning rate: 4.4589e-05]
	Learning Rate: 4.45893e-05
	LOSS [training: 0.6380332879194829 | validation: 0.5339496451429818]
	TIME [epoch: 5.4 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6355121516905883		[learning rate: 4.4432e-05]
	Learning Rate: 4.44316e-05
	LOSS [training: 0.6355121516905883 | validation: 0.5391390941630592]
	TIME [epoch: 5.41 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6379959535224636		[learning rate: 4.4275e-05]
	Learning Rate: 4.42745e-05
	LOSS [training: 0.6379959535224636 | validation: 0.5283214939067313]
	TIME [epoch: 5.41 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6363771395272086		[learning rate: 4.4118e-05]
	Learning Rate: 4.4118e-05
	LOSS [training: 0.6363771395272086 | validation: 0.526450441016124]
	TIME [epoch: 5.4 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6376421155071068		[learning rate: 4.3962e-05]
	Learning Rate: 4.3962e-05
	LOSS [training: 0.6376421155071068 | validation: 0.536811907776595]
	TIME [epoch: 5.41 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6380147593325132		[learning rate: 4.3807e-05]
	Learning Rate: 4.38065e-05
	LOSS [training: 0.6380147593325132 | validation: 0.5307620001676886]
	TIME [epoch: 5.39 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.635833645177995		[learning rate: 4.3652e-05]
	Learning Rate: 4.36516e-05
	LOSS [training: 0.635833645177995 | validation: 0.5325588679781189]
	TIME [epoch: 5.41 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6340189821958463		[learning rate: 4.3497e-05]
	Learning Rate: 4.34972e-05
	LOSS [training: 0.6340189821958463 | validation: 0.5307549026040106]
	TIME [epoch: 5.41 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6367070960209414		[learning rate: 4.3343e-05]
	Learning Rate: 4.33434e-05
	LOSS [training: 0.6367070960209414 | validation: 0.535880694293244]
	TIME [epoch: 5.41 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6356132239674244		[learning rate: 4.319e-05]
	Learning Rate: 4.31902e-05
	LOSS [training: 0.6356132239674244 | validation: 0.5298493809646708]
	TIME [epoch: 5.41 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6364938436354978		[learning rate: 4.3037e-05]
	Learning Rate: 4.30374e-05
	LOSS [training: 0.6364938436354978 | validation: 0.5273372110620568]
	TIME [epoch: 5.4 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.637826069506685		[learning rate: 4.2885e-05]
	Learning Rate: 4.28852e-05
	LOSS [training: 0.637826069506685 | validation: 0.5291352910715169]
	TIME [epoch: 5.41 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6370371650167956		[learning rate: 4.2734e-05]
	Learning Rate: 4.27336e-05
	LOSS [training: 0.6370371650167956 | validation: 0.5227751181965836]
	TIME [epoch: 5.41 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6349263174418781		[learning rate: 4.2582e-05]
	Learning Rate: 4.25825e-05
	LOSS [training: 0.6349263174418781 | validation: 0.523655576781115]
	TIME [epoch: 5.41 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6391424963509212		[learning rate: 4.2432e-05]
	Learning Rate: 4.24319e-05
	LOSS [training: 0.6391424963509212 | validation: 0.5328358653146575]
	TIME [epoch: 5.4 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6355651787930218		[learning rate: 4.2282e-05]
	Learning Rate: 4.22819e-05
	LOSS [training: 0.6355651787930218 | validation: 0.5236684378674367]
	TIME [epoch: 5.41 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6325502869514262		[learning rate: 4.2132e-05]
	Learning Rate: 4.21323e-05
	LOSS [training: 0.6325502869514262 | validation: 0.5295297223529047]
	TIME [epoch: 5.41 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6373764333660377		[learning rate: 4.1983e-05]
	Learning Rate: 4.19833e-05
	LOSS [training: 0.6373764333660377 | validation: 0.5327398148599143]
	TIME [epoch: 5.4 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6378528507450179		[learning rate: 4.1835e-05]
	Learning Rate: 4.18349e-05
	LOSS [training: 0.6378528507450179 | validation: 0.5267538925301045]
	TIME [epoch: 5.41 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6348916137436508		[learning rate: 4.1687e-05]
	Learning Rate: 4.1687e-05
	LOSS [training: 0.6348916137436508 | validation: 0.5264815937395745]
	TIME [epoch: 5.39 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6394687486145972		[learning rate: 4.154e-05]
	Learning Rate: 4.15395e-05
	LOSS [training: 0.6394687486145972 | validation: 0.5303979844773014]
	TIME [epoch: 5.41 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6411760672970368		[learning rate: 4.1393e-05]
	Learning Rate: 4.13926e-05
	LOSS [training: 0.6411760672970368 | validation: 0.5275900580602338]
	TIME [epoch: 5.4 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.636076152005622		[learning rate: 4.1246e-05]
	Learning Rate: 4.12463e-05
	LOSS [training: 0.636076152005622 | validation: 0.5308573995595413]
	TIME [epoch: 5.41 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6350448904871352		[learning rate: 4.11e-05]
	Learning Rate: 4.11004e-05
	LOSS [training: 0.6350448904871352 | validation: 0.5411424485588018]
	TIME [epoch: 5.41 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.632354276003015		[learning rate: 4.0955e-05]
	Learning Rate: 4.09551e-05
	LOSS [training: 0.632354276003015 | validation: 0.5256445503947088]
	TIME [epoch: 5.41 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.633121820308616		[learning rate: 4.081e-05]
	Learning Rate: 4.08103e-05
	LOSS [training: 0.633121820308616 | validation: 0.5195106707169862]
	TIME [epoch: 5.44 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6406161684309604		[learning rate: 4.0666e-05]
	Learning Rate: 4.06659e-05
	LOSS [training: 0.6406161684309604 | validation: 0.5310997930569572]
	TIME [epoch: 5.4 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6366891788105342		[learning rate: 4.0522e-05]
	Learning Rate: 4.05221e-05
	LOSS [training: 0.6366891788105342 | validation: 0.5363197186590245]
	TIME [epoch: 5.4 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6334628599615777		[learning rate: 4.0379e-05]
	Learning Rate: 4.03788e-05
	LOSS [training: 0.6334628599615777 | validation: 0.5323550529402631]
	TIME [epoch: 5.4 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6378084113527787		[learning rate: 4.0236e-05]
	Learning Rate: 4.02361e-05
	LOSS [training: 0.6378084113527787 | validation: 0.529257924999887]
	TIME [epoch: 5.4 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6333798743297566		[learning rate: 4.0094e-05]
	Learning Rate: 4.00938e-05
	LOSS [training: 0.6333798743297566 | validation: 0.5267246008807899]
	TIME [epoch: 5.4 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6346683591323862		[learning rate: 3.9952e-05]
	Learning Rate: 3.9952e-05
	LOSS [training: 0.6346683591323862 | validation: 0.5215901967393488]
	TIME [epoch: 5.41 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6344937955542471		[learning rate: 3.9811e-05]
	Learning Rate: 3.98107e-05
	LOSS [training: 0.6344937955542471 | validation: 0.5314636418567902]
	TIME [epoch: 5.4 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6340465077339541		[learning rate: 3.967e-05]
	Learning Rate: 3.967e-05
	LOSS [training: 0.6340465077339541 | validation: 0.533926216799226]
	TIME [epoch: 5.41 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6343744502200915		[learning rate: 3.953e-05]
	Learning Rate: 3.95297e-05
	LOSS [training: 0.6343744502200915 | validation: 0.5278426036465288]
	TIME [epoch: 5.41 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6333328877549392		[learning rate: 3.939e-05]
	Learning Rate: 3.93899e-05
	LOSS [training: 0.6333328877549392 | validation: 0.5301014618994521]
	TIME [epoch: 5.4 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6358943323647782		[learning rate: 3.9251e-05]
	Learning Rate: 3.92506e-05
	LOSS [training: 0.6358943323647782 | validation: 0.5297780780589972]
	TIME [epoch: 5.4 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6300855733018353		[learning rate: 3.9112e-05]
	Learning Rate: 3.91118e-05
	LOSS [training: 0.6300855733018353 | validation: 0.529041930651671]
	TIME [epoch: 5.4 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6327664246158505		[learning rate: 3.8973e-05]
	Learning Rate: 3.89735e-05
	LOSS [training: 0.6327664246158505 | validation: 0.5320300681855067]
	TIME [epoch: 5.41 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6348206779917751		[learning rate: 3.8836e-05]
	Learning Rate: 3.88357e-05
	LOSS [training: 0.6348206779917751 | validation: 0.5212257648727056]
	TIME [epoch: 5.4 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6329523561525637		[learning rate: 3.8698e-05]
	Learning Rate: 3.86983e-05
	LOSS [training: 0.6329523561525637 | validation: 0.5361898294426312]
	TIME [epoch: 5.4 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6332356334995641		[learning rate: 3.8561e-05]
	Learning Rate: 3.85615e-05
	LOSS [training: 0.6332356334995641 | validation: 0.5290906287189797]
	TIME [epoch: 5.4 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6345841231728144		[learning rate: 3.8425e-05]
	Learning Rate: 3.84251e-05
	LOSS [training: 0.6345841231728144 | validation: 0.5358057524890273]
	TIME [epoch: 5.41 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6300227075102616		[learning rate: 3.8289e-05]
	Learning Rate: 3.82893e-05
	LOSS [training: 0.6300227075102616 | validation: 0.5297663873619288]
	TIME [epoch: 5.41 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6343664296729937		[learning rate: 3.8154e-05]
	Learning Rate: 3.81539e-05
	LOSS [training: 0.6343664296729937 | validation: 0.5291093207954713]
	TIME [epoch: 5.41 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6337557314733172		[learning rate: 3.8019e-05]
	Learning Rate: 3.8019e-05
	LOSS [training: 0.6337557314733172 | validation: 0.534751516525603]
	TIME [epoch: 5.4 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6344511861903481		[learning rate: 3.7885e-05]
	Learning Rate: 3.78845e-05
	LOSS [training: 0.6344511861903481 | validation: 0.5248224130420558]
	TIME [epoch: 5.41 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.634857434712407		[learning rate: 3.7751e-05]
	Learning Rate: 3.77505e-05
	LOSS [training: 0.634857434712407 | validation: 0.5248477774593182]
	TIME [epoch: 5.4 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6333778249771055		[learning rate: 3.7617e-05]
	Learning Rate: 3.7617e-05
	LOSS [training: 0.6333778249771055 | validation: 0.5305148413641618]
	TIME [epoch: 5.41 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6350667294612353		[learning rate: 3.7484e-05]
	Learning Rate: 3.7484e-05
	LOSS [training: 0.6350667294612353 | validation: 0.5317404738931163]
	TIME [epoch: 5.4 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6292725683461883		[learning rate: 3.7351e-05]
	Learning Rate: 3.73515e-05
	LOSS [training: 0.6292725683461883 | validation: 0.5228544513316273]
	TIME [epoch: 5.41 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.635917927817517		[learning rate: 3.7219e-05]
	Learning Rate: 3.72194e-05
	LOSS [training: 0.635917927817517 | validation: 0.5371878739316521]
	TIME [epoch: 5.4 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6343375779524542		[learning rate: 3.7088e-05]
	Learning Rate: 3.70878e-05
	LOSS [training: 0.6343375779524542 | validation: 0.5246838083462979]
	TIME [epoch: 5.41 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6351009820830315		[learning rate: 3.6957e-05]
	Learning Rate: 3.69566e-05
	LOSS [training: 0.6351009820830315 | validation: 0.5244942356232147]
	TIME [epoch: 5.4 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6340770513796546		[learning rate: 3.6826e-05]
	Learning Rate: 3.68259e-05
	LOSS [training: 0.6340770513796546 | validation: 0.5273812977115538]
	TIME [epoch: 5.41 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6359074182062583		[learning rate: 3.6696e-05]
	Learning Rate: 3.66957e-05
	LOSS [training: 0.6359074182062583 | validation: 0.5291918381408632]
	TIME [epoch: 5.4 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.635690775858201		[learning rate: 3.6566e-05]
	Learning Rate: 3.6566e-05
	LOSS [training: 0.635690775858201 | validation: 0.5291934960383083]
	TIME [epoch: 5.4 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6313008336942398		[learning rate: 3.6437e-05]
	Learning Rate: 3.64367e-05
	LOSS [training: 0.6313008336942398 | validation: 0.5283910615686812]
	TIME [epoch: 5.4 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6306150608771381		[learning rate: 3.6308e-05]
	Learning Rate: 3.63078e-05
	LOSS [training: 0.6306150608771381 | validation: 0.5204370269218881]
	TIME [epoch: 5.42 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6356033358546804		[learning rate: 3.6179e-05]
	Learning Rate: 3.61794e-05
	LOSS [training: 0.6356033358546804 | validation: 0.5239724387212092]
	TIME [epoch: 5.4 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6335942184934049		[learning rate: 3.6051e-05]
	Learning Rate: 3.60515e-05
	LOSS [training: 0.6335942184934049 | validation: 0.5247595147550239]
	TIME [epoch: 5.4 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6318650430985171		[learning rate: 3.5924e-05]
	Learning Rate: 3.5924e-05
	LOSS [training: 0.6318650430985171 | validation: 0.5391830668911551]
	TIME [epoch: 5.47 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6376104620956005		[learning rate: 3.5797e-05]
	Learning Rate: 3.5797e-05
	LOSS [training: 0.6376104620956005 | validation: 0.5291072186622128]
	TIME [epoch: 5.4 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6325010304920224		[learning rate: 3.567e-05]
	Learning Rate: 3.56704e-05
	LOSS [training: 0.6325010304920224 | validation: 0.5249267362765903]
	TIME [epoch: 5.4 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6299643618245196		[learning rate: 3.5544e-05]
	Learning Rate: 3.55442e-05
	LOSS [training: 0.6299643618245196 | validation: 0.5194129040680094]
	TIME [epoch: 5.4 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1643.pth
	Model improved!!!
EPOCH 1644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6360320115853301		[learning rate: 3.5419e-05]
	Learning Rate: 3.54186e-05
	LOSS [training: 0.6360320115853301 | validation: 0.5231911391862814]
	TIME [epoch: 5.42 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6328790621943025		[learning rate: 3.5293e-05]
	Learning Rate: 3.52933e-05
	LOSS [training: 0.6328790621943025 | validation: 0.5239361373092585]
	TIME [epoch: 5.4 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6337841597313743		[learning rate: 3.5169e-05]
	Learning Rate: 3.51685e-05
	LOSS [training: 0.6337841597313743 | validation: 0.5237049437829494]
	TIME [epoch: 5.4 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6310419750672448		[learning rate: 3.5044e-05]
	Learning Rate: 3.50441e-05
	LOSS [training: 0.6310419750672448 | validation: 0.5262066871167116]
	TIME [epoch: 5.41 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6288595627495698		[learning rate: 3.492e-05]
	Learning Rate: 3.49202e-05
	LOSS [training: 0.6288595627495698 | validation: 0.5275905870970858]
	TIME [epoch: 5.41 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6317802112565828		[learning rate: 3.4797e-05]
	Learning Rate: 3.47967e-05
	LOSS [training: 0.6317802112565828 | validation: 0.5236481553605151]
	TIME [epoch: 5.41 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6334235010674155		[learning rate: 3.4674e-05]
	Learning Rate: 3.46737e-05
	LOSS [training: 0.6334235010674155 | validation: 0.5279074262111987]
	TIME [epoch: 5.4 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6360705017418825		[learning rate: 3.4551e-05]
	Learning Rate: 3.45511e-05
	LOSS [training: 0.6360705017418825 | validation: 0.5313283064394518]
	TIME [epoch: 5.4 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6330331857835572		[learning rate: 3.4429e-05]
	Learning Rate: 3.44289e-05
	LOSS [training: 0.6330331857835572 | validation: 0.5308252957966043]
	TIME [epoch: 5.4 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6360227860888577		[learning rate: 3.4307e-05]
	Learning Rate: 3.43072e-05
	LOSS [training: 0.6360227860888577 | validation: 0.5263077453833318]
	TIME [epoch: 5.4 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6392868811748761		[learning rate: 3.4186e-05]
	Learning Rate: 3.41858e-05
	LOSS [training: 0.6392868811748761 | validation: 0.5237101344790535]
	TIME [epoch: 5.4 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6330072179820614		[learning rate: 3.4065e-05]
	Learning Rate: 3.4065e-05
	LOSS [training: 0.6330072179820614 | validation: 0.5191134980127284]
	TIME [epoch: 5.41 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1655.pth
	Model improved!!!
EPOCH 1656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6329228749242625		[learning rate: 3.3944e-05]
	Learning Rate: 3.39445e-05
	LOSS [training: 0.6329228749242625 | validation: 0.5228762560721117]
	TIME [epoch: 5.41 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6317653608739835		[learning rate: 3.3824e-05]
	Learning Rate: 3.38245e-05
	LOSS [training: 0.6317653608739835 | validation: 0.5292982422052724]
	TIME [epoch: 5.42 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6348740451999392		[learning rate: 3.3705e-05]
	Learning Rate: 3.37049e-05
	LOSS [training: 0.6348740451999392 | validation: 0.5248742812081426]
	TIME [epoch: 5.41 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6335877479385406		[learning rate: 3.3586e-05]
	Learning Rate: 3.35857e-05
	LOSS [training: 0.6335877479385406 | validation: 0.5282645430975791]
	TIME [epoch: 5.4 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6310355712453658		[learning rate: 3.3467e-05]
	Learning Rate: 3.34669e-05
	LOSS [training: 0.6310355712453658 | validation: 0.5309174149409023]
	TIME [epoch: 5.4 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6336661773180775		[learning rate: 3.3349e-05]
	Learning Rate: 3.33486e-05
	LOSS [training: 0.6336661773180775 | validation: 0.5254097314975772]
	TIME [epoch: 5.4 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6342877483440547		[learning rate: 3.3231e-05]
	Learning Rate: 3.32306e-05
	LOSS [training: 0.6342877483440547 | validation: 0.5270836691770153]
	TIME [epoch: 5.4 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6350862706951206		[learning rate: 3.3113e-05]
	Learning Rate: 3.31131e-05
	LOSS [training: 0.6350862706951206 | validation: 0.5221355887112621]
	TIME [epoch: 5.4 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6338128806056309		[learning rate: 3.2996e-05]
	Learning Rate: 3.2996e-05
	LOSS [training: 0.6338128806056309 | validation: 0.5268310674297817]
	TIME [epoch: 5.4 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6333130315520322		[learning rate: 3.2879e-05]
	Learning Rate: 3.28794e-05
	LOSS [training: 0.6333130315520322 | validation: 0.5303921194051741]
	TIME [epoch: 5.41 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6360487146622531		[learning rate: 3.2763e-05]
	Learning Rate: 3.27631e-05
	LOSS [training: 0.6360487146622531 | validation: 0.5344820546178604]
	TIME [epoch: 5.41 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6322801295076461		[learning rate: 3.2647e-05]
	Learning Rate: 3.26472e-05
	LOSS [training: 0.6322801295076461 | validation: 0.5290963497371656]
	TIME [epoch: 5.41 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6318251339339654		[learning rate: 3.2532e-05]
	Learning Rate: 3.25318e-05
	LOSS [training: 0.6318251339339654 | validation: 0.5181673995001143]
	TIME [epoch: 5.41 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1668.pth
	Model improved!!!
EPOCH 1669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6354366269088526		[learning rate: 3.2417e-05]
	Learning Rate: 3.24167e-05
	LOSS [training: 0.6354366269088526 | validation: 0.5320703932711794]
	TIME [epoch: 5.41 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6332293535811813		[learning rate: 3.2302e-05]
	Learning Rate: 3.23021e-05
	LOSS [training: 0.6332293535811813 | validation: 0.5296722626528084]
	TIME [epoch: 5.4 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6305782295303815		[learning rate: 3.2188e-05]
	Learning Rate: 3.21879e-05
	LOSS [training: 0.6305782295303815 | validation: 0.5149498983602072]
	TIME [epoch: 5.41 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1671.pth
	Model improved!!!
EPOCH 1672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6330890252116701		[learning rate: 3.2074e-05]
	Learning Rate: 3.20741e-05
	LOSS [training: 0.6330890252116701 | validation: 0.5241445867164327]
	TIME [epoch: 5.41 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.631134674387763		[learning rate: 3.1961e-05]
	Learning Rate: 3.19606e-05
	LOSS [training: 0.631134674387763 | validation: 0.5209122127749478]
	TIME [epoch: 5.41 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6326832777952158		[learning rate: 3.1848e-05]
	Learning Rate: 3.18476e-05
	LOSS [training: 0.6326832777952158 | validation: 0.5298892443522155]
	TIME [epoch: 5.42 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6330062805169474		[learning rate: 3.1735e-05]
	Learning Rate: 3.1735e-05
	LOSS [training: 0.6330062805169474 | validation: 0.5310275658012877]
	TIME [epoch: 5.41 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6332476826694164		[learning rate: 3.1623e-05]
	Learning Rate: 3.16228e-05
	LOSS [training: 0.6332476826694164 | validation: 0.5295088271285966]
	TIME [epoch: 5.4 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6331734141467223		[learning rate: 3.1511e-05]
	Learning Rate: 3.1511e-05
	LOSS [training: 0.6331734141467223 | validation: 0.5269425424261929]
	TIME [epoch: 5.41 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6321456697857435		[learning rate: 3.14e-05]
	Learning Rate: 3.13995e-05
	LOSS [training: 0.6321456697857435 | validation: 0.5343831274926109]
	TIME [epoch: 5.4 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6314459007192085		[learning rate: 3.1288e-05]
	Learning Rate: 3.12885e-05
	LOSS [training: 0.6314459007192085 | validation: 0.5297702937501495]
	TIME [epoch: 5.4 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.630580166885835		[learning rate: 3.1178e-05]
	Learning Rate: 3.11779e-05
	LOSS [training: 0.630580166885835 | validation: 0.5245872650677983]
	TIME [epoch: 5.4 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6296936476077872		[learning rate: 3.1068e-05]
	Learning Rate: 3.10676e-05
	LOSS [training: 0.6296936476077872 | validation: 0.5315433016361358]
	TIME [epoch: 5.4 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6324173030252537		[learning rate: 3.0958e-05]
	Learning Rate: 3.09577e-05
	LOSS [training: 0.6324173030252537 | validation: 0.5223186732321013]
	TIME [epoch: 5.41 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6329303539862298		[learning rate: 3.0848e-05]
	Learning Rate: 3.08483e-05
	LOSS [training: 0.6329303539862298 | validation: 0.5275973714272668]
	TIME [epoch: 5.4 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6295729847835608		[learning rate: 3.0739e-05]
	Learning Rate: 3.07392e-05
	LOSS [training: 0.6295729847835608 | validation: 0.5267521476311864]
	TIME [epoch: 5.4 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6350011349710907		[learning rate: 3.063e-05]
	Learning Rate: 3.06305e-05
	LOSS [training: 0.6350011349710907 | validation: 0.5256454413291144]
	TIME [epoch: 5.4 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6280058149818877		[learning rate: 3.0522e-05]
	Learning Rate: 3.05222e-05
	LOSS [training: 0.6280058149818877 | validation: 0.5359602029282388]
	TIME [epoch: 5.4 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6318056578973144		[learning rate: 3.0414e-05]
	Learning Rate: 3.04142e-05
	LOSS [training: 0.6318056578973144 | validation: 0.5254004177092056]
	TIME [epoch: 5.41 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6313178196597178		[learning rate: 3.0307e-05]
	Learning Rate: 3.03067e-05
	LOSS [training: 0.6313178196597178 | validation: 0.5287141232707501]
	TIME [epoch: 5.41 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6311334029026187		[learning rate: 3.02e-05]
	Learning Rate: 3.01995e-05
	LOSS [training: 0.6311334029026187 | validation: 0.5257309460386119]
	TIME [epoch: 5.4 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6324443422531418		[learning rate: 3.0093e-05]
	Learning Rate: 3.00927e-05
	LOSS [training: 0.6324443422531418 | validation: 0.5259614246894289]
	TIME [epoch: 5.41 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6329078041659381		[learning rate: 2.9986e-05]
	Learning Rate: 2.99863e-05
	LOSS [training: 0.6329078041659381 | validation: 0.5251302169347131]
	TIME [epoch: 5.4 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6310847044961662		[learning rate: 2.988e-05]
	Learning Rate: 2.98803e-05
	LOSS [training: 0.6310847044961662 | validation: 0.5215777584024972]
	TIME [epoch: 5.41 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6294837712053923		[learning rate: 2.9775e-05]
	Learning Rate: 2.97746e-05
	LOSS [training: 0.6294837712053923 | validation: 0.5267387918390137]
	TIME [epoch: 5.4 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6305380408595572		[learning rate: 2.9669e-05]
	Learning Rate: 2.96693e-05
	LOSS [training: 0.6305380408595572 | validation: 0.5267385802642178]
	TIME [epoch: 5.41 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6318074141467834		[learning rate: 2.9564e-05]
	Learning Rate: 2.95644e-05
	LOSS [training: 0.6318074141467834 | validation: 0.527128861569698]
	TIME [epoch: 5.4 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6320686753733227		[learning rate: 2.946e-05]
	Learning Rate: 2.94599e-05
	LOSS [training: 0.6320686753733227 | validation: 0.5225146401328257]
	TIME [epoch: 5.41 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6289506776976049		[learning rate: 2.9356e-05]
	Learning Rate: 2.93557e-05
	LOSS [training: 0.6289506776976049 | validation: 0.5254429918426806]
	TIME [epoch: 5.41 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6346567191617112		[learning rate: 2.9252e-05]
	Learning Rate: 2.92519e-05
	LOSS [training: 0.6346567191617112 | validation: 0.5197814022957207]
	TIME [epoch: 5.41 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6308128947782979		[learning rate: 2.9148e-05]
	Learning Rate: 2.91485e-05
	LOSS [training: 0.6308128947782979 | validation: 0.5222409316727336]
	TIME [epoch: 5.4 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6296084697096651		[learning rate: 2.9045e-05]
	Learning Rate: 2.90454e-05
	LOSS [training: 0.6296084697096651 | validation: 0.5235183054143007]
	TIME [epoch: 5.41 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6297964623693934		[learning rate: 2.8943e-05]
	Learning Rate: 2.89427e-05
	LOSS [training: 0.6297964623693934 | validation: 0.5206941678336892]
	TIME [epoch: 5.4 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6297067572377708		[learning rate: 2.884e-05]
	Learning Rate: 2.88403e-05
	LOSS [training: 0.6297067572377708 | validation: 0.5229463711733298]
	TIME [epoch: 5.42 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6308252003355035		[learning rate: 2.8738e-05]
	Learning Rate: 2.87383e-05
	LOSS [training: 0.6308252003355035 | validation: 0.5180873705182945]
	TIME [epoch: 5.4 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6291625124830117		[learning rate: 2.8637e-05]
	Learning Rate: 2.86367e-05
	LOSS [training: 0.6291625124830117 | validation: 0.522023158175655]
	TIME [epoch: 5.41 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6296575421549421		[learning rate: 2.8535e-05]
	Learning Rate: 2.85355e-05
	LOSS [training: 0.6296575421549421 | validation: 0.5254232231811419]
	TIME [epoch: 5.4 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6322140170821788		[learning rate: 2.8435e-05]
	Learning Rate: 2.84345e-05
	LOSS [training: 0.6322140170821788 | validation: 0.5188867581368367]
	TIME [epoch: 5.41 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.629456386499723		[learning rate: 2.8334e-05]
	Learning Rate: 2.8334e-05
	LOSS [training: 0.629456386499723 | validation: 0.5203552636410226]
	TIME [epoch: 5.41 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6320594300938089		[learning rate: 2.8234e-05]
	Learning Rate: 2.82338e-05
	LOSS [training: 0.6320594300938089 | validation: 0.5263602267024413]
	TIME [epoch: 5.41 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6291988917063135		[learning rate: 2.8134e-05]
	Learning Rate: 2.8134e-05
	LOSS [training: 0.6291988917063135 | validation: 0.513354270222857]
	TIME [epoch: 5.4 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1709.pth
	Model improved!!!
EPOCH 1710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6300029104992136		[learning rate: 2.8034e-05]
	Learning Rate: 2.80345e-05
	LOSS [training: 0.6300029104992136 | validation: 0.523398934748343]
	TIME [epoch: 5.41 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6330902519473861		[learning rate: 2.7935e-05]
	Learning Rate: 2.79353e-05
	LOSS [training: 0.6330902519473861 | validation: 0.5292447940843287]
	TIME [epoch: 5.4 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6316263437243801		[learning rate: 2.7837e-05]
	Learning Rate: 2.78366e-05
	LOSS [training: 0.6316263437243801 | validation: 0.5226676816514241]
	TIME [epoch: 5.41 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6306785863106723		[learning rate: 2.7738e-05]
	Learning Rate: 2.77381e-05
	LOSS [training: 0.6306785863106723 | validation: 0.5210523219576249]
	TIME [epoch: 5.41 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6318067864021601		[learning rate: 2.764e-05]
	Learning Rate: 2.764e-05
	LOSS [training: 0.6318067864021601 | validation: 0.5237413837859457]
	TIME [epoch: 5.41 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6323655306331487		[learning rate: 2.7542e-05]
	Learning Rate: 2.75423e-05
	LOSS [training: 0.6323655306331487 | validation: 0.5255056285753764]
	TIME [epoch: 5.41 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6312978651249006		[learning rate: 2.7445e-05]
	Learning Rate: 2.74449e-05
	LOSS [training: 0.6312978651249006 | validation: 0.525575599068501]
	TIME [epoch: 5.4 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6336690748621935		[learning rate: 2.7348e-05]
	Learning Rate: 2.73478e-05
	LOSS [training: 0.6336690748621935 | validation: 0.521043871094137]
	TIME [epoch: 5.41 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6282609892084117		[learning rate: 2.7251e-05]
	Learning Rate: 2.72511e-05
	LOSS [training: 0.6282609892084117 | validation: 0.5153902939192758]
	TIME [epoch: 5.4 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6311648321391705		[learning rate: 2.7155e-05]
	Learning Rate: 2.71548e-05
	LOSS [training: 0.6311648321391705 | validation: 0.5263937834659674]
	TIME [epoch: 5.4 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6319724537138032		[learning rate: 2.7059e-05]
	Learning Rate: 2.70587e-05
	LOSS [training: 0.6319724537138032 | validation: 0.5232922949681196]
	TIME [epoch: 5.41 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6321702207561143		[learning rate: 2.6963e-05]
	Learning Rate: 2.69631e-05
	LOSS [training: 0.6321702207561143 | validation: 0.5197499159945882]
	TIME [epoch: 5.4 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6302371749620282		[learning rate: 2.6868e-05]
	Learning Rate: 2.68677e-05
	LOSS [training: 0.6302371749620282 | validation: 0.5312947871762108]
	TIME [epoch: 5.42 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.630201838441013		[learning rate: 2.6773e-05]
	Learning Rate: 2.67727e-05
	LOSS [training: 0.630201838441013 | validation: 0.5137123378524239]
	TIME [epoch: 5.4 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.627807102185737		[learning rate: 2.6678e-05]
	Learning Rate: 2.6678e-05
	LOSS [training: 0.627807102185737 | validation: 0.5178168111700088]
	TIME [epoch: 5.41 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6290195444737159		[learning rate: 2.6584e-05]
	Learning Rate: 2.65837e-05
	LOSS [training: 0.6290195444737159 | validation: 0.5179033418567103]
	TIME [epoch: 5.4 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6287154144986054		[learning rate: 2.649e-05]
	Learning Rate: 2.64897e-05
	LOSS [training: 0.6287154144986054 | validation: 0.525288365762634]
	TIME [epoch: 5.4 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6309175722034535		[learning rate: 2.6396e-05]
	Learning Rate: 2.6396e-05
	LOSS [training: 0.6309175722034535 | validation: 0.5224411707092322]
	TIME [epoch: 5.4 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6298847751459857		[learning rate: 2.6303e-05]
	Learning Rate: 2.63027e-05
	LOSS [training: 0.6298847751459857 | validation: 0.5184945083921716]
	TIME [epoch: 5.4 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6333554557823703		[learning rate: 2.621e-05]
	Learning Rate: 2.62097e-05
	LOSS [training: 0.6333554557823703 | validation: 0.5216053435657856]
	TIME [epoch: 5.39 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6355365048573296		[learning rate: 2.6117e-05]
	Learning Rate: 2.6117e-05
	LOSS [training: 0.6355365048573296 | validation: 0.5357796454199697]
	TIME [epoch: 5.4 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6338645243530212		[learning rate: 2.6025e-05]
	Learning Rate: 2.60246e-05
	LOSS [training: 0.6338645243530212 | validation: 0.5175864621693567]
	TIME [epoch: 5.41 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6298084688641923		[learning rate: 2.5933e-05]
	Learning Rate: 2.59326e-05
	LOSS [training: 0.6298084688641923 | validation: 0.5307413603245723]
	TIME [epoch: 5.42 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6281551198308863		[learning rate: 2.5841e-05]
	Learning Rate: 2.58409e-05
	LOSS [training: 0.6281551198308863 | validation: 0.5324670290723571]
	TIME [epoch: 5.41 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.627489769982862		[learning rate: 2.575e-05]
	Learning Rate: 2.57495e-05
	LOSS [training: 0.627489769982862 | validation: 0.5266655276534644]
	TIME [epoch: 5.41 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6266615559357395		[learning rate: 2.5658e-05]
	Learning Rate: 2.56585e-05
	LOSS [training: 0.6266615559357395 | validation: 0.5200476773423757]
	TIME [epoch: 5.41 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6311718251081692		[learning rate: 2.5568e-05]
	Learning Rate: 2.55677e-05
	LOSS [training: 0.6311718251081692 | validation: 0.5259504565320964]
	TIME [epoch: 5.41 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6274945164188511		[learning rate: 2.5477e-05]
	Learning Rate: 2.54773e-05
	LOSS [training: 0.6274945164188511 | validation: 0.5166409254150953]
	TIME [epoch: 5.41 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6286407163586232		[learning rate: 2.5387e-05]
	Learning Rate: 2.53872e-05
	LOSS [training: 0.6286407163586232 | validation: 0.5185833064223481]
	TIME [epoch: 5.4 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6285035643075844		[learning rate: 2.5297e-05]
	Learning Rate: 2.52975e-05
	LOSS [training: 0.6285035643075844 | validation: 0.5163598536896189]
	TIME [epoch: 5.4 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6296751585138524		[learning rate: 2.5208e-05]
	Learning Rate: 2.5208e-05
	LOSS [training: 0.6296751585138524 | validation: 0.526346300487781]
	TIME [epoch: 5.42 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6309738738609053		[learning rate: 2.5119e-05]
	Learning Rate: 2.51189e-05
	LOSS [training: 0.6309738738609053 | validation: 0.525303913796086]
	TIME [epoch: 5.4 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6307142916615592		[learning rate: 2.503e-05]
	Learning Rate: 2.503e-05
	LOSS [training: 0.6307142916615592 | validation: 0.5173833231539291]
	TIME [epoch: 5.42 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6315414348973754		[learning rate: 2.4942e-05]
	Learning Rate: 2.49415e-05
	LOSS [training: 0.6315414348973754 | validation: 0.5196330604588967]
	TIME [epoch: 5.41 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6312152922017943		[learning rate: 2.4853e-05]
	Learning Rate: 2.48533e-05
	LOSS [training: 0.6312152922017943 | validation: 0.516505320123943]
	TIME [epoch: 5.4 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6292804733220096		[learning rate: 2.4765e-05]
	Learning Rate: 2.47655e-05
	LOSS [training: 0.6292804733220096 | validation: 0.5191810242601678]
	TIME [epoch: 5.41 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6290157448572096		[learning rate: 2.4678e-05]
	Learning Rate: 2.46779e-05
	LOSS [training: 0.6290157448572096 | validation: 0.5211457561804597]
	TIME [epoch: 5.4 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6313558757751178		[learning rate: 2.4591e-05]
	Learning Rate: 2.45906e-05
	LOSS [training: 0.6313558757751178 | validation: 0.5213628437283222]
	TIME [epoch: 5.4 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6318175677678343		[learning rate: 2.4504e-05]
	Learning Rate: 2.45037e-05
	LOSS [training: 0.6318175677678343 | validation: 0.5188691812003515]
	TIME [epoch: 5.4 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6311455684663537		[learning rate: 2.4417e-05]
	Learning Rate: 2.4417e-05
	LOSS [training: 0.6311455684663537 | validation: 0.5277513854940988]
	TIME [epoch: 5.4 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6286805731474187		[learning rate: 2.4331e-05]
	Learning Rate: 2.43307e-05
	LOSS [training: 0.6286805731474187 | validation: 0.5211097173878539]
	TIME [epoch: 5.41 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6294027693244264		[learning rate: 2.4245e-05]
	Learning Rate: 2.42446e-05
	LOSS [training: 0.6294027693244264 | validation: 0.5216619826134997]
	TIME [epoch: 5.41 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.625418372629371		[learning rate: 2.4159e-05]
	Learning Rate: 2.41589e-05
	LOSS [training: 0.625418372629371 | validation: 0.5229642220544747]
	TIME [epoch: 5.42 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6330721033385152		[learning rate: 2.4073e-05]
	Learning Rate: 2.40735e-05
	LOSS [training: 0.6330721033385152 | validation: 0.5219548218656279]
	TIME [epoch: 5.4 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6309210564328649		[learning rate: 2.3988e-05]
	Learning Rate: 2.39883e-05
	LOSS [training: 0.6309210564328649 | validation: 0.5254627190043218]
	TIME [epoch: 5.41 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6285002822020281		[learning rate: 2.3904e-05]
	Learning Rate: 2.39035e-05
	LOSS [training: 0.6285002822020281 | validation: 0.5289102595707157]
	TIME [epoch: 5.4 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6257456126302734		[learning rate: 2.3819e-05]
	Learning Rate: 2.3819e-05
	LOSS [training: 0.6257456126302734 | validation: 0.5261081606224577]
	TIME [epoch: 5.39 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6278590481933505		[learning rate: 2.3735e-05]
	Learning Rate: 2.37347e-05
	LOSS [training: 0.6278590481933505 | validation: 0.5241451399019941]
	TIME [epoch: 5.39 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.626800141928534		[learning rate: 2.3651e-05]
	Learning Rate: 2.36508e-05
	LOSS [training: 0.626800141928534 | validation: 0.5236147222339999]
	TIME [epoch: 5.41 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6293520456241064		[learning rate: 2.3567e-05]
	Learning Rate: 2.35672e-05
	LOSS [training: 0.6293520456241064 | validation: 0.5273999299080226]
	TIME [epoch: 5.41 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6272438366792771		[learning rate: 2.3484e-05]
	Learning Rate: 2.34838e-05
	LOSS [training: 0.6272438366792771 | validation: 0.5212440727398543]
	TIME [epoch: 5.4 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6306183981703967		[learning rate: 2.3401e-05]
	Learning Rate: 2.34008e-05
	LOSS [training: 0.6306183981703967 | validation: 0.5247398856008164]
	TIME [epoch: 5.41 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6282284968172163		[learning rate: 2.3318e-05]
	Learning Rate: 2.33181e-05
	LOSS [training: 0.6282284968172163 | validation: 0.5202600470426563]
	TIME [epoch: 5.39 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6293634163378704		[learning rate: 2.3236e-05]
	Learning Rate: 2.32356e-05
	LOSS [training: 0.6293634163378704 | validation: 0.526328309229597]
	TIME [epoch: 5.38 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6306530537318531		[learning rate: 2.3153e-05]
	Learning Rate: 2.31534e-05
	LOSS [training: 0.6306530537318531 | validation: 0.5289127970295551]
	TIME [epoch: 5.4 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6268605199232995		[learning rate: 2.3072e-05]
	Learning Rate: 2.30716e-05
	LOSS [training: 0.6268605199232995 | validation: 0.5205639616082861]
	TIME [epoch: 5.43 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6297370391747152		[learning rate: 2.299e-05]
	Learning Rate: 2.299e-05
	LOSS [training: 0.6297370391747152 | validation: 0.5170167288936426]
	TIME [epoch: 5.44 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6269822148946728		[learning rate: 2.2909e-05]
	Learning Rate: 2.29087e-05
	LOSS [training: 0.6269822148946728 | validation: 0.5238582298858584]
	TIME [epoch: 5.42 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6270881763808539		[learning rate: 2.2828e-05]
	Learning Rate: 2.28277e-05
	LOSS [training: 0.6270881763808539 | validation: 0.5259841024986666]
	TIME [epoch: 5.43 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6284547300423637		[learning rate: 2.2747e-05]
	Learning Rate: 2.27469e-05
	LOSS [training: 0.6284547300423637 | validation: 0.5204459727777769]
	TIME [epoch: 5.42 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6274307076379525		[learning rate: 2.2667e-05]
	Learning Rate: 2.26665e-05
	LOSS [training: 0.6274307076379525 | validation: 0.5266082597939256]
	TIME [epoch: 5.44 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6283862110599675		[learning rate: 2.2586e-05]
	Learning Rate: 2.25864e-05
	LOSS [training: 0.6283862110599675 | validation: 0.5261873496913353]
	TIME [epoch: 5.43 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.630668119079858		[learning rate: 2.2506e-05]
	Learning Rate: 2.25065e-05
	LOSS [training: 0.630668119079858 | validation: 0.5253782531739739]
	TIME [epoch: 5.43 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6286669403390566		[learning rate: 2.2427e-05]
	Learning Rate: 2.24269e-05
	LOSS [training: 0.6286669403390566 | validation: 0.5165246891747634]
	TIME [epoch: 5.42 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6283013242051811		[learning rate: 2.2348e-05]
	Learning Rate: 2.23476e-05
	LOSS [training: 0.6283013242051811 | validation: 0.5246031005594353]
	TIME [epoch: 5.43 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6312956219983596		[learning rate: 2.2269e-05]
	Learning Rate: 2.22686e-05
	LOSS [training: 0.6312956219983596 | validation: 0.5260365793233099]
	TIME [epoch: 5.41 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6290467008292867		[learning rate: 2.219e-05]
	Learning Rate: 2.21898e-05
	LOSS [training: 0.6290467008292867 | validation: 0.5154499607230363]
	TIME [epoch: 5.42 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6293163991188377		[learning rate: 2.2111e-05]
	Learning Rate: 2.21114e-05
	LOSS [training: 0.6293163991188377 | validation: 0.5169159452719673]
	TIME [epoch: 5.41 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.625625201984394		[learning rate: 2.2033e-05]
	Learning Rate: 2.20332e-05
	LOSS [training: 0.625625201984394 | validation: 0.5215485574807256]
	TIME [epoch: 5.42 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6278015322392296		[learning rate: 2.1955e-05]
	Learning Rate: 2.19553e-05
	LOSS [training: 0.6278015322392296 | validation: 0.5144829813298429]
	TIME [epoch: 5.42 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6278526429795623		[learning rate: 2.1878e-05]
	Learning Rate: 2.18776e-05
	LOSS [training: 0.6278526429795623 | validation: 0.5134186649888934]
	TIME [epoch: 5.42 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6290204861033547		[learning rate: 2.18e-05]
	Learning Rate: 2.18003e-05
	LOSS [training: 0.6290204861033547 | validation: 0.5124376742256356]
	TIME [epoch: 5.42 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1781.pth
	Model improved!!!
EPOCH 1782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6276993042840464		[learning rate: 2.1723e-05]
	Learning Rate: 2.17232e-05
	LOSS [training: 0.6276993042840464 | validation: 0.5237367040508868]
	TIME [epoch: 5.4 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6278531638563323		[learning rate: 2.1646e-05]
	Learning Rate: 2.16464e-05
	LOSS [training: 0.6278531638563323 | validation: 0.518329436111625]
	TIME [epoch: 5.37 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.625009442784023		[learning rate: 2.157e-05]
	Learning Rate: 2.15698e-05
	LOSS [training: 0.625009442784023 | validation: 0.5242900386362068]
	TIME [epoch: 5.38 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6286301957626543		[learning rate: 2.1494e-05]
	Learning Rate: 2.14935e-05
	LOSS [training: 0.6286301957626543 | validation: 0.5196664111119297]
	TIME [epoch: 5.37 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6276948619482229		[learning rate: 2.1418e-05]
	Learning Rate: 2.14175e-05
	LOSS [training: 0.6276948619482229 | validation: 0.5239603807109224]
	TIME [epoch: 5.37 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6289724340642507		[learning rate: 2.1342e-05]
	Learning Rate: 2.13418e-05
	LOSS [training: 0.6289724340642507 | validation: 0.5223086122839427]
	TIME [epoch: 5.37 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6285408136636156		[learning rate: 2.1266e-05]
	Learning Rate: 2.12663e-05
	LOSS [training: 0.6285408136636156 | validation: 0.5244760731736439]
	TIME [epoch: 5.4 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6235616180510547		[learning rate: 2.1191e-05]
	Learning Rate: 2.11911e-05
	LOSS [training: 0.6235616180510547 | validation: 0.5210106414450252]
	TIME [epoch: 5.39 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6293163429742054		[learning rate: 2.1116e-05]
	Learning Rate: 2.11162e-05
	LOSS [training: 0.6293163429742054 | validation: 0.521545673630439]
	TIME [epoch: 5.38 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6256004353896429		[learning rate: 2.1042e-05]
	Learning Rate: 2.10415e-05
	LOSS [training: 0.6256004353896429 | validation: 0.5213759023469839]
	TIME [epoch: 5.38 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.631454685220765		[learning rate: 2.0967e-05]
	Learning Rate: 2.09671e-05
	LOSS [training: 0.631454685220765 | validation: 0.5255235901005043]
	TIME [epoch: 5.38 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6273277089311291		[learning rate: 2.0893e-05]
	Learning Rate: 2.0893e-05
	LOSS [training: 0.6273277089311291 | validation: 0.5183479196670617]
	TIME [epoch: 5.38 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6287817275305834		[learning rate: 2.0819e-05]
	Learning Rate: 2.08191e-05
	LOSS [training: 0.6287817275305834 | validation: 0.5216233402477048]
	TIME [epoch: 5.38 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6245319411318795		[learning rate: 2.0745e-05]
	Learning Rate: 2.07455e-05
	LOSS [training: 0.6245319411318795 | validation: 0.515522201660814]
	TIME [epoch: 5.38 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6272468202313193		[learning rate: 2.0672e-05]
	Learning Rate: 2.06721e-05
	LOSS [training: 0.6272468202313193 | validation: 0.5211761332784415]
	TIME [epoch: 5.37 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6262743053257102		[learning rate: 2.0599e-05]
	Learning Rate: 2.0599e-05
	LOSS [training: 0.6262743053257102 | validation: 0.5198806726139484]
	TIME [epoch: 5.38 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6304429343098176		[learning rate: 2.0526e-05]
	Learning Rate: 2.05262e-05
	LOSS [training: 0.6304429343098176 | validation: 0.5190613660517982]
	TIME [epoch: 5.38 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6268772938091977		[learning rate: 2.0454e-05]
	Learning Rate: 2.04536e-05
	LOSS [training: 0.6268772938091977 | validation: 0.525628911492964]
	TIME [epoch: 5.38 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6264230473234708		[learning rate: 2.0381e-05]
	Learning Rate: 2.03812e-05
	LOSS [training: 0.6264230473234708 | validation: 0.524489082558573]
	TIME [epoch: 5.38 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.626614556113405		[learning rate: 2.0309e-05]
	Learning Rate: 2.03092e-05
	LOSS [training: 0.626614556113405 | validation: 0.5256742301763789]
	TIME [epoch: 5.42 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6249841653573948		[learning rate: 2.0237e-05]
	Learning Rate: 2.02374e-05
	LOSS [training: 0.6249841653573948 | validation: 0.5261148236136706]
	TIME [epoch: 5.38 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6272167751969786		[learning rate: 2.0166e-05]
	Learning Rate: 2.01658e-05
	LOSS [training: 0.6272167751969786 | validation: 0.521510590787419]
	TIME [epoch: 5.39 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6247081449450489		[learning rate: 2.0094e-05]
	Learning Rate: 2.00945e-05
	LOSS [training: 0.6247081449450489 | validation: 0.5183796082071408]
	TIME [epoch: 5.39 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6289138640903362		[learning rate: 2.0023e-05]
	Learning Rate: 2.00234e-05
	LOSS [training: 0.6289138640903362 | validation: 0.5197327164974253]
	TIME [epoch: 5.39 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6277629755541818		[learning rate: 1.9953e-05]
	Learning Rate: 1.99526e-05
	LOSS [training: 0.6277629755541818 | validation: 0.5223452244557016]
	TIME [epoch: 5.38 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6263179455397228		[learning rate: 1.9882e-05]
	Learning Rate: 1.98821e-05
	LOSS [training: 0.6263179455397228 | validation: 0.5205998590243232]
	TIME [epoch: 5.38 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6282198470862046		[learning rate: 1.9812e-05]
	Learning Rate: 1.98118e-05
	LOSS [training: 0.6282198470862046 | validation: 0.5214785056348458]
	TIME [epoch: 5.39 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6272597156604337		[learning rate: 1.9742e-05]
	Learning Rate: 1.97417e-05
	LOSS [training: 0.6272597156604337 | validation: 0.5171129661844969]
	TIME [epoch: 5.39 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.630362565002703		[learning rate: 1.9672e-05]
	Learning Rate: 1.96719e-05
	LOSS [training: 0.630362565002703 | validation: 0.5192591013254108]
	TIME [epoch: 5.39 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6236548931844984		[learning rate: 1.9602e-05]
	Learning Rate: 1.96023e-05
	LOSS [training: 0.6236548931844984 | validation: 0.5053662591671629]
	TIME [epoch: 5.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1811.pth
	Model improved!!!
EPOCH 1812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6263308239671269		[learning rate: 1.9533e-05]
	Learning Rate: 1.9533e-05
	LOSS [training: 0.6263308239671269 | validation: 0.5279156489940882]
	TIME [epoch: 5.38 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.627321306000417		[learning rate: 1.9464e-05]
	Learning Rate: 1.94639e-05
	LOSS [training: 0.627321306000417 | validation: 0.5263565259756046]
	TIME [epoch: 5.37 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6291258639200596		[learning rate: 1.9395e-05]
	Learning Rate: 1.93951e-05
	LOSS [training: 0.6291258639200596 | validation: 0.5161222509330876]
	TIME [epoch: 5.39 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6263499619051769		[learning rate: 1.9327e-05]
	Learning Rate: 1.93265e-05
	LOSS [training: 0.6263499619051769 | validation: 0.5172833899601196]
	TIME [epoch: 5.36 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6248894769488914		[learning rate: 1.9258e-05]
	Learning Rate: 1.92582e-05
	LOSS [training: 0.6248894769488914 | validation: 0.5156229845950471]
	TIME [epoch: 5.38 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6257082682771882		[learning rate: 1.919e-05]
	Learning Rate: 1.91901e-05
	LOSS [training: 0.6257082682771882 | validation: 0.5186188524519378]
	TIME [epoch: 5.36 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6264006240215617		[learning rate: 1.9122e-05]
	Learning Rate: 1.91222e-05
	LOSS [training: 0.6264006240215617 | validation: 0.520668502328125]
	TIME [epoch: 5.37 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6272261712579189		[learning rate: 1.9055e-05]
	Learning Rate: 1.90546e-05
	LOSS [training: 0.6272261712579189 | validation: 0.5195233498399727]
	TIME [epoch: 5.37 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6278193133708502		[learning rate: 1.8987e-05]
	Learning Rate: 1.89872e-05
	LOSS [training: 0.6278193133708502 | validation: 0.5241855998486281]
	TIME [epoch: 5.38 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6273634223017124		[learning rate: 1.892e-05]
	Learning Rate: 1.89201e-05
	LOSS [training: 0.6273634223017124 | validation: 0.5216085930949468]
	TIME [epoch: 5.37 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6273996875963951		[learning rate: 1.8853e-05]
	Learning Rate: 1.88532e-05
	LOSS [training: 0.6273996875963951 | validation: 0.5178227818879683]
	TIME [epoch: 5.38 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6280608224113423		[learning rate: 1.8787e-05]
	Learning Rate: 1.87865e-05
	LOSS [training: 0.6280608224113423 | validation: 0.5170664571253897]
	TIME [epoch: 5.36 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6271779409661395		[learning rate: 1.872e-05]
	Learning Rate: 1.87201e-05
	LOSS [training: 0.6271779409661395 | validation: 0.5242653926170611]
	TIME [epoch: 5.38 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6292255399794054		[learning rate: 1.8654e-05]
	Learning Rate: 1.86539e-05
	LOSS [training: 0.6292255399794054 | validation: 0.5154078738689288]
	TIME [epoch: 5.37 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6288199245674897		[learning rate: 1.8588e-05]
	Learning Rate: 1.85879e-05
	LOSS [training: 0.6288199245674897 | validation: 0.5206595706403452]
	TIME [epoch: 5.38 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6304319814587599		[learning rate: 1.8522e-05]
	Learning Rate: 1.85222e-05
	LOSS [training: 0.6304319814587599 | validation: 0.5198591889879836]
	TIME [epoch: 5.37 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.627054648037407		[learning rate: 1.8457e-05]
	Learning Rate: 1.84567e-05
	LOSS [training: 0.627054648037407 | validation: 0.5173237713740695]
	TIME [epoch: 5.38 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.625259102505834		[learning rate: 1.8391e-05]
	Learning Rate: 1.83914e-05
	LOSS [training: 0.625259102505834 | validation: 0.5225792584520356]
	TIME [epoch: 5.37 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6273363960247903		[learning rate: 1.8326e-05]
	Learning Rate: 1.83264e-05
	LOSS [training: 0.6273363960247903 | validation: 0.521054880623594]
	TIME [epoch: 5.38 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6274194636968933		[learning rate: 1.8262e-05]
	Learning Rate: 1.82616e-05
	LOSS [training: 0.6274194636968933 | validation: 0.5253846212475229]
	TIME [epoch: 5.37 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6278557399040067		[learning rate: 1.8197e-05]
	Learning Rate: 1.8197e-05
	LOSS [training: 0.6278557399040067 | validation: 0.5246883672821213]
	TIME [epoch: 5.38 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6237191344446545		[learning rate: 1.8133e-05]
	Learning Rate: 1.81327e-05
	LOSS [training: 0.6237191344446545 | validation: 0.5129069235910084]
	TIME [epoch: 5.37 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6272889523468221		[learning rate: 1.8069e-05]
	Learning Rate: 1.80685e-05
	LOSS [training: 0.6272889523468221 | validation: 0.5090056853127239]
	TIME [epoch: 5.39 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6275739038006971		[learning rate: 1.8005e-05]
	Learning Rate: 1.80047e-05
	LOSS [training: 0.6275739038006971 | validation: 0.5152766052518304]
	TIME [epoch: 5.41 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6251169877961817		[learning rate: 1.7941e-05]
	Learning Rate: 1.7941e-05
	LOSS [training: 0.6251169877961817 | validation: 0.5150338454533144]
	TIME [epoch: 5.39 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6253902098351108		[learning rate: 1.7878e-05]
	Learning Rate: 1.78775e-05
	LOSS [training: 0.6253902098351108 | validation: 0.5249283279670757]
	TIME [epoch: 5.38 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6294541662672313		[learning rate: 1.7814e-05]
	Learning Rate: 1.78143e-05
	LOSS [training: 0.6294541662672313 | validation: 0.512259616895915]
	TIME [epoch: 5.37 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6268224487004432		[learning rate: 1.7751e-05]
	Learning Rate: 1.77513e-05
	LOSS [training: 0.6268224487004432 | validation: 0.5197773730937401]
	TIME [epoch: 5.37 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6287841400536369		[learning rate: 1.7689e-05]
	Learning Rate: 1.76886e-05
	LOSS [training: 0.6287841400536369 | validation: 0.5162033536664462]
	TIME [epoch: 5.39 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6260665881981019		[learning rate: 1.7626e-05]
	Learning Rate: 1.7626e-05
	LOSS [training: 0.6260665881981019 | validation: 0.5205917344459527]
	TIME [epoch: 5.37 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6271036666105507		[learning rate: 1.7564e-05]
	Learning Rate: 1.75637e-05
	LOSS [training: 0.6271036666105507 | validation: 0.513197158520897]
	TIME [epoch: 5.38 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6242159917362884		[learning rate: 1.7502e-05]
	Learning Rate: 1.75016e-05
	LOSS [training: 0.6242159917362884 | validation: 0.5197517607416633]
	TIME [epoch: 5.38 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6264764450022282		[learning rate: 1.744e-05]
	Learning Rate: 1.74397e-05
	LOSS [training: 0.6264764450022282 | validation: 0.5211728364573921]
	TIME [epoch: 5.38 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6255923252410734		[learning rate: 1.7378e-05]
	Learning Rate: 1.7378e-05
	LOSS [training: 0.6255923252410734 | validation: 0.5206683834281387]
	TIME [epoch: 5.37 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6266851657967901		[learning rate: 1.7317e-05]
	Learning Rate: 1.73166e-05
	LOSS [training: 0.6266851657967901 | validation: 0.5195835287016534]
	TIME [epoch: 5.38 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6279423173823031		[learning rate: 1.7255e-05]
	Learning Rate: 1.72553e-05
	LOSS [training: 0.6279423173823031 | validation: 0.5155292693422323]
	TIME [epoch: 5.36 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6276706353676859		[learning rate: 1.7194e-05]
	Learning Rate: 1.71943e-05
	LOSS [training: 0.6276706353676859 | validation: 0.5258135804273621]
	TIME [epoch: 5.38 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6239172900025338		[learning rate: 1.7134e-05]
	Learning Rate: 1.71335e-05
	LOSS [training: 0.6239172900025338 | validation: 0.5189961472436713]
	TIME [epoch: 5.36 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6239297527609646		[learning rate: 1.7073e-05]
	Learning Rate: 1.70729e-05
	LOSS [training: 0.6239297527609646 | validation: 0.5167445528292279]
	TIME [epoch: 5.37 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6242924033284769		[learning rate: 1.7013e-05]
	Learning Rate: 1.70125e-05
	LOSS [training: 0.6242924033284769 | validation: 0.519137789813242]
	TIME [epoch: 5.36 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6255330475938254		[learning rate: 1.6952e-05]
	Learning Rate: 1.69524e-05
	LOSS [training: 0.6255330475938254 | validation: 0.5153366855469994]
	TIME [epoch: 5.38 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6233053931930526		[learning rate: 1.6892e-05]
	Learning Rate: 1.68924e-05
	LOSS [training: 0.6233053931930526 | validation: 0.5182688371295724]
	TIME [epoch: 5.36 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.627984403508314		[learning rate: 1.6833e-05]
	Learning Rate: 1.68327e-05
	LOSS [training: 0.627984403508314 | validation: 0.5200953989736177]
	TIME [epoch: 5.37 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6260260867085936		[learning rate: 1.6773e-05]
	Learning Rate: 1.67732e-05
	LOSS [training: 0.6260260867085936 | validation: 0.5234915952010523]
	TIME [epoch: 5.37 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6238254799955455		[learning rate: 1.6714e-05]
	Learning Rate: 1.67139e-05
	LOSS [training: 0.6238254799955455 | validation: 0.5126484291829639]
	TIME [epoch: 5.36 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6264180331944652		[learning rate: 1.6655e-05]
	Learning Rate: 1.66548e-05
	LOSS [training: 0.6264180331944652 | validation: 0.5226875204294175]
	TIME [epoch: 5.36 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6258238869686248		[learning rate: 1.6596e-05]
	Learning Rate: 1.65959e-05
	LOSS [training: 0.6258238869686248 | validation: 0.5131222174950976]
	TIME [epoch: 5.37 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6276085048474996		[learning rate: 1.6537e-05]
	Learning Rate: 1.65372e-05
	LOSS [training: 0.6276085048474996 | validation: 0.5212772957302904]
	TIME [epoch: 5.37 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6248832249934122		[learning rate: 1.6479e-05]
	Learning Rate: 1.64787e-05
	LOSS [training: 0.6248832249934122 | validation: 0.5212482709633547]
	TIME [epoch: 5.38 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6278367248691266		[learning rate: 1.642e-05]
	Learning Rate: 1.64204e-05
	LOSS [training: 0.6278367248691266 | validation: 0.5190690010201598]
	TIME [epoch: 5.36 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.626499238772321		[learning rate: 1.6362e-05]
	Learning Rate: 1.63624e-05
	LOSS [training: 0.626499238772321 | validation: 0.5159102399856789]
	TIME [epoch: 5.37 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6264629740001291		[learning rate: 1.6305e-05]
	Learning Rate: 1.63045e-05
	LOSS [training: 0.6264629740001291 | validation: 0.5211350154332104]
	TIME [epoch: 5.37 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6231107002283759		[learning rate: 1.6247e-05]
	Learning Rate: 1.62469e-05
	LOSS [training: 0.6231107002283759 | validation: 0.5243537259426913]
	TIME [epoch: 5.36 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6245006285565158		[learning rate: 1.6189e-05]
	Learning Rate: 1.61894e-05
	LOSS [training: 0.6245006285565158 | validation: 0.5192299520998498]
	TIME [epoch: 5.37 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6258322641979305		[learning rate: 1.6132e-05]
	Learning Rate: 1.61322e-05
	LOSS [training: 0.6258322641979305 | validation: 0.5287328653107117]
	TIME [epoch: 5.37 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6235474821332835		[learning rate: 1.6075e-05]
	Learning Rate: 1.60751e-05
	LOSS [training: 0.6235474821332835 | validation: 0.523434052603761]
	TIME [epoch: 5.37 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6264540872923259		[learning rate: 1.6018e-05]
	Learning Rate: 1.60183e-05
	LOSS [training: 0.6264540872923259 | validation: 0.5104642492085253]
	TIME [epoch: 5.36 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6277462287270953		[learning rate: 1.5962e-05]
	Learning Rate: 1.59616e-05
	LOSS [training: 0.6277462287270953 | validation: 0.5127562710605588]
	TIME [epoch: 5.36 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6256766567828073		[learning rate: 1.5905e-05]
	Learning Rate: 1.59052e-05
	LOSS [training: 0.6256766567828073 | validation: 0.5141892895971306]
	TIME [epoch: 5.36 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6261336603202399		[learning rate: 1.5849e-05]
	Learning Rate: 1.58489e-05
	LOSS [training: 0.6261336603202399 | validation: 0.519471529418994]
	TIME [epoch: 5.37 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.62627667981833		[learning rate: 1.5793e-05]
	Learning Rate: 1.57929e-05
	LOSS [training: 0.62627667981833 | validation: 0.5238945256295573]
	TIME [epoch: 5.36 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6225192858689017		[learning rate: 1.5737e-05]
	Learning Rate: 1.5737e-05
	LOSS [training: 0.6225192858689017 | validation: 0.5220565244997244]
	TIME [epoch: 5.36 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6232526478457134		[learning rate: 1.5681e-05]
	Learning Rate: 1.56814e-05
	LOSS [training: 0.6232526478457134 | validation: 0.5230666022406102]
	TIME [epoch: 5.36 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6251764714026958		[learning rate: 1.5626e-05]
	Learning Rate: 1.56259e-05
	LOSS [training: 0.6251764714026958 | validation: 0.5173050332111423]
	TIME [epoch: 5.36 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6273590692631141		[learning rate: 1.5571e-05]
	Learning Rate: 1.55707e-05
	LOSS [training: 0.6273590692631141 | validation: 0.5196965702685489]
	TIME [epoch: 5.37 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6266781010919966		[learning rate: 1.5516e-05]
	Learning Rate: 1.55156e-05
	LOSS [training: 0.6266781010919966 | validation: 0.5258761427988745]
	TIME [epoch: 5.37 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6293623433452789		[learning rate: 1.5461e-05]
	Learning Rate: 1.54608e-05
	LOSS [training: 0.6293623433452789 | validation: 0.5181368138457881]
	TIME [epoch: 5.37 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6243631767663491		[learning rate: 1.5406e-05]
	Learning Rate: 1.54061e-05
	LOSS [training: 0.6243631767663491 | validation: 0.517770045039787]
	TIME [epoch: 5.38 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6270867578379973		[learning rate: 1.5352e-05]
	Learning Rate: 1.53516e-05
	LOSS [training: 0.6270867578379973 | validation: 0.5232780524035493]
	TIME [epoch: 5.36 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6291832286893991		[learning rate: 1.5297e-05]
	Learning Rate: 1.52973e-05
	LOSS [training: 0.6291832286893991 | validation: 0.5169015931741033]
	TIME [epoch: 5.37 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6274930660706957		[learning rate: 1.5243e-05]
	Learning Rate: 1.52432e-05
	LOSS [training: 0.6274930660706957 | validation: 0.5292785095717049]
	TIME [epoch: 5.37 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6242029588610395		[learning rate: 1.5189e-05]
	Learning Rate: 1.51893e-05
	LOSS [training: 0.6242029588610395 | validation: 0.5129242343332753]
	TIME [epoch: 5.37 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6270585100157485		[learning rate: 1.5136e-05]
	Learning Rate: 1.51356e-05
	LOSS [training: 0.6270585100157485 | validation: 0.5209234407501873]
	TIME [epoch: 5.37 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6235086384906833		[learning rate: 1.5082e-05]
	Learning Rate: 1.50821e-05
	LOSS [training: 0.6235086384906833 | validation: 0.5256456681396108]
	TIME [epoch: 5.36 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6280873488155325		[learning rate: 1.5029e-05]
	Learning Rate: 1.50288e-05
	LOSS [training: 0.6280873488155325 | validation: 0.5228776580098508]
	TIME [epoch: 5.37 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6285409884113341		[learning rate: 1.4976e-05]
	Learning Rate: 1.49756e-05
	LOSS [training: 0.6285409884113341 | validation: 0.516614444135417]
	TIME [epoch: 5.38 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6265578664968822		[learning rate: 1.4923e-05]
	Learning Rate: 1.49227e-05
	LOSS [training: 0.6265578664968822 | validation: 0.5155581283583363]
	TIME [epoch: 5.37 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6272145048991105		[learning rate: 1.487e-05]
	Learning Rate: 1.48699e-05
	LOSS [training: 0.6272145048991105 | validation: 0.5280842181817527]
	TIME [epoch: 5.37 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6259161757536325		[learning rate: 1.4817e-05]
	Learning Rate: 1.48173e-05
	LOSS [training: 0.6259161757536325 | validation: 0.5287332138704376]
	TIME [epoch: 5.37 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6259561222160601		[learning rate: 1.4765e-05]
	Learning Rate: 1.47649e-05
	LOSS [training: 0.6259561222160601 | validation: 0.5198501413412517]
	TIME [epoch: 5.36 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6255847444792872		[learning rate: 1.4713e-05]
	Learning Rate: 1.47127e-05
	LOSS [training: 0.6255847444792872 | validation: 0.521172967238528]
	TIME [epoch: 5.38 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6229249219124807		[learning rate: 1.4661e-05]
	Learning Rate: 1.46607e-05
	LOSS [training: 0.6229249219124807 | validation: 0.5185323463999308]
	TIME [epoch: 5.36 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6239869990326729		[learning rate: 1.4609e-05]
	Learning Rate: 1.46088e-05
	LOSS [training: 0.6239869990326729 | validation: 0.5297060871467381]
	TIME [epoch: 5.36 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6273655090309872		[learning rate: 1.4557e-05]
	Learning Rate: 1.45572e-05
	LOSS [training: 0.6273655090309872 | validation: 0.5159179226207928]
	TIME [epoch: 5.37 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6268588024858984		[learning rate: 1.4506e-05]
	Learning Rate: 1.45057e-05
	LOSS [training: 0.6268588024858984 | validation: 0.5241873311626395]
	TIME [epoch: 5.37 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6248353951348758		[learning rate: 1.4454e-05]
	Learning Rate: 1.44544e-05
	LOSS [training: 0.6248353951348758 | validation: 0.5052808872137672]
	TIME [epoch: 5.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1897.pth
	Model improved!!!
EPOCH 1898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6221847165873081		[learning rate: 1.4403e-05]
	Learning Rate: 1.44033e-05
	LOSS [training: 0.6221847165873081 | validation: 0.5204335549524334]
	TIME [epoch: 5.37 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6261375874672644		[learning rate: 1.4352e-05]
	Learning Rate: 1.43524e-05
	LOSS [training: 0.6261375874672644 | validation: 0.5041760876662982]
	TIME [epoch: 5.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_1899.pth
	Model improved!!!
EPOCH 1900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6264884696252987		[learning rate: 1.4302e-05]
	Learning Rate: 1.43016e-05
	LOSS [training: 0.6264884696252987 | validation: 0.5185832076098754]
	TIME [epoch: 5.37 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6265437619719663		[learning rate: 1.4251e-05]
	Learning Rate: 1.4251e-05
	LOSS [training: 0.6265437619719663 | validation: 0.5250269225832963]
	TIME [epoch: 5.38 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.625028057019201		[learning rate: 1.4201e-05]
	Learning Rate: 1.42006e-05
	LOSS [training: 0.625028057019201 | validation: 0.5164471857567072]
	TIME [epoch: 5.38 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6249742024315501		[learning rate: 1.415e-05]
	Learning Rate: 1.41504e-05
	LOSS [training: 0.6249742024315501 | validation: 0.5146473363992131]
	TIME [epoch: 5.38 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6306343484702984		[learning rate: 1.41e-05]
	Learning Rate: 1.41004e-05
	LOSS [training: 0.6306343484702984 | validation: 0.520024138050501]
	TIME [epoch: 5.4 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6274458664766032		[learning rate: 1.4051e-05]
	Learning Rate: 1.40505e-05
	LOSS [training: 0.6274458664766032 | validation: 0.5160608327925068]
	TIME [epoch: 5.4 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6224574230523094		[learning rate: 1.4001e-05]
	Learning Rate: 1.40008e-05
	LOSS [training: 0.6224574230523094 | validation: 0.5181192721166599]
	TIME [epoch: 5.4 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6239971807284052		[learning rate: 1.3951e-05]
	Learning Rate: 1.39513e-05
	LOSS [training: 0.6239971807284052 | validation: 0.5204832192315644]
	TIME [epoch: 5.39 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6264940913418727		[learning rate: 1.3902e-05]
	Learning Rate: 1.3902e-05
	LOSS [training: 0.6264940913418727 | validation: 0.5166791474900801]
	TIME [epoch: 5.39 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6239317835304568		[learning rate: 1.3853e-05]
	Learning Rate: 1.38528e-05
	LOSS [training: 0.6239317835304568 | validation: 0.5221978401866717]
	TIME [epoch: 5.38 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6249183068256586		[learning rate: 1.3804e-05]
	Learning Rate: 1.38038e-05
	LOSS [training: 0.6249183068256586 | validation: 0.5153200238353971]
	TIME [epoch: 5.38 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6232913553931614		[learning rate: 1.3755e-05]
	Learning Rate: 1.3755e-05
	LOSS [training: 0.6232913553931614 | validation: 0.5140269439648094]
	TIME [epoch: 5.39 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.625893274681239		[learning rate: 1.3706e-05]
	Learning Rate: 1.37064e-05
	LOSS [training: 0.625893274681239 | validation: 0.5151737982794992]
	TIME [epoch: 5.39 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6251987224648781		[learning rate: 1.3658e-05]
	Learning Rate: 1.36579e-05
	LOSS [training: 0.6251987224648781 | validation: 0.5172430656531212]
	TIME [epoch: 5.39 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6246134668780033		[learning rate: 1.361e-05]
	Learning Rate: 1.36096e-05
	LOSS [training: 0.6246134668780033 | validation: 0.520604459713525]
	TIME [epoch: 5.38 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6253870592855049		[learning rate: 1.3562e-05]
	Learning Rate: 1.35615e-05
	LOSS [training: 0.6253870592855049 | validation: 0.5144983033233516]
	TIME [epoch: 5.37 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6227474042874639		[learning rate: 1.3514e-05]
	Learning Rate: 1.35135e-05
	LOSS [training: 0.6227474042874639 | validation: 0.5205984793173206]
	TIME [epoch: 5.39 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6258106461245174		[learning rate: 1.3466e-05]
	Learning Rate: 1.34658e-05
	LOSS [training: 0.6258106461245174 | validation: 0.5202219420742191]
	TIME [epoch: 5.39 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6269819853929673		[learning rate: 1.3418e-05]
	Learning Rate: 1.34181e-05
	LOSS [training: 0.6269819853929673 | validation: 0.5143996157618591]
	TIME [epoch: 5.39 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6254752170478353		[learning rate: 1.3371e-05]
	Learning Rate: 1.33707e-05
	LOSS [training: 0.6254752170478353 | validation: 0.5183028514056]
	TIME [epoch: 5.37 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6249831349162224		[learning rate: 1.3323e-05]
	Learning Rate: 1.33234e-05
	LOSS [training: 0.6249831349162224 | validation: 0.5210567956025729]
	TIME [epoch: 5.37 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6235561864393622		[learning rate: 1.3276e-05]
	Learning Rate: 1.32763e-05
	LOSS [training: 0.6235561864393622 | validation: 0.5185510501217164]
	TIME [epoch: 5.38 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6224502808570533		[learning rate: 1.3229e-05]
	Learning Rate: 1.32294e-05
	LOSS [training: 0.6224502808570533 | validation: 0.5097117538763197]
	TIME [epoch: 5.37 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6272265826866188		[learning rate: 1.3183e-05]
	Learning Rate: 1.31826e-05
	LOSS [training: 0.6272265826866188 | validation: 0.5157319782985087]
	TIME [epoch: 5.37 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6234036407872703		[learning rate: 1.3136e-05]
	Learning Rate: 1.3136e-05
	LOSS [training: 0.6234036407872703 | validation: 0.5184690157839061]
	TIME [epoch: 5.38 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6228982717663664		[learning rate: 1.309e-05]
	Learning Rate: 1.30895e-05
	LOSS [training: 0.6228982717663664 | validation: 0.520647347053813]
	TIME [epoch: 5.38 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6250383522388158		[learning rate: 1.3043e-05]
	Learning Rate: 1.30432e-05
	LOSS [training: 0.6250383522388158 | validation: 0.5136603261931806]
	TIME [epoch: 5.37 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6271864584314755		[learning rate: 1.2997e-05]
	Learning Rate: 1.29971e-05
	LOSS [training: 0.6271864584314755 | validation: 0.512940701864865]
	TIME [epoch: 5.37 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.627655993682742		[learning rate: 1.2951e-05]
	Learning Rate: 1.29511e-05
	LOSS [training: 0.627655993682742 | validation: 0.5131470643766355]
	TIME [epoch: 5.37 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6244553810796838		[learning rate: 1.2905e-05]
	Learning Rate: 1.29053e-05
	LOSS [training: 0.6244553810796838 | validation: 0.5118692048800958]
	TIME [epoch: 5.38 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6261754353689264		[learning rate: 1.286e-05]
	Learning Rate: 1.28597e-05
	LOSS [training: 0.6261754353689264 | validation: 0.515090146526647]
	TIME [epoch: 5.38 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6244426300048089		[learning rate: 1.2814e-05]
	Learning Rate: 1.28142e-05
	LOSS [training: 0.6244426300048089 | validation: 0.5216703524463879]
	TIME [epoch: 5.39 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.62656771785419		[learning rate: 1.2769e-05]
	Learning Rate: 1.27689e-05
	LOSS [training: 0.62656771785419 | validation: 0.5113919501968249]
	TIME [epoch: 5.38 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6233292356106899		[learning rate: 1.2724e-05]
	Learning Rate: 1.27238e-05
	LOSS [training: 0.6233292356106899 | validation: 0.522784314592529]
	TIME [epoch: 5.36 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6289391216238954		[learning rate: 1.2679e-05]
	Learning Rate: 1.26788e-05
	LOSS [training: 0.6289391216238954 | validation: 0.525809612855172]
	TIME [epoch: 5.38 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6247280416744089		[learning rate: 1.2634e-05]
	Learning Rate: 1.26339e-05
	LOSS [training: 0.6247280416744089 | validation: 0.5143452560647371]
	TIME [epoch: 5.37 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.625596583759598		[learning rate: 1.2589e-05]
	Learning Rate: 1.25893e-05
	LOSS [training: 0.625596583759598 | validation: 0.5226900788260481]
	TIME [epoch: 5.39 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6222704324979783		[learning rate: 1.2545e-05]
	Learning Rate: 1.25447e-05
	LOSS [training: 0.6222704324979783 | validation: 0.5158477818208]
	TIME [epoch: 5.37 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6245932411792028		[learning rate: 1.25e-05]
	Learning Rate: 1.25004e-05
	LOSS [training: 0.6245932411792028 | validation: 0.520882456622613]
	TIME [epoch: 5.38 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6259389124692585		[learning rate: 1.2456e-05]
	Learning Rate: 1.24562e-05
	LOSS [training: 0.6259389124692585 | validation: 0.5163503677648117]
	TIME [epoch: 5.41 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6237777255886191		[learning rate: 1.2412e-05]
	Learning Rate: 1.24121e-05
	LOSS [training: 0.6237777255886191 | validation: 0.5177902164105362]
	TIME [epoch: 5.37 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6284048260543228		[learning rate: 1.2368e-05]
	Learning Rate: 1.23682e-05
	LOSS [training: 0.6284048260543228 | validation: 0.5137816035940169]
	TIME [epoch: 5.37 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6236358304890528		[learning rate: 1.2325e-05]
	Learning Rate: 1.23245e-05
	LOSS [training: 0.6236358304890528 | validation: 0.5143390544896278]
	TIME [epoch: 5.38 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6199707660478653		[learning rate: 1.2281e-05]
	Learning Rate: 1.22809e-05
	LOSS [training: 0.6199707660478653 | validation: 0.5159361643255841]
	TIME [epoch: 5.37 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6214495269636398		[learning rate: 1.2237e-05]
	Learning Rate: 1.22375e-05
	LOSS [training: 0.6214495269636398 | validation: 0.5204015646458349]
	TIME [epoch: 5.37 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6239090992178978		[learning rate: 1.2194e-05]
	Learning Rate: 1.21942e-05
	LOSS [training: 0.6239090992178978 | validation: 0.5187697023270579]
	TIME [epoch: 5.37 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6274248862621982		[learning rate: 1.2151e-05]
	Learning Rate: 1.21511e-05
	LOSS [training: 0.6274248862621982 | validation: 0.5169908886414315]
	TIME [epoch: 5.37 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6272343705545882		[learning rate: 1.2108e-05]
	Learning Rate: 1.21081e-05
	LOSS [training: 0.6272343705545882 | validation: 0.5211993599668474]
	TIME [epoch: 5.38 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6221084782959746		[learning rate: 1.2065e-05]
	Learning Rate: 1.20653e-05
	LOSS [training: 0.6221084782959746 | validation: 0.5074675494084157]
	TIME [epoch: 5.38 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6256928441026969		[learning rate: 1.2023e-05]
	Learning Rate: 1.20226e-05
	LOSS [training: 0.6256928441026969 | validation: 0.5240888518381296]
	TIME [epoch: 5.39 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.622113612456823		[learning rate: 1.198e-05]
	Learning Rate: 1.19801e-05
	LOSS [training: 0.622113612456823 | validation: 0.5156776103101175]
	TIME [epoch: 5.41 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6247501682506681		[learning rate: 1.1938e-05]
	Learning Rate: 1.19378e-05
	LOSS [training: 0.6247501682506681 | validation: 0.5153523799760168]
	TIME [epoch: 5.38 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6232156662394336		[learning rate: 1.1896e-05]
	Learning Rate: 1.18956e-05
	LOSS [training: 0.6232156662394336 | validation: 0.5152377485176588]
	TIME [epoch: 5.39 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6253099088873597		[learning rate: 1.1853e-05]
	Learning Rate: 1.18535e-05
	LOSS [training: 0.6253099088873597 | validation: 0.5171594386310753]
	TIME [epoch: 5.37 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6219225202113043		[learning rate: 1.1812e-05]
	Learning Rate: 1.18116e-05
	LOSS [training: 0.6219225202113043 | validation: 0.5129438119696618]
	TIME [epoch: 5.36 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6207919724615631		[learning rate: 1.177e-05]
	Learning Rate: 1.17698e-05
	LOSS [training: 0.6207919724615631 | validation: 0.5142932186265597]
	TIME [epoch: 5.39 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.626591616688511		[learning rate: 1.1728e-05]
	Learning Rate: 1.17282e-05
	LOSS [training: 0.626591616688511 | validation: 0.5078967074152685]
	TIME [epoch: 5.37 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6219620741402038		[learning rate: 1.1687e-05]
	Learning Rate: 1.16867e-05
	LOSS [training: 0.6219620741402038 | validation: 0.5088541774804919]
	TIME [epoch: 5.37 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.622379531106263		[learning rate: 1.1645e-05]
	Learning Rate: 1.16454e-05
	LOSS [training: 0.622379531106263 | validation: 0.5243241396450445]
	TIME [epoch: 5.37 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6273904771296922		[learning rate: 1.1604e-05]
	Learning Rate: 1.16042e-05
	LOSS [training: 0.6273904771296922 | validation: 0.5239241518586056]
	TIME [epoch: 5.37 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6246317679884862		[learning rate: 1.1563e-05]
	Learning Rate: 1.15632e-05
	LOSS [training: 0.6246317679884862 | validation: 0.5086092028555911]
	TIME [epoch: 5.38 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6226735357524396		[learning rate: 1.1522e-05]
	Learning Rate: 1.15223e-05
	LOSS [training: 0.6226735357524396 | validation: 0.5189431183068015]
	TIME [epoch: 5.37 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6214192276503185		[learning rate: 1.1482e-05]
	Learning Rate: 1.14815e-05
	LOSS [training: 0.6214192276503185 | validation: 0.5125773607359781]
	TIME [epoch: 5.36 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6255656148690412		[learning rate: 1.1441e-05]
	Learning Rate: 1.14409e-05
	LOSS [training: 0.6255656148690412 | validation: 0.5126102486939607]
	TIME [epoch: 5.37 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6255346291040217		[learning rate: 1.14e-05]
	Learning Rate: 1.14005e-05
	LOSS [training: 0.6255346291040217 | validation: 0.5203230981413794]
	TIME [epoch: 5.37 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6227254480651236		[learning rate: 1.136e-05]
	Learning Rate: 1.13602e-05
	LOSS [training: 0.6227254480651236 | validation: 0.5128377617295624]
	TIME [epoch: 5.37 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6204885878239346		[learning rate: 1.132e-05]
	Learning Rate: 1.132e-05
	LOSS [training: 0.6204885878239346 | validation: 0.5188228949934336]
	TIME [epoch: 5.37 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6241259279695218		[learning rate: 1.128e-05]
	Learning Rate: 1.128e-05
	LOSS [training: 0.6241259279695218 | validation: 0.5152038426601914]
	TIME [epoch: 5.36 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.624603701545662		[learning rate: 1.124e-05]
	Learning Rate: 1.12401e-05
	LOSS [training: 0.624603701545662 | validation: 0.5186637688079363]
	TIME [epoch: 5.38 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6243313130301792		[learning rate: 1.12e-05]
	Learning Rate: 1.12003e-05
	LOSS [training: 0.6243313130301792 | validation: 0.5171577264777916]
	TIME [epoch: 5.36 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6200259179722085		[learning rate: 1.1161e-05]
	Learning Rate: 1.11607e-05
	LOSS [training: 0.6200259179722085 | validation: 0.5105266073913425]
	TIME [epoch: 5.37 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6234817720722055		[learning rate: 1.1121e-05]
	Learning Rate: 1.11213e-05
	LOSS [training: 0.6234817720722055 | validation: 0.5192880386923276]
	TIME [epoch: 5.37 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6224571910879143		[learning rate: 1.1082e-05]
	Learning Rate: 1.10819e-05
	LOSS [training: 0.6224571910879143 | validation: 0.5208124052514701]
	TIME [epoch: 5.37 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6214183745215575		[learning rate: 1.1043e-05]
	Learning Rate: 1.10427e-05
	LOSS [training: 0.6214183745215575 | validation: 0.5164446846739733]
	TIME [epoch: 5.37 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6232680036767144		[learning rate: 1.1004e-05]
	Learning Rate: 1.10037e-05
	LOSS [training: 0.6232680036767144 | validation: 0.5202317730331635]
	TIME [epoch: 5.38 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6249126644280083		[learning rate: 1.0965e-05]
	Learning Rate: 1.09648e-05
	LOSS [training: 0.6249126644280083 | validation: 0.5172029677041322]
	TIME [epoch: 5.37 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6231493686766976		[learning rate: 1.0926e-05]
	Learning Rate: 1.0926e-05
	LOSS [training: 0.6231493686766976 | validation: 0.5121218030222611]
	TIME [epoch: 5.38 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6239874917577802		[learning rate: 1.0887e-05]
	Learning Rate: 1.08874e-05
	LOSS [training: 0.6239874917577802 | validation: 0.5159424398464174]
	TIME [epoch: 5.37 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6228640319691922		[learning rate: 1.0849e-05]
	Learning Rate: 1.08489e-05
	LOSS [training: 0.6228640319691922 | validation: 0.5159282426200201]
	TIME [epoch: 5.37 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6220722951002214		[learning rate: 1.0811e-05]
	Learning Rate: 1.08105e-05
	LOSS [training: 0.6220722951002214 | validation: 0.5117193131990503]
	TIME [epoch: 5.37 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6261611275218232		[learning rate: 1.0772e-05]
	Learning Rate: 1.07723e-05
	LOSS [training: 0.6261611275218232 | validation: 0.5171811207561148]
	TIME [epoch: 5.37 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6227395385246288		[learning rate: 1.0734e-05]
	Learning Rate: 1.07342e-05
	LOSS [training: 0.6227395385246288 | validation: 0.5229544542918653]
	TIME [epoch: 5.37 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6235926910410583		[learning rate: 1.0696e-05]
	Learning Rate: 1.06962e-05
	LOSS [training: 0.6235926910410583 | validation: 0.5112314310762935]
	TIME [epoch: 5.38 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6213549991860406		[learning rate: 1.0658e-05]
	Learning Rate: 1.06584e-05
	LOSS [training: 0.6213549991860406 | validation: 0.5224217102119065]
	TIME [epoch: 5.39 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6238464420983256		[learning rate: 1.0621e-05]
	Learning Rate: 1.06207e-05
	LOSS [training: 0.6238464420983256 | validation: 0.5143632577074669]
	TIME [epoch: 5.37 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6267993375038521		[learning rate: 1.0583e-05]
	Learning Rate: 1.05832e-05
	LOSS [training: 0.6267993375038521 | validation: 0.5142401396735433]
	TIME [epoch: 5.39 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6233147528920424		[learning rate: 1.0546e-05]
	Learning Rate: 1.05457e-05
	LOSS [training: 0.6233147528920424 | validation: 0.5165508878046637]
	TIME [epoch: 5.37 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6222508088633799		[learning rate: 1.0508e-05]
	Learning Rate: 1.05084e-05
	LOSS [training: 0.6222508088633799 | validation: 0.5191878302754643]
	TIME [epoch: 5.37 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6249848007246586		[learning rate: 1.0471e-05]
	Learning Rate: 1.04713e-05
	LOSS [training: 0.6249848007246586 | validation: 0.5194031171281034]
	TIME [epoch: 5.36 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6251882602003991		[learning rate: 1.0434e-05]
	Learning Rate: 1.04343e-05
	LOSS [training: 0.6251882602003991 | validation: 0.5236182364780453]
	TIME [epoch: 5.38 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6236638883701422		[learning rate: 1.0397e-05]
	Learning Rate: 1.03974e-05
	LOSS [training: 0.6236638883701422 | validation: 0.5209337077470971]
	TIME [epoch: 5.36 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6225898677850275		[learning rate: 1.0361e-05]
	Learning Rate: 1.03606e-05
	LOSS [training: 0.6225898677850275 | validation: 0.5127933847099991]
	TIME [epoch: 5.37 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6226923226097383		[learning rate: 1.0324e-05]
	Learning Rate: 1.0324e-05
	LOSS [training: 0.6226923226097383 | validation: 0.5106954027087367]
	TIME [epoch: 5.37 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6265794699276056		[learning rate: 1.0287e-05]
	Learning Rate: 1.02875e-05
	LOSS [training: 0.6265794699276056 | validation: 0.520851169687651]
	TIME [epoch: 5.36 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6228655688949775		[learning rate: 1.0251e-05]
	Learning Rate: 1.02511e-05
	LOSS [training: 0.6228655688949775 | validation: 0.5169382595359979]
	TIME [epoch: 5.37 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.624487280228276		[learning rate: 1.0215e-05]
	Learning Rate: 1.02148e-05
	LOSS [training: 0.624487280228276 | validation: 0.5162837806164869]
	TIME [epoch: 5.37 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6238545663878606		[learning rate: 1.0179e-05]
	Learning Rate: 1.01787e-05
	LOSS [training: 0.6238545663878606 | validation: 0.5129267691625062]
	TIME [epoch: 5.37 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6214439805192677		[learning rate: 1.0143e-05]
	Learning Rate: 1.01427e-05
	LOSS [training: 0.6214439805192677 | validation: 0.5131126101607129]
	TIME [epoch: 5.38 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6244939715645446		[learning rate: 1.0107e-05]
	Learning Rate: 1.01068e-05
	LOSS [training: 0.6244939715645446 | validation: 0.5219803054976238]
	TIME [epoch: 5.35 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6215900685308293		[learning rate: 1.0071e-05]
	Learning Rate: 1.00711e-05
	LOSS [training: 0.6215900685308293 | validation: 0.5178915456471103]
	TIME [epoch: 5.37 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6255642435825766		[learning rate: 1.0035e-05]
	Learning Rate: 1.00355e-05
	LOSS [training: 0.6255642435825766 | validation: 0.5134106275266118]
	TIME [epoch: 5.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd3_20250509_105701/states/model_phi1_4a_distortion_v2_1_v_mmd3_2000.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 8473.397 seconds.
