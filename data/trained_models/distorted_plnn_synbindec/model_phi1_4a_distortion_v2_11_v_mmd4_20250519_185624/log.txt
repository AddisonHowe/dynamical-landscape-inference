Args:
Namespace(name='model_phi1_4a_distortion_v2_11_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_11/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_11/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.01826576, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2974374911

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.1864539862112204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.1864539862112204 | validation: 4.892837620863161]
	TIME [epoch: 126 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.856712361724903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.856712361724903 | validation: 5.277449438439972]
	TIME [epoch: 0.485 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.541468930739413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.541468930739413 | validation: 5.04649957084462]
	TIME [epoch: 0.479 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.4761239645267095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4761239645267095 | validation: 4.771156901803765]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.144616327908311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.144616327908311 | validation: 4.548102797280234]
	TIME [epoch: 0.481 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9345741305023245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9345741305023245 | validation: 4.344502639566922]
	TIME [epoch: 0.477 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7361874550739356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7361874550739356 | validation: 4.148227258354165]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.543662441537052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.543662441537052 | validation: 3.9842358281722845]
	TIME [epoch: 0.477 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.386476193645267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.386476193645267 | validation: 3.7200318543179955]
	TIME [epoch: 0.477 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1738646499464527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1738646499464527 | validation: 3.612809437843997]
	TIME [epoch: 0.479 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9905617430031124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9905617430031124 | validation: 3.275407829712706]
	TIME [epoch: 0.478 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7636049352900005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7636049352900005 | validation: 3.0405764387178]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7300616932053874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7300616932053874 | validation: 2.828951704424743]
	TIME [epoch: 0.478 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.836198950079675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.836198950079675 | validation: 2.9257316783956764]
	TIME [epoch: 0.478 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.630396699414422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.630396699414422 | validation: 2.7332104735230676]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.474370286543561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.474370286543561 | validation: 2.6488161470358045]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3271829442151546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3271829442151546 | validation: 2.1434018020555783]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2489695454835705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2489695454835705 | validation: 2.1242264559333335]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.157771071420731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.157771071420731 | validation: 2.262420710145265]
	TIME [epoch: 0.477 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1340521424140166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1340521424140166 | validation: 2.062641184691903]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.116938484451694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.116938484451694 | validation: 2.106301791514848]
	TIME [epoch: 0.477 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.079736243197859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.079736243197859 | validation: 2.099001584568455]
	TIME [epoch: 0.477 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.093038823141413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.093038823141413 | validation: 1.8963477066651686]
	TIME [epoch: 0.477 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.944537022997653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.944537022997653 | validation: 1.7630283770938762]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.899674489947756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.899674489947756 | validation: 1.6779244975684258]
	TIME [epoch: 0.477 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.867264002369946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.867264002369946 | validation: 1.847980061248531]
	TIME [epoch: 0.483 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.889797475048233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.889797475048233 | validation: 2.316726846634436]
	TIME [epoch: 0.475 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.156761502250357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.156761502250357 | validation: 1.6644071357367094]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7899063514290416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7899063514290416 | validation: 2.109950660710807]
	TIME [epoch: 0.478 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9247766606909182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9247766606909182 | validation: 2.0861222068604435]
	TIME [epoch: 0.476 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8901641104530067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8901641104530067 | validation: 1.9071065550269695]
	TIME [epoch: 0.476 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8107508932049357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8107508932049357 | validation: 1.52048035860595]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7123825939746848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7123825939746848 | validation: 1.718290727702398]
	TIME [epoch: 0.479 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.689010927482693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.689010927482693 | validation: 1.7048386754353595]
	TIME [epoch: 0.476 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6643315655532402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6643315655532402 | validation: 1.5311629844420336]
	TIME [epoch: 0.476 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6151226642545686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6151226642545686 | validation: 1.5312267423671981]
	TIME [epoch: 0.476 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6199478181171254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6199478181171254 | validation: 1.6014361626208875]
	TIME [epoch: 0.476 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6050376351617324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6050376351617324 | validation: 1.565454904807834]
	TIME [epoch: 0.477 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.576358463211896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.576358463211896 | validation: 1.513446812871273]
	TIME [epoch: 0.477 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5611526474528803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5611526474528803 | validation: 1.647024101648902]
	TIME [epoch: 0.476 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5600181477598292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5600181477598292 | validation: 1.4929629290714865]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.552205604750489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.552205604750489 | validation: 1.7573849998915811]
	TIME [epoch: 0.477 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5727167745093364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5727167745093364 | validation: 1.643213959231192]
	TIME [epoch: 0.477 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.69458441150819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.69458441150819 | validation: 2.1020492485893514]
	TIME [epoch: 0.476 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6991375469009193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6991375469009193 | validation: 1.7385521071521577]
	TIME [epoch: 0.477 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5717074037517964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5717074037517964 | validation: 1.5492777616641904]
	TIME [epoch: 0.477 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5653068082378563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5653068082378563 | validation: 1.6605006148344863]
	TIME [epoch: 0.477 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5362192211622556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5362192211622556 | validation: 1.6565533605992937]
	TIME [epoch: 0.476 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5310707204173821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5310707204173821 | validation: 1.5687805173698086]
	TIME [epoch: 0.477 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5262553066314763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5262553066314763 | validation: 1.5621450404638595]
	TIME [epoch: 0.477 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5164761466791812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5164761466791812 | validation: 1.5905752402055071]
	TIME [epoch: 0.477 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5205357698406954		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.5205357698406954 | validation: 1.6193577676965738]
	TIME [epoch: 0.476 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5256203299405169		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.5256203299405169 | validation: 1.61519956505995]
	TIME [epoch: 0.476 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.570291228654823		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.570291228654823 | validation: 2.001610226922848]
	TIME [epoch: 0.477 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6497840598488256		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.6497840598488256 | validation: 1.5716165615456614]
	TIME [epoch: 0.477 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7363452523392817		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.7363452523392817 | validation: 1.798322427751092]
	TIME [epoch: 0.476 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5592755687537576		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.5592755687537576 | validation: 1.8201266957304356]
	TIME [epoch: 0.476 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5630762909169955		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.5630762909169955 | validation: 1.5629359005424144]
	TIME [epoch: 0.476 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5169187452750512		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.5169187452750512 | validation: 1.4990584155895172]
	TIME [epoch: 0.478 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.51547757779896		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.51547757779896 | validation: 1.590259726323218]
	TIME [epoch: 0.477 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5025872862442509		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.5025872862442509 | validation: 1.5999919809039111]
	TIME [epoch: 0.476 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5003281835456215		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.5003281835456215 | validation: 1.526669599345015]
	TIME [epoch: 0.476 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4942689118869192		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.4942689118869192 | validation: 1.5631703910621262]
	TIME [epoch: 0.477 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4822072737107619		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.4822072737107619 | validation: 1.560378491687374]
	TIME [epoch: 0.478 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4787884343316648		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.4787884343316648 | validation: 1.5092041718865024]
	TIME [epoch: 0.477 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.469642778121274		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.469642778121274 | validation: 1.7883099739991082]
	TIME [epoch: 0.477 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5243318184599026		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.5243318184599026 | validation: 1.864430508236984]
	TIME [epoch: 0.476 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0651338974413007		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 2.0651338974413007 | validation: 1.9211932493230461]
	TIME [epoch: 0.477 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6053041073485776		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.6053041073485776 | validation: 1.9613680671118765]
	TIME [epoch: 0.476 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6126377943114096		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.6126377943114096 | validation: 1.638595149044695]
	TIME [epoch: 0.476 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5038582884384457		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.5038582884384457 | validation: 1.5203808897345175]
	TIME [epoch: 0.477 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5115264928241792		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.5115264928241792 | validation: 1.5209925729513158]
	TIME [epoch: 0.476 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4965445031995779		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.4965445031995779 | validation: 1.5432976227862794]
	TIME [epoch: 0.477 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.48266662489963		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.48266662489963 | validation: 1.545319937401969]
	TIME [epoch: 0.477 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4596397480848702		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.4596397480848702 | validation: 1.503880103944647]
	TIME [epoch: 0.476 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.460722806860204		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.460722806860204 | validation: 1.5605824832189614]
	TIME [epoch: 0.476 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4530645844295407		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.4530645844295407 | validation: 1.5033729419432542]
	TIME [epoch: 0.48 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4609194764073714		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.4609194764073714 | validation: 1.7334172124163125]
	TIME [epoch: 0.477 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5089499537318773		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.5089499537318773 | validation: 1.6854684123293808]
	TIME [epoch: 0.476 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.74509979769864		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.74509979769864 | validation: 1.7508950488973778]
	TIME [epoch: 0.477 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5083302761989938		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.5083302761989938 | validation: 1.6691746666150367]
	TIME [epoch: 0.477 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.467972940986071		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.467972940986071 | validation: 1.4110137259955264]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4732027861437478		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.4732027861437478 | validation: 1.4589366599157343]
	TIME [epoch: 0.477 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4122643090544102		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.4122643090544102 | validation: 1.4994907737065657]
	TIME [epoch: 0.481 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.408497821397535		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.408497821397535 | validation: 1.4292563816586574]
	TIME [epoch: 0.476 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3938366476526343		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.3938366476526343 | validation: 1.5755132529192695]
	TIME [epoch: 0.478 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4123063987062017		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.4123063987062017 | validation: 1.4641008409872]
	TIME [epoch: 0.478 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.576479801140133		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.576479801140133 | validation: 2.189568587346222]
	TIME [epoch: 0.477 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.714798442269987		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.714798442269987 | validation: 1.5684873174420408]
	TIME [epoch: 0.476 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4918798771312494		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.4918798771312494 | validation: 1.3153045562910146]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4278821266753032		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.4278821266753032 | validation: 1.5278836530010558]
	TIME [epoch: 0.478 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.412060050095457		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.412060050095457 | validation: 1.3690271823906448]
	TIME [epoch: 0.476 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.363580498503502		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.363580498503502 | validation: 1.4349855534823355]
	TIME [epoch: 0.477 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3341654959780493		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.3341654959780493 | validation: 1.3287134773565015]
	TIME [epoch: 0.478 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.314683627665105		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.314683627665105 | validation: 1.3256083171649122]
	TIME [epoch: 0.477 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2905469335572741		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.2905469335572741 | validation: 1.320446144880198]
	TIME [epoch: 0.476 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2723292245313618		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.2723292245313618 | validation: 1.2235721195367946]
	TIME [epoch: 0.477 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.282382442942739		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.282382442942739 | validation: 2.0386983708300153]
	TIME [epoch: 0.477 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6291222732382968		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.6291222732382968 | validation: 1.6214895429653806]
	TIME [epoch: 0.478 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9861078395457268		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.9861078395457268 | validation: 1.4416629524421751]
	TIME [epoch: 0.477 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.432531010217213		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.432531010217213 | validation: 1.7130943779889414]
	TIME [epoch: 0.477 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4597495624500096		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.4597495624500096 | validation: 1.2316277064343002]
	TIME [epoch: 0.477 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2689973589016512		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.2689973589016512 | validation: 1.164185452536296]
	TIME [epoch: 0.477 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2716376133442564		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.2716376133442564 | validation: 1.2758202641274832]
	TIME [epoch: 0.477 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2547707030514914		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.2547707030514914 | validation: 1.1923998438872936]
	TIME [epoch: 0.478 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2433570705904626		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.2433570705904626 | validation: 1.2236211753463475]
	TIME [epoch: 0.477 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2107957691954578		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.2107957691954578 | validation: 1.1138508818426816]
	TIME [epoch: 0.477 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1864542115082213		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.1864542115082213 | validation: 1.1918854392397233]
	TIME [epoch: 0.477 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1872751094547385		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.1872751094547385 | validation: 1.1365574623812549]
	TIME [epoch: 0.476 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2210575590110113		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.2210575590110113 | validation: 1.6803510136006166]
	TIME [epoch: 0.477 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4115967528535354		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.4115967528535354 | validation: 1.170348310145411]
	TIME [epoch: 0.477 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4542659493824572		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.4542659493824572 | validation: 1.1846812926477257]
	TIME [epoch: 0.476 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.173898122988299		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.173898122988299 | validation: 1.183246209434375]
	TIME [epoch: 0.476 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1756226729135317		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.1756226729135317 | validation: 1.0240835679859395]
	TIME [epoch: 0.477 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2168170564067493		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.2168170564067493 | validation: 1.2922261107284112]
	TIME [epoch: 0.477 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2002802843322096		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.2002802843322096 | validation: 1.1227676681817156]
	TIME [epoch: 0.476 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1941534861594045		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.1941534861594045 | validation: 1.1974300011840147]
	TIME [epoch: 0.476 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1556807623254515		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.1556807623254515 | validation: 0.9690731363463437]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1390977798580704		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.1390977798580704 | validation: 1.1102762101902715]
	TIME [epoch: 0.478 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1181129729407133		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.1181129729407133 | validation: 0.9725184689828374]
	TIME [epoch: 0.475 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1213922386871724		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.1213922386871724 | validation: 1.25166191206296]
	TIME [epoch: 0.476 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1554182715596975		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.1554182715596975 | validation: 1.0163713290316625]
	TIME [epoch: 0.476 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2290116172654		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.2290116172654 | validation: 1.3209898503425292]
	TIME [epoch: 0.476 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1918282467603551		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.1918282467603551 | validation: 0.9528335852814863]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0837521719614007		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.0837521719614007 | validation: 1.072579661970567]
	TIME [epoch: 0.476 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.066133656323222		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.066133656323222 | validation: 0.9246690361419937]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0773241191228617		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.0773241191228617 | validation: 1.1204826241435233]
	TIME [epoch: 0.478 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0853950936033607		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.0853950936033607 | validation: 0.9065995209295515]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1613012104190237		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.1613012104190237 | validation: 1.3014305422493369]
	TIME [epoch: 0.477 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1815354903933053		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.1815354903933053 | validation: 0.9241290552484671]
	TIME [epoch: 0.477 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1052248314776876		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.1052248314776876 | validation: 1.1547540447924918]
	TIME [epoch: 0.476 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0949993524533457		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.0949993524533457 | validation: 0.8759408504285219]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0622261870584684		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.0622261870584684 | validation: 1.0803191092587017]
	TIME [epoch: 0.477 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0479349872381616		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.0479349872381616 | validation: 0.895261700901131]
	TIME [epoch: 0.477 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0584888511191084		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.0584888511191084 | validation: 1.1540021795000013]
	TIME [epoch: 0.477 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0787916834477358		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.0787916834477358 | validation: 0.8775517016903316]
	TIME [epoch: 0.476 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.123514339136272		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.123514339136272 | validation: 1.2159974741064778]
	TIME [epoch: 0.482 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1096323306827507		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.1096323306827507 | validation: 0.8883913939117861]
	TIME [epoch: 0.478 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0266089926392044		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.0266089926392044 | validation: 1.0207414447995102]
	TIME [epoch: 0.476 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.01823640306176		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.01823640306176 | validation: 0.8631800530235095]
	TIME [epoch: 0.477 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0193692120254045		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.0193692120254045 | validation: 1.144347896851104]
	TIME [epoch: 0.477 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0757067201998607		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.0757067201998607 | validation: 0.8702192191322068]
	TIME [epoch: 0.477 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.096751943450603		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.096751943450603 | validation: 1.1019942597307544]
	TIME [epoch: 0.477 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0341415417004922		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.0341415417004922 | validation: 0.8394795370678774]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_144.pth
	Model improved!!!
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0080394719570078		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.0080394719570078 | validation: 1.130370386323596]
	TIME [epoch: 0.476 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.048150242836392		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.048150242836392 | validation: 0.8570607596911156]
	TIME [epoch: 0.476 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0626943890744325		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.0626943890744325 | validation: 1.1804237122668437]
	TIME [epoch: 0.476 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0820576227537761		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.0820576227537761 | validation: 0.8644569369118434]
	TIME [epoch: 0.475 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0049885288137268		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.0049885288137268 | validation: 0.9986787926235885]
	TIME [epoch: 0.476 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9779003140343545		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.9779003140343545 | validation: 0.8643416802616081]
	TIME [epoch: 0.477 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9724067248795548		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.9724067248795548 | validation: 1.030337442956883]
	TIME [epoch: 0.477 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9923426240425373		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.9923426240425373 | validation: 0.8020998231742816]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_152.pth
	Model improved!!!
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0542743467422009		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.0542743467422009 | validation: 1.2635158758543845]
	TIME [epoch: 0.476 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1228642968609521		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.1228642968609521 | validation: 0.8505819841198655]
	TIME [epoch: 0.479 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0198255209655935		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.0198255209655935 | validation: 1.0424289162239504]
	TIME [epoch: 0.476 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0080941093623854		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.0080941093623854 | validation: 0.8349366498240963]
	TIME [epoch: 0.476 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9869757741381203		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.9869757741381203 | validation: 1.0452848189347215]
	TIME [epoch: 0.476 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9884153570405214		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.9884153570405214 | validation: 0.8211673495335456]
	TIME [epoch: 0.475 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9945879874637562		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.9945879874637562 | validation: 1.1104033266742779]
	TIME [epoch: 0.477 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0344641851744851		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.0344641851744851 | validation: 0.8329461750292787]
	TIME [epoch: 0.476 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0307167741622238		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.0307167741622238 | validation: 1.1444218939655255]
	TIME [epoch: 0.476 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0421946014012644		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.0421946014012644 | validation: 0.8383306384580074]
	TIME [epoch: 0.475 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9617429777481582		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.9617429777481582 | validation: 0.9971355019658941]
	TIME [epoch: 0.477 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9528265494825096		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.9528265494825096 | validation: 0.8249990002287086]
	TIME [epoch: 0.477 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9621564232171039		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.9621564232171039 | validation: 1.0747001205057205]
	TIME [epoch: 0.476 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9946919800757229		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.9946919800757229 | validation: 0.8417664184294527]
	TIME [epoch: 0.475 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.030668741196491		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.030668741196491 | validation: 1.1202565251498136]
	TIME [epoch: 0.476 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0276531600805843		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.0276531600805843 | validation: 0.8599588476997528]
	TIME [epoch: 0.476 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9939177721506559		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.9939177721506559 | validation: 1.072368880084715]
	TIME [epoch: 0.477 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9839297998487524		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.9839297998487524 | validation: 0.7934664133995158]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.950823476931441		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.950823476931441 | validation: 0.9847880263788596]
	TIME [epoch: 0.477 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9432256121403552		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.9432256121403552 | validation: 0.7970585181781665]
	TIME [epoch: 0.476 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9371972480682443		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.9371972480682443 | validation: 1.066362989172849]
	TIME [epoch: 0.477 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.979739440696618		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.979739440696618 | validation: 0.8309786407338173]
	TIME [epoch: 0.476 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0527217952807093		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.0527217952807093 | validation: 1.1981968744039528]
	TIME [epoch: 0.476 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0856457869756084		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.0856457869756084 | validation: 0.8504749776442688]
	TIME [epoch: 0.476 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9283480729033481		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.9283480729033481 | validation: 0.9303589160562563]
	TIME [epoch: 0.478 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9183409026568277		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.9183409026568277 | validation: 0.8529339178601829]
	TIME [epoch: 0.476 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9120307308377074		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.9120307308377074 | validation: 0.9639265430547225]
	TIME [epoch: 0.476 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.921902507198098		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.921902507198098 | validation: 0.8038815863989474]
	TIME [epoch: 0.475 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.973047699051034		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.973047699051034 | validation: 1.2675456812648709]
	TIME [epoch: 0.477 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1023855866458205		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.1023855866458205 | validation: 0.8042038814962393]
	TIME [epoch: 0.477 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9761788282622902		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.9761788282622902 | validation: 1.031096577616442]
	TIME [epoch: 0.476 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9490997799662497		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.9490997799662497 | validation: 0.8125624042758528]
	TIME [epoch: 0.476 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9225067394303544		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.9225067394303544 | validation: 0.9919595029800601]
	TIME [epoch: 0.475 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9335884694412596		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.9335884694412596 | validation: 0.8037483777337437]
	TIME [epoch: 0.476 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9458302888686103		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.9458302888686103 | validation: 1.083390107497796]
	TIME [epoch: 0.476 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9780779319018819		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.9780779319018819 | validation: 0.8245472578831846]
	TIME [epoch: 0.476 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9606125707404811		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.9606125707404811 | validation: 1.0480935306072567]
	TIME [epoch: 0.476 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9628157910236834		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.9628157910236834 | validation: 0.7910043540002274]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_190.pth
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9369278436039747		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.9369278436039747 | validation: 1.0191718036179083]
	TIME [epoch: 0.478 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9512553778659978		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.9512553778659978 | validation: 0.7921488987533781]
	TIME [epoch: 0.477 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9216973442586612		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.9216973442586612 | validation: 1.0218416288806258]
	TIME [epoch: 0.477 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.932502541698185		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.932502541698185 | validation: 0.8063755695256166]
	TIME [epoch: 0.477 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9275532485148057		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.9275532485148057 | validation: 1.0772765414865073]
	TIME [epoch: 0.482 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9635349307588729		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.9635349307588729 | validation: 0.8056860720233978]
	TIME [epoch: 0.476 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9457928980967859		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.9457928980967859 | validation: 1.0283794769780428]
	TIME [epoch: 0.477 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9548695734517831		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.9548695734517831 | validation: 0.858890641498234]
	TIME [epoch: 0.476 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9325141048224709		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.9325141048224709 | validation: 0.9895074218799528]
	TIME [epoch: 0.478 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9133618385793776		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.9133618385793776 | validation: 0.8047095707244388]
	TIME [epoch: 0.477 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9017332795969257		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.9017332795969257 | validation: 1.037476054576295]
	TIME [epoch: 133 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9338582225570793		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.9338582225570793 | validation: 0.7866866600997625]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9375490172609768		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.9375490172609768 | validation: 1.1200335795321725]
	TIME [epoch: 0.933 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9837969918970193		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.9837969918970193 | validation: 0.8174122813851241]
	TIME [epoch: 0.931 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9122445664167863		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.9122445664167863 | validation: 0.9872168937410161]
	TIME [epoch: 0.931 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9113447706088539		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.9113447706088539 | validation: 0.8100920786769379]
	TIME [epoch: 0.93 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8860612110983709		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.8860612110983709 | validation: 0.9676899018314343]
	TIME [epoch: 0.931 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8889288719001732		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.8889288719001732 | validation: 0.7875246863193163]
	TIME [epoch: 0.93 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9174427812455861		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.9174427812455861 | validation: 1.1047418312600985]
	TIME [epoch: 0.931 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9811002063727154		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.9811002063727154 | validation: 0.796300384286271]
	TIME [epoch: 0.93 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9221015289953423		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.9221015289953423 | validation: 0.9896101468080014]
	TIME [epoch: 0.93 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8924355144980465		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.8924355144980465 | validation: 0.8195004367740417]
	TIME [epoch: 0.93 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8739074536711854		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.8739074536711854 | validation: 0.9686935211555708]
	TIME [epoch: 0.933 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8763017213728559		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.8763017213728559 | validation: 0.7778495058138404]
	TIME [epoch: 0.93 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_214.pth
	Model improved!!!
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.90270773282499		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.90270773282499 | validation: 1.089486365535327]
	TIME [epoch: 0.933 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9747925907048689		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.9747925907048689 | validation: 0.8414747867148565]
	TIME [epoch: 0.934 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.927742029183844		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.927742029183844 | validation: 0.9746939298659673]
	TIME [epoch: 0.934 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8800955953172891		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.8800955953172891 | validation: 0.8546651876599811]
	TIME [epoch: 0.932 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8491712760806667		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.8491712760806667 | validation: 0.8868997861215807]
	TIME [epoch: 0.932 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8331705363691739		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.8331705363691739 | validation: 0.7974123387999439]
	TIME [epoch: 0.931 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.843647288556318		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.843647288556318 | validation: 1.062253413642269]
	TIME [epoch: 0.932 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9387453490874313		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.9387453490874313 | validation: 0.7905326845068704]
	TIME [epoch: 0.931 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0618646970677337		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 1.0618646970677337 | validation: 1.0904984934134838]
	TIME [epoch: 0.933 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9266989682246509		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.9266989682246509 | validation: 0.907056735136459]
	TIME [epoch: 0.932 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.841614321469188		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.841614321469188 | validation: 0.7924274138560645]
	TIME [epoch: 0.933 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8625422239333821		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.8625422239333821 | validation: 1.0044453605575583]
	TIME [epoch: 0.932 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9023385554849577		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.9023385554849577 | validation: 0.7999248168992811]
	TIME [epoch: 0.932 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8672465602886333		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.8672465602886333 | validation: 0.9897862256966921]
	TIME [epoch: 0.932 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8769531978788089		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.8769531978788089 | validation: 0.819634547272311]
	TIME [epoch: 0.933 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8725198377340015		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.8725198377340015 | validation: 1.0176844798869078]
	TIME [epoch: 0.932 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8750073167158462		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.8750073167158462 | validation: 0.8216122498849864]
	TIME [epoch: 0.93 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8666803416892859		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.8666803416892859 | validation: 1.015885172411284]
	TIME [epoch: 0.932 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8810999497486935		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.8810999497486935 | validation: 0.8244115817711943]
	TIME [epoch: 0.931 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8488304643113329		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.8488304643113329 | validation: 0.9480225849001989]
	TIME [epoch: 0.932 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8469763919924873		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.8469763919924873 | validation: 0.82064807475088]
	TIME [epoch: 0.932 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8374135189297087		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.8374135189297087 | validation: 0.9759573499292211]
	TIME [epoch: 0.932 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8346138076427692		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.8346138076427692 | validation: 0.7900135958699098]
	TIME [epoch: 0.931 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8785720559128012		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.8785720559128012 | validation: 1.0942897366377662]
	TIME [epoch: 0.931 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9372288889903204		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.9372288889903204 | validation: 0.8411167755711899]
	TIME [epoch: 0.931 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8702813292521049		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.8702813292521049 | validation: 0.9394563763866718]
	TIME [epoch: 0.932 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.815364008613948		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.815364008613948 | validation: 0.8215283405274454]
	TIME [epoch: 0.932 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7889753538858709		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.7889753538858709 | validation: 0.9147347437619637]
	TIME [epoch: 0.931 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.806259729096819		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.806259729096819 | validation: 0.7963531908529088]
	TIME [epoch: 0.932 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8772995907958756		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.8772995907958756 | validation: 1.188497812045285]
	TIME [epoch: 0.931 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0202410007870453		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 1.0202410007870453 | validation: 0.8846282301362948]
	TIME [epoch: 0.932 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8031813262871514		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.8031813262871514 | validation: 0.8339671919912597]
	TIME [epoch: 0.931 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7935318937595307		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.7935318937595307 | validation: 0.9011453897329998]
	TIME [epoch: 0.934 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7879871569749334		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.7879871569749334 | validation: 0.7871725259308509]
	TIME [epoch: 0.931 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8216744464588385		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.8216744464588385 | validation: 1.0743383288548987]
	TIME [epoch: 0.931 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9124665096340043		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.9124665096340043 | validation: 0.8104883778164086]
	TIME [epoch: 0.931 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8178000196618402		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.8178000196618402 | validation: 0.9489431076180822]
	TIME [epoch: 0.931 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7997343568756025		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.7997343568756025 | validation: 0.8131603750731359]
	TIME [epoch: 0.931 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8469278208793167		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.8469278208793167 | validation: 0.9894144955980146]
	TIME [epoch: 0.932 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8851621168208439		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.8851621168208439 | validation: 0.8530081472391026]
	TIME [epoch: 0.931 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.848632358740364		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.848632358740364 | validation: 0.9544938197266802]
	TIME [epoch: 0.931 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8094269768501762		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.8094269768501762 | validation: 0.7963125893897566]
	TIME [epoch: 0.93 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8183705903442205		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.8183705903442205 | validation: 1.0201759438893505]
	TIME [epoch: 0.932 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8677865011641983		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.8677865011641983 | validation: 0.7564396486931528]
	TIME [epoch: 0.931 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8259299973722403		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.8259299973722403 | validation: 1.008543168121321]
	TIME [epoch: 0.928 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8148154237048274		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.8148154237048274 | validation: 0.8214953848943062]
	TIME [epoch: 0.927 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.789197575238807		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.789197575238807 | validation: 0.8938410322726478]
	TIME [epoch: 0.931 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7856005368719174		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.7856005368719174 | validation: 0.7482505008822735]
	TIME [epoch: 0.932 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_262.pth
	Model improved!!!
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8043300174155293		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.8043300174155293 | validation: 1.0724181687907117]
	TIME [epoch: 0.927 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8724544859212875		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.8724544859212875 | validation: 0.8038899704214416]
	TIME [epoch: 0.926 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8169981662045299		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.8169981662045299 | validation: 0.9177539999891746]
	TIME [epoch: 0.927 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7929832505987671		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.7929832505987671 | validation: 0.7503175225096037]
	TIME [epoch: 0.927 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7564350742298822		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.7564350742298822 | validation: 0.9453699458758156]
	TIME [epoch: 0.928 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7610802432621102		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.7610802432621102 | validation: 0.757824985913266]
	TIME [epoch: 0.928 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7976155399893248		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.7976155399893248 | validation: 1.139098384094061]
	TIME [epoch: 0.927 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9630694580779764		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.9630694580779764 | validation: 0.7866140900646538]
	TIME [epoch: 0.928 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8103310450762584		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.8103310450762584 | validation: 0.8872960850341441]
	TIME [epoch: 0.926 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7554746682106245		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.7554746682106245 | validation: 0.8167115223729795]
	TIME [epoch: 0.926 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7952581732623644		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.7952581732623644 | validation: 1.0164935068222964]
	TIME [epoch: 0.928 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.855091851140819		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.855091851140819 | validation: 0.7814181865325263]
	TIME [epoch: 0.927 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8298328372696212		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.8298328372696212 | validation: 0.963713970236732]
	TIME [epoch: 0.927 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7888396528331056		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.7888396528331056 | validation: 0.7930830836763134]
	TIME [epoch: 0.927 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7515207464179872		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.7515207464179872 | validation: 0.8793285608640207]
	TIME [epoch: 0.927 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7520746707098441		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.7520746707098441 | validation: 0.7421603679389759]
	TIME [epoch: 0.927 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7679108404370553		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.7679108404370553 | validation: 1.0034257562359208]
	TIME [epoch: 0.932 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8266012815103229		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.8266012815103229 | validation: 0.8242537826194595]
	TIME [epoch: 0.929 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8172226843021606		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.8172226843021606 | validation: 0.8843955478351534]
	TIME [epoch: 0.931 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7930611725397941		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.7930611725397941 | validation: 0.7883621288440447]
	TIME [epoch: 0.929 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8079698668140054		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.8079698668140054 | validation: 0.9654521531022211]
	TIME [epoch: 0.93 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.776522763794926		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.776522763794926 | validation: 0.7719624156678124]
	TIME [epoch: 0.929 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7419804967647984		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.7419804967647984 | validation: 0.8821417750318359]
	TIME [epoch: 0.93 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7402660958551297		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.7402660958551297 | validation: 0.7609939336190905]
	TIME [epoch: 0.928 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8104833451364385		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.8104833451364385 | validation: 1.0583524932648272]
	TIME [epoch: 0.931 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8579712740649414		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.8579712740649414 | validation: 0.7630074784437038]
	TIME [epoch: 0.929 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7726770540149025		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.7726770540149025 | validation: 0.90680902966817]
	TIME [epoch: 0.928 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7458199435853663		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.7458199435853663 | validation: 0.7911266079249458]
	TIME [epoch: 0.929 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7475542746038241		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.7475542746038241 | validation: 0.8039939659521519]
	TIME [epoch: 0.929 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7855131234884075		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.7855131234884075 | validation: 1.0047841920905014]
	TIME [epoch: 0.93 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.829490905080802		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.829490905080802 | validation: 0.7896624423510069]
	TIME [epoch: 0.929 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8104938932660776		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.8104938932660776 | validation: 0.8450930111171777]
	TIME [epoch: 0.929 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7373853540704246		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.7373853540704246 | validation: 0.7440164130173859]
	TIME [epoch: 0.93 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.721480071803841		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.721480071803841 | validation: 0.9271883436415342]
	TIME [epoch: 0.929 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7335381348882803		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.7335381348882803 | validation: 0.7047294391711971]
	TIME [epoch: 0.93 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.791139277342299		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.791139277342299 | validation: 1.0197240328954187]
	TIME [epoch: 0.929 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8131527935401759		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.8131527935401759 | validation: 0.7460679769720269]
	TIME [epoch: 0.927 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7578046761164586		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.7578046761164586 | validation: 0.8867961923622553]
	TIME [epoch: 0.927 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7416411428787549		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.7416411428787549 | validation: 0.7364664477229752]
	TIME [epoch: 0.928 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7488586300255808		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.7488586300255808 | validation: 0.9625738371221164]
	TIME [epoch: 0.929 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7865094558134		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.7865094558134 | validation: 0.7420616248778242]
	TIME [epoch: 0.93 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.783743373145061		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.783743373145061 | validation: 0.871623570099009]
	TIME [epoch: 0.931 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7220895904487489		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.7220895904487489 | validation: 0.7587670777491676]
	TIME [epoch: 0.928 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7021793485635591		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.7021793485635591 | validation: 0.7864040433860725]
	TIME [epoch: 0.929 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6993062773055344		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.6993062773055344 | validation: 0.8090562717761186]
	TIME [epoch: 0.931 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6983672560283861		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.6983672560283861 | validation: 0.7233378213841465]
	TIME [epoch: 0.93 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7188193916659421		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.7188193916659421 | validation: 0.9816348380280449]
	TIME [epoch: 0.931 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7698867461474578		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.7698867461474578 | validation: 0.6810955006688915]
	TIME [epoch: 0.932 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_310.pth
	Model improved!!!
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9282857677283203		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.9282857677283203 | validation: 0.977738203024769]
	TIME [epoch: 0.932 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7563261925478532		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.7563261925478532 | validation: 0.8358481516560149]
	TIME [epoch: 0.931 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7477479164235337		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.7477479164235337 | validation: 0.6358636452409888]
	TIME [epoch: 0.932 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_313.pth
	Model improved!!!
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8310344811825059		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.8310344811825059 | validation: 1.0914514484004558]
	TIME [epoch: 0.931 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8371788919951471		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.8371788919951471 | validation: 0.9865051771309983]
	TIME [epoch: 0.931 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8805443090096384		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.8805443090096384 | validation: 0.6900549057872127]
	TIME [epoch: 0.931 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7446350678389223		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.7446350678389223 | validation: 0.8399005668772089]
	TIME [epoch: 0.931 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7414118392663344		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.7414118392663344 | validation: 0.7385490172932353]
	TIME [epoch: 0.931 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7122664785485088		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.7122664785485088 | validation: 0.8537276161563231]
	TIME [epoch: 0.931 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7037981825625415		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.7037981825625415 | validation: 0.759310596081614]
	TIME [epoch: 0.931 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7317023278724903		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.7317023278724903 | validation: 0.9321594850105397]
	TIME [epoch: 0.93 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8099746556981308		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.8099746556981308 | validation: 0.7988875465030256]
	TIME [epoch: 0.931 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8268890762549098		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.8268890762549098 | validation: 0.8990497026661732]
	TIME [epoch: 0.932 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7395535347511296		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.7395535347511296 | validation: 0.7376919306727046]
	TIME [epoch: 0.931 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7042944933609415		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.7042944933609415 | validation: 0.8621220618057189]
	TIME [epoch: 0.932 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7059096058551453		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.7059096058551453 | validation: 0.6938851355956235]
	TIME [epoch: 0.931 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7119769695860309		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.7119769695860309 | validation: 0.8892614953380887]
	TIME [epoch: 0.932 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7228782174290856		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.7228782174290856 | validation: 0.6921547625136486]
	TIME [epoch: 0.931 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7155025854695238		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.7155025854695238 | validation: 0.9023889384101423]
	TIME [epoch: 0.931 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7178501465270953		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.7178501465270953 | validation: 0.6767229429904149]
	TIME [epoch: 0.932 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7060888109637893		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.7060888109637893 | validation: 0.8853718856663542]
	TIME [epoch: 0.931 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7082605580505549		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.7082605580505549 | validation: 0.6916984655592943]
	TIME [epoch: 0.931 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6989184375172084		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.6989184375172084 | validation: 0.8831613079099478]
	TIME [epoch: 0.931 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7212503369240036		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.7212503369240036 | validation: 0.6552476843485927]
	TIME [epoch: 0.931 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7340083603714086		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.7340083603714086 | validation: 0.8996083061953428]
	TIME [epoch: 0.932 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7050887003499365		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.7050887003499365 | validation: 0.69653710477842]
	TIME [epoch: 0.931 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.686806341317312		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.686806341317312 | validation: 0.7914442369234019]
	TIME [epoch: 0.931 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6852027381403955		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.6852027381403955 | validation: 0.773344885214689]
	TIME [epoch: 0.931 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8665461928087783		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.8665461928087783 | validation: 1.056827175300876]
	TIME [epoch: 0.932 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9733066528374871		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.9733066528374871 | validation: 0.8525671954818954]
	TIME [epoch: 0.936 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.732738272983928		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.732738272983928 | validation: 0.72860404579693]
	TIME [epoch: 0.932 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6871630510739928		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.6871630510739928 | validation: 0.7965438885218863]
	TIME [epoch: 0.933 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7218221458100683		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.7218221458100683 | validation: 0.7003107317452133]
	TIME [epoch: 0.931 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.734979743663052		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.734979743663052 | validation: 0.8864829708985495]
	TIME [epoch: 0.931 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.710583560713727		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.710583560713727 | validation: 0.7545797106152916]
	TIME [epoch: 0.932 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6857082771970856		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.6857082771970856 | validation: 0.7599964520175228]
	TIME [epoch: 0.931 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6630589578599364		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.6630589578599364 | validation: 0.6989411981488859]
	TIME [epoch: 0.933 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6668724074070497		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.6668724074070497 | validation: 0.804019649413926]
	TIME [epoch: 0.931 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6859291080259121		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.6859291080259121 | validation: 0.7530750263780241]
	TIME [epoch: 0.931 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7626807598523524		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.7626807598523524 | validation: 0.9283934761042636]
	TIME [epoch: 0.93 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7808494399251368		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.7808494399251368 | validation: 0.7513464511163234]
	TIME [epoch: 0.932 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7367527801577453		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.7367527801577453 | validation: 0.789788839291934]
	TIME [epoch: 0.932 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6680847463179319		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.6680847463179319 | validation: 0.732174951326647]
	TIME [epoch: 0.932 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6576497947490616		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.6576497947490616 | validation: 0.7195521112891078]
	TIME [epoch: 0.932 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6529199637017424		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.6529199637017424 | validation: 0.741959730874513]
	TIME [epoch: 0.931 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6634579757625652		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.6634579757625652 | validation: 0.7460472466742724]
	TIME [epoch: 0.933 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6969764832514914		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.6969764832514914 | validation: 0.828550901317624]
	TIME [epoch: 0.932 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7660145884456651		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.7660145884456651 | validation: 0.861234340293292]
	TIME [epoch: 0.932 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.752220260365399		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.752220260365399 | validation: 0.6466219121072615]
	TIME [epoch: 0.93 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7243886537394733		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.7243886537394733 | validation: 0.9356999154443497]
	TIME [epoch: 0.932 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7249720986457683		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.7249720986457683 | validation: 0.7105501975980988]
	TIME [epoch: 0.93 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6767941837009098		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.6767941837009098 | validation: 0.6975883514848686]
	TIME [epoch: 0.932 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6430002394983222		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.6430002394983222 | validation: 0.7254995885250187]
	TIME [epoch: 0.93 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6538547194647913		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.6538547194647913 | validation: 0.714338149632907]
	TIME [epoch: 0.931 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7338365637379541		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.7338365637379541 | validation: 0.8628486918541413]
	TIME [epoch: 0.93 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7361049000288773		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.7361049000288773 | validation: 0.7373175574839602]
	TIME [epoch: 0.929 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7015721249784832		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.7015721249784832 | validation: 0.7339554097140888]
	TIME [epoch: 0.93 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.663752503950806		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.663752503950806 | validation: 0.7352991497141432]
	TIME [epoch: 0.931 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6555236698361334		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.6555236698361334 | validation: 0.6756981509407254]
	TIME [epoch: 0.93 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6672307171174603		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.6672307171174603 | validation: 0.8600001032541332]
	TIME [epoch: 0.93 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6902305987807055		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.6902305987807055 | validation: 0.5528983522017071]
	TIME [epoch: 0.928 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_371.pth
	Model improved!!!
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.762025725955366		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.762025725955366 | validation: 0.9347703426889877]
	TIME [epoch: 0.937 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6969539913500025		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.6969539913500025 | validation: 0.702440093270033]
	TIME [epoch: 0.93 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6270513943351743		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.6270513943351743 | validation: 0.6182301305664065]
	TIME [epoch: 0.931 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6372801468998199		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.6372801468998199 | validation: 0.8614770951217627]
	TIME [epoch: 0.929 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6718133062576975		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.6718133062576975 | validation: 0.6866058823008478]
	TIME [epoch: 0.928 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7321832097579634		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.7321832097579634 | validation: 0.7844009409854572]
	TIME [epoch: 0.928 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7063742550483977		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.7063742550483977 | validation: 0.8003142143834223]
	TIME [epoch: 0.928 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7380508644079256		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.7380508644079256 | validation: 0.6983697153315853]
	TIME [epoch: 0.93 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6686694979295251		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.6686694979295251 | validation: 0.692665709323055]
	TIME [epoch: 0.928 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6311343105732906		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.6311343105732906 | validation: 0.740631856909171]
	TIME [epoch: 0.93 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6313205994945896		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.6313205994945896 | validation: 0.6107938865673773]
	TIME [epoch: 0.928 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6422220439500466		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.6422220439500466 | validation: 0.7963236490236407]
	TIME [epoch: 0.93 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6466611701096059		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.6466611701096059 | validation: 0.6592334309940937]
	TIME [epoch: 0.927 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6589092106235375		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.6589092106235375 | validation: 0.797904733230117]
	TIME [epoch: 0.931 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7174459339494271		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.7174459339494271 | validation: 0.8106740167032256]
	TIME [epoch: 0.93 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7767825445133714		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.7767825445133714 | validation: 0.656560242167919]
	TIME [epoch: 0.928 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6545971594256176		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.6545971594256176 | validation: 0.7215322252856375]
	TIME [epoch: 0.926 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6105309560422605		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.6105309560422605 | validation: 0.6092393952750094]
	TIME [epoch: 0.931 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6159603071179615		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.6159603071179615 | validation: 0.7801494269473853]
	TIME [epoch: 0.931 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6280953345422401		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.6280953345422401 | validation: 0.5372731744903106]
	TIME [epoch: 0.928 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_391.pth
	Model improved!!!
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6569558658945697		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.6569558658945697 | validation: 0.9508616767716307]
	TIME [epoch: 0.929 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.717812496119369		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.717812496119369 | validation: 0.6358390684267304]
	TIME [epoch: 0.928 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.67231061449294		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.67231061449294 | validation: 0.6954970044309444]
	TIME [epoch: 0.927 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6478561051757622		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.6478561051757622 | validation: 0.8184834845116788]
	TIME [epoch: 0.927 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7100549829444305		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.7100549829444305 | validation: 0.6271978821085762]
	TIME [epoch: 0.927 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7111694595866672		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.7111694595866672 | validation: 0.7091691503532861]
	TIME [epoch: 0.928 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6291035282393509		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.6291035282393509 | validation: 0.7248754365065279]
	TIME [epoch: 0.928 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6141068684032844		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.6141068684032844 | validation: 0.552550408483173]
	TIME [epoch: 0.928 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6140570356114488		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.6140570356114488 | validation: 0.8223685330288518]
	TIME [epoch: 0.928 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.632029305752921		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.632029305752921 | validation: 0.5969482709426844]
	TIME [epoch: 0.93 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6325221976299574		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.6325221976299574 | validation: 0.7654538465791378]
	TIME [epoch: 0.931 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6645170222267759		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.6645170222267759 | validation: 0.6666974542859292]
	TIME [epoch: 0.932 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6641072530623383		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.6641072530623383 | validation: 0.6909444581811567]
	TIME [epoch: 0.929 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6548554434232231		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.6548554434232231 | validation: 0.7807000732616973]
	TIME [epoch: 0.928 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6453417677572669		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.6453417677572669 | validation: 0.5856051614473995]
	TIME [epoch: 0.928 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6607045376102179		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.6607045376102179 | validation: 0.7529347581900359]
	TIME [epoch: 0.926 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.61656648618221		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.61656648618221 | validation: 0.6690338740901028]
	TIME [epoch: 0.928 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.597938535493649		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.597938535493649 | validation: 0.5546421954423915]
	TIME [epoch: 0.926 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6287049106725836		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.6287049106725836 | validation: 0.809835896982325]
	TIME [epoch: 0.929 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.653998219433998		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.653998219433998 | validation: 0.5555268844004355]
	TIME [epoch: 0.928 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.635342879431003		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.635342879431003 | validation: 0.7360553102642104]
	TIME [epoch: 0.929 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6003849107693323		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.6003849107693323 | validation: 0.5820150599817707]
	TIME [epoch: 0.928 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5908437743756784		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.5908437743756784 | validation: 0.6987098354951083]
	TIME [epoch: 0.929 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5975891993379244		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.5975891993379244 | validation: 0.578393671666707]
	TIME [epoch: 0.928 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6156825445279657		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.6156825445279657 | validation: 0.7313429545428823]
	TIME [epoch: 0.93 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6516884947578925		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.6516884947578925 | validation: 0.7167919847563846]
	TIME [epoch: 0.929 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6579401355130289		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.6579401355130289 | validation: 0.5695275574436732]
	TIME [epoch: 0.929 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6953695458880432		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.6953695458880432 | validation: 0.877475563142535]
	TIME [epoch: 0.93 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6732484876672697		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.6732484876672697 | validation: 0.7249243509087862]
	TIME [epoch: 0.928 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6078816635896603		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.6078816635896603 | validation: 0.47965665079667374]
	TIME [epoch: 0.93 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_421.pth
	Model improved!!!
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5901734118479331		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.5901734118479331 | validation: 0.8077040358994856]
	TIME [epoch: 0.93 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6326472497350522		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.6326472497350522 | validation: 0.5087484505743988]
	TIME [epoch: 0.93 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6048888194547438		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.6048888194547438 | validation: 0.6949010059596014]
	TIME [epoch: 0.929 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5816543313320877		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.5816543313320877 | validation: 0.6315300774177135]
	TIME [epoch: 0.931 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6260315746291001		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.6260315746291001 | validation: 0.6651289379946518]
	TIME [epoch: 0.928 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6769450837403291		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.6769450837403291 | validation: 0.7085167327791515]
	TIME [epoch: 0.928 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6244665918468536		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.6244665918468536 | validation: 0.66703309630814]
	TIME [epoch: 0.93 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5812668047808706		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.5812668047808706 | validation: 0.5808131957218433]
	TIME [epoch: 0.93 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5715720304341168		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.5715720304341168 | validation: 0.639132963931775]
	TIME [epoch: 0.932 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5865444395204172		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.5865444395204172 | validation: 0.542613304865943]
	TIME [epoch: 0.931 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5944494132594151		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.5944494132594151 | validation: 0.7493647430956394]
	TIME [epoch: 0.932 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6129475675963847		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.6129475675963847 | validation: 0.5128694919839024]
	TIME [epoch: 0.928 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.609340810400126		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.609340810400126 | validation: 0.7669072428703858]
	TIME [epoch: 0.93 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5984086818504782		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.5984086818504782 | validation: 0.4633068271365158]
	TIME [epoch: 0.932 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_435.pth
	Model improved!!!
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6030656099455031		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.6030656099455031 | validation: 0.8321830808533224]
	TIME [epoch: 0.929 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6065679906169175		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.6065679906169175 | validation: 0.5175262650459653]
	TIME [epoch: 0.927 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5634552198432714		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.5634552198432714 | validation: 0.6447950354219705]
	TIME [epoch: 0.928 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.551458867515786		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.551458867515786 | validation: 0.5160564084738757]
	TIME [epoch: 0.928 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5500731869094615		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.5500731869094615 | validation: 0.6831596890390316]
	TIME [epoch: 0.927 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5758218413119446		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.5758218413119446 | validation: 0.48781831654699337]
	TIME [epoch: 0.929 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6564584483443738		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.6564584483443738 | validation: 0.8297917803423854]
	TIME [epoch: 0.928 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6744561814311475		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.6744561814311475 | validation: 0.521187985534643]
	TIME [epoch: 0.928 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5751950457849787		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.5751950457849787 | validation: 0.5718922039712308]
	TIME [epoch: 0.928 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5339226930245965		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.5339226930245965 | validation: 0.654854846389538]
	TIME [epoch: 0.928 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5562826989697854		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.5562826989697854 | validation: 0.4915272971208694]
	TIME [epoch: 0.929 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6230630681049171		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.6230630681049171 | validation: 0.7652858440870978]
	TIME [epoch: 0.928 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6402059879033941		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.6402059879033941 | validation: 0.6926267039661425]
	TIME [epoch: 0.929 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6133064987186023		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.6133064987186023 | validation: 0.48055577362173135]
	TIME [epoch: 0.929 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5852256981425606		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.5852256981425606 | validation: 0.6893889594836873]
	TIME [epoch: 0.929 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5732598377563954		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.5732598377563954 | validation: 0.46213436737152763]
	TIME [epoch: 0.928 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_451.pth
	Model improved!!!
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5452936611818762		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.5452936611818762 | validation: 0.6678991769041449]
	TIME [epoch: 0.93 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5326545507253799		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.5326545507253799 | validation: 0.46783171065969553]
	TIME [epoch: 0.93 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5412155450434518		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.5412155450434518 | validation: 0.6860483774341941]
	TIME [epoch: 0.929 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.558884607436907		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.558884607436907 | validation: 0.46846820531049416]
	TIME [epoch: 0.929 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5926264227629894		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.5926264227629894 | validation: 0.7508467636626327]
	TIME [epoch: 0.929 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6122677121312755		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.6122677121312755 | validation: 0.464709224455424]
	TIME [epoch: 0.929 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5664801914260784		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.5664801914260784 | validation: 0.6320703987009066]
	TIME [epoch: 0.929 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5306896888152062		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.5306896888152062 | validation: 0.5081203567772917]
	TIME [epoch: 0.928 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5335177485513152		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.5335177485513152 | validation: 0.5545019323299378]
	TIME [epoch: 0.929 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.56258080677827		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.56258080677827 | validation: 0.7825052247850964]
	TIME [epoch: 0.928 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6682282725871617		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.6682282725871617 | validation: 0.538674416502659]
	TIME [epoch: 0.929 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6175588185808865		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.6175588185808865 | validation: 0.5482404032891877]
	TIME [epoch: 0.929 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5365785866800494		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.5365785866800494 | validation: 0.6367181203479382]
	TIME [epoch: 0.929 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5235739996806502		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.5235739996806502 | validation: 0.390104982013227]
	TIME [epoch: 0.929 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_465.pth
	Model improved!!!
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5492056610022268		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.5492056610022268 | validation: 0.7659943507256459]
	TIME [epoch: 0.928 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5867545453090394		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.5867545453090394 | validation: 0.4667318704702857]
	TIME [epoch: 0.927 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.553645250415158		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.553645250415158 | validation: 0.6067080756485297]
	TIME [epoch: 0.926 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5320889760541908		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.5320889760541908 | validation: 0.5276745672046528]
	TIME [epoch: 0.927 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.526800335981105		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.526800335981105 | validation: 0.5729891062109965]
	TIME [epoch: 0.927 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5483763834187019		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.5483763834187019 | validation: 0.5864222819023313]
	TIME [epoch: 0.927 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.559471776976169		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.559471776976169 | validation: 0.6192865209011997]
	TIME [epoch: 0.927 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5547965583925969		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.5547965583925969 | validation: 0.5250772793700401]
	TIME [epoch: 0.926 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5266674460970356		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.5266674460970356 | validation: 0.5370675315500653]
	TIME [epoch: 0.927 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5209556951028268		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.5209556951028268 | validation: 0.554886872740817]
	TIME [epoch: 0.927 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5425125838863719		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.5425125838863719 | validation: 0.5151119443844043]
	TIME [epoch: 0.927 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5602480940519451		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.5602480940519451 | validation: 0.6003607617072764]
	TIME [epoch: 0.927 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5323354788711891		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.5323354788711891 | validation: 0.5026719264773692]
	TIME [epoch: 0.927 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5200009930116283		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.5200009930116283 | validation: 0.5574075107262336]
	TIME [epoch: 0.925 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5112340300947369		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.5112340300947369 | validation: 0.531946024059742]
	TIME [epoch: 0.927 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5260001084916556		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.5260001084916556 | validation: 0.514181701730405]
	TIME [epoch: 0.926 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5424297086702429		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.5424297086702429 | validation: 0.7119285851381681]
	TIME [epoch: 0.927 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5710883275656151		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.5710883275656151 | validation: 0.3499258992304539]
	TIME [epoch: 0.927 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_483.pth
	Model improved!!!
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5985061876678845		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.5985061876678845 | validation: 0.7709645657698463]
	TIME [epoch: 0.93 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5726104130761995		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.5726104130761995 | validation: 0.4770689184870002]
	TIME [epoch: 0.929 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4936421575191334		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.4936421575191334 | validation: 0.4926492826855781]
	TIME [epoch: 0.928 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4694024129680412		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.4694024129680412 | validation: 0.5017283954350423]
	TIME [epoch: 0.929 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47901423012104816		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.47901423012104816 | validation: 0.4845118630691772]
	TIME [epoch: 0.929 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4845188623248459		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.4845188623248459 | validation: 0.5888182792787988]
	TIME [epoch: 0.928 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5025194112497868		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.5025194112497868 | validation: 0.5514244054691811]
	TIME [epoch: 0.929 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5434492124385675		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.5434492124385675 | validation: 0.5368649066408296]
	TIME [epoch: 0.929 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5659983040473285		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.5659983040473285 | validation: 0.6427658456938808]
	TIME [epoch: 0.93 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5321335265985622		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.5321335265985622 | validation: 0.3663035101815668]
	TIME [epoch: 0.929 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5175585487443122		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.5175585487443122 | validation: 0.7143637932574839]
	TIME [epoch: 0.929 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5378932614414303		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.5378932614414303 | validation: 0.37733796365034716]
	TIME [epoch: 0.929 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5048354471583585		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.5048354471583585 | validation: 0.6150660906010108]
	TIME [epoch: 0.929 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4866026186164271		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.4866026186164271 | validation: 0.4096541296601146]
	TIME [epoch: 0.935 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.488457605304678		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.488457605304678 | validation: 0.5506125683042467]
	TIME [epoch: 0.929 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49371403482620324		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.49371403482620324 | validation: 0.4919957144887713]
	TIME [epoch: 0.929 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49999821245192805		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.49999821245192805 | validation: 0.5250594701666932]
	TIME [epoch: 0.929 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5170822061543011		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.5170822061543011 | validation: 0.523363211465495]
	TIME [epoch: 136 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49214968796047376		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.49214968796047376 | validation: 0.5316048075040903]
	TIME [epoch: 1.84 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4781138929578977		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.4781138929578977 | validation: 0.41354792798629325]
	TIME [epoch: 1.83 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47083785404557843		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.47083785404557843 | validation: 0.6049450172286139]
	TIME [epoch: 1.83 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4896406207484802		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.4896406207484802 | validation: 0.3454519137028503]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_505.pth
	Model improved!!!
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.503532400291539		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.503532400291539 | validation: 0.6905933173919774]
	TIME [epoch: 1.83 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5124922095649214		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.5124922095649214 | validation: 0.32943100965347866]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_507.pth
	Model improved!!!
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4874956043589429		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.4874956043589429 | validation: 0.5516141019531597]
	TIME [epoch: 1.83 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4644576664069238		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.4644576664069238 | validation: 0.35683529851117457]
	TIME [epoch: 1.83 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46302705832711577		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.46302705832711577 | validation: 0.6072669146729909]
	TIME [epoch: 1.83 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4662738700015568		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.4662738700015568 | validation: 0.36945973731274057]
	TIME [epoch: 1.83 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47345476477977905		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.47345476477977905 | validation: 0.5702791106785344]
	TIME [epoch: 1.83 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48065660596660137		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.48065660596660137 | validation: 0.41369984973113927]
	TIME [epoch: 1.83 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4721845753956647		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.4721845753956647 | validation: 0.5322840154926926]
	TIME [epoch: 1.83 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4828885460165553		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.4828885460165553 | validation: 0.5567910692864161]
	TIME [epoch: 1.83 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4939262134255218		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.4939262134255218 | validation: 0.4062817591879041]
	TIME [epoch: 1.83 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.491444376699325		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.491444376699325 | validation: 0.5655748430787118]
	TIME [epoch: 1.83 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4560028192911682		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.4560028192911682 | validation: 0.42194857074642833]
	TIME [epoch: 1.83 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41095596833952386		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.41095596833952386 | validation: 0.4130122601584618]
	TIME [epoch: 1.83 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4029139271168905		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.4029139271168905 | validation: 0.48063089152478594]
	TIME [epoch: 1.83 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.415239472513665		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.415239472513665 | validation: 0.3526320785157712]
	TIME [epoch: 1.83 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4432868203962913		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.4432868203962913 | validation: 0.6895616426367647]
	TIME [epoch: 1.83 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5530475349598076		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.5530475349598076 | validation: 0.3329645599250916]
	TIME [epoch: 1.84 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5568560910516825		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.5568560910516825 | validation: 0.5815701433244741]
	TIME [epoch: 1.83 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4394465046095134		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.4394465046095134 | validation: 0.41188472884713545]
	TIME [epoch: 1.83 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40234859460378325		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.40234859460378325 | validation: 0.4165570232930705]
	TIME [epoch: 1.83 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.401813121735469		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.401813121735469 | validation: 0.44534179532348994]
	TIME [epoch: 1.83 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4010938120297571		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.4010938120297571 | validation: 0.4545236523878233]
	TIME [epoch: 1.83 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4308105994534334		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.4308105994534334 | validation: 0.48000124782780945]
	TIME [epoch: 1.83 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4771374791750354		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.4771374791750354 | validation: 0.5302463505231442]
	TIME [epoch: 1.83 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4781095379168512		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.4781095379168512 | validation: 0.3616825868958821]
	TIME [epoch: 1.83 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43931842898890267		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.43931842898890267 | validation: 0.5619875243755328]
	TIME [epoch: 1.83 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43409551958419756		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.43409551958419756 | validation: 0.2639218296964556]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_533.pth
	Model improved!!!
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4942011964062584		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.4942011964062584 | validation: 0.628381193450938]
	TIME [epoch: 1.83 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45441741568963084		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.45441741568963084 | validation: 0.34697429120288087]
	TIME [epoch: 1.83 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41009229734020947		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.41009229734020947 | validation: 0.4578104664654687]
	TIME [epoch: 1.83 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40779157646985364		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.40779157646985364 | validation: 0.41236678383690734]
	TIME [epoch: 1.83 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40831823323804145		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.40831823323804145 | validation: 0.46741392807699206]
	TIME [epoch: 1.83 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41674198073575414		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.41674198073575414 | validation: 0.42999653767823126]
	TIME [epoch: 1.84 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4104178926090926		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.4104178926090926 | validation: 0.5136692266017406]
	TIME [epoch: 1.83 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4072412175267489		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.4072412175267489 | validation: 0.3198320822377637]
	TIME [epoch: 1.83 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41619215684669053		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.41619215684669053 | validation: 0.59561290457181]
	TIME [epoch: 1.83 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4481701649401883		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.4481701649401883 | validation: 0.2843086091894394]
	TIME [epoch: 1.83 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4376721999182833		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.4376721999182833 | validation: 0.5274719287558692]
	TIME [epoch: 1.83 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3971987502456095		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.3971987502456095 | validation: 0.3713206584024481]
	TIME [epoch: 1.83 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3782367362568488		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.3782367362568488 | validation: 0.40976500085752265]
	TIME [epoch: 1.83 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36061111473648166		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.36061111473648166 | validation: 0.40461087103432797]
	TIME [epoch: 1.83 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3588352518484051		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.3588352518484051 | validation: 0.42148721962206004]
	TIME [epoch: 1.83 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3780107525809946		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.3780107525809946 | validation: 0.4436046247230057]
	TIME [epoch: 1.83 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4160084293463486		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.4160084293463486 | validation: 0.5398371440138748]
	TIME [epoch: 1.83 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44522589767963866		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.44522589767963866 | validation: 0.3539415748857992]
	TIME [epoch: 1.83 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41350142312716587		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.41350142312716587 | validation: 0.4849358525298111]
	TIME [epoch: 1.83 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37354978438290587		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.37354978438290587 | validation: 0.3157117722404798]
	TIME [epoch: 1.83 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3735028590278786		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.3735028590278786 | validation: 0.5323278211797984]
	TIME [epoch: 1.83 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3992046056880851		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.3992046056880851 | validation: 0.2551795836439877]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_555.pth
	Model improved!!!
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42084626973022227		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.42084626973022227 | validation: 0.5351310070054071]
	TIME [epoch: 1.83 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39782699354253176		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.39782699354253176 | validation: 0.32492601782803854]
	TIME [epoch: 1.83 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3616911777768326		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.3616911777768326 | validation: 0.44573842139125397]
	TIME [epoch: 1.83 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3478041088869215		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.3478041088869215 | validation: 0.3462008606070025]
	TIME [epoch: 1.83 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36718014652701736		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.36718014652701736 | validation: 0.49372085994607834]
	TIME [epoch: 1.83 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3711679720179757		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.3711679720179757 | validation: 0.36384354268270275]
	TIME [epoch: 1.83 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37543019084603374		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.37543019084603374 | validation: 0.4887081060748056]
	TIME [epoch: 1.83 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3687524374406119		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.3687524374406119 | validation: 0.3204734520610842]
	TIME [epoch: 1.83 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3592888193013415		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.3592888193013415 | validation: 0.47605933486875823]
	TIME [epoch: 1.83 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35086209385039124		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.35086209385039124 | validation: 0.31299395840573196]
	TIME [epoch: 1.83 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34337186315365875		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.34337186315365875 | validation: 0.5292686291283134]
	TIME [epoch: 1.83 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3825183421479092		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.3825183421479092 | validation: 0.23879993655740794]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_567.pth
	Model improved!!!
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40271475195169637		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.40271475195169637 | validation: 0.46110691736112525]
	TIME [epoch: 1.83 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33901362191006246		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.33901362191006246 | validation: 0.3589898186462186]
	TIME [epoch: 1.83 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31996023828696957		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.31996023828696957 | validation: 0.38257778063835385]
	TIME [epoch: 1.83 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3153988715795336		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.3153988715795336 | validation: 0.3613682591456012]
	TIME [epoch: 1.84 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3236566586050589		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.3236566586050589 | validation: 0.46296858540678026]
	TIME [epoch: 1.83 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33951408071678457		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.33951408071678457 | validation: 0.3770713791994352]
	TIME [epoch: 1.83 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3814434720387149		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.3814434720387149 | validation: 0.5077352197612766]
	TIME [epoch: 1.83 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3953972606710001		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.3953972606710001 | validation: 0.32712102417536587]
	TIME [epoch: 1.83 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3383872802098311		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.3383872802098311 | validation: 0.4357835635358474]
	TIME [epoch: 1.83 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31258220114384616		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.31258220114384616 | validation: 0.29173686651417496]
	TIME [epoch: 1.83 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3161425217871209		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.3161425217871209 | validation: 0.48743004699144915]
	TIME [epoch: 1.83 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34452959409726375		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.34452959409726375 | validation: 0.2399736536995155]
	TIME [epoch: 1.83 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37081796079767243		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.37081796079767243 | validation: 0.4348879465108768]
	TIME [epoch: 1.83 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31765254394398		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.31765254394398 | validation: 0.3362757618996392]
	TIME [epoch: 1.83 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3084791670048813		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.3084791670048813 | validation: 0.4576458797767158]
	TIME [epoch: 1.83 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32037179883920275		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.32037179883920275 | validation: 0.36991030458676627]
	TIME [epoch: 1.83 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33172127710681293		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.33172127710681293 | validation: 0.4429912199471308]
	TIME [epoch: 1.83 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32287276659753916		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.32287276659753916 | validation: 0.36044813158179095]
	TIME [epoch: 1.83 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.317310630444457		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.317310630444457 | validation: 0.3824234492415222]
	TIME [epoch: 1.83 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3223989800795091		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.3223989800795091 | validation: 0.39963432345813027]
	TIME [epoch: 1.84 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31092834615209397		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.31092834615209397 | validation: 0.38062081329319036]
	TIME [epoch: 1.83 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2943770730994782		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.2943770730994782 | validation: 0.3417477971445668]
	TIME [epoch: 1.83 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2877532012758394		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.2877532012758394 | validation: 0.4237405069152662]
	TIME [epoch: 1.83 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29398847361645486		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.29398847361645486 | validation: 0.2524032554195694]
	TIME [epoch: 1.83 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3375539793150616		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.3375539793150616 | validation: 0.518414955759744]
	TIME [epoch: 1.83 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37465166727116866		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.37465166727116866 | validation: 0.2347730576587762]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_593.pth
	Model improved!!!
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34677679702418773		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.34677679702418773 | validation: 0.3915708031118538]
	TIME [epoch: 1.83 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27677183339949857		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.27677183339949857 | validation: 0.3509447476873626]
	TIME [epoch: 1.83 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2675418124999788		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.2675418124999788 | validation: 0.32407515247823265]
	TIME [epoch: 1.83 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2734042674763639		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.2734042674763639 | validation: 0.37289477064814114]
	TIME [epoch: 1.83 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28040584730024015		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.28040584730024015 | validation: 0.3466018542107726]
	TIME [epoch: 1.83 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2873334750496614		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.2873334750496614 | validation: 0.34971992372636684]
	TIME [epoch: 1.83 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27795259579786663		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.27795259579786663 | validation: 0.45240370169651717]
	TIME [epoch: 1.83 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29592871614628397		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.29592871614628397 | validation: 0.23383162273534236]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_601.pth
	Model improved!!!
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3408009464536814		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.3408009464536814 | validation: 0.49152202582077537]
	TIME [epoch: 1.83 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33509327841808656		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.33509327841808656 | validation: 0.23123400594943752]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_603.pth
	Model improved!!!
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31462360966287556		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.31462360966287556 | validation: 0.3989889663558354]
	TIME [epoch: 1.83 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25501877717009047		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.25501877717009047 | validation: 0.3253635777051301]
	TIME [epoch: 1.83 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.245654889827893		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.245654889827893 | validation: 0.36632555339784]
	TIME [epoch: 1.83 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24834893287111492		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.24834893287111492 | validation: 0.30847848592370597]
	TIME [epoch: 1.83 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2611583039155272		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.2611583039155272 | validation: 0.44630237227368713]
	TIME [epoch: 1.83 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2859382249212564		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.2859382249212564 | validation: 0.2861362240680479]
	TIME [epoch: 1.83 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31573427339249777		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.31573427339249777 | validation: 0.4651586966338462]
	TIME [epoch: 1.83 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2950319315064509		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.2950319315064509 | validation: 0.28705581052803447]
	TIME [epoch: 1.83 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2649812027055055		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.2649812027055055 | validation: 0.3676371522646697]
	TIME [epoch: 1.83 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24812364093067643		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.24812364093067643 | validation: 0.3208618453600034]
	TIME [epoch: 1.83 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2478705012433993		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.2478705012433993 | validation: 0.3884749516461383]
	TIME [epoch: 1.83 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.271752961856961		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.271752961856961 | validation: 0.39493283185487515]
	TIME [epoch: 1.83 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30672631013073004		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.30672631013073004 | validation: 0.3398896748811325]
	TIME [epoch: 1.83 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27205807082260963		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.27205807082260963 | validation: 0.3123057938999833]
	TIME [epoch: 1.83 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23427984316830658		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.23427984316830658 | validation: 0.36017552422890386]
	TIME [epoch: 1.84 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23283731462925186		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.23283731462925186 | validation: 0.264734478012331]
	TIME [epoch: 1.83 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24354152496521456		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.24354152496521456 | validation: 0.4237514019250818]
	TIME [epoch: 1.83 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27196750957695126		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.27196750957695126 | validation: 0.20177835391286167]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_621.pth
	Model improved!!!
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.308097399384802		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.308097399384802 | validation: 0.4496265876865641]
	TIME [epoch: 1.83 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2836377900345434		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.2836377900345434 | validation: 0.26568328346836845]
	TIME [epoch: 1.83 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.263957833215818		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.263957833215818 | validation: 0.3616601536491511]
	TIME [epoch: 1.83 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23758439549660637		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.23758439549660637 | validation: 0.3344863256967975]
	TIME [epoch: 1.83 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23103158607156313		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.23103158607156313 | validation: 0.32674058117607685]
	TIME [epoch: 1.83 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23766754176084695		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.23766754176084695 | validation: 0.30646919698708697]
	TIME [epoch: 1.83 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24513508000228876		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.24513508000228876 | validation: 0.41590489678073106]
	TIME [epoch: 1.83 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2570931670729036		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.2570931670729036 | validation: 0.24537934075278445]
	TIME [epoch: 1.83 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26507476092123994		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.26507476092123994 | validation: 0.38817133281833993]
	TIME [epoch: 1.83 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23911779931312624		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.23911779931312624 | validation: 0.22994580959041358]
	TIME [epoch: 1.83 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2389981727947887		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.2389981727947887 | validation: 0.38457524822278505]
	TIME [epoch: 1.83 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23073976373401947		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.23073976373401947 | validation: 0.24616140698093117]
	TIME [epoch: 1.83 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23649809524863238		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.23649809524863238 | validation: 0.4090928677080399]
	TIME [epoch: 1.83 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24054027648689805		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.24054027648689805 | validation: 0.19543278852992585]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_635.pth
	Model improved!!!
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2555837638291105		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.2555837638291105 | validation: 0.4020499681303929]
	TIME [epoch: 1.83 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24200259319699632		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.24200259319699632 | validation: 0.24716775442528302]
	TIME [epoch: 1.83 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21822070623483641		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.21822070623483641 | validation: 0.3669212960927386]
	TIME [epoch: 1.83 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21133091889553057		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.21133091889553057 | validation: 0.2433805280838091]
	TIME [epoch: 1.83 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22100586792744065		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.22100586792744065 | validation: 0.3906326516333758]
	TIME [epoch: 1.83 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23539985699300325		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.23539985699300325 | validation: 0.33618418140990514]
	TIME [epoch: 1.83 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2660031906957198		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.2660031906957198 | validation: 0.3624102385771049]
	TIME [epoch: 1.83 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24693917826264575		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.24693917826264575 | validation: 0.30712133680637876]
	TIME [epoch: 1.83 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2272158267177715		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.2272158267177715 | validation: 0.29084876436341606]
	TIME [epoch: 1.83 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21961225545837498		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.21961225545837498 | validation: 0.3013420417244061]
	TIME [epoch: 1.83 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20172698258820101		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.20172698258820101 | validation: 0.27638083665798824]
	TIME [epoch: 1.83 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19600399745216365		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.19600399745216365 | validation: 0.3312352021995666]
	TIME [epoch: 1.83 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19956629414566174		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.19956629414566174 | validation: 0.25927247519451857]
	TIME [epoch: 1.83 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19329695270856703		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.19329695270856703 | validation: 0.314267926439103]
	TIME [epoch: 1.83 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1961942884375116		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.1961942884375116 | validation: 0.25214775554837104]
	TIME [epoch: 1.83 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1948173624339587		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.1948173624339587 | validation: 0.3598965606157156]
	TIME [epoch: 1.84 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2172511756992373		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.2172511756992373 | validation: 0.2810140278492163]
	TIME [epoch: 1.83 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2855810727661123		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.2855810727661123 | validation: 0.4919284007971033]
	TIME [epoch: 1.83 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2931571221611704		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.2931571221611704 | validation: 0.2016566149743814]
	TIME [epoch: 1.83 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2290933830431294		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.2290933830431294 | validation: 0.3348776014653215]
	TIME [epoch: 1.83 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19453964169398139		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.19453964169398139 | validation: 0.2777721198206212]
	TIME [epoch: 1.83 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18593055336365116		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.18593055336365116 | validation: 0.31300746857404793]
	TIME [epoch: 1.83 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.185885363795854		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.185885363795854 | validation: 0.2834260806570891]
	TIME [epoch: 1.83 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19089072866104131		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.19089072866104131 | validation: 0.3012205274509381]
	TIME [epoch: 1.83 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2079767664697358		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.2079767664697358 | validation: 0.3018317144277013]
	TIME [epoch: 1.83 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22285118119888553		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.22285118119888553 | validation: 0.352941582477768]
	TIME [epoch: 1.83 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2137458340016817		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.2137458340016817 | validation: 0.2982350027176383]
	TIME [epoch: 1.83 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20004335920568073		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.20004335920568073 | validation: 0.2293134308302307]
	TIME [epoch: 1.83 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21227657005796446		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.21227657005796446 | validation: 0.38308778976171737]
	TIME [epoch: 1.83 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23233615654044149		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.23233615654044149 | validation: 0.1906898577472848]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_665.pth
	Model improved!!!
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23866588450782186		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.23866588450782186 | validation: 0.3468342545244178]
	TIME [epoch: 1.83 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19760927242636417		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.19760927242636417 | validation: 0.25397980003721277]
	TIME [epoch: 1.84 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18190308908122507		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.18190308908122507 | validation: 0.32860990505567433]
	TIME [epoch: 1.83 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18588937662169805		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.18588937662169805 | validation: 0.20267069012536]
	TIME [epoch: 1.83 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.192650069541345		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.192650069541345 | validation: 0.3327547206280402]
	TIME [epoch: 1.83 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19226180392684633		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.19226180392684633 | validation: 0.24453178870012435]
	TIME [epoch: 1.83 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1980165359042364		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.1980165359042364 | validation: 0.3461883086130585]
	TIME [epoch: 1.83 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19865879524788946		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.19865879524788946 | validation: 0.25035495167200333]
	TIME [epoch: 1.83 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19422490209640983		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.19422490209640983 | validation: 0.2973758381450657]
	TIME [epoch: 1.83 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18654196036987053		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.18654196036987053 | validation: 0.27520346307662163]
	TIME [epoch: 1.83 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18504490059724646		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.18504490059724646 | validation: 0.31246094900217847]
	TIME [epoch: 1.83 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17870125757150837		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.17870125757150837 | validation: 0.30025643577378924]
	TIME [epoch: 1.83 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19172692632120047		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.19172692632120047 | validation: 0.23918535027921176]
	TIME [epoch: 1.83 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20294650724888239		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.20294650724888239 | validation: 0.30201699827364026]
	TIME [epoch: 1.83 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1824255786017492		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.1824255786017492 | validation: 0.24425516291234134]
	TIME [epoch: 1.83 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17893885105483479		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.17893885105483479 | validation: 0.3058050015751973]
	TIME [epoch: 1.83 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17126673890799354		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.17126673890799354 | validation: 0.23362617862242657]
	TIME [epoch: 1.83 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16880737957705283		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.16880737957705283 | validation: 0.301924206177422]
	TIME [epoch: 1.84 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1710255641604577		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.1710255641604577 | validation: 0.1579898622035634]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_684.pth
	Model improved!!!
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2044071518430956		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.2044071518430956 | validation: 0.4281320929038745]
	TIME [epoch: 1.83 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24766583911247375		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.24766583911247375 | validation: 0.1937349315650838]
	TIME [epoch: 1.83 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22410014681122709		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.22410014681122709 | validation: 0.3094575530903057]
	TIME [epoch: 1.83 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1675000227438263		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.1675000227438263 | validation: 0.23797158055703005]
	TIME [epoch: 1.83 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15168128626579072		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.15168128626579072 | validation: 0.27793087470057953]
	TIME [epoch: 1.83 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1505910196615381		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.1505910196615381 | validation: 0.2352134633109551]
	TIME [epoch: 1.83 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15288378311306144		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.15288378311306144 | validation: 0.2846879110803191]
	TIME [epoch: 1.83 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15788419660370415		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.15788419660370415 | validation: 0.22457095138084437]
	TIME [epoch: 1.83 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17937675100460376		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.17937675100460376 | validation: 0.35195470636575216]
	TIME [epoch: 1.83 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21021823244327395		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.21021823244327395 | validation: 0.2861659362078839]
	TIME [epoch: 1.83 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19717780114210448		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.19717780114210448 | validation: 0.3247516489598876]
	TIME [epoch: 1.83 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17075850893353325		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.17075850893353325 | validation: 0.17916332339507768]
	TIME [epoch: 1.83 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17326404182262728		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.17326404182262728 | validation: 0.3547581133797816]
	TIME [epoch: 1.83 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1872284728198804		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.1872284728198804 | validation: 0.14964441559217548]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_698.pth
	Model improved!!!
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18006941519756198		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.18006941519756198 | validation: 0.26967963147267127]
	TIME [epoch: 1.84 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14848031749481747		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.14848031749481747 | validation: 0.2329141004086943]
	TIME [epoch: 1.84 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1554531952693155		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.1554531952693155 | validation: 0.2839372333789169]
	TIME [epoch: 1.84 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17298782384711905		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.17298782384711905 | validation: 0.2519073889965968]
	TIME [epoch: 1.84 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1916793545025266		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.1916793545025266 | validation: 0.3052044693663003]
	TIME [epoch: 1.84 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17452637194918652		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.17452637194918652 | validation: 0.1770939120646418]
	TIME [epoch: 1.84 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17262329288436248		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.17262329288436248 | validation: 0.37199315924047344]
	TIME [epoch: 1.84 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1995060773949078		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.1995060773949078 | validation: 0.1559887917550788]
	TIME [epoch: 1.83 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16737828012235145		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.16737828012235145 | validation: 0.27005069593855946]
	TIME [epoch: 1.84 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15070201533208344		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.15070201533208344 | validation: 0.2506799004825238]
	TIME [epoch: 1.84 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1490883303622819		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.1490883303622819 | validation: 0.23188758876539783]
	TIME [epoch: 1.83 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1519349709561961		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.1519349709561961 | validation: 0.25711250899790306]
	TIME [epoch: 1.84 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15927926546580973		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.15927926546580973 | validation: 0.27039324284287336]
	TIME [epoch: 1.83 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1700507581869939		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.1700507581869939 | validation: 0.21859609792808743]
	TIME [epoch: 1.84 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1555335313516509		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.1555335313516509 | validation: 0.26939078613577316]
	TIME [epoch: 1.84 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14808255022421882		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.14808255022421882 | validation: 0.2476059550138528]
	TIME [epoch: 1.84 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14893048449408772		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.14893048449408772 | validation: 0.27330275723784964]
	TIME [epoch: 1.84 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1672739132290839		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.1672739132290839 | validation: 0.20994750266697723]
	TIME [epoch: 1.84 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17720048052118859		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.17720048052118859 | validation: 0.3396054364698526]
	TIME [epoch: 1.83 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1669946722475591		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.1669946722475591 | validation: 0.14954770953725513]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_718.pth
	Model improved!!!
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17436570985998617		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.17436570985998617 | validation: 0.3453454481968996]
	TIME [epoch: 1.83 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17517968149215724		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.17517968149215724 | validation: 0.15456673922958553]
	TIME [epoch: 1.83 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1578951597209945		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.1578951597209945 | validation: 0.2745164438954083]
	TIME [epoch: 1.83 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14416709463008295		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.14416709463008295 | validation: 0.21867582990313827]
	TIME [epoch: 1.83 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13820944848979186		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.13820944848979186 | validation: 0.26298662301142295]
	TIME [epoch: 1.83 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14283611699799006		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.14283611699799006 | validation: 0.22741557984425712]
	TIME [epoch: 1.82 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14512373250048297		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.14512373250048297 | validation: 0.26343676934827587]
	TIME [epoch: 1.83 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14568330375769745		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.14568330375769745 | validation: 0.19361467531006102]
	TIME [epoch: 1.83 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14937005919884483		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.14937005919884483 | validation: 0.2940615964054554]
	TIME [epoch: 1.83 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15274927107080402		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.15274927107080402 | validation: 0.18890672330759525]
	TIME [epoch: 1.83 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15720553945198026		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.15720553945198026 | validation: 0.29194214147480924]
	TIME [epoch: 1.82 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.149146392198316		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.149146392198316 | validation: 0.18752916025458688]
	TIME [epoch: 1.83 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14300449632743642		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.14300449632743642 | validation: 0.20680412876394272]
	TIME [epoch: 1.84 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14842341645084267		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.14842341645084267 | validation: 0.2743636133300803]
	TIME [epoch: 1.83 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1500978305319463		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.1500978305319463 | validation: 0.22384961005309828]
	TIME [epoch: 1.82 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13534181757774902		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.13534181757774902 | validation: 0.191143930880201]
	TIME [epoch: 1.83 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13412247645921435		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.13412247645921435 | validation: 0.2788234622572234]
	TIME [epoch: 1.83 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13818381321939055		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.13818381321939055 | validation: 0.16206968403970845]
	TIME [epoch: 1.83 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13804910147693342		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.13804910147693342 | validation: 0.2605660975415222]
	TIME [epoch: 1.83 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13380216321163196		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.13380216321163196 | validation: 0.1562327997923525]
	TIME [epoch: 1.83 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13177750308012567		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.13177750308012567 | validation: 0.3641325138196396]
	TIME [epoch: 1.83 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19017811760543665		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.19017811760543665 | validation: 0.16502092445876157]
	TIME [epoch: 1.83 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19619032096657968		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.19619032096657968 | validation: 0.27304285555615165]
	TIME [epoch: 1.83 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14912856192050017		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.14912856192050017 | validation: 0.24342111816784007]
	TIME [epoch: 1.83 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12822305010306526		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.12822305010306526 | validation: 0.18559373379155808]
	TIME [epoch: 1.83 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12121712824817539		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.12121712824817539 | validation: 0.20287274911037284]
	TIME [epoch: 1.83 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11758006388354257		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.11758006388354257 | validation: 0.214894422270383]
	TIME [epoch: 1.83 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11846930032624058		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.11846930032624058 | validation: 0.2205437168433122]
	TIME [epoch: 1.83 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12527851986437974		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.12527851986437974 | validation: 0.20719494061974209]
	TIME [epoch: 1.83 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13776513545136418		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.13776513545136418 | validation: 0.27775993552974304]
	TIME [epoch: 1.83 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1731222027603224		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.1731222027603224 | validation: 0.2176352974319814]
	TIME [epoch: 1.82 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1553619301290464		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.1553619301290464 | validation: 0.25926242534199784]
	TIME [epoch: 1.83 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1359897649856793		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.1359897649856793 | validation: 0.13399565737221084]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_751.pth
	Model improved!!!
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1466416092023138		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.1466416092023138 | validation: 0.2983221603882981]
	TIME [epoch: 1.83 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1485371351205757		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.1485371351205757 | validation: 0.12957544463276702]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_753.pth
	Model improved!!!
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1375277023589418		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.1375277023589418 | validation: 0.24902154831409914]
	TIME [epoch: 1.83 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12449185943696868		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.12449185943696868 | validation: 0.1828716510105698]
	TIME [epoch: 1.83 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1262442163025143		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.1262442163025143 | validation: 0.2228215300958596]
	TIME [epoch: 1.83 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13193137657734158		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.13193137657734158 | validation: 0.20258932433829477]
	TIME [epoch: 1.83 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14023067117466648		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.14023067117466648 | validation: 0.27558493873564566]
	TIME [epoch: 1.83 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1374726031844467		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.1374726031844467 | validation: 0.12893671666894213]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_759.pth
	Model improved!!!
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14105038496348243		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.14105038496348243 | validation: 0.2883248675110632]
	TIME [epoch: 1.83 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13782122344103598		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.13782122344103598 | validation: 0.13965713041788938]
	TIME [epoch: 1.83 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13299095406373776		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.13299095406373776 | validation: 0.26471232576881737]
	TIME [epoch: 1.83 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1312062794243611		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.1312062794243611 | validation: 0.1790708660508159]
	TIME [epoch: 1.84 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1256558918857105		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.1256558918857105 | validation: 0.20153882132954576]
	TIME [epoch: 1.84 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12334072056367362		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.12334072056367362 | validation: 0.28970637771632857]
	TIME [epoch: 1.84 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14575442606514327		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.14575442606514327 | validation: 0.1679123908265298]
	TIME [epoch: 1.84 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11747155943405925		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.11747155943405925 | validation: 0.15952207434513088]
	TIME [epoch: 1.83 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11462040141256324		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.11462040141256324 | validation: 0.2773917311880854]
	TIME [epoch: 1.83 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13523640320155264		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.13523640320155264 | validation: 0.1539339606010098]
	TIME [epoch: 1.83 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13213068683396506		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.13213068683396506 | validation: 0.23940860319178184]
	TIME [epoch: 1.82 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13516782743266872		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.13516782743266872 | validation: 0.2178237704873367]
	TIME [epoch: 1.84 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.134756565571774		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.134756565571774 | validation: 0.2541240334477958]
	TIME [epoch: 1.82 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13066154445672984		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.13066154445672984 | validation: 0.13589745508354442]
	TIME [epoch: 1.83 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13252519274877922		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.13252519274877922 | validation: 0.2563822502621785]
	TIME [epoch: 1.83 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13509793480137924		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.13509793480137924 | validation: 0.16891022746821233]
	TIME [epoch: 1.83 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12797580446768347		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.12797580446768347 | validation: 0.2100131072479826]
	TIME [epoch: 1.83 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11780774591869583		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.11780774591869583 | validation: 0.17951729495647453]
	TIME [epoch: 1.83 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1147357700465301		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.1147357700465301 | validation: 0.20476228232198093]
	TIME [epoch: 1.83 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11662249484117945		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.11662249484117945 | validation: 0.1742018644736808]
	TIME [epoch: 1.84 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12326830649524535		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.12326830649524535 | validation: 0.21782498533444064]
	TIME [epoch: 1.83 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12613532041490302		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.12613532041490302 | validation: 0.16952018010776626]
	TIME [epoch: 1.83 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12396738752665128		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.12396738752665128 | validation: 0.26712978083261313]
	TIME [epoch: 1.83 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12958255769012747		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.12958255769012747 | validation: 0.1284955424665288]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_783.pth
	Model improved!!!
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12175115948608985		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.12175115948608985 | validation: 0.25342202946421166]
	TIME [epoch: 1.84 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12327021000268719		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.12327021000268719 | validation: 0.1216298767137975]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_785.pth
	Model improved!!!
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12549190525423484		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.12549190525423484 | validation: 0.2030930985441947]
	TIME [epoch: 1.83 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1143069768736715		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.1143069768736715 | validation: 0.16170776124025776]
	TIME [epoch: 1.83 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10825922487052472		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.10825922487052472 | validation: 0.19617294470046379]
	TIME [epoch: 1.83 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11000627080952186		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.11000627080952186 | validation: 0.20725228806579254]
	TIME [epoch: 1.83 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11714980157433509		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.11714980157433509 | validation: 0.23600133116200342]
	TIME [epoch: 1.83 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12412443706511496		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.12412443706511496 | validation: 0.13531374237081562]
	TIME [epoch: 1.83 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14337867536080626		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.14337867536080626 | validation: 0.24871037553721367]
	TIME [epoch: 1.83 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1267418212450917		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.1267418212450917 | validation: 0.1536592822042253]
	TIME [epoch: 1.83 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1105602297842735		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.1105602297842735 | validation: 0.22455060875664534]
	TIME [epoch: 1.83 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10747335384055953		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.10747335384055953 | validation: 0.14282022368014613]
	TIME [epoch: 1.84 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10707549030289207		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.10707549030289207 | validation: 0.2254207090181303]
	TIME [epoch: 1.83 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11679078086499907		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.11679078086499907 | validation: 0.11450009356969573]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_797.pth
	Model improved!!!
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12500976452469006		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.12500976452469006 | validation: 0.2533429348710534]
	TIME [epoch: 1.83 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11623653458565256		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.11623653458565256 | validation: 0.10716946162791574]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_799.pth
	Model improved!!!
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11624638143567953		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.11624638143567953 | validation: 0.23219452430899776]
	TIME [epoch: 1.83 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11497370504854257		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.11497370504854257 | validation: 0.14149690095427958]
	TIME [epoch: 1.84 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11388405044387323		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.11388405044387323 | validation: 0.2725506502652989]
	TIME [epoch: 1.83 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12728429935535668		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.12728429935535668 | validation: 0.14079000068486278]
	TIME [epoch: 1.84 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11904747326702739		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.11904747326702739 | validation: 0.1833765932063379]
	TIME [epoch: 1.84 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10794016909012553		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.10794016909012553 | validation: 0.22151020321165116]
	TIME [epoch: 1.84 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11167151159115335		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.11167151159115335 | validation: 0.14534228748727523]
	TIME [epoch: 1.84 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11496494273547316		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.11496494273547316 | validation: 0.15860550052578537]
	TIME [epoch: 1.84 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10400492597906265		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.10400492597906265 | validation: 0.2191528934886895]
	TIME [epoch: 1.84 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11207723543847692		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.11207723543847692 | validation: 0.11919119998967057]
	TIME [epoch: 1.84 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11485381291817753		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.11485381291817753 | validation: 0.22003506936684097]
	TIME [epoch: 1.84 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11795642108848092		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.11795642108848092 | validation: 0.15179181702946162]
	TIME [epoch: 1.84 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11852881336128228		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.11852881336128228 | validation: 0.20050027388809696]
	TIME [epoch: 1.84 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10980209160615206		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.10980209160615206 | validation: 0.13309555344409707]
	TIME [epoch: 1.84 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10085098326905158		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.10085098326905158 | validation: 0.1922854461122483]
	TIME [epoch: 1.84 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09912947005720288		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.09912947005720288 | validation: 0.13036389149414943]
	TIME [epoch: 1.84 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10090952569112291		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.10090952569112291 | validation: 0.16927595824700195]
	TIME [epoch: 1.84 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11080094546799689		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.11080094546799689 | validation: 0.21583592286714995]
	TIME [epoch: 1.84 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12236903114531934		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.12236903114531934 | validation: 0.1565177586349847]
	TIME [epoch: 1.84 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11213299810388777		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.11213299810388777 | validation: 0.15908065784772396]
	TIME [epoch: 1.84 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10032895820515587		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.10032895820515587 | validation: 0.17795710182963817]
	TIME [epoch: 1.84 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09978488894969377		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.09978488894969377 | validation: 0.14849338100870088]
	TIME [epoch: 1.84 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09601960552911758		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.09601960552911758 | validation: 0.12937320014049647]
	TIME [epoch: 1.83 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09463875585274564		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.09463875585274564 | validation: 0.17956637870915537]
	TIME [epoch: 1.83 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09622335462984104		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.09622335462984104 | validation: 0.1440623451644309]
	TIME [epoch: 1.84 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09983272496529143		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.09983272496529143 | validation: 0.1936173928920923]
	TIME [epoch: 1.84 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10606744772090375		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.10606744772090375 | validation: 0.18105634022880968]
	TIME [epoch: 1.84 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11587464070923044		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.11587464070923044 | validation: 0.1497932893646805]
	TIME [epoch: 1.84 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1110557084732264		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.1110557084732264 | validation: 0.22165332793321246]
	TIME [epoch: 1.83 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11578277628644303		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.11578277628644303 | validation: 0.09693220317576767]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_829.pth
	Model improved!!!
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13129121311738254		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.13129121311738254 | validation: 0.27559819152184656]
	TIME [epoch: 1.83 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1326328288789743		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.1326328288789743 | validation: 0.11363727718936115]
	TIME [epoch: 1.83 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10018610400597808		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.10018610400597808 | validation: 0.1214654485888605]
	TIME [epoch: 1.83 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09223813423792226		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.09223813423792226 | validation: 0.1855534074861264]
	TIME [epoch: 1.82 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09128183595005475		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.09128183595005475 | validation: 0.12377605993514207]
	TIME [epoch: 1.83 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08996676841490402		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.08996676841490402 | validation: 0.1470940827566632]
	TIME [epoch: 1.83 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0894413826714652		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.0894413826714652 | validation: 0.13805670178537452]
	TIME [epoch: 1.83 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09231494562382025		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.09231494562382025 | validation: 0.22371264083439574]
	TIME [epoch: 1.84 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10723694809722208		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.10723694809722208 | validation: 0.09506161893722252]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_838.pth
	Model improved!!!
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12816215941262402		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.12816215941262402 | validation: 0.2173688585181832]
	TIME [epoch: 1.84 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11963744463814704		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.11963744463814704 | validation: 0.14508486937293497]
	TIME [epoch: 1.84 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10082214876027216		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.10082214876027216 | validation: 0.14197019221417825]
	TIME [epoch: 1.83 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09152107358356197		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.09152107358356197 | validation: 0.12305682483756782]
	TIME [epoch: 1.84 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09092536467789211		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.09092536467789211 | validation: 0.15582083355503812]
	TIME [epoch: 1.84 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09114065829520396		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.09114065829520396 | validation: 0.13796628140584452]
	TIME [epoch: 1.84 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09707822894975532		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.09707822894975532 | validation: 0.1496749075724456]
	TIME [epoch: 1.84 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09783269552175851		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.09783269552175851 | validation: 0.14365800275664278]
	TIME [epoch: 1.84 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09092766479870307		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.09092766479870307 | validation: 0.17382264867638383]
	TIME [epoch: 1.83 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08848998032904856		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.08848998032904856 | validation: 0.10995585832145718]
	TIME [epoch: 2.58 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09071367553206136		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.09071367553206136 | validation: 0.17123105491825982]
	TIME [epoch: 1.84 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0879447988416614		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.0879447988416614 | validation: 0.13269350634810398]
	TIME [epoch: 1.83 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08628064521437034		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.08628064521437034 | validation: 0.1678210182602124]
	TIME [epoch: 1.83 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08963089691103898		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.08963089691103898 | validation: 0.09472063110203337]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_852.pth
	Model improved!!!
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10342285464864875		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.10342285464864875 | validation: 0.3410449276275257]
	TIME [epoch: 1.84 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17229646630462117		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.17229646630462117 | validation: 0.12794625344618407]
	TIME [epoch: 1.84 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1186672628795001		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.1186672628795001 | validation: 0.11950095119636597]
	TIME [epoch: 1.84 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1030740707434206		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.1030740707434206 | validation: 0.1796878859817146]
	TIME [epoch: 1.83 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09141199221620187		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.09141199221620187 | validation: 0.14180465380548823]
	TIME [epoch: 1.84 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0858154930812756		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.0858154930812756 | validation: 0.1283086579712129]
	TIME [epoch: 1.84 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08569572192138074		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.08569572192138074 | validation: 0.15199803341066442]
	TIME [epoch: 1.84 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08686098063238648		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.08686098063238648 | validation: 0.14872289277677345]
	TIME [epoch: 1.83 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0840994404004065		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.0840994404004065 | validation: 0.12148503442724548]
	TIME [epoch: 1.84 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08721807986281743		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.08721807986281743 | validation: 0.14568198889272568]
	TIME [epoch: 1.84 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08878975339755979		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.08878975339755979 | validation: 0.15807468333406233]
	TIME [epoch: 1.84 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0901232216659382		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.0901232216659382 | validation: 0.1578853231570486]
	TIME [epoch: 1.84 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09101703336041726		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.09101703336041726 | validation: 0.11040024353890968]
	TIME [epoch: 1.84 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0862864649000176		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.0862864649000176 | validation: 0.16961813390306013]
	TIME [epoch: 1.83 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08598154055682003		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.08598154055682003 | validation: 0.10133370004913232]
	TIME [epoch: 1.84 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09393063802827206		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.09393063802827206 | validation: 0.22295788045248896]
	TIME [epoch: 1.83 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12252820260138997		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.12252820260138997 | validation: 0.14951289039731472]
	TIME [epoch: 1.84 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12935051313893647		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.12935051313893647 | validation: 0.15164139539691546]
	TIME [epoch: 1.84 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.092278560154225		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.092278560154225 | validation: 0.14741488994809357]
	TIME [epoch: 1.84 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08198589889425074		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.08198589889425074 | validation: 0.0960211684928933]
	TIME [epoch: 1.84 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.084054581393677		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.084054581393677 | validation: 0.17871766231148975]
	TIME [epoch: 1.84 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0862842691463473		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.0862842691463473 | validation: 0.10630651897242692]
	TIME [epoch: 1.84 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08546260079550873		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.08546260079550873 | validation: 0.17481645043490446]
	TIME [epoch: 1.84 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08646193958332758		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.08646193958332758 | validation: 0.09662156785063647]
	TIME [epoch: 1.83 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0905309295442975		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.0905309295442975 | validation: 0.15755252046380863]
	TIME [epoch: 1.84 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08668231704700037		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.08668231704700037 | validation: 0.12718412685957045]
	TIME [epoch: 1.84 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08153159626089301		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.08153159626089301 | validation: 0.14825096018856385]
	TIME [epoch: 1.84 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08203083602428869		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.08203083602428869 | validation: 0.12319774824744933]
	TIME [epoch: 1.83 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07939234181470588		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.07939234181470588 | validation: 0.1298301806407292]
	TIME [epoch: 1.84 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08339488448566958		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.08339488448566958 | validation: 0.14809239162558016]
	TIME [epoch: 1.84 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09507286401430351		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.09507286401430351 | validation: 0.1433297871615643]
	TIME [epoch: 1.84 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1329849023677505		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.1329849023677505 | validation: 0.20608388548209533]
	TIME [epoch: 1.84 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11000195205987769		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.11000195205987769 | validation: 0.10777700536123208]
	TIME [epoch: 1.84 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08435776690985033		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.08435776690985033 | validation: 0.11002210777102908]
	TIME [epoch: 1.84 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08515318944026401		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.08515318944026401 | validation: 0.17084194555535703]
	TIME [epoch: 1.84 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08857030186650681		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.08857030186650681 | validation: 0.11007510322511871]
	TIME [epoch: 1.84 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0811343007056491		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.0811343007056491 | validation: 0.13212067182016038]
	TIME [epoch: 1.84 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07607447271653961		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.07607447271653961 | validation: 0.13305798936701607]
	TIME [epoch: 1.84 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07705660982445886		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.07705660982445886 | validation: 0.12357118385543729]
	TIME [epoch: 1.84 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08166375397252448		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.08166375397252448 | validation: 0.14396509408007432]
	TIME [epoch: 1.83 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08839706182076187		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.08839706182076187 | validation: 0.1631450671972995]
	TIME [epoch: 1.84 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09583201892860946		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.09583201892860946 | validation: 0.09250316004748285]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_894.pth
	Model improved!!!
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09803861669433982		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.09803861669433982 | validation: 0.1874068056491329]
	TIME [epoch: 1.84 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09274837109073741		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.09274837109073741 | validation: 0.09375804447474657]
	TIME [epoch: 1.84 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08585771313933173		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.08585771313933173 | validation: 0.15948597222067618]
	TIME [epoch: 1.84 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08524986123629567		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.08524986123629567 | validation: 0.12413322327930616]
	TIME [epoch: 1.84 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0807890844831752		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.0807890844831752 | validation: 0.13046960482962397]
	TIME [epoch: 1.84 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08462122346291116		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.08462122346291116 | validation: 0.15690831724251872]
	TIME [epoch: 1.84 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08506260888839277		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.08506260888839277 | validation: 0.1180466010493885]
	TIME [epoch: 1.84 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08258488430304597		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.08258488430304597 | validation: 0.12392755518636074]
	TIME [epoch: 1.83 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07817366209903685		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.07817366209903685 | validation: 0.17505511519088812]
	TIME [epoch: 1.84 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08308348514521374		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.08308348514521374 | validation: 0.08292438309651055]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_904.pth
	Model improved!!!
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08450436848687268		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.08450436848687268 | validation: 0.18123942849845615]
	TIME [epoch: 1.84 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0888782356128947		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.0888782356128947 | validation: 0.1063158316740822]
	TIME [epoch: 1.84 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08671811349131084		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.08671811349131084 | validation: 0.19804304474679998]
	TIME [epoch: 1.83 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09064339626122074		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.09064339626122074 | validation: 0.0808698235908045]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_908.pth
	Model improved!!!
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08319840782969633		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.08319840782969633 | validation: 0.1562905705425423]
	TIME [epoch: 1.84 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07967404089200067		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.07967404089200067 | validation: 0.09771412127741086]
	TIME [epoch: 1.84 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0751939238921633		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.0751939238921633 | validation: 0.1231517550026406]
	TIME [epoch: 1.84 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07392317689915794		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.07392317689915794 | validation: 0.1269767777839157]
	TIME [epoch: 1.84 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07956769257572976		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.07956769257572976 | validation: 0.10587797838417877]
	TIME [epoch: 1.84 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08734317652897972		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.08734317652897972 | validation: 0.14164769634718458]
	TIME [epoch: 1.84 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09089349769530351		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.09089349769530351 | validation: 0.1237356536817924]
	TIME [epoch: 1.84 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08134324406946018		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.08134324406946018 | validation: 0.11089995977520291]
	TIME [epoch: 1.83 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07383222859097104		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.07383222859097104 | validation: 0.1252338387357909]
	TIME [epoch: 1.83 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07417857242497605		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.07417857242497605 | validation: 0.10855774129169365]
	TIME [epoch: 1.84 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07155307162245465		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.07155307162245465 | validation: 0.10689698063644021]
	TIME [epoch: 1.83 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07493698026268038		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.07493698026268038 | validation: 0.16000812772536788]
	TIME [epoch: 1.84 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08511039584987108		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.08511039584987108 | validation: 0.14158202973449066]
	TIME [epoch: 1.84 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0859506457483865		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.0859506457483865 | validation: 0.13338471180580855]
	TIME [epoch: 1.84 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08704318669935628		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.08704318669935628 | validation: 0.1634955399065246]
	TIME [epoch: 1.83 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08021144665891948		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.08021144665891948 | validation: 0.07282772932874]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_924.pth
	Model improved!!!
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08494103242396875		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.08494103242396875 | validation: 0.17475102903025286]
	TIME [epoch: 1.84 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08455125200509864		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.08455125200509864 | validation: 0.11020828593314214]
	TIME [epoch: 1.84 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07918859078419974		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.07918859078419974 | validation: 0.13638599651806224]
	TIME [epoch: 1.84 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07891404432124242		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.07891404432124242 | validation: 0.09401307977366635]
	TIME [epoch: 1.84 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07859483864890622		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.07859483864890622 | validation: 0.11591519903411383]
	TIME [epoch: 1.84 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07954902332736893		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.07954902332736893 | validation: 0.11521074838443535]
	TIME [epoch: 1.84 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07640569610591062		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.07640569610591062 | validation: 0.1086158041467876]
	TIME [epoch: 1.84 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07437283436215894		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.07437283436215894 | validation: 0.12231395064427666]
	TIME [epoch: 1.84 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07309185239068153		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.07309185239068153 | validation: 0.13329412657634004]
	TIME [epoch: 1.84 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07207952581194874		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.07207952581194874 | validation: 0.0856546731344272]
	TIME [epoch: 1.83 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07471453222237603		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.07471453222237603 | validation: 0.15142938574378634]
	TIME [epoch: 1.83 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07846780915633776		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.07846780915633776 | validation: 0.09282572844151528]
	TIME [epoch: 1.83 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07875860762259451		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.07875860762259451 | validation: 0.10473507383211406]
	TIME [epoch: 1.84 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08347259350685106		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.08347259350685106 | validation: 0.12899791056780216]
	TIME [epoch: 1.84 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07663779016759031		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.07663779016759031 | validation: 0.1234575004398228]
	TIME [epoch: 1.84 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07066755231598246		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.07066755231598246 | validation: 0.08991751429201349]
	TIME [epoch: 1.84 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07469626560712282		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.07469626560712282 | validation: 0.1502433393126988]
	TIME [epoch: 1.83 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07811416051386343		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.07811416051386343 | validation: 0.0842677264659685]
	TIME [epoch: 1.84 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07841586107108496		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.07841586107108496 | validation: 0.13063936115963234]
	TIME [epoch: 1.84 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0743813332021199		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.0743813332021199 | validation: 0.08063259781312329]
	TIME [epoch: 1.83 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0794856977734267		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.0794856977734267 | validation: 0.19478691777825077]
	TIME [epoch: 1.84 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09473234117220504		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.09473234117220504 | validation: 0.08348794560058315]
	TIME [epoch: 1.84 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08549134753857437		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.08549134753857437 | validation: 0.09007823307907584]
	TIME [epoch: 1.84 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0722925698894738		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.0722925698894738 | validation: 0.13180836548920052]
	TIME [epoch: 1.83 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0705442869797629		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.0705442869797629 | validation: 0.07476444569897188]
	TIME [epoch: 1.83 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07335389749868342		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.07335389749868342 | validation: 0.14083556623603488]
	TIME [epoch: 1.84 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07163355273238789		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.07163355273238789 | validation: 0.10427191485415643]
	TIME [epoch: 1.84 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0703656787680423		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.0703656787680423 | validation: 0.10774871621665894]
	TIME [epoch: 1.84 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07199225244780143		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.07199225244780143 | validation: 0.10186490348964812]
	TIME [epoch: 1.84 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07397889542026798		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.07397889542026798 | validation: 0.14415158925183175]
	TIME [epoch: 1.84 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07341751081709803		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.07341751081709803 | validation: 0.06961982781492251]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_955.pth
	Model improved!!!
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07545253434105285		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.07545253434105285 | validation: 0.17792507931960955]
	TIME [epoch: 1.83 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08087395152797411		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.08087395152797411 | validation: 0.08471615736954802]
	TIME [epoch: 1.84 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07103764653618472		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.07103764653618472 | validation: 0.10880150711180465]
	TIME [epoch: 1.83 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06911609764968044		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.06911609764968044 | validation: 0.11017986342343132]
	TIME [epoch: 1.83 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07350970706610649		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.07350970706610649 | validation: 0.09784310826417658]
	TIME [epoch: 1.84 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0735882883322462		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.0735882883322462 | validation: 0.09726944895090027]
	TIME [epoch: 1.83 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06906539273856495		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.06906539273856495 | validation: 0.11820950553514478]
	TIME [epoch: 1.84 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06865800350818428		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.06865800350818428 | validation: 0.10187591776255928]
	TIME [epoch: 1.84 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0707143419749189		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.0707143419749189 | validation: 0.1405744424506223]
	TIME [epoch: 1.83 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07072132173016359		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.07072132173016359 | validation: 0.13024404003885592]
	TIME [epoch: 1.83 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0723402046097835		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.0723402046097835 | validation: 0.09658201905354034]
	TIME [epoch: 1.83 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0760340343473811		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.0760340343473811 | validation: 0.13063720183533964]
	TIME [epoch: 1.83 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0765449378860448		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.0765449378860448 | validation: 0.088525546124048]
	TIME [epoch: 1.83 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07224049494527834		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.07224049494527834 | validation: 0.11175613412242398]
	TIME [epoch: 1.83 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06940665007417325		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.06940665007417325 | validation: 0.08679078090105767]
	TIME [epoch: 1.84 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0687912683778663		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.0687912683778663 | validation: 0.10992777092296352]
	TIME [epoch: 1.83 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06964014789642495		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.06964014789642495 | validation: 0.07575471942939908]
	TIME [epoch: 1.84 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0689703952889811		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.0689703952889811 | validation: 0.11429789486559248]
	TIME [epoch: 1.83 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07345607688303439		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.07345607688303439 | validation: 0.10949987070885157]
	TIME [epoch: 1.84 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06949382887099455		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.06949382887099455 | validation: 0.10114548982487195]
	TIME [epoch: 1.84 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06461458634751052		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.06461458634751052 | validation: 0.08560520868256526]
	TIME [epoch: 1.83 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06209159240219004		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.06209159240219004 | validation: 0.0979956655606012]
	TIME [epoch: 1.83 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06184098675421553		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.06184098675421553 | validation: 0.09515533570702404]
	TIME [epoch: 1.83 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06395114093389397		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.06395114093389397 | validation: 0.10824923828646393]
	TIME [epoch: 1.83 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06358457958858317		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.06358457958858317 | validation: 0.07638471375989031]
	TIME [epoch: 1.84 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0676328535163707		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.0676328535163707 | validation: 0.1918177230762671]
	TIME [epoch: 1.83 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08461052699330605		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.08461052699330605 | validation: 0.06717333751315643]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_982.pth
	Model improved!!!
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09300817360484946		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.09300817360484946 | validation: 0.14604670948226206]
	TIME [epoch: 1.83 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07870961058879296		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.07870961058879296 | validation: 0.10997189224696449]
	TIME [epoch: 1.83 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07863703810698511		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.07863703810698511 | validation: 0.09346533182269615]
	TIME [epoch: 1.83 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07211114662010508		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.07211114662010508 | validation: 0.08290074520662213]
	TIME [epoch: 1.84 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07427107821377413		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.07427107821377413 | validation: 0.11586289853889836]
	TIME [epoch: 1.83 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06600099948019325		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.06600099948019325 | validation: 0.09349083799260244]
	TIME [epoch: 1.84 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06430148076733465		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.06430148076733465 | validation: 0.08812791258308078]
	TIME [epoch: 1.84 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0649574549544044		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.0649574549544044 | validation: 0.11392499632838167]
	TIME [epoch: 1.84 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06496313038359146		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.06496313038359146 | validation: 0.09619974951045229]
	TIME [epoch: 1.84 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06510958257763759		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.06510958257763759 | validation: 0.09043596873011188]
	TIME [epoch: 1.83 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062401510596647294		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.062401510596647294 | validation: 0.10364573448079997]
	TIME [epoch: 1.84 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06634207859626677		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.06634207859626677 | validation: 0.10302946092411469]
	TIME [epoch: 1.83 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06823130777239021		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.06823130777239021 | validation: 0.0898811318641779]
	TIME [epoch: 1.83 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07021666938663691		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.07021666938663691 | validation: 0.10858711415351674]
	TIME [epoch: 1.84 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07275093716070422		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.07275093716070422 | validation: 0.08741724733821478]
	TIME [epoch: 1.83 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06773036090788251		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.06773036090788251 | validation: 0.09947540036674436]
	TIME [epoch: 1.83 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06521817330730804		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.06521817330730804 | validation: 0.08469547314548106]
	TIME [epoch: 1.84 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06371403847938527		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.06371403847938527 | validation: 0.09653983318156863]
	TIME [epoch: 1.83 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06680764254375453		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.06680764254375453 | validation: 0.07762137091174356]
	TIME [epoch: 140 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0654465994107364		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.0654465994107364 | validation: 0.1170200768843339]
	TIME [epoch: 3.97 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06715602827604547		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.06715602827604547 | validation: 0.08061010625248104]
	TIME [epoch: 3.96 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06480113377383516		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.06480113377383516 | validation: 0.0926103757512137]
	TIME [epoch: 3.97 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06161980081387827		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.06161980081387827 | validation: 0.07403755069586655]
	TIME [epoch: 3.97 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06242364234916835		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.06242364234916835 | validation: 0.1263172262705732]
	TIME [epoch: 3.97 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.067463113126907		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.067463113126907 | validation: 0.07415554973676822]
	TIME [epoch: 3.97 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07495536355535437		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.07495536355535437 | validation: 0.14672514812137624]
	TIME [epoch: 3.96 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0766951934620813		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.0766951934620813 | validation: 0.09639781163466735]
	TIME [epoch: 3.97 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062356015660503736		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.062356015660503736 | validation: 0.06056255910679288]
	TIME [epoch: 3.96 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_1010.pth
	Model improved!!!
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06581548311140979		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.06581548311140979 | validation: 0.12379949782482542]
	TIME [epoch: 3.96 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06519471688446879		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.06519471688446879 | validation: 0.08185244675213572]
	TIME [epoch: 3.97 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06441321249981095		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.06441321249981095 | validation: 0.11028109321467013]
	TIME [epoch: 3.97 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06311138991609315		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.06311138991609315 | validation: 0.0893600605980723]
	TIME [epoch: 3.97 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06233056565865593		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.06233056565865593 | validation: 0.09001646809789943]
	TIME [epoch: 3.96 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06130760108140049		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.06130760108140049 | validation: 0.0925521017682393]
	TIME [epoch: 3.97 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061213753902382775		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.061213753902382775 | validation: 0.11383692337043297]
	TIME [epoch: 3.96 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06481441700497836		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.06481441700497836 | validation: 0.05765109608961001]
	TIME [epoch: 3.96 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_1018.pth
	Model improved!!!
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07490372910576752		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.07490372910576752 | validation: 0.12928050628231488]
	TIME [epoch: 3.97 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07145842461062353		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.07145842461062353 | validation: 0.08585661965099645]
	TIME [epoch: 3.96 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06389458722265096		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.06389458722265096 | validation: 0.08913642084415649]
	TIME [epoch: 3.96 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06387162782339126		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.06387162782339126 | validation: 0.11667367705023364]
	TIME [epoch: 3.97 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0644677699894447		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.0644677699894447 | validation: 0.08934968412676775]
	TIME [epoch: 3.96 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061451382653198924		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.061451382653198924 | validation: 0.07871217229173912]
	TIME [epoch: 3.96 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06791849923847076		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.06791849923847076 | validation: 0.08787317935451033]
	TIME [epoch: 3.96 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06587965823179856		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.06587965823179856 | validation: 0.08570057798775109]
	TIME [epoch: 3.96 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060713587979789006		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.060713587979789006 | validation: 0.1086583897975618]
	TIME [epoch: 3.97 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061720139825095756		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.061720139825095756 | validation: 0.07810477466143671]
	TIME [epoch: 3.96 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06470634989928739		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.06470634989928739 | validation: 0.11284146410321463]
	TIME [epoch: 3.96 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06516291895512862		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.06516291895512862 | validation: 0.08220954152140007]
	TIME [epoch: 3.96 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0634277797893123		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.0634277797893123 | validation: 0.08941574013804789]
	TIME [epoch: 3.96 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06152607769687153		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.06152607769687153 | validation: 0.08673323245082706]
	TIME [epoch: 3.96 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05969045867121528		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.05969045867121528 | validation: 0.10458336669553298]
	TIME [epoch: 3.96 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05960984158980079		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.05960984158980079 | validation: 0.10873705072020104]
	TIME [epoch: 3.97 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058324265902062765		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.058324265902062765 | validation: 0.07489666942550349]
	TIME [epoch: 3.96 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062047969687439794		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.062047969687439794 | validation: 0.11512833439332898]
	TIME [epoch: 3.97 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06443702288022687		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.06443702288022687 | validation: 0.06564399678955882]
	TIME [epoch: 3.96 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06181789146065965		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.06181789146065965 | validation: 0.09437008624941423]
	TIME [epoch: 3.96 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05892934679571482		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.05892934679571482 | validation: 0.10686500700396447]
	TIME [epoch: 3.96 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05941744305353625		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.05941744305353625 | validation: 0.07670476259777391]
	TIME [epoch: 3.96 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05853122913080408		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.05853122913080408 | validation: 0.1305829921330373]
	TIME [epoch: 3.96 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07072183922927079		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.07072183922927079 | validation: 0.07187914920077759]
	TIME [epoch: 3.97 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07206356107791245		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.07206356107791245 | validation: 0.09625402534801834]
	TIME [epoch: 3.96 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06219576432909969		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.06219576432909969 | validation: 0.07373923995665788]
	TIME [epoch: 3.96 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056087869700420734		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.056087869700420734 | validation: 0.08627863782013204]
	TIME [epoch: 3.96 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05781796343673855		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.05781796343673855 | validation: 0.09183046092890607]
	TIME [epoch: 3.96 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05967945483478894		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.05967945483478894 | validation: 0.09169172987053546]
	TIME [epoch: 3.96 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05925113047906784		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.05925113047906784 | validation: 0.08807606005965919]
	TIME [epoch: 3.96 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060788884272341255		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.060788884272341255 | validation: 0.0705464264512695]
	TIME [epoch: 3.97 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05921993272585725		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.05921993272585725 | validation: 0.10059840157732605]
	TIME [epoch: 3.96 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06331084055583952		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.06331084055583952 | validation: 0.07446073125254361]
	TIME [epoch: 3.97 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06323848515344546		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.06323848515344546 | validation: 0.09716614927963409]
	TIME [epoch: 3.96 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06186310170424859		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.06186310170424859 | validation: 0.08532160578794674]
	TIME [epoch: 3.96 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057148528656101955		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.057148528656101955 | validation: 0.06057193400901483]
	TIME [epoch: 3.96 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058538362250432525		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.058538362250432525 | validation: 0.09333538162628713]
	TIME [epoch: 3.96 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05826060821398337		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.05826060821398337 | validation: 0.07493871494537473]
	TIME [epoch: 3.96 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05900897833434608		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.05900897833434608 | validation: 0.10726257508225834]
	TIME [epoch: 3.97 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060034720949958285		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.060034720949958285 | validation: 0.07590696468183102]
	TIME [epoch: 3.96 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06152675608037795		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.06152675608037795 | validation: 0.09815025845898884]
	TIME [epoch: 3.96 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06095922157125726		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.06095922157125726 | validation: 0.08579207445870825]
	TIME [epoch: 3.97 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06148410156237501		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.06148410156237501 | validation: 0.07786298323860012]
	TIME [epoch: 3.96 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059896745362659785		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.059896745362659785 | validation: 0.0791519841057224]
	TIME [epoch: 3.96 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05960981309619008		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.05960981309619008 | validation: 0.10381642016535389]
	TIME [epoch: 3.96 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058355853875974466		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.058355853875974466 | validation: 0.06703951742988912]
	TIME [epoch: 3.97 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057850618055256896		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.057850618055256896 | validation: 0.08742115605811078]
	TIME [epoch: 3.96 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058844684445497356		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.058844684445497356 | validation: 0.08630267455997423]
	TIME [epoch: 3.96 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05837547685893903		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.05837547685893903 | validation: 0.08656267922867614]
	TIME [epoch: 3.96 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056059077812710856		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.056059077812710856 | validation: 0.06963763251098562]
	TIME [epoch: 3.96 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05663410386970556		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.05663410386970556 | validation: 0.1382556806096846]
	TIME [epoch: 3.96 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0645326832321365		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.0645326832321365 | validation: 0.0707051009711435]
	TIME [epoch: 3.96 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0584059987183065		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.0584059987183065 | validation: 0.07999525261232629]
	TIME [epoch: 3.96 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057461239256262175		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.057461239256262175 | validation: 0.08338907602989828]
	TIME [epoch: 3.97 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056873715072262326		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.056873715072262326 | validation: 0.10029913533679223]
	TIME [epoch: 3.96 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05819279665449333		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.05819279665449333 | validation: 0.0736614891352856]
	TIME [epoch: 3.96 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05996086925896808		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.05996086925896808 | validation: 0.07490402944841447]
	TIME [epoch: 3.96 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05826784019293996		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.05826784019293996 | validation: 0.08419940810976813]
	TIME [epoch: 3.96 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05798822203987511		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.05798822203987511 | validation: 0.09462631832787953]
	TIME [epoch: 3.96 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055331482943741905		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.055331482943741905 | validation: 0.07192243664220797]
	TIME [epoch: 3.96 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05455641388355496		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.05455641388355496 | validation: 0.08900067509243786]
	TIME [epoch: 3.97 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056075994141365965		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.056075994141365965 | validation: 0.06271203935992622]
	TIME [epoch: 3.96 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058459369741565174		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.058459369741565174 | validation: 0.09478787215228478]
	TIME [epoch: 3.96 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05842630982724147		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.05842630982724147 | validation: 0.06250334135720381]
	TIME [epoch: 3.96 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05973361519917043		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.05973361519917043 | validation: 0.11079076480438942]
	TIME [epoch: 3.96 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0577294000415559		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.0577294000415559 | validation: 0.0608187112935987]
	TIME [epoch: 3.96 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0558479275835613		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.0558479275835613 | validation: 0.07889445490475311]
	TIME [epoch: 3.96 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053410846207152744		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.053410846207152744 | validation: 0.08660302623431726]
	TIME [epoch: 3.96 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053482887368761674		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.053482887368761674 | validation: 0.06537765394402885]
	TIME [epoch: 3.97 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054920242309343975		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.054920242309343975 | validation: 0.09105050201182639]
	TIME [epoch: 3.96 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05539664864320736		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.05539664864320736 | validation: 0.06906985319065843]
	TIME [epoch: 3.96 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056890974650060835		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.056890974650060835 | validation: 0.07172345200073342]
	TIME [epoch: 3.96 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05423827125170595		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.05423827125170595 | validation: 0.08364412227015773]
	TIME [epoch: 3.96 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057264363012991135		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.057264363012991135 | validation: 0.07914344828648548]
	TIME [epoch: 3.97 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06051315011834969		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.06051315011834969 | validation: 0.09543445086166619]
	TIME [epoch: 3.96 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05924045624385732		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.05924045624385732 | validation: 0.06309761838939097]
	TIME [epoch: 3.96 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05445884069862416		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.05445884069862416 | validation: 0.0808399553469113]
	TIME [epoch: 3.97 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05167803311774944		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.05167803311774944 | validation: 0.0843639087482909]
	TIME [epoch: 3.96 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05375364203069417		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.05375364203069417 | validation: 0.06770487834749751]
	TIME [epoch: 3.96 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05151261216092575		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.05151261216092575 | validation: 0.0694979831507666]
	TIME [epoch: 3.96 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05442663100071057		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.05442663100071057 | validation: 0.08723083089701873]
	TIME [epoch: 3.97 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05496658381800351		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.05496658381800351 | validation: 0.07823623844036628]
	TIME [epoch: 3.97 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05347167777310503		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.05347167777310503 | validation: 0.10401174507862446]
	TIME [epoch: 3.96 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05551747352743492		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.05551747352743492 | validation: 0.05350543863475324]
	TIME [epoch: 3.97 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_1102.pth
	Model improved!!!
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051237989895916894		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.051237989895916894 | validation: 0.07977673952039827]
	TIME [epoch: 3.96 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05541292967936134		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.05541292967936134 | validation: 0.06270151895754224]
	TIME [epoch: 3.97 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056482606853352664		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.056482606853352664 | validation: 0.1265570582287084]
	TIME [epoch: 3.96 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06445911382041156		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.06445911382041156 | validation: 0.0802812207387162]
	TIME [epoch: 3.97 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056999381335600736		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.056999381335600736 | validation: 0.06310246342537167]
	TIME [epoch: 3.96 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052438408774994574		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.052438408774994574 | validation: 0.10491171041835323]
	TIME [epoch: 3.96 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05407932803863472		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.05407932803863472 | validation: 0.07611134800233077]
	TIME [epoch: 3.97 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05432027461767985		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.05432027461767985 | validation: 0.08004988888669974]
	TIME [epoch: 3.96 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05285538394375021		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.05285538394375021 | validation: 0.0662737335558452]
	TIME [epoch: 3.96 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051976957149660274		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.051976957149660274 | validation: 0.09171324607432732]
	TIME [epoch: 3.96 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05122656995682045		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.05122656995682045 | validation: 0.07093532901318074]
	TIME [epoch: 3.96 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059210473236342286		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.059210473236342286 | validation: 0.07554656705981028]
	TIME [epoch: 3.96 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06226861634241066		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.06226861634241066 | validation: 0.07304823891294819]
	TIME [epoch: 3.96 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05164989070091689		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.05164989070091689 | validation: 0.06993075308335767]
	TIME [epoch: 3.96 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04996919601969165		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.04996919601969165 | validation: 0.06784825202757788]
	TIME [epoch: 3.97 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05083083404585688		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.05083083404585688 | validation: 0.08034895342694426]
	TIME [epoch: 3.96 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05306655919745846		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.05306655919745846 | validation: 0.063749141868739]
	TIME [epoch: 3.96 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05024773854126243		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.05024773854126243 | validation: 0.09751166731718718]
	TIME [epoch: 3.96 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0580644576792003		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.0580644576792003 | validation: 0.07224748912725285]
	TIME [epoch: 3.96 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061128112777993904		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.061128112777993904 | validation: 0.08240338198722251]
	TIME [epoch: 3.96 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05396848617631545		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.05396848617631545 | validation: 0.07584713455309108]
	TIME [epoch: 3.96 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0519994185871181		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.0519994185871181 | validation: 0.07509658083700985]
	TIME [epoch: 3.97 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05150022411154529		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.05150022411154529 | validation: 0.0777744147729396]
	TIME [epoch: 3.96 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05114158914908613		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.05114158914908613 | validation: 0.06520533416751063]
	TIME [epoch: 3.96 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052856966084381896		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.052856966084381896 | validation: 0.08576504040539405]
	TIME [epoch: 3.96 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0526218902534439		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.0526218902534439 | validation: 0.04899551569131118]
	TIME [epoch: 3.97 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_1128.pth
	Model improved!!!
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05987863321218599		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.05987863321218599 | validation: 0.07840778003816089]
	TIME [epoch: 3.96 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05189963435954668		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.05189963435954668 | validation: 0.07262344943673125]
	TIME [epoch: 3.96 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051458066757489464		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.051458066757489464 | validation: 0.09433037326970879]
	TIME [epoch: 3.96 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05205502819083344		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.05205502819083344 | validation: 0.05606551037972623]
	TIME [epoch: 3.97 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05237578660565956		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.05237578660565956 | validation: 0.07725878057383978]
	TIME [epoch: 3.96 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055006516786604245		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.055006516786604245 | validation: 0.0679055056762654]
	TIME [epoch: 3.96 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05327599089015903		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.05327599089015903 | validation: 0.06584986214088782]
	TIME [epoch: 3.96 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05235579795212694		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.05235579795212694 | validation: 0.07455904232115486]
	TIME [epoch: 3.97 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05020022944733726		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.05020022944733726 | validation: 0.05832682837535789]
	TIME [epoch: 3.96 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051469022360822514		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.051469022360822514 | validation: 0.08305970411018734]
	TIME [epoch: 3.96 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050344158616396475		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.050344158616396475 | validation: 0.06648698176714532]
	TIME [epoch: 3.97 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051107585290444596		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.051107585290444596 | validation: 0.0721924710319233]
	TIME [epoch: 3.96 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049990625261470946		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.049990625261470946 | validation: 0.0685344868685697]
	TIME [epoch: 3.96 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051690923133638156		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.051690923133638156 | validation: 0.07177976151348585]
	TIME [epoch: 3.96 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0523854941802287		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.0523854941802287 | validation: 0.0692863367881087]
	TIME [epoch: 3.96 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05256083273713136		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.05256083273713136 | validation: 0.06701688000527488]
	TIME [epoch: 3.96 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05145486430731874		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.05145486430731874 | validation: 0.07179777668721157]
	TIME [epoch: 3.96 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04866615405035231		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.04866615405035231 | validation: 0.09185120957127357]
	TIME [epoch: 3.96 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05246879992042691		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.05246879992042691 | validation: 0.05952859064418029]
	TIME [epoch: 3.97 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05054084824853493		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.05054084824853493 | validation: 0.07332787192892785]
	TIME [epoch: 3.96 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05131521584980872		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.05131521584980872 | validation: 0.07641999349637224]
	TIME [epoch: 3.96 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05265709904269427		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.05265709904269427 | validation: 0.06999490000315954]
	TIME [epoch: 3.96 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05452111316654053		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.05452111316654053 | validation: 0.06153976632119837]
	TIME [epoch: 3.96 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05480291073144247		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.05480291073144247 | validation: 0.0982995959588622]
	TIME [epoch: 3.96 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05536859468850938		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.05536859468850938 | validation: 0.07207043658473576]
	TIME [epoch: 3.96 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05035351503140555		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.05035351503140555 | validation: 0.06925560883519952]
	TIME [epoch: 3.97 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04831874714782912		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.04831874714782912 | validation: 0.05854533187558465]
	TIME [epoch: 3.96 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049367720563955615		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.049367720563955615 | validation: 0.0774938469101783]
	TIME [epoch: 3.96 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04989213623993288		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.04989213623993288 | validation: 0.05971148280436266]
	TIME [epoch: 3.96 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05018514842429527		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.05018514842429527 | validation: 0.07311640736314189]
	TIME [epoch: 3.96 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05142482363444341		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.05142482363444341 | validation: 0.07902734935746773]
	TIME [epoch: 3.96 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04879640743328232		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.04879640743328232 | validation: 0.062474280937792684]
	TIME [epoch: 3.96 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05087778161168968		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.05087778161168968 | validation: 0.06710027921589339]
	TIME [epoch: 3.97 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04749802780910703		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.04749802780910703 | validation: 0.07258493894545671]
	TIME [epoch: 3.97 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049787060780322634		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.049787060780322634 | validation: 0.07662297919086865]
	TIME [epoch: 3.96 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05224796401851858		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.05224796401851858 | validation: 0.05682091402672973]
	TIME [epoch: 3.96 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051157561582674226		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.051157561582674226 | validation: 0.09105381756267272]
	TIME [epoch: 3.96 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05101422456902894		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.05101422456902894 | validation: 0.06190278707855049]
	TIME [epoch: 3.96 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04912176314215289		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.04912176314215289 | validation: 0.05713766526055977]
	TIME [epoch: 3.96 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05244891655267505		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.05244891655267505 | validation: 0.06829256732829754]
	TIME [epoch: 3.96 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0484687831004188		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.0484687831004188 | validation: 0.07104480813248991]
	TIME [epoch: 3.96 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04761520074060416		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.04761520074060416 | validation: 0.053095751852885224]
	TIME [epoch: 3.97 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05039235017576619		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.05039235017576619 | validation: 0.0897857850539837]
	TIME [epoch: 3.96 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05024452617175155		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.05024452617175155 | validation: 0.059907784163203984]
	TIME [epoch: 3.96 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04895922210233641		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.04895922210233641 | validation: 0.09483309544939787]
	TIME [epoch: 3.96 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051316998756293995		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.051316998756293995 | validation: 0.060692671885339056]
	TIME [epoch: 3.96 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04594376723370295		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.04594376723370295 | validation: 0.06285219176373051]
	TIME [epoch: 3.97 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04710900164938662		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.04710900164938662 | validation: 0.06490590746485524]
	TIME [epoch: 3.96 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04668982972809751		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.04668982972809751 | validation: 0.07052413615548063]
	TIME [epoch: 3.97 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047398438525682964		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.047398438525682964 | validation: 0.06309204284186887]
	TIME [epoch: 3.96 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05347406599817985		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.05347406599817985 | validation: 0.08672427109653534]
	TIME [epoch: 3.96 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053406660451718276		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.053406660451718276 | validation: 0.06811591190550101]
	TIME [epoch: 3.96 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047804431610549064		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.047804431610549064 | validation: 0.060591814487698015]
	TIME [epoch: 3.96 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04941803416521685		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.04941803416521685 | validation: 0.07475242446091178]
	TIME [epoch: 3.96 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04920267751068299		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.04920267751068299 | validation: 0.05112663756476621]
	TIME [epoch: 3.96 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051141211373397846		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.051141211373397846 | validation: 0.0647162560359369]
	TIME [epoch: 3.96 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04922216554906345		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.04922216554906345 | validation: 0.06152910512368085]
	TIME [epoch: 3.97 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04785098122085242		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.04785098122085242 | validation: 0.06577057291162279]
	TIME [epoch: 3.96 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04752903171833843		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.04752903171833843 | validation: 0.0654776415479084]
	TIME [epoch: 3.96 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04614971974332655		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.04614971974332655 | validation: 0.0687044736760992]
	TIME [epoch: 3.96 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05004915155064125		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.05004915155064125 | validation: 0.06619999002285444]
	TIME [epoch: 3.96 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04731106568650938		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.04731106568650938 | validation: 0.06998075256096785]
	TIME [epoch: 3.96 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04652479275942065		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.04652479275942065 | validation: 0.06911934682534565]
	TIME [epoch: 3.96 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046708603918230604		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.046708603918230604 | validation: 0.06882878274593535]
	TIME [epoch: 3.97 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04722145035385653		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.04722145035385653 | validation: 0.057181827451236594]
	TIME [epoch: 3.96 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047072349712875175		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.047072349712875175 | validation: 0.055572492796423616]
	TIME [epoch: 3.96 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048400985215558974		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.048400985215558974 | validation: 0.06953670360567786]
	TIME [epoch: 3.96 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04636877220664377		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.04636877220664377 | validation: 0.05156592145495832]
	TIME [epoch: 3.96 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047674287539691135		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.047674287539691135 | validation: 0.09359305406428238]
	TIME [epoch: 3.96 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05320283782121025		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.05320283782121025 | validation: 0.053757095102235676]
	TIME [epoch: 3.96 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04745697434849868		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.04745697434849868 | validation: 0.060984385595630776]
	TIME [epoch: 3.96 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04616402473165555		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.04616402473165555 | validation: 0.06844390763266144]
	TIME [epoch: 3.97 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04606808367051473		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.04606808367051473 | validation: 0.06137098793108038]
	TIME [epoch: 3.96 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04620265779906037		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.04620265779906037 | validation: 0.06341976689631056]
	TIME [epoch: 3.97 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04458751025585727		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.04458751025585727 | validation: 0.0619864779717578]
	TIME [epoch: 3.96 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049198189957826176		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.049198189957826176 | validation: 0.06466770248401325]
	TIME [epoch: 3.96 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050975265436446676		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.050975265436446676 | validation: 0.06678564773287962]
	TIME [epoch: 3.96 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04925644956939593		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.04925644956939593 | validation: 0.06723294090232693]
	TIME [epoch: 3.96 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04546030889289832		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.04546030889289832 | validation: 0.07001448768584805]
	TIME [epoch: 3.97 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04564757103967531		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.04564757103967531 | validation: 0.04999442686307618]
	TIME [epoch: 3.96 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04792691909889173		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.04792691909889173 | validation: 0.05564822864740332]
	TIME [epoch: 3.96 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04524178581522561		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.04524178581522561 | validation: 0.06257960904380779]
	TIME [epoch: 3.96 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04617019610876952		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.04617019610876952 | validation: 0.05470142341485541]
	TIME [epoch: 3.96 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04734118614538682		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.04734118614538682 | validation: 0.06986743828675594]
	TIME [epoch: 3.97 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046543662649715614		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.046543662649715614 | validation: 0.05031333545861946]
	TIME [epoch: 3.96 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04642625466618172		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.04642625466618172 | validation: 0.06470038751333793]
	TIME [epoch: 3.96 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044201941543933455		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.044201941543933455 | validation: 0.0624239864769458]
	TIME [epoch: 3.97 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04752104033918376		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.04752104033918376 | validation: 0.06386661182940027]
	TIME [epoch: 3.96 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047706407797216785		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.047706407797216785 | validation: 0.07643626221248226]
	TIME [epoch: 3.96 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04994030497559324		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.04994030497559324 | validation: 0.055664717629739825]
	TIME [epoch: 3.96 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04570797790635831		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.04570797790635831 | validation: 0.05176746634245896]
	TIME [epoch: 3.96 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047771098097513065		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.047771098097513065 | validation: 0.0678144676778956]
	TIME [epoch: 3.96 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04545568919180405		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.04545568919180405 | validation: 0.05611646956380271]
	TIME [epoch: 3.96 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04669913414066429		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.04669913414066429 | validation: 0.054673239609101934]
	TIME [epoch: 3.97 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045994595253197657		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.045994595253197657 | validation: 0.07269018660164663]
	TIME [epoch: 3.96 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04743887416358931		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.04743887416358931 | validation: 0.07276529648523714]
	TIME [epoch: 3.96 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0503487867126476		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.0503487867126476 | validation: 0.06550774205934506]
	TIME [epoch: 3.96 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044729119297389486		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.044729119297389486 | validation: 0.05494217127929055]
	TIME [epoch: 3.96 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04796406106314811		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.04796406106314811 | validation: 0.06963627734151534]
	TIME [epoch: 3.96 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045922590626375874		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.045922590626375874 | validation: 0.05615197223927482]
	TIME [epoch: 3.96 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0459567331961853		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.0459567331961853 | validation: 0.0680494992729614]
	TIME [epoch: 3.96 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_11_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_11_v_mmd4_1229.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 2787.262 seconds.
