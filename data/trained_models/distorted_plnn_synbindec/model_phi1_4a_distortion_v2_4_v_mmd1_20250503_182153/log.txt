Args:
Namespace(name='model_phi1_4a_distortion_v2_4_v_mmd1', outdir='out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_4/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_4/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2550492158

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.61853644612059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.61853644612059 | validation: 3.8616320863235942]
	TIME [epoch: 157 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.115825408476544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.115825408476544 | validation: 4.883595416885236]
	TIME [epoch: 1.03 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.415002532119644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.415002532119644 | validation: 3.933707810540068]
	TIME [epoch: 0.706 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.942247648965676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.942247648965676 | validation: 3.2671858408688372]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7396850730379483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7396850730379483 | validation: 3.90439108602679]
	TIME [epoch: 0.705 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.823396897519647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.823396897519647 | validation: 3.5368614729642602]
	TIME [epoch: 0.705 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4831563088680504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4831563088680504 | validation: 2.852060221145873]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.178071766367515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.178071766367515 | validation: 2.5885876610573764]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9458870004013273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9458870004013273 | validation: 2.785511031084572]
	TIME [epoch: 0.708 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7710648819164523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7710648819164523 | validation: 3.1889530168764884]
	TIME [epoch: 0.703 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.25492824641659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.25492824641659 | validation: 2.397150969954403]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.455014411700318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.455014411700318 | validation: 2.0427441741570056]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.301506972861562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.301506972861562 | validation: 2.6258476270883224]
	TIME [epoch: 0.699 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5301720842997173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5301720842997173 | validation: 2.1956766062972664]
	TIME [epoch: 0.701 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4377880938727587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4377880938727587 | validation: 1.9887539989836747]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9762288679460613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9762288679460613 | validation: 1.7128041329994521]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8198209744621034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8198209744621034 | validation: 1.7392723850116596]
	TIME [epoch: 0.7 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7270974809670412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7270974809670412 | validation: 1.5869283168986372]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8633046604458758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8633046604458758 | validation: 2.9148503566846484]
	TIME [epoch: 0.699 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.765074048259962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.765074048259962 | validation: 1.56886878376256]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6307547501479138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6307547501479138 | validation: 1.0871857767237725]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8988835342025185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8988835342025185 | validation: 2.2971382490378516]
	TIME [epoch: 0.705 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2025381898173455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2025381898173455 | validation: 1.6163915838219283]
	TIME [epoch: 0.704 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7287277978311317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7287277978311317 | validation: 1.0820338435378063]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6302212615230962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6302212615230962 | validation: 1.2665675608406284]
	TIME [epoch: 0.704 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.523646666619998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.523646666619998 | validation: 1.3995369396944002]
	TIME [epoch: 0.7 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.530822115239434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.530822115239434 | validation: 1.1659588075923573]
	TIME [epoch: 0.703 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4980872537628216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4980872537628216 | validation: 1.1778438661255306]
	TIME [epoch: 0.703 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5001944365905957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5001944365905957 | validation: 1.425118485082647]
	TIME [epoch: 0.702 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.589106762927763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.589106762927763 | validation: 1.4179119317226103]
	TIME [epoch: 0.699 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6831103724792886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6831103724792886 | validation: 1.3316136280908009]
	TIME [epoch: 0.697 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5492290645984679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5492290645984679 | validation: 1.2385980114422201]
	TIME [epoch: 0.697 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.465553243727007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.465553243727007 | validation: 1.0902109051649302]
	TIME [epoch: 0.698 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4302521818795406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4302521818795406 | validation: 1.1806340331682632]
	TIME [epoch: 0.697 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4193263682444208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4193263682444208 | validation: 1.18301439757138]
	TIME [epoch: 0.698 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4134576893109574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4134576893109574 | validation: 1.0898580725305849]
	TIME [epoch: 0.697 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4163297998075945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4163297998075945 | validation: 1.2370422364682165]
	TIME [epoch: 0.699 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4117626673804875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4117626673804875 | validation: 1.0507718262140957]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4108373700300445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4108373700300445 | validation: 1.238684092703548]
	TIME [epoch: 0.703 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4127782248376184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4127782248376184 | validation: 1.0907111768971933]
	TIME [epoch: 0.701 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4026752478570288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4026752478570288 | validation: 1.2156909593119523]
	TIME [epoch: 0.707 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4062374773748127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4062374773748127 | validation: 1.0808942938982424]
	TIME [epoch: 0.701 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.409438549102757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.409438549102757 | validation: 1.2899005005261956]
	TIME [epoch: 0.701 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.415447632300264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.415447632300264 | validation: 1.0201429964802333]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4142111049218136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4142111049218136 | validation: 1.3098962014215645]
	TIME [epoch: 0.702 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4194242878567016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4194242878567016 | validation: 1.078447100577494]
	TIME [epoch: 0.7 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.385858614911316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.385858614911316 | validation: 1.1946420426150144]
	TIME [epoch: 0.699 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.37323323243244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.37323323243244 | validation: 1.170698311504274]
	TIME [epoch: 0.7 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3654595973235741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3654595973235741 | validation: 1.1580222717636361]
	TIME [epoch: 0.698 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3607047842043287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3607047842043287 | validation: 1.1513971107393832]
	TIME [epoch: 0.699 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.367086508357348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.367086508357348 | validation: 1.157756140765423]
	TIME [epoch: 0.7 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.360223468008483		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.360223468008483 | validation: 1.1457687314218163]
	TIME [epoch: 0.699 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3594589275475193		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.3594589275475193 | validation: 1.1502838016228212]
	TIME [epoch: 0.698 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3554682606756858		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.3554682606756858 | validation: 1.1738486330380227]
	TIME [epoch: 0.699 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.358438701904702		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.358438701904702 | validation: 1.0990076605637575]
	TIME [epoch: 0.699 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.367508227280364		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.367508227280364 | validation: 1.435842659695241]
	TIME [epoch: 0.697 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4362128142737391		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.4362128142737391 | validation: 1.01661836600854]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3939119449705422		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.3939119449705422 | validation: 1.4017562479266739]
	TIME [epoch: 0.707 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.401377284403437		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.401377284403437 | validation: 0.9655527393488829]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4159836984660696		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.4159836984660696 | validation: 1.4935074063615947]
	TIME [epoch: 0.703 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4940219121474672		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.4940219121474672 | validation: 1.210887910650055]
	TIME [epoch: 0.703 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3442054856243824		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.3442054856243824 | validation: 0.9792980984091259]
	TIME [epoch: 0.7 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3997025771132905		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.3997025771132905 | validation: 1.3575084304780143]
	TIME [epoch: 0.698 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3993859141411347		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.3993859141411347 | validation: 1.2009272090742564]
	TIME [epoch: 0.698 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3434462591081435		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.3434462591081435 | validation: 1.0012944046305219]
	TIME [epoch: 0.698 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3587414616266218		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.3587414616266218 | validation: 1.2786025650503505]
	TIME [epoch: 0.7 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3540280455462566		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.3540280455462566 | validation: 1.1983448266108307]
	TIME [epoch: 0.7 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3273292000277455		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.3273292000277455 | validation: 1.0253971969573816]
	TIME [epoch: 0.701 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.337206634935385		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.337206634935385 | validation: 1.261345892570002]
	TIME [epoch: 0.699 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3330314272067367		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.3330314272067367 | validation: 1.1059895325148548]
	TIME [epoch: 0.698 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3274074382551333		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.3274074382551333 | validation: 1.1468672807849964]
	TIME [epoch: 0.698 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3277128650197256		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.3277128650197256 | validation: 1.1693288873767684]
	TIME [epoch: 0.696 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3250886882585025		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.3250886882585025 | validation: 1.1102856946296293]
	TIME [epoch: 0.697 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3387027882445057		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.3387027882445057 | validation: 1.287271907184698]
	TIME [epoch: 0.699 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4068861113149678		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.4068861113149678 | validation: 1.391402720220084]
	TIME [epoch: 0.698 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.451984268100433		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.451984268100433 | validation: 1.1601614393133957]
	TIME [epoch: 0.697 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.32996842644884		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.32996842644884 | validation: 1.1305718500638695]
	TIME [epoch: 0.698 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3296581902171902		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.3296581902171902 | validation: 1.2029346674289458]
	TIME [epoch: 0.698 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3227692807999314		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.3227692807999314 | validation: 1.1129976187199473]
	TIME [epoch: 0.697 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3150776446054562		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.3150776446054562 | validation: 1.1619515335507125]
	TIME [epoch: 0.697 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3264350744614664		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.3264350744614664 | validation: 1.1562110217203478]
	TIME [epoch: 0.702 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3150325304502701		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.3150325304502701 | validation: 1.1272880251516708]
	TIME [epoch: 0.698 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3052608546543578		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.3052608546543578 | validation: 1.2167498135424097]
	TIME [epoch: 0.699 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3124548963804545		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.3124548963804545 | validation: 1.0060420711764269]
	TIME [epoch: 0.698 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3582509301639838		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.3582509301639838 | validation: 1.5786709072127836]
	TIME [epoch: 0.699 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5162393376256749		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.5162393376256749 | validation: 1.2331910929648107]
	TIME [epoch: 0.698 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3180173755583264		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.3180173755583264 | validation: 0.7897572234194001]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.490406321467939		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.490406321467939 | validation: 1.4502736682593453]
	TIME [epoch: 0.705 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4059595922447903		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.4059595922447903 | validation: 1.4165326956368811]
	TIME [epoch: 0.701 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3893490519222513		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.3893490519222513 | validation: 1.0491744287160474]
	TIME [epoch: 0.699 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3181987665216548		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.3181987665216548 | validation: 1.061904078026582]
	TIME [epoch: 0.699 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3284090399886574		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.3284090399886574 | validation: 1.2052147346029487]
	TIME [epoch: 0.699 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3066812100242606		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.3066812100242606 | validation: 1.216811668261257]
	TIME [epoch: 0.7 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3088539713588665		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.3088539713588665 | validation: 1.0485510215286624]
	TIME [epoch: 0.7 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3045842167680142		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.3045842167680142 | validation: 1.2109503680784144]
	TIME [epoch: 0.7 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3072287133929488		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.3072287133929488 | validation: 1.1469514801606147]
	TIME [epoch: 0.699 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3401321119708205		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.3401321119708205 | validation: 1.2726200616380519]
	TIME [epoch: 0.699 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.370825650675411		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.370825650675411 | validation: 1.1318493718765124]
	TIME [epoch: 0.699 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3202994447161174		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.3202994447161174 | validation: 1.131301883817367]
	TIME [epoch: 0.699 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2872802907659513		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.2872802907659513 | validation: 1.0806935845397891]
	TIME [epoch: 0.699 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2892342284329874		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.2892342284329874 | validation: 1.164327962196057]
	TIME [epoch: 0.703 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2855952432099451		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.2855952432099451 | validation: 1.1084621867930957]
	TIME [epoch: 0.701 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2757242140864857		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.2757242140864857 | validation: 1.0971581138344284]
	TIME [epoch: 0.701 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2787533145536456		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.2787533145536456 | validation: 1.1885584597246066]
	TIME [epoch: 0.699 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.30203397098162		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.30203397098162 | validation: 1.1100125036760866]
	TIME [epoch: 0.697 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3509941510971062		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.3509941510971062 | validation: 1.3227347695214267]
	TIME [epoch: 0.698 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.360947603041402		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.360947603041402 | validation: 0.9877693749511938]
	TIME [epoch: 0.697 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2747318059926236		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.2747318059926236 | validation: 1.206324083763769]
	TIME [epoch: 0.701 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2727248287758102		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.2727248287758102 | validation: 0.9120421013644604]
	TIME [epoch: 0.701 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3223773596702777		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.3223773596702777 | validation: 1.4629489858086995]
	TIME [epoch: 0.7 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3923372064588961		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.3923372064588961 | validation: 0.9991604492942046]
	TIME [epoch: 0.699 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.259557269536875		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.259557269536875 | validation: 0.9766945915091143]
	TIME [epoch: 0.699 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2631517138104673		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.2631517138104673 | validation: 1.2252011922466424]
	TIME [epoch: 0.697 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2728632697166504		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.2728632697166504 | validation: 0.939111500182137]
	TIME [epoch: 0.698 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2539928683924482		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.2539928683924482 | validation: 1.1637884639223086]
	TIME [epoch: 0.698 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2469790432669978		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.2469790432669978 | validation: 0.9127784119054149]
	TIME [epoch: 0.697 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2430951958162098		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.2430951958162098 | validation: 1.3277959088730034]
	TIME [epoch: 0.698 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2832407055661395		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.2832407055661395 | validation: 0.7014308639041632]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.347818474035447		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.347818474035447 | validation: 1.4007779532825912]
	TIME [epoch: 0.702 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3375712795405892		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.3375712795405892 | validation: 1.029021946832243]
	TIME [epoch: 0.7 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2330465979158647		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.2330465979158647 | validation: 0.8864875995921454]
	TIME [epoch: 0.709 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2677567075835712		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.2677567075835712 | validation: 1.1763041397373362]
	TIME [epoch: 0.699 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.240374941667333		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.240374941667333 | validation: 1.0023142938283003]
	TIME [epoch: 0.698 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2040545147417212		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.2040545147417212 | validation: 0.9670148990865584]
	TIME [epoch: 0.704 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1975196639811125		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.1975196639811125 | validation: 1.220995251991266]
	TIME [epoch: 0.701 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2359127867832995		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.2359127867832995 | validation: 0.7228876925628]
	TIME [epoch: 0.701 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3777040274808559		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.3777040274808559 | validation: 1.4772940278454716]
	TIME [epoch: 0.7 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3815521499988053		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.3815521499988053 | validation: 0.9921588526872251]
	TIME [epoch: 0.701 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1833285914359721		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.1833285914359721 | validation: 0.8144643887058065]
	TIME [epoch: 0.7 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.268714491852206		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.268714491852206 | validation: 1.164972568719212]
	TIME [epoch: 0.701 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.215952805530036		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.215952805530036 | validation: 0.9886009695881524]
	TIME [epoch: 0.699 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1812305993181198		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.1812305993181198 | validation: 0.933713462175294]
	TIME [epoch: 0.7 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.171674950900675		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.171674950900675 | validation: 1.1147001922883042]
	TIME [epoch: 0.699 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1709908240034574		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.1709908240034574 | validation: 0.7970820849995182]
	TIME [epoch: 0.7 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1591118945032322		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.1591118945032322 | validation: 1.364965838467762]
	TIME [epoch: 0.7 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2616088983269793		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.2616088983269793 | validation: 0.6206624085635063]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5318888893849028		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.5318888893849028 | validation: 1.2191122879488383]
	TIME [epoch: 0.7 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.212715384098104		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.212715384098104 | validation: 1.0901927009741714]
	TIME [epoch: 0.7 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1596645774260974		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.1596645774260974 | validation: 0.7840037488060574]
	TIME [epoch: 0.697 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1801783301114757		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.1801783301114757 | validation: 1.0616572219505114]
	TIME [epoch: 0.696 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1333013892595352		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.1333013892595352 | validation: 0.9279718862914831]
	TIME [epoch: 0.697 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1212181977346871		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.1212181977346871 | validation: 0.9468628507444841]
	TIME [epoch: 0.696 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1057761173187373		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.1057761173187373 | validation: 0.8863580325094432]
	TIME [epoch: 0.696 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.092767633611568		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.092767633611568 | validation: 1.0407487494202368]
	TIME [epoch: 0.697 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1012167974353768		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.1012167974353768 | validation: 0.5999416856862041]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3032387357623652		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.3032387357623652 | validation: 2.088346299130104]
	TIME [epoch: 0.707 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8816261887002896		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.8816261887002896 | validation: 0.9347849872839884]
	TIME [epoch: 0.705 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0899478240815876		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.0899478240815876 | validation: 0.6206289793432518]
	TIME [epoch: 0.703 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2671893439044268		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.2671893439044268 | validation: 1.0718164711896596]
	TIME [epoch: 0.703 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1349942146716858		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.1349942146716858 | validation: 1.0312285854183518]
	TIME [epoch: 0.703 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1150294693035945		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.1150294693035945 | validation: 0.7554311090443261]
	TIME [epoch: 0.702 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.096858863982829		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.096858863982829 | validation: 0.9585044915032945]
	TIME [epoch: 0.702 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0619491949030817		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.0619491949030817 | validation: 0.9012959919127227]
	TIME [epoch: 0.702 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.043307927923276		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.043307927923276 | validation: 0.8015414455957732]
	TIME [epoch: 0.703 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0276217003014143		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.0276217003014143 | validation: 0.9845102124296224]
	TIME [epoch: 0.701 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0185610447678324		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.0185610447678324 | validation: 0.5780875304657312]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1320962602001354		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.1320962602001354 | validation: 1.8247926034417419]
	TIME [epoch: 0.705 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5850905291735962		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.5850905291735962 | validation: 0.6443521279286922]
	TIME [epoch: 0.705 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0902197837967331		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.0902197837967331 | validation: 0.8548584008988565]
	TIME [epoch: 0.702 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9983740158959057		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.9983740158959057 | validation: 0.9123786898743989]
	TIME [epoch: 0.703 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.006161612243715		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.006161612243715 | validation: 0.7035935382374332]
	TIME [epoch: 0.709 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9901852394114846		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.9901852394114846 | validation: 1.0559048056427778]
	TIME [epoch: 0.703 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0158005881760426		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.0158005881760426 | validation: 0.5555996645039326]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1734437327523874		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.1734437327523874 | validation: 1.6663997238617647]
	TIME [epoch: 0.706 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4148890993418382		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.4148890993418382 | validation: 0.687659947748438]
	TIME [epoch: 0.705 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9855017425073476		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.9855017425073476 | validation: 0.7313600997652939]
	TIME [epoch: 0.704 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9706714195239841		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.9706714195239841 | validation: 1.026761408264451]
	TIME [epoch: 0.703 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0106053370649024		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.0106053370649024 | validation: 0.587720231461853]
	TIME [epoch: 0.703 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0355826545513076		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.0355826545513076 | validation: 1.2158370044731925]
	TIME [epoch: 0.703 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.087211210011077		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.087211210011077 | validation: 0.5966330123899647]
	TIME [epoch: 0.702 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0548209209993784		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.0548209209993784 | validation: 1.1404439172483678]
	TIME [epoch: 0.702 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0479069978405644		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.0479069978405644 | validation: 0.6159779836684328]
	TIME [epoch: 0.703 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9700955219252613		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.9700955219252613 | validation: 0.9556630065463964]
	TIME [epoch: 0.702 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9429433572475167		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.9429433572475167 | validation: 0.6217137753801799]
	TIME [epoch: 0.701 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9494518749413048		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.9494518749413048 | validation: 1.1665912788068085]
	TIME [epoch: 0.701 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0623332731996504		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.0623332731996504 | validation: 0.5728349135250536]
	TIME [epoch: 0.701 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1610440115335836		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.1610440115335836 | validation: 1.3666966907267286]
	TIME [epoch: 0.702 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1884712439791476		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.1884712439791476 | validation: 0.6694056198462294]
	TIME [epoch: 0.702 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9196257165503531		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.9196257165503531 | validation: 0.7230098134976934]
	TIME [epoch: 0.701 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.891123186886476		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.891123186886476 | validation: 0.8547166708232089]
	TIME [epoch: 0.702 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8900128048986238		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.8900128048986238 | validation: 0.6197555108027974]
	TIME [epoch: 0.701 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9082774519966839		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.9082774519966839 | validation: 1.2059223404933725]
	TIME [epoch: 0.702 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0520258353064134		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.0520258353064134 | validation: 0.6328759680248229]
	TIME [epoch: 0.703 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3277872835699043		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.3277872835699043 | validation: 1.35547608025379]
	TIME [epoch: 0.705 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1651909470172896		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.1651909470172896 | validation: 0.7383430760257325]
	TIME [epoch: 0.702 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8686767549190844		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.8686767549190844 | validation: 0.5918814923022488]
	TIME [epoch: 0.701 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9284699314288432		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.9284699314288432 | validation: 1.0217060759287229]
	TIME [epoch: 0.702 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9537680276373898		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.9537680276373898 | validation: 0.6437903534169653]
	TIME [epoch: 0.704 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8916671485186689		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.8916671485186689 | validation: 0.8760989741793179]
	TIME [epoch: 0.707 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8536273017971885		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.8536273017971885 | validation: 0.6119331600097779]
	TIME [epoch: 0.706 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8507286636855363		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.8507286636855363 | validation: 1.0059023304842]
	TIME [epoch: 0.704 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.911272482076939		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.911272482076939 | validation: 0.6327080428387593]
	TIME [epoch: 0.704 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1592963132934975		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.1592963132934975 | validation: 1.555749316032349]
	TIME [epoch: 0.703 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3066568936146206		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.3066568936146206 | validation: 0.7019719225645616]
	TIME [epoch: 0.702 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8287299335607157		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.8287299335607157 | validation: 0.5825283787435517]
	TIME [epoch: 0.705 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8968374839822869		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.8968374839822869 | validation: 1.0668020077465126]
	TIME [epoch: 0.703 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9576068997887752		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.9576068997887752 | validation: 0.5992468520102783]
	TIME [epoch: 0.702 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.866244476694857		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.866244476694857 | validation: 0.8169814912906598]
	TIME [epoch: 0.702 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8222163041952939		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.8222163041952939 | validation: 0.6300191832882689]
	TIME [epoch: 0.702 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8104270865646389		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.8104270865646389 | validation: 0.9179041018000726]
	TIME [epoch: 0.702 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8370779968370299		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.8370779968370299 | validation: 0.5960922069503541]
	TIME [epoch: 166 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.057574106201962		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.057574106201962 | validation: 1.6526516284884107]
	TIME [epoch: 1.4 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3474271316069377		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.3474271316069377 | validation: 0.6930345144024308]
	TIME [epoch: 1.38 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7949657289724086		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.7949657289724086 | validation: 0.5778383753158343]
	TIME [epoch: 1.37 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8781930552090523		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.8781930552090523 | validation: 1.1120567043369944]
	TIME [epoch: 1.37 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9816075997433197		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.9816075997433197 | validation: 0.584580090497169]
	TIME [epoch: 1.37 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8462931674057143		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.8462931674057143 | validation: 0.7906005911229812]
	TIME [epoch: 1.38 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7843071212266227		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.7843071212266227 | validation: 0.6560850468223838]
	TIME [epoch: 1.37 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7651261250133331		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.7651261250133331 | validation: 0.8078208802659227]
	TIME [epoch: 1.38 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7774163511052654		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.7774163511052654 | validation: 0.5882725665113079]
	TIME [epoch: 1.37 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8849559930899801		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.8849559930899801 | validation: 1.5363472567870533]
	TIME [epoch: 1.38 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2633939568697927		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.2633939568697927 | validation: 0.5820686139871558]
	TIME [epoch: 1.38 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8524032359608021		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.8524032359608021 | validation: 0.8217854361710999]
	TIME [epoch: 1.37 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7745497493306958		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.7745497493306958 | validation: 0.6250943411058716]
	TIME [epoch: 1.38 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7591776370825434		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.7591776370825434 | validation: 0.8346912865631326]
	TIME [epoch: 1.38 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7843936617280831		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.7843936617280831 | validation: 0.57324562999769]
	TIME [epoch: 1.37 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8670597114656013		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.8670597114656013 | validation: 1.3289252968198118]
	TIME [epoch: 1.38 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0975706114002526		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.0975706114002526 | validation: 0.5532027192660335]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8385973164059086		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.8385973164059086 | validation: 0.8344750992126482]
	TIME [epoch: 1.38 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7815469878170687		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.7815469878170687 | validation: 0.5795632418287915]
	TIME [epoch: 1.38 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7742898646926153		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.7742898646926153 | validation: 0.9180175237720066]
	TIME [epoch: 1.38 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8093236250199191		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.8093236250199191 | validation: 0.5529553080522289]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_222.pth
	Model improved!!!
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8858372556691115		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.8858372556691115 | validation: 1.2056874432929137]
	TIME [epoch: 1.38 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0283540736729284		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.0283540736729284 | validation: 0.57038690348197]
	TIME [epoch: 1.38 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7836713941166892		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.7836713941166892 | validation: 0.7740571675429258]
	TIME [epoch: 1.38 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7387659659215808		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.7387659659215808 | validation: 0.6064845943015197]
	TIME [epoch: 1.38 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7299506535101871		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.7299506535101871 | validation: 0.821743502506336]
	TIME [epoch: 1.38 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7378773722483787		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.7378773722483787 | validation: 0.5559576475136325]
	TIME [epoch: 1.38 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8443784116923128		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.8443784116923128 | validation: 1.3939454101868138]
	TIME [epoch: 1.38 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1486589292146456		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.1486589292146456 | validation: 0.5554976270447988]
	TIME [epoch: 1.38 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7684938156996868		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.7684938156996868 | validation: 0.7288735592017659]
	TIME [epoch: 1.38 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7207656220735627		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.7207656220735627 | validation: 0.6407768244903078]
	TIME [epoch: 1.37 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7146113395921312		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.7146113395921312 | validation: 0.7494143845927408]
	TIME [epoch: 1.38 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7113853381947124		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.7113853381947124 | validation: 0.5713392447804835]
	TIME [epoch: 1.38 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.758808909490611		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.758808909490611 | validation: 1.245340523663287]
	TIME [epoch: 1.37 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0380786657639245		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.0380786657639245 | validation: 0.5466813654771058]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9530294037918088		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.9530294037918088 | validation: 1.124775320462121]
	TIME [epoch: 1.38 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9150884561088627		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.9150884561088627 | validation: 0.5940138398399125]
	TIME [epoch: 1.38 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.718660479953914		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.718660479953914 | validation: 0.6853683379889843]
	TIME [epoch: 1.38 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7040307680679839		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.7040307680679839 | validation: 0.6912494839215056]
	TIME [epoch: 1.38 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6962310221621906		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.6962310221621906 | validation: 0.6018262024676496]
	TIME [epoch: 1.39 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7026017450120553		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.7026017450120553 | validation: 0.9008508171707753]
	TIME [epoch: 1.38 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7706996956847609		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.7706996956847609 | validation: 0.5655826912074144]
	TIME [epoch: 1.38 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.998213698000942		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.998213698000942 | validation: 1.3555926941820502]
	TIME [epoch: 1.38 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0823462920953426		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 1.0823462920953426 | validation: 0.6402437807121776]
	TIME [epoch: 1.38 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7002563170662353		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.7002563170662353 | validation: 0.5504478347438096]
	TIME [epoch: 1.38 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7715253145752445		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.7715253145752445 | validation: 1.0126852069396068]
	TIME [epoch: 1.38 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8613413245050242		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.8613413245050242 | validation: 0.566408999500107]
	TIME [epoch: 1.38 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7312551401398916		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.7312551401398916 | validation: 0.769445718780597]
	TIME [epoch: 1.38 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7014739918037165		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.7014739918037165 | validation: 0.616556888238548]
	TIME [epoch: 1.38 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6894888585695034		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.6894888585695034 | validation: 0.7976897383748031]
	TIME [epoch: 1.38 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7029342646348126		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.7029342646348126 | validation: 0.5578789319577767]
	TIME [epoch: 1.38 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7805697858341841		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.7805697858341841 | validation: 1.2249279775700943]
	TIME [epoch: 1.38 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9952138709633309		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.9952138709633309 | validation: 0.5408731826408032]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_254.pth
	Model improved!!!
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7401986324220953		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.7401986324220953 | validation: 0.7830480872933879]
	TIME [epoch: 1.38 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7021624330068863		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.7021624330068863 | validation: 0.5890304459009542]
	TIME [epoch: 1.38 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.69651523133286		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.69651523133286 | validation: 0.8147032902915307]
	TIME [epoch: 1.38 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7079402568653326		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.7079402568653326 | validation: 0.5422111818172284]
	TIME [epoch: 1.38 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8021345389662775		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.8021345389662775 | validation: 1.1639609656549996]
	TIME [epoch: 1.38 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9439846942210014		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.9439846942210014 | validation: 0.545191786407975]
	TIME [epoch: 1.38 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7081503073811737		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.7081503073811737 | validation: 0.7198826547471296]
	TIME [epoch: 1.38 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6712657469156059		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.6712657469156059 | validation: 0.6119508579076145]
	TIME [epoch: 1.38 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6576990500071784		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.6576990500071784 | validation: 0.7184510684481689]
	TIME [epoch: 1.38 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6653334252443327		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.6653334252443327 | validation: 0.5700611292043204]
	TIME [epoch: 1.38 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7054852760218645		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.7054852760218645 | validation: 1.0980605240182377]
	TIME [epoch: 1.38 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8910496368979386		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.8910496368979386 | validation: 0.521280348541613]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8926323756676606		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.8926323756676606 | validation: 1.0712255597825813]
	TIME [epoch: 1.38 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8655478733776454		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.8655478733776454 | validation: 0.5887162308800525]
	TIME [epoch: 1.37 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6586141544010232		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.6586141544010232 | validation: 0.6496723423782844]
	TIME [epoch: 1.37 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6540413923074891		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.6540413923074891 | validation: 0.7454430448128543]
	TIME [epoch: 1.37 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6603325399648188		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.6603325399648188 | validation: 0.5470817889904815]
	TIME [epoch: 1.37 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7066046717029454		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.7066046717029454 | validation: 1.0426232911032098]
	TIME [epoch: 1.37 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8541501894178394		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.8541501894178394 | validation: 0.5216932130694756]
	TIME [epoch: 1.37 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7763809895723575		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.7763809895723575 | validation: 0.9078303416690517]
	TIME [epoch: 1.37 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7576446275392149		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.7576446275392149 | validation: 0.5587047019560688]
	TIME [epoch: 1.38 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6869071144025648		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.6869071144025648 | validation: 0.7852676863100704]
	TIME [epoch: 1.37 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6825938894620717		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.6825938894620717 | validation: 0.5612324674526497]
	TIME [epoch: 1.37 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.685149325688998		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.685149325688998 | validation: 0.9007595041378095]
	TIME [epoch: 1.37 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7514482143595873		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.7514482143595873 | validation: 0.5216382960566724]
	TIME [epoch: 1.37 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7583337165586957		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.7583337165586957 | validation: 0.9881586082204716]
	TIME [epoch: 1.37 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7968626503745142		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.7968626503745142 | validation: 0.5540210601405212]
	TIME [epoch: 1.37 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.699530928243329		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.699530928243329 | validation: 0.7653454624059324]
	TIME [epoch: 1.37 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6749534039742491		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.6749534039742491 | validation: 0.5535084454723073]
	TIME [epoch: 1.37 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6791859218756685		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.6791859218756685 | validation: 0.8728574414803032]
	TIME [epoch: 1.37 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7297393974099675		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.7297393974099675 | validation: 0.5221863167196379]
	TIME [epoch: 1.37 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7379704944500256		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.7379704944500256 | validation: 0.9719017394408912]
	TIME [epoch: 1.37 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7826990183698805		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.7826990183698805 | validation: 0.5200080631213039]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_287.pth
	Model improved!!!
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7020469253883037		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.7020469253883037 | validation: 0.8132324426994911]
	TIME [epoch: 1.37 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6875988246225654		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.6875988246225654 | validation: 0.5622083931397636]
	TIME [epoch: 1.37 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6797234550844083		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.6797234550844083 | validation: 0.8653803935485116]
	TIME [epoch: 1.37 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7142923946363976		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.7142923946363976 | validation: 0.5232024949907607]
	TIME [epoch: 1.37 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7080632955997156		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.7080632955997156 | validation: 0.9005021026178013]
	TIME [epoch: 1.37 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7476726177654568		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.7476726177654568 | validation: 0.5340261064980868]
	TIME [epoch: 1.37 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7016357249136519		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.7016357249136519 | validation: 0.8563458385417939]
	TIME [epoch: 1.37 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7118250187657025		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.7118250187657025 | validation: 0.539592918182628]
	TIME [epoch: 1.37 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6887129197452169		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.6887129197452169 | validation: 0.8409375373450907]
	TIME [epoch: 1.37 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6967639441537972		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.6967639441537972 | validation: 0.5334914950639726]
	TIME [epoch: 1.37 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6903133748196476		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.6903133748196476 | validation: 0.8841005008620808]
	TIME [epoch: 1.37 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7187449318919596		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.7187449318919596 | validation: 0.5343752666930485]
	TIME [epoch: 1.37 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7003921428456389		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.7003921428456389 | validation: 0.8754529103409325]
	TIME [epoch: 1.37 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7300661298498858		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.7300661298498858 | validation: 0.5441659748859479]
	TIME [epoch: 1.38 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6858974254260518		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.6858974254260518 | validation: 0.8616075925499493]
	TIME [epoch: 1.38 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6881004501926872		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.6881004501926872 | validation: 0.5306202058992627]
	TIME [epoch: 1.37 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6772896172677579		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.6772896172677579 | validation: 0.8249963934985662]
	TIME [epoch: 1.38 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6897405339003104		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.6897405339003104 | validation: 0.505818291753981]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7011650127693011		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.7011650127693011 | validation: 0.9016738811656192]
	TIME [epoch: 1.38 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7357530406998185		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.7357530406998185 | validation: 0.5248567564634338]
	TIME [epoch: 1.38 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6764888522673388		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.6764888522673388 | validation: 0.8239729274247378]
	TIME [epoch: 1.38 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6732073678872941		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.6732073678872941 | validation: 0.5252471067796644]
	TIME [epoch: 1.38 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.671222968385346		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.671222968385346 | validation: 0.8489626224551092]
	TIME [epoch: 1.38 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7009951848203926		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.7009951848203926 | validation: 0.5271377254455942]
	TIME [epoch: 1.38 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6942117125114449		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.6942117125114449 | validation: 0.8695211821440835]
	TIME [epoch: 1.38 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7033226846318682		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.7033226846318682 | validation: 0.5406353440095844]
	TIME [epoch: 1.38 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6695076740329818		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.6695076740329818 | validation: 0.8346606628852528]
	TIME [epoch: 1.38 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6804686372396229		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.6804686372396229 | validation: 0.5490987145121784]
	TIME [epoch: 1.38 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6713558429903012		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.6713558429903012 | validation: 0.8317810383327295]
	TIME [epoch: 1.38 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6893714167192314		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.6893714167192314 | validation: 0.5219660646775619]
	TIME [epoch: 1.38 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6714229735633651		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.6714229735633651 | validation: 0.8482190996503784]
	TIME [epoch: 1.38 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6928496051472144		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.6928496051472144 | validation: 0.5209534522388815]
	TIME [epoch: 1.38 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6771459816200601		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.6771459816200601 | validation: 0.8415569843292577]
	TIME [epoch: 1.38 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6859224991110506		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.6859224991110506 | validation: 0.5345651983347607]
	TIME [epoch: 1.38 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6673725575840245		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.6673725575840245 | validation: 0.8402592338466669]
	TIME [epoch: 1.38 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6931312898763606		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.6931312898763606 | validation: 0.5045561125417314]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_323.pth
	Model improved!!!
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6636202223738356		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.6636202223738356 | validation: 0.8141121229787134]
	TIME [epoch: 1.37 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6743680343190053		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.6743680343190053 | validation: 0.5220233250083979]
	TIME [epoch: 1.37 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6567800293198296		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.6567800293198296 | validation: 0.8122547564990227]
	TIME [epoch: 1.37 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6729113903434911		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.6729113903434911 | validation: 0.5347260998117808]
	TIME [epoch: 1.36 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6648591157913011		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.6648591157913011 | validation: 0.8399708984321013]
	TIME [epoch: 1.37 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6881494877555713		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.6881494877555713 | validation: 0.5295208041243552]
	TIME [epoch: 1.37 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6775319237007225		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.6775319237007225 | validation: 0.8586072374736085]
	TIME [epoch: 1.37 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6824166970660903		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.6824166970660903 | validation: 0.5189637525465323]
	TIME [epoch: 1.37 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6544412043585334		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.6544412043585334 | validation: 0.7993568756316775]
	TIME [epoch: 1.37 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.660940673139027		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.660940673139027 | validation: 0.5232842827775257]
	TIME [epoch: 1.38 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6553492236506075		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.6553492236506075 | validation: 0.8183553197790691]
	TIME [epoch: 1.38 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6732410115010408		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.6732410115010408 | validation: 0.5151115297047423]
	TIME [epoch: 1.38 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6579053306652576		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.6579053306652576 | validation: 0.8402177249520477]
	TIME [epoch: 1.38 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6670516069919583		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.6670516069919583 | validation: 0.5280505813470473]
	TIME [epoch: 1.38 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6442872613714883		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.6442872613714883 | validation: 0.7949821413887709]
	TIME [epoch: 1.38 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6558816200430267		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.6558816200430267 | validation: 0.5050378557752849]
	TIME [epoch: 1.38 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6598016097731062		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.6598016097731062 | validation: 0.8504136332135985]
	TIME [epoch: 1.38 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6871300744861405		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.6871300744861405 | validation: 0.5069792396379217]
	TIME [epoch: 1.38 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.657091670815796		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.657091670815796 | validation: 0.7967005489651424]
	TIME [epoch: 1.38 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6638874487601646		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.6638874487601646 | validation: 0.5296195764523901]
	TIME [epoch: 1.38 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6422236924486663		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.6422236924486663 | validation: 0.8036962961602424]
	TIME [epoch: 1.38 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6594167800718261		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.6594167800718261 | validation: 0.5235244474980068]
	TIME [epoch: 1.38 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.647361914601487		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.647361914601487 | validation: 0.8058309109661995]
	TIME [epoch: 1.38 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6597075763054252		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.6597075763054252 | validation: 0.5168215911264914]
	TIME [epoch: 1.38 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.637280038105716		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.637280038105716 | validation: 0.7625793252262685]
	TIME [epoch: 1.38 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6464737920804866		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.6464737920804866 | validation: 0.5169459858345119]
	TIME [epoch: 1.38 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6376882237439461		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.6376882237439461 | validation: 0.7921641759006186]
	TIME [epoch: 1.38 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6431471567697321		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.6431471567697321 | validation: 0.525369421665786]
	TIME [epoch: 1.38 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6560823234390418		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.6560823234390418 | validation: 0.8890182838792122]
	TIME [epoch: 1.38 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7086929084983266		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.7086929084983266 | validation: 0.5057387539683249]
	TIME [epoch: 1.38 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6332029467383565		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.6332029467383565 | validation: 0.7486331908125679]
	TIME [epoch: 1.38 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6238356354423396		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.6238356354423396 | validation: 0.528741588760177]
	TIME [epoch: 1.38 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6319777626675528		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.6319777626675528 | validation: 0.8134392393733375]
	TIME [epoch: 1.38 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6555159592049802		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.6555159592049802 | validation: 0.5156047077514415]
	TIME [epoch: 1.38 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6483156747596882		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.6483156747596882 | validation: 0.8120012263947326]
	TIME [epoch: 1.38 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6603002255190944		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.6603002255190944 | validation: 0.5186728803484112]
	TIME [epoch: 1.38 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6254154577721401		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.6254154577721401 | validation: 0.8113811238132661]
	TIME [epoch: 1.38 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6486740295941092		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.6486740295941092 | validation: 0.5182398108843703]
	TIME [epoch: 1.38 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6267262180838828		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.6267262180838828 | validation: 0.7670004374335536]
	TIME [epoch: 1.38 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6288117797319531		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.6288117797319531 | validation: 0.5162380885579319]
	TIME [epoch: 1.38 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6458253626518388		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.6458253626518388 | validation: 0.8389395902370723]
	TIME [epoch: 1.38 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6681897493580827		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.6681897493580827 | validation: 0.5332337406906217]
	TIME [epoch: 1.38 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6189341558068003		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.6189341558068003 | validation: 0.7417832261894525]
	TIME [epoch: 1.38 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6174797284537912		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.6174797284537912 | validation: 0.529697821690734]
	TIME [epoch: 1.38 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6149061630617662		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.6149061630617662 | validation: 0.8124805362262525]
	TIME [epoch: 1.38 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6524769706936359		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.6524769706936359 | validation: 0.4944329552568565]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_369.pth
	Model improved!!!
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6474581091063679		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.6474581091063679 | validation: 0.807795338624737]
	TIME [epoch: 1.37 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6514858466745769		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.6514858466745769 | validation: 0.49033691008152885]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_371.pth
	Model improved!!!
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6136594791915928		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.6136594791915928 | validation: 0.7408913915327707]
	TIME [epoch: 1.38 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6143905325166491		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.6143905325166491 | validation: 0.5161061864209703]
	TIME [epoch: 1.38 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.619840201899673		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.619840201899673 | validation: 0.7837106779908445]
	TIME [epoch: 1.38 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6423840168352419		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.6423840168352419 | validation: 0.5097052954222013]
	TIME [epoch: 1.37 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6337907562877537		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.6337907562877537 | validation: 0.8176203208586289]
	TIME [epoch: 1.37 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6577210180513415		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.6577210180513415 | validation: 0.5247004972203886]
	TIME [epoch: 1.37 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6143325517487164		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.6143325517487164 | validation: 0.7472630281774065]
	TIME [epoch: 1.37 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6144872347644288		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.6144872347644288 | validation: 0.5153534197046394]
	TIME [epoch: 1.37 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6098377833606591		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.6098377833606591 | validation: 0.7726821606554118]
	TIME [epoch: 1.38 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6305929285380714		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.6305929285380714 | validation: 0.49491021345714525]
	TIME [epoch: 1.37 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6286608453234147		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.6286608453234147 | validation: 0.7776548512031165]
	TIME [epoch: 1.37 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6327680963110764		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.6327680963110764 | validation: 0.4967016256639991]
	TIME [epoch: 1.38 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6092467693402748		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.6092467693402748 | validation: 0.7751336023976262]
	TIME [epoch: 1.37 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6245532972444822		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.6245532972444822 | validation: 0.5096035456162092]
	TIME [epoch: 1.37 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6160668774116661		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.6160668774116661 | validation: 0.7630782649958717]
	TIME [epoch: 1.37 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.625276635084334		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.625276635084334 | validation: 0.4907266465410916]
	TIME [epoch: 1.37 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6238663440846649		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.6238663440846649 | validation: 0.7895908181235077]
	TIME [epoch: 1.37 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.631484702622791		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.631484702622791 | validation: 0.4994680588357139]
	TIME [epoch: 1.37 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.615267940162517		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.615267940162517 | validation: 0.757418054007875]
	TIME [epoch: 1.37 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6165384871506694		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.6165384871506694 | validation: 0.4870669881174651]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_391.pth
	Model improved!!!
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5914585176610531		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.5914585176610531 | validation: 0.766096563382225]
	TIME [epoch: 1.38 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6112561356218583		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.6112561356218583 | validation: 0.5060257478058047]
	TIME [epoch: 1.37 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6106551869127697		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.6106551869127697 | validation: 0.7668284899935438]
	TIME [epoch: 1.37 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6324300189181977		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.6324300189181977 | validation: 0.49187905596652937]
	TIME [epoch: 1.37 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6121972999936115		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.6121972999936115 | validation: 0.7683653881342085]
	TIME [epoch: 1.37 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6104191941967221		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.6104191941967221 | validation: 0.4878657635437842]
	TIME [epoch: 1.37 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6031832691723603		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.6031832691723603 | validation: 0.7523878076814784]
	TIME [epoch: 1.37 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6037710229593619		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.6037710229593619 | validation: 0.5041689526688292]
	TIME [epoch: 1.37 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5887810557113157		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.5887810557113157 | validation: 0.7488283078987591]
	TIME [epoch: 1.38 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6167516799284943		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.6167516799284943 | validation: 0.4938141168905201]
	TIME [epoch: 1.38 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6179575668709089		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.6179575668709089 | validation: 0.8276179682037654]
	TIME [epoch: 1.38 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6518175739347843		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.6518175739347843 | validation: 0.5090584468899224]
	TIME [epoch: 1.37 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5805565716820774		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.5805565716820774 | validation: 0.6634398293772762]
	TIME [epoch: 1.37 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5714818426073832		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.5714818426073832 | validation: 0.5160984964516541]
	TIME [epoch: 1.37 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5720485772832317		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.5720485772832317 | validation: 0.7305745692558868]
	TIME [epoch: 1.37 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5988584704854673		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.5988584704854673 | validation: 0.46471731308828573]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_407.pth
	Model improved!!!
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6318965064807924		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.6318965064807924 | validation: 0.8236874316251526]
	TIME [epoch: 1.38 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6587401911300174		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.6587401911300174 | validation: 0.511282776053318]
	TIME [epoch: 1.38 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5784290602795901		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.5784290602795901 | validation: 0.681206737566714]
	TIME [epoch: 1.37 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5687273138374294		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.5687273138374294 | validation: 0.5162997699764228]
	TIME [epoch: 1.38 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5721858861267185		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.5721858861267185 | validation: 0.7345621998524416]
	TIME [epoch: 1.38 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5873567573146847		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.5873567573146847 | validation: 0.4903144874523719]
	TIME [epoch: 1.38 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5955990920565454		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.5955990920565454 | validation: 0.8002317761475647]
	TIME [epoch: 1.38 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6371551646803039		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.6371551646803039 | validation: 0.4696292865408591]
	TIME [epoch: 1.38 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5906046538739839		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.5906046538739839 | validation: 0.7159730580444497]
	TIME [epoch: 1.38 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5860839147305952		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.5860839147305952 | validation: 0.49128277786471614]
	TIME [epoch: 1.38 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5708966192414432		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.5708966192414432 | validation: 0.7309365126074798]
	TIME [epoch: 1.38 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5854845021909257		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.5854845021909257 | validation: 0.46876828305088997]
	TIME [epoch: 1.38 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5802743089125685		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.5802743089125685 | validation: 0.7766526433821395]
	TIME [epoch: 1.38 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.609481832500213		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.609481832500213 | validation: 0.4640760179100969]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_421.pth
	Model improved!!!
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5978563414847764		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.5978563414847764 | validation: 0.7595212325124098]
	TIME [epoch: 1.38 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5976455580397666		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.5976455580397666 | validation: 0.4699527550312107]
	TIME [epoch: 1.37 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5779437625827616		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.5779437625827616 | validation: 0.7212267041425513]
	TIME [epoch: 1.37 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5927159356841136		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.5927159356841136 | validation: 0.4959764848909822]
	TIME [epoch: 1.37 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5599884682265688		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.5599884682265688 | validation: 0.6885145581227569]
	TIME [epoch: 1.37 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5729302226789502		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.5729302226789502 | validation: 0.47207606971869653]
	TIME [epoch: 1.37 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5798411102743873		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.5798411102743873 | validation: 0.7650963885791476]
	TIME [epoch: 1.37 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6042345179703947		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.6042345179703947 | validation: 0.46868033408385207]
	TIME [epoch: 1.37 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5838074674672974		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.5838074674672974 | validation: 0.6961197716846119]
	TIME [epoch: 1.37 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5658347829854007		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.5658347829854007 | validation: 0.48542815301644604]
	TIME [epoch: 1.37 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5682862151726912		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.5682862151726912 | validation: 0.7396167009008732]
	TIME [epoch: 1.37 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5924596275263919		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.5924596275263919 | validation: 0.4426039259792667]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_433.pth
	Model improved!!!
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5786117156489174		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.5786117156489174 | validation: 0.7511627056008321]
	TIME [epoch: 1.38 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5945214344504619		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.5945214344504619 | validation: 0.4799189635269514]
	TIME [epoch: 1.37 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5623234259994518		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.5623234259994518 | validation: 0.6848240620802057]
	TIME [epoch: 1.37 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5564709731151675		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.5564709731151675 | validation: 0.4592693404203484]
	TIME [epoch: 1.37 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5636768076081826		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.5636768076081826 | validation: 0.7381964092435487]
	TIME [epoch: 1.37 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5981064752317017		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.5981064752317017 | validation: 0.4561720094058413]
	TIME [epoch: 1.37 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5687635042211993		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.5687635042211993 | validation: 0.7369121551915024]
	TIME [epoch: 1.37 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5855237464531947		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.5855237464531947 | validation: 0.4913365489568454]
	TIME [epoch: 1.37 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5438562888831446		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.5438562888831446 | validation: 0.6575581631080966]
	TIME [epoch: 1.37 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5440437300550327		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.5440437300550327 | validation: 0.46490158761752814]
	TIME [epoch: 1.37 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5445195602354199		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.5445195602354199 | validation: 0.720060835366583]
	TIME [epoch: 1.36 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5813417982330067		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.5813417982330067 | validation: 0.4349030204260058]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_445.pth
	Model improved!!!
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5794125576591976		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.5794125576591976 | validation: 0.7524994534898307]
	TIME [epoch: 1.37 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5825527889556789		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.5825527889556789 | validation: 0.46122141093241215]
	TIME [epoch: 1.37 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5533840159763301		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.5533840159763301 | validation: 0.6975339551910675]
	TIME [epoch: 1.37 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.562278488224481		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.562278488224481 | validation: 0.4649694225289247]
	TIME [epoch: 1.37 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5663410225843256		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.5663410225843256 | validation: 0.7256722622928912]
	TIME [epoch: 1.37 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5846214909973846		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.5846214909973846 | validation: 0.4413746699003105]
	TIME [epoch: 1.36 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5466957298615734		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.5466957298615734 | validation: 0.6564938277628232]
	TIME [epoch: 1.37 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.546026095470474		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.546026095470474 | validation: 0.4589378529731776]
	TIME [epoch: 1.36 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5523446692218241		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.5523446692218241 | validation: 0.6784441031040122]
	TIME [epoch: 1.37 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.554136242797104		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.554136242797104 | validation: 0.4337423917313668]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_455.pth
	Model improved!!!
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5581826497988266		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.5581826497988266 | validation: 0.715314523069357]
	TIME [epoch: 1.37 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5789348029674225		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.5789348029674225 | validation: 0.456786671869003]
	TIME [epoch: 1.37 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.550873493504135		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.550873493504135 | validation: 0.6842336234827235]
	TIME [epoch: 1.37 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.564585917431902		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.564585917431902 | validation: 0.44108574208526563]
	TIME [epoch: 1.37 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5467672402813297		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.5467672402813297 | validation: 0.7019482772217494]
	TIME [epoch: 1.37 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5464688604481056		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.5464688604481056 | validation: 0.44900635640576647]
	TIME [epoch: 1.37 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5383560377912906		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.5383560377912906 | validation: 0.6761952996739243]
	TIME [epoch: 1.37 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5449617957006059		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.5449617957006059 | validation: 0.4476786742668337]
	TIME [epoch: 1.37 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5426868314849009		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.5426868314849009 | validation: 0.6888625627302061]
	TIME [epoch: 1.37 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5529289089748195		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.5529289089748195 | validation: 0.43665824993717184]
	TIME [epoch: 1.37 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5467360910749172		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.5467360910749172 | validation: 0.711899125898753]
	TIME [epoch: 1.36 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5530859992553808		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.5530859992553808 | validation: 0.42857923810488147]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5420249823361523		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.5420249823361523 | validation: 0.7137319585119921]
	TIME [epoch: 1.37 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5593280847317265		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.5593280847317265 | validation: 0.46572573794379135]
	TIME [epoch: 1.37 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5314695838873973		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.5314695838873973 | validation: 0.6549438121665114]
	TIME [epoch: 1.37 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5353891086205427		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.5353891086205427 | validation: 0.4297775872998198]
	TIME [epoch: 1.37 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5292636950479109		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.5292636950479109 | validation: 0.6850254448260094]
	TIME [epoch: 1.37 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5407667723885586		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.5407667723885586 | validation: 0.43455154244723904]
	TIME [epoch: 1.37 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5392581775297239		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.5392581775297239 | validation: 0.680849296927196]
	TIME [epoch: 1.37 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5470843481880985		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.5470843481880985 | validation: 0.4249748826118202]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_475.pth
	Model improved!!!
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.533417697438876		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.533417697438876 | validation: 0.6717593489852371]
	TIME [epoch: 1.37 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5367697343024459		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.5367697343024459 | validation: 0.43373797537656433]
	TIME [epoch: 1.37 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5318098871060438		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.5318098871060438 | validation: 0.7185289044699479]
	TIME [epoch: 1.37 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5671507957719087		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.5671507957719087 | validation: 0.43074217856413083]
	TIME [epoch: 1.37 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5271932988668637		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.5271932988668637 | validation: 0.6526172433877162]
	TIME [epoch: 1.36 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5190993793224783		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.5190993793224783 | validation: 0.43345277369326163]
	TIME [epoch: 1.37 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.514603984766923		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.514603984766923 | validation: 0.6857102939618308]
	TIME [epoch: 1.36 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5470510676959668		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.5470510676959668 | validation: 0.39946776402274176]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_483.pth
	Model improved!!!
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5380213759987875		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.5380213759987875 | validation: 0.6656533779475835]
	TIME [epoch: 1.37 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5355978121706337		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.5355978121706337 | validation: 0.4088223034042231]
	TIME [epoch: 1.37 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5213384097507461		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.5213384097507461 | validation: 0.6727273198543209]
	TIME [epoch: 1.37 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5307614145934605		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.5307614145934605 | validation: 0.44802132126694927]
	TIME [epoch: 1.37 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5151577241256996		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.5151577241256996 | validation: 0.7134168868037857]
	TIME [epoch: 1.37 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5478777359367614		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.5478777359367614 | validation: 0.43822687023866946]
	TIME [epoch: 1.37 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5158672226536423		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.5158672226536423 | validation: 0.6398565567108152]
	TIME [epoch: 1.37 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5289121469400394		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.5289121469400394 | validation: 0.40571988930871106]
	TIME [epoch: 1.37 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5110264395997439		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.5110264395997439 | validation: 0.6605693521974603]
	TIME [epoch: 1.37 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5173443415040054		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.5173443415040054 | validation: 0.41016774215418944]
	TIME [epoch: 1.37 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5128161357518077		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.5128161357518077 | validation: 0.6613265162326802]
	TIME [epoch: 1.37 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5214629267646361		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.5214629267646361 | validation: 0.3993066955867675]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_495.pth
	Model improved!!!
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.517397415312242		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.517397415312242 | validation: 0.6683259864575398]
	TIME [epoch: 1.37 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5274704623721019		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.5274704623721019 | validation: 0.4022602249814889]
	TIME [epoch: 1.37 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5083491026504333		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.5083491026504333 | validation: 0.6391787393520998]
	TIME [epoch: 1.37 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5193259834697015		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.5193259834697015 | validation: 0.42458140327826954]
	TIME [epoch: 1.37 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5036038709459113		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.5036038709459113 | validation: 0.6369463473648983]
	TIME [epoch: 1.37 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5169119481636382		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.5169119481636382 | validation: 0.4030766498274299]
	TIME [epoch: 170 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5139316708028793		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.5139316708028793 | validation: 0.6716464839536984]
	TIME [epoch: 2.72 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.541326946449908		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.541326946449908 | validation: 0.4028085730953279]
	TIME [epoch: 2.71 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.501776315585584		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.501776315585584 | validation: 0.6264222025517442]
	TIME [epoch: 2.71 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5027927764545767		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.5027927764545767 | validation: 0.42208055133554234]
	TIME [epoch: 2.71 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5125550987904631		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.5125550987904631 | validation: 0.6086701196913453]
	TIME [epoch: 2.7 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4982216273526368		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.4982216273526368 | validation: 0.40397674569810316]
	TIME [epoch: 2.71 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.493240082522959		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.493240082522959 | validation: 0.6615798789576784]
	TIME [epoch: 2.71 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5154937583906097		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.5154937583906097 | validation: 0.3975745764206061]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_509.pth
	Model improved!!!
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5142255339625182		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.5142255339625182 | validation: 0.7031108181603858]
	TIME [epoch: 2.71 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5443540657270285		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.5443540657270285 | validation: 0.4312680680794982]
	TIME [epoch: 2.7 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49184155946110253		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.49184155946110253 | validation: 0.582145751900436]
	TIME [epoch: 2.71 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4850814679080245		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.4850814679080245 | validation: 0.4211668434475906]
	TIME [epoch: 2.71 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47917625201025943		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.47917625201025943 | validation: 0.6331879584048365]
	TIME [epoch: 2.7 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5010991523248842		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.5010991523248842 | validation: 0.37659245949888615]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_515.pth
	Model improved!!!
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5131076100457481		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.5131076100457481 | validation: 0.6298454251742006]
	TIME [epoch: 2.7 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5120221883543024		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.5120221883543024 | validation: 0.4008919525250246]
	TIME [epoch: 2.7 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4887205954132402		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.4887205954132402 | validation: 0.6290646878887439]
	TIME [epoch: 2.71 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4953365519905442		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.4953365519905442 | validation: 0.4124009323747004]
	TIME [epoch: 2.7 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5025139788243266		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.5025139788243266 | validation: 0.6914950414729453]
	TIME [epoch: 2.7 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5374568463898006		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.5374568463898006 | validation: 0.40288733751521155]
	TIME [epoch: 2.7 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4768202032891571		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.4768202032891571 | validation: 0.5665394089303158]
	TIME [epoch: 2.7 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4743212424380765		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.4743212424380765 | validation: 0.39160001846122244]
	TIME [epoch: 2.7 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48660178841325114		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.48660178841325114 | validation: 0.6583608960560725]
	TIME [epoch: 2.71 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5109997845008895		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.5109997845008895 | validation: 0.370524933325786]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_525.pth
	Model improved!!!
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48774042803300055		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.48774042803300055 | validation: 0.592819369015493]
	TIME [epoch: 2.7 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4783755234094975		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.4783755234094975 | validation: 0.3833559103960792]
	TIME [epoch: 2.7 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46663340986412233		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.46663340986412233 | validation: 0.59876967831993]
	TIME [epoch: 2.7 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48376911596625094		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.48376911596625094 | validation: 0.3923254468934392]
	TIME [epoch: 2.71 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4871226787665283		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.4871226787665283 | validation: 0.6697935607897205]
	TIME [epoch: 2.7 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5170103546022984		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.5170103546022984 | validation: 0.40147831516868354]
	TIME [epoch: 2.7 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48656049345457486		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.48656049345457486 | validation: 0.6008622726896072]
	TIME [epoch: 2.7 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48360698803727614		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.48360698803727614 | validation: 0.3920817557219725]
	TIME [epoch: 2.7 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.475795797585212		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.475795797585212 | validation: 0.628643784439753]
	TIME [epoch: 2.7 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4938117533982708		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.4938117533982708 | validation: 0.37267739966894414]
	TIME [epoch: 2.7 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47858913420512295		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.47858913420512295 | validation: 0.5962667983531513]
	TIME [epoch: 2.7 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47942126619612163		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.47942126619612163 | validation: 0.37634886153847164]
	TIME [epoch: 2.7 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47152083761901964		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.47152083761901964 | validation: 0.5985396853200281]
	TIME [epoch: 2.7 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.483206843991401		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.483206843991401 | validation: 0.37409734186734106]
	TIME [epoch: 2.7 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46867264378796014		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.46867264378796014 | validation: 0.5907243615310919]
	TIME [epoch: 2.71 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47338148491342175		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.47338148491342175 | validation: 0.3845131164644667]
	TIME [epoch: 2.7 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47089363414965263		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.47089363414965263 | validation: 0.609642075034917]
	TIME [epoch: 2.7 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4838210312299245		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.4838210312299245 | validation: 0.38291504035296886]
	TIME [epoch: 2.7 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4728800326082285		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.4728800326082285 | validation: 0.6395574123279804]
	TIME [epoch: 2.7 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4977496881433477		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.4977496881433477 | validation: 0.37341865118936735]
	TIME [epoch: 2.7 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46796375449053673		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.46796375449053673 | validation: 0.5593510669951283]
	TIME [epoch: 2.7 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46296394855213907		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.46296394855213907 | validation: 0.36156053353964507]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_547.pth
	Model improved!!!
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4665590190527783		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.4665590190527783 | validation: 0.6003175330472867]
	TIME [epoch: 2.7 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47457040073556644		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.47457040073556644 | validation: 0.3674693171630148]
	TIME [epoch: 2.71 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46006883280486144		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.46006883280486144 | validation: 0.5754239169451995]
	TIME [epoch: 2.71 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46554892794131136		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.46554892794131136 | validation: 0.3520396957955443]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_551.pth
	Model improved!!!
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45902514502987773		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.45902514502987773 | validation: 0.6044433471899411]
	TIME [epoch: 2.7 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47733769473986243		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.47733769473986243 | validation: 0.36891281393936803]
	TIME [epoch: 2.7 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4694894602919109		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.4694894602919109 | validation: 0.5819221060377102]
	TIME [epoch: 2.7 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4612824970623594		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.4612824970623594 | validation: 0.36772349650497443]
	TIME [epoch: 2.71 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44364601032372314		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.44364601032372314 | validation: 0.5690886144325277]
	TIME [epoch: 2.7 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45389860672727705		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.45389860672727705 | validation: 0.35770571541307367]
	TIME [epoch: 2.71 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4591688694691669		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.4591688694691669 | validation: 0.6020739434492449]
	TIME [epoch: 2.7 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47372928522475866		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.47372928522475866 | validation: 0.36268951049069637]
	TIME [epoch: 2.7 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45977453773935384		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.45977453773935384 | validation: 0.5505034537034178]
	TIME [epoch: 2.7 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4551095479144225		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.4551095479144225 | validation: 0.3600156474758547]
	TIME [epoch: 2.71 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44336447090993075		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.44336447090993075 | validation: 0.5574834206642432]
	TIME [epoch: 2.7 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.461397166485787		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.461397166485787 | validation: 0.35189364147105434]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_563.pth
	Model improved!!!
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4703797411257923		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.4703797411257923 | validation: 0.6363599762582164]
	TIME [epoch: 2.72 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4990103948533516		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.4990103948533516 | validation: 0.3884921633268503]
	TIME [epoch: 2.73 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43700429205436103		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.43700429205436103 | validation: 0.493039602252183]
	TIME [epoch: 2.73 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4198915301029818		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.4198915301029818 | validation: 0.3777206081214539]
	TIME [epoch: 2.73 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4226312493682807		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.4226312493682807 | validation: 0.5386594346362266]
	TIME [epoch: 2.73 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4491984854166287		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.4491984854166287 | validation: 0.3232105467023283]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_569.pth
	Model improved!!!
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47670069420040895		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.47670069420040895 | validation: 0.6289913536101173]
	TIME [epoch: 2.73 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4961099155287335		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.4961099155287335 | validation: 0.3371714039226823]
	TIME [epoch: 2.73 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44041542975982423		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.44041542975982423 | validation: 0.49219321672525806]
	TIME [epoch: 2.73 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42284082695884495		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.42284082695884495 | validation: 0.38328206386282976]
	TIME [epoch: 2.72 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42537716195287917		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.42537716195287917 | validation: 0.5587085446948724]
	TIME [epoch: 2.72 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45238932375866375		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.45238932375866375 | validation: 0.35002474997521393]
	TIME [epoch: 2.72 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4858421382965948		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.4858421382965948 | validation: 0.6555361319102871]
	TIME [epoch: 2.73 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5101688758906251		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.5101688758906251 | validation: 0.3880368749584113]
	TIME [epoch: 2.72 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4175685551162206		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.4175685551162206 | validation: 0.44805523826517235]
	TIME [epoch: 2.73 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4067306063603371		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.4067306063603371 | validation: 0.4220438469510933]
	TIME [epoch: 2.73 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4117775983689738		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.4117775983689738 | validation: 0.45444238403637666]
	TIME [epoch: 2.73 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4084703167006164		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.4084703167006164 | validation: 0.4152739564594483]
	TIME [epoch: 2.73 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41262423507160534		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.41262423507160534 | validation: 0.46387644975861086]
	TIME [epoch: 2.72 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41366363910764625		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.41366363910764625 | validation: 0.3356671312751758]
	TIME [epoch: 2.73 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4471234819293514		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.4471234819293514 | validation: 0.6697658658470396]
	TIME [epoch: 2.72 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5295564764784851		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.5295564764784851 | validation: 0.2977902104801644]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_585.pth
	Model improved!!!
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45902044747578724		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.45902044747578724 | validation: 0.55043680139619]
	TIME [epoch: 2.72 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4412704862560857		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.4412704862560857 | validation: 0.3499867987130672]
	TIME [epoch: 2.72 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43195061833784637		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.43195061833784637 | validation: 0.6010650518816076]
	TIME [epoch: 2.72 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46488022445976657		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.46488022445976657 | validation: 0.34449346216596927]
	TIME [epoch: 2.72 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4275271985782564		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.4275271985782564 | validation: 0.5486833736527367]
	TIME [epoch: 2.72 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.432400199082836		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.432400199082836 | validation: 0.3357378685274588]
	TIME [epoch: 2.72 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4225307051183446		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.4225307051183446 | validation: 0.5486705425664271]
	TIME [epoch: 2.71 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4322015216658035		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.4322015216658035 | validation: 0.3324270484952626]
	TIME [epoch: 2.72 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4381325818439957		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.4381325818439957 | validation: 0.523315470702894]
	TIME [epoch: 2.71 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4316463909524792		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.4316463909524792 | validation: 0.3453551308751177]
	TIME [epoch: 2.71 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4195350966646164		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.4195350966646164 | validation: 0.5239096051517645]
	TIME [epoch: 2.71 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42883357168329045		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.42883357168329045 | validation: 0.3289699143572238]
	TIME [epoch: 2.71 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42012179678197664		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.42012179678197664 | validation: 0.5314886613748717]
	TIME [epoch: 2.72 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42812968629568177		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.42812968629568177 | validation: 0.34285149439724727]
	TIME [epoch: 2.72 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43048372988535966		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.43048372988535966 | validation: 0.5719411899014716]
	TIME [epoch: 2.72 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45272539990337135		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.45272539990337135 | validation: 0.3605255112123955]
	TIME [epoch: 2.72 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42338878131652685		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.42338878131652685 | validation: 0.4953449533423709]
	TIME [epoch: 2.72 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41464997095984746		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.41464997095984746 | validation: 0.33909466350432826]
	TIME [epoch: 2.74 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4075959219717445		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.4075959219717445 | validation: 0.49620007236789454]
	TIME [epoch: 2.72 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41587304884702875		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.41587304884702875 | validation: 0.3104100338580364]
	TIME [epoch: 2.72 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42202962309312525		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.42202962309312525 | validation: 0.5330304232534375]
	TIME [epoch: 2.71 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43150924781646416		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.43150924781646416 | validation: 0.31607307808288915]
	TIME [epoch: 2.72 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4172584907956869		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.4172584907956869 | validation: 0.5055483140195302]
	TIME [epoch: 2.72 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4174433720523142		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.4174433720523142 | validation: 0.3359082886556346]
	TIME [epoch: 2.72 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4074069715892189		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.4074069715892189 | validation: 0.5245944834919847]
	TIME [epoch: 2.71 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4259722303575817		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.4259722303575817 | validation: 0.34968421395408694]
	TIME [epoch: 2.72 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4262119004080898		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.4262119004080898 | validation: 0.5487689721942428]
	TIME [epoch: 2.72 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42755222004426585		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.42755222004426585 | validation: 0.32518290757464063]
	TIME [epoch: 2.72 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39788452871039826		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.39788452871039826 | validation: 0.47686794895480794]
	TIME [epoch: 2.7 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3923897395536066		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.3923897395536066 | validation: 0.3218291846058352]
	TIME [epoch: 2.73 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4048045044624351		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.4048045044624351 | validation: 0.5165478398226077]
	TIME [epoch: 2.72 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42033093957475454		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.42033093957475454 | validation: 0.29635109511615604]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_617.pth
	Model improved!!!
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4246537249239682		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.4246537249239682 | validation: 0.5188122851344392]
	TIME [epoch: 2.72 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4210912847963113		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.4210912847963113 | validation: 0.3174043713256188]
	TIME [epoch: 2.72 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40101884995649245		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.40101884995649245 | validation: 0.5200614495437406]
	TIME [epoch: 2.72 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41682310162141817		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.41682310162141817 | validation: 0.32751717013948795]
	TIME [epoch: 2.72 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4137439657895056		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.4137439657895056 | validation: 0.5387765356539985]
	TIME [epoch: 2.72 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42746755279502835		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.42746755279502835 | validation: 0.3356156494555682]
	TIME [epoch: 2.72 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4087550598699612		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.4087550598699612 | validation: 0.47969683139912017]
	TIME [epoch: 2.72 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4044100517616465		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.4044100517616465 | validation: 0.3229025719673271]
	TIME [epoch: 2.72 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40307728868134673		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.40307728868134673 | validation: 0.5288990730410232]
	TIME [epoch: 2.72 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4167115733101302		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.4167115733101302 | validation: 0.2985035496728763]
	TIME [epoch: 2.72 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4045410275417629		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.4045410275417629 | validation: 0.4741737097762239]
	TIME [epoch: 2.72 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40814458279307714		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.40814458279307714 | validation: 0.3213613340509711]
	TIME [epoch: 2.72 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3943433194105298		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.3943433194105298 | validation: 0.4925329101016985]
	TIME [epoch: 2.71 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3903845962690358		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.3903845962690358 | validation: 0.30257743420177974]
	TIME [epoch: 2.72 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3996271374972008		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.3996271374972008 | validation: 0.5319864052509787]
	TIME [epoch: 2.72 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4085264894072445		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.4085264894072445 | validation: 0.3279469283228978]
	TIME [epoch: 2.72 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4076803414958404		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.4076803414958404 | validation: 0.5264174734317332]
	TIME [epoch: 2.72 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4183466866534653		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.4183466866534653 | validation: 0.33477003804151456]
	TIME [epoch: 2.72 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3909347877166081		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.3909347877166081 | validation: 0.4591287111738807]
	TIME [epoch: 2.72 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38876923788405054		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.38876923788405054 | validation: 0.30323817190530494]
	TIME [epoch: 2.73 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3937494307876523		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.3937494307876523 | validation: 0.5019442314997574]
	TIME [epoch: 2.72 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4037968787082193		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.4037968787082193 | validation: 0.29966847251401707]
	TIME [epoch: 2.72 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3960520425063315		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.3960520425063315 | validation: 0.4763075976044957]
	TIME [epoch: 2.72 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3974524920631559		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.3974524920631559 | validation: 0.2874045744373387]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_641.pth
	Model improved!!!
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3904014810126313		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.3904014810126313 | validation: 0.49357948969234355]
	TIME [epoch: 2.71 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4050970105668397		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.4050970105668397 | validation: 0.31041608231531204]
	TIME [epoch: 2.72 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38840238699704244		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.38840238699704244 | validation: 0.4568271830397769]
	TIME [epoch: 2.72 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38786752544119696		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.38786752544119696 | validation: 0.30975743120578825]
	TIME [epoch: 2.72 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.390060690852828		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.390060690852828 | validation: 0.5222902709207632]
	TIME [epoch: 2.71 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41145351568061744		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.41145351568061744 | validation: 0.3103014731644895]
	TIME [epoch: 2.72 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40131899102677926		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.40131899102677926 | validation: 0.49385876096560993]
	TIME [epoch: 2.71 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39580086971828593		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.39580086971828593 | validation: 0.33415309083437456]
	TIME [epoch: 2.71 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37127949140561		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.37127949140561 | validation: 0.4541196405039342]
	TIME [epoch: 2.71 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37434189121346356		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.37434189121346356 | validation: 0.3089942875898276]
	TIME [epoch: 2.71 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38856411071130326		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.38856411071130326 | validation: 0.4932574876536746]
	TIME [epoch: 2.72 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4121503203074826		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.4121503203074826 | validation: 0.2826062838603356]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_653.pth
	Model improved!!!
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4069181660997232		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.4069181660997232 | validation: 0.4671154027249845]
	TIME [epoch: 2.72 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39143788828188264		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.39143788828188264 | validation: 0.31899338564926377]
	TIME [epoch: 2.72 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37355283657029503		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.37355283657029503 | validation: 0.44376508989052105]
	TIME [epoch: 2.71 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37913983373333565		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.37913983373333565 | validation: 0.3302761194224894]
	TIME [epoch: 2.71 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38436242317563685		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.38436242317563685 | validation: 0.4843751562435643]
	TIME [epoch: 2.72 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39796043315094815		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.39796043315094815 | validation: 0.2896537533691185]
	TIME [epoch: 2.71 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39436875022081913		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.39436875022081913 | validation: 0.4614788150114375]
	TIME [epoch: 2.71 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3903302369946313		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.3903302369946313 | validation: 0.30318992849589155]
	TIME [epoch: 2.71 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3750629488382212		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.3750629488382212 | validation: 0.42730828928222797]
	TIME [epoch: 2.72 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3627001903426607		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.3627001903426607 | validation: 0.3144228766741595]
	TIME [epoch: 2.72 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37081395162714986		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.37081395162714986 | validation: 0.4519516933101313]
	TIME [epoch: 2.72 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.380444179789685		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.380444179789685 | validation: 0.2691752480124437]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_665.pth
	Model improved!!!
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3929695758927697		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.3929695758927697 | validation: 0.5046326741085192]
	TIME [epoch: 2.72 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40666442439430045		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.40666442439430045 | validation: 0.2802060664555311]
	TIME [epoch: 2.72 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3686931391565127		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.3686931391565127 | validation: 0.40496295934777876]
	TIME [epoch: 2.72 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3582596862234362		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.3582596862234362 | validation: 0.3239413721017794]
	TIME [epoch: 2.73 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3611750840623211		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.3611750840623211 | validation: 0.42210879218171266]
	TIME [epoch: 2.72 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36514111543152644		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.36514111543152644 | validation: 0.29231552731453864]
	TIME [epoch: 2.72 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39766904580428253		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.39766904580428253 | validation: 0.5520925616046407]
	TIME [epoch: 2.71 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43440740571230185		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.43440740571230185 | validation: 0.31307426169918817]
	TIME [epoch: 2.72 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37705618160005266		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.37705618160005266 | validation: 0.40187810638447313]
	TIME [epoch: 2.72 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3530582339373254		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.3530582339373254 | validation: 0.3058028784633408]
	TIME [epoch: 3.47 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3583024009294697		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.3583024009294697 | validation: 0.43908670674016803]
	TIME [epoch: 2.72 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3732767464027667		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.3732767464027667 | validation: 0.27105549395623274]
	TIME [epoch: 2.71 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3864181879922953		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.3864181879922953 | validation: 0.48028745227022684]
	TIME [epoch: 2.72 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38698187226315744		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.38698187226315744 | validation: 0.30336109616507595]
	TIME [epoch: 2.72 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35562831998191097		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.35562831998191097 | validation: 0.4036676506896356]
	TIME [epoch: 2.73 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35357857386929975		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.35357857386929975 | validation: 0.3225714336702238]
	TIME [epoch: 2.72 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3555026104240012		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.3555026104240012 | validation: 0.43424576935930903]
	TIME [epoch: 2.72 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36505163434801885		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.36505163434801885 | validation: 0.27017305997613345]
	TIME [epoch: 2.72 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3792277324720885		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.3792277324720885 | validation: 0.4881253117591525]
	TIME [epoch: 2.72 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40162460707877307		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.40162460707877307 | validation: 0.2927722085168157]
	TIME [epoch: 2.72 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3584080318736972		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.3584080318736972 | validation: 0.3991856493222294]
	TIME [epoch: 2.72 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35635052895745667		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.35635052895745667 | validation: 0.3071145915625406]
	TIME [epoch: 2.72 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34871693952449606		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.34871693952449606 | validation: 0.4415234812786432]
	TIME [epoch: 2.72 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36813515302960725		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.36813515302960725 | validation: 0.24823794316447634]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_689.pth
	Model improved!!!
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38410243250762927		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.38410243250762927 | validation: 0.47182525067059666]
	TIME [epoch: 2.72 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38266031566426506		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.38266031566426506 | validation: 0.28558555475059866]
	TIME [epoch: 2.72 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3512247582610036		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.3512247582610036 | validation: 0.38086449950537526]
	TIME [epoch: 2.72 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3384214718183961		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.3384214718183961 | validation: 0.3481716302084446]
	TIME [epoch: 2.72 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33997767258278916		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.33997767258278916 | validation: 0.34981616812415234]
	TIME [epoch: 2.72 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3413419797509523		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.3413419797509523 | validation: 0.35574503900743587]
	TIME [epoch: 2.72 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3394126125957259		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.3394126125957259 | validation: 0.35901849050829543]
	TIME [epoch: 2.72 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36083906400766		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.36083906400766 | validation: 0.424998063235813]
	TIME [epoch: 2.72 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3649000371832648		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.3649000371832648 | validation: 0.27383712973927604]
	TIME [epoch: 2.72 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38935094998252057		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.38935094998252057 | validation: 0.5293339675522434]
	TIME [epoch: 2.72 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4319564541399684		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.4319564541399684 | validation: 0.3028836789638719]
	TIME [epoch: 2.72 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.358626461132872		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.358626461132872 | validation: 0.3620359367365442]
	TIME [epoch: 2.72 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3311502879075669		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.3311502879075669 | validation: 0.32696766560833357]
	TIME [epoch: 2.72 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32809409592968025		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.32809409592968025 | validation: 0.40450480985803106]
	TIME [epoch: 2.72 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3434534278608555		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.3434534278608555 | validation: 0.2586826558631993]
	TIME [epoch: 2.72 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36843467706292443		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.36843467706292443 | validation: 0.5397026635739309]
	TIME [epoch: 2.72 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4275909390053012		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.4275909390053012 | validation: 0.2936216434822489]
	TIME [epoch: 2.72 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33403131826224325		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.33403131826224325 | validation: 0.3246693706518795]
	TIME [epoch: 2.72 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3299046280036208		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.3299046280036208 | validation: 0.3893404453020398]
	TIME [epoch: 2.72 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3331872748154332		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.3331872748154332 | validation: 0.26053064532315007]
	TIME [epoch: 2.72 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3595939264071325		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.3595939264071325 | validation: 0.48786276352438035]
	TIME [epoch: 2.72 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40043714918818735		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.40043714918818735 | validation: 0.28069769091147356]
	TIME [epoch: 2.72 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3501014797145789		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.3501014797145789 | validation: 0.3987896556257968]
	TIME [epoch: 2.72 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34192491011566506		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.34192491011566506 | validation: 0.2692670736447772]
	TIME [epoch: 2.72 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3418519089251389		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.3418519089251389 | validation: 0.43272690168274563]
	TIME [epoch: 2.72 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35819458991710235		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.35819458991710235 | validation: 0.27106772871069307]
	TIME [epoch: 2.72 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3537757808783125		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.3537757808783125 | validation: 0.41185671621420905]
	TIME [epoch: 2.72 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35231562082300316		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.35231562082300316 | validation: 0.2979661530367252]
	TIME [epoch: 2.71 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3420168026264591		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.3420168026264591 | validation: 0.392425953281548]
	TIME [epoch: 2.72 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.335962973281962		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.335962973281962 | validation: 0.296261797556411]
	TIME [epoch: 2.71 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33882417365589035		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.33882417365589035 | validation: 0.41057821070117284]
	TIME [epoch: 2.72 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3547145768190185		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.3547145768190185 | validation: 0.25053937309760854]
	TIME [epoch: 2.72 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35226643338218805		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.35226643338218805 | validation: 0.42198607924157394]
	TIME [epoch: 2.72 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3491060123206796		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.3491060123206796 | validation: 0.2762072096071586]
	TIME [epoch: 2.72 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33923022274623743		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.33923022274623743 | validation: 0.39478732501472424]
	TIME [epoch: 2.72 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33597240045744287		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.33597240045744287 | validation: 0.27221363163201506]
	TIME [epoch: 2.72 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34280997542833985		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.34280997542833985 | validation: 0.4045081626171387]
	TIME [epoch: 2.72 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3458490630493195		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.3458490630493195 | validation: 0.2749130296583353]
	TIME [epoch: 2.72 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3444708528761331		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.3444708528761331 | validation: 0.40788030460897257]
	TIME [epoch: 2.72 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34792879860094045		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.34792879860094045 | validation: 0.28257101635098164]
	TIME [epoch: 2.72 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3367626248686662		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.3367626248686662 | validation: 0.4147821754909924]
	TIME [epoch: 2.72 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3421628389530747		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.3421628389530747 | validation: 0.26223209823719335]
	TIME [epoch: 2.72 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34227141140099065		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.34227141140099065 | validation: 0.3988371451843715]
	TIME [epoch: 2.72 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34318345778111986		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.34318345778111986 | validation: 0.2640548348088825]
	TIME [epoch: 2.72 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33638074326352885		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.33638074326352885 | validation: 0.37482021732661636]
	TIME [epoch: 2.72 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3215487183825377		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.3215487183825377 | validation: 0.2715319091739626]
	TIME [epoch: 2.72 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3360438186725644		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.3360438186725644 | validation: 0.412859672092089]
	TIME [epoch: 2.72 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34455029390284536		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.34455029390284536 | validation: 0.24593836187536106]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_737.pth
	Model improved!!!
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3550245726497179		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.3550245726497179 | validation: 0.40457996178953676]
	TIME [epoch: 2.72 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3511885280978997		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.3511885280978997 | validation: 0.2814035404551384]
	TIME [epoch: 2.72 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3300952682981672		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.3300952682981672 | validation: 0.36602887400865286]
	TIME [epoch: 2.72 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32764542300930133		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.32764542300930133 | validation: 0.2891776413914063]
	TIME [epoch: 2.72 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33608771488462935		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.33608771488462935 | validation: 0.42291793335491934]
	TIME [epoch: 2.72 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3525534838350614		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.3525534838350614 | validation: 0.27234716677139864]
	TIME [epoch: 2.72 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33970863727594847		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.33970863727594847 | validation: 0.3955139170995923]
	TIME [epoch: 2.72 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33736372935297765		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.33736372935297765 | validation: 0.2675372546056524]
	TIME [epoch: 2.72 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3260071919989596		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.3260071919989596 | validation: 0.38769990049828085]
	TIME [epoch: 2.72 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32754143796450225		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.32754143796450225 | validation: 0.2661842600036771]
	TIME [epoch: 2.72 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3306309928264571		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.3306309928264571 | validation: 0.39288979103384364]
	TIME [epoch: 2.72 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33670157277821877		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.33670157277821877 | validation: 0.268337229987549]
	TIME [epoch: 2.72 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33764777584847133		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.33764777584847133 | validation: 0.3968115178010527]
	TIME [epoch: 2.72 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33421550405480305		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.33421550405480305 | validation: 0.28686679847999985]
	TIME [epoch: 2.72 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3212433314999471		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.3212433314999471 | validation: 0.37558868924809163]
	TIME [epoch: 2.72 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32685641450782216		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.32685641450782216 | validation: 0.2873711220108061]
	TIME [epoch: 2.73 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3297644430277296		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.3297644430277296 | validation: 0.3767723232795411]
	TIME [epoch: 2.72 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3263876278943873		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.3263876278943873 | validation: 0.26398020126963184]
	TIME [epoch: 2.73 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32934291390023634		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.32934291390023634 | validation: 0.4252987525386642]
	TIME [epoch: 2.72 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.341457863111382		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.341457863111382 | validation: 0.26491017452494103]
	TIME [epoch: 2.72 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3281583124394348		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.3281583124394348 | validation: 0.38624878343539465]
	TIME [epoch: 2.72 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3291151262177164		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.3291151262177164 | validation: 0.2788937909544538]
	TIME [epoch: 2.73 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3206480441965291		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.3206480441965291 | validation: 0.37865356509072245]
	TIME [epoch: 2.72 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31761303445795663		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.31761303445795663 | validation: 0.2665796884615039]
	TIME [epoch: 2.72 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31989136371019594		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.31989136371019594 | validation: 0.37870828648552807]
	TIME [epoch: 2.72 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3281214690195725		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.3281214690195725 | validation: 0.2551167879705276]
	TIME [epoch: 2.72 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3312671575732339		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.3312671575732339 | validation: 0.41707895525519345]
	TIME [epoch: 2.72 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3404678938119195		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.3404678938119195 | validation: 0.27311787722208924]
	TIME [epoch: 2.72 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3285200818426932		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.3285200818426932 | validation: 0.3646848882367496]
	TIME [epoch: 2.73 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32236581674753945		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.32236581674753945 | validation: 0.29173790632897095]
	TIME [epoch: 2.72 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3098769829177104		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.3098769829177104 | validation: 0.29635843422933483]
	TIME [epoch: 2.72 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3066388718632593		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.3066388718632593 | validation: 0.33561432759353543]
	TIME [epoch: 2.71 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30648794434297416		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.30648794434297416 | validation: 0.2658716732831544]
	TIME [epoch: 2.72 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3112480737067079		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.3112480737067079 | validation: 0.39671084323712147]
	TIME [epoch: 2.72 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3342702982833694		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.3342702982833694 | validation: 0.2438109018616615]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_772.pth
	Model improved!!!
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3392125121195072		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.3392125121195072 | validation: 0.4088040754730973]
	TIME [epoch: 2.72 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3411565124255175		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.3411565124255175 | validation: 0.25692295425424655]
	TIME [epoch: 2.72 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31698046160192034		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.31698046160192034 | validation: 0.34043295070865254]
	TIME [epoch: 2.73 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.301563667133606		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.301563667133606 | validation: 0.27694730618181645]
	TIME [epoch: 2.72 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3059408081395717		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.3059408081395717 | validation: 0.344840262062596]
	TIME [epoch: 2.73 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30729790429024867		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.30729790429024867 | validation: 0.28223766609412315]
	TIME [epoch: 2.72 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3135866538890534		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.3135866538890534 | validation: 0.3698012867596554]
	TIME [epoch: 2.72 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31735214156150215		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.31735214156150215 | validation: 0.26068265823142917]
	TIME [epoch: 2.72 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33961459806297245		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.33961459806297245 | validation: 0.41379007120145617]
	TIME [epoch: 2.72 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3427500188988309		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.3427500188988309 | validation: 0.2706896739164691]
	TIME [epoch: 2.73 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3122855484233599		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.3122855484233599 | validation: 0.3620741301917004]
	TIME [epoch: 2.73 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3085217026111116		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.3085217026111116 | validation: 0.2645773772816347]
	TIME [epoch: 2.73 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3130693247683212		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.3130693247683212 | validation: 0.38964241906906216]
	TIME [epoch: 2.72 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32095601755514025		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.32095601755514025 | validation: 0.2379872171082371]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_786.pth
	Model improved!!!
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32710844180972176		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.32710844180972176 | validation: 0.3442212697532903]
	TIME [epoch: 2.72 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31157271034811934		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.31157271034811934 | validation: 0.2628101784523243]
	TIME [epoch: 2.73 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3052685176031027		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.3052685176031027 | validation: 0.34427417420112555]
	TIME [epoch: 2.73 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3063059359025991		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.3063059359025991 | validation: 0.26837482450709116]
	TIME [epoch: 2.72 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3015636823518078		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.3015636823518078 | validation: 0.3612198363366007]
	TIME [epoch: 2.73 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.321387018146762		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.321387018146762 | validation: 0.22359353612884147]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_792.pth
	Model improved!!!
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3260646958735703		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.3260646958735703 | validation: 0.3954880466476615]
	TIME [epoch: 2.72 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32945932739972483		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.32945932739972483 | validation: 0.25986793729814966]
	TIME [epoch: 2.72 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3018700825444379		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.3018700825444379 | validation: 0.3395721388900045]
	TIME [epoch: 2.72 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2975761129056758		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.2975761129056758 | validation: 0.25396702726112214]
	TIME [epoch: 2.72 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30320753104855164		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.30320753104855164 | validation: 0.35123564428620946]
	TIME [epoch: 2.73 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3107928569168496		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.3107928569168496 | validation: 0.2411979259044129]
	TIME [epoch: 2.73 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31553968887332867		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.31553968887332867 | validation: 0.3942456014091998]
	TIME [epoch: 2.73 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3248736166894625		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.3248736166894625 | validation: 0.25052747703944095]
	TIME [epoch: 2.73 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30321520115639755		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.30321520115639755 | validation: 0.3381297067511229]
	TIME [epoch: 2.73 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.298357278195515		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.298357278195515 | validation: 0.26438424240197483]
	TIME [epoch: 2.73 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3034566536573249		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.3034566536573249 | validation: 0.35818699470072246]
	TIME [epoch: 2.73 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30806336236488474		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.30806336236488474 | validation: 0.23693926130556733]
	TIME [epoch: 2.73 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31165539077323673		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.31165539077323673 | validation: 0.3755339951869575]
	TIME [epoch: 2.73 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3166663006242707		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.3166663006242707 | validation: 0.26413016193402805]
	TIME [epoch: 2.73 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3081800464611103		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.3081800464611103 | validation: 0.3477974450155836]
	TIME [epoch: 2.73 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3066709497271478		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.3066709497271478 | validation: 0.25126668543639263]
	TIME [epoch: 2.73 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31867453808047036		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.31867453808047036 | validation: 0.3491691626075662]
	TIME [epoch: 2.75 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31216565105004246		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.31216565105004246 | validation: 0.2620313631185196]
	TIME [epoch: 2.73 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30706874346699065		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.30706874346699065 | validation: 0.3333262300724504]
	TIME [epoch: 2.73 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2971653869009213		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.2971653869009213 | validation: 0.2629131752155604]
	TIME [epoch: 2.72 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2986891526881006		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.2986891526881006 | validation: 0.3367964005254145]
	TIME [epoch: 2.72 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.302872507775681		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.302872507775681 | validation: 0.23986700153896026]
	TIME [epoch: 2.72 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30911712542523534		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.30911712542523534 | validation: 0.3893732760689117]
	TIME [epoch: 2.72 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31669240427017914		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.31669240427017914 | validation: 0.22831686858338585]
	TIME [epoch: 2.72 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31220516448878594		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.31220516448878594 | validation: 0.3713271141268487]
	TIME [epoch: 2.72 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3131883868921975		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.3131883868921975 | validation: 0.2659364001109225]
	TIME [epoch: 2.72 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30374726419720494		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.30374726419720494 | validation: 0.33342844854591996]
	TIME [epoch: 2.72 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29872151594240104		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.29872151594240104 | validation: 0.2766780550497865]
	TIME [epoch: 2.73 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29499608426314594		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.29499608426314594 | validation: 0.3247684469019816]
	TIME [epoch: 2.72 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2939071062769369		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.2939071062769369 | validation: 0.26882888450411374]
	TIME [epoch: 2.72 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2987583413028721		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.2987583413028721 | validation: 0.3604888651605507]
	TIME [epoch: 2.72 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.303563137467483		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.303563137467483 | validation: 0.24011360471318716]
	TIME [epoch: 2.73 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3205028996777248		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.3205028996777248 | validation: 0.3823160647610624]
	TIME [epoch: 2.72 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3138752468020899		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.3138752468020899 | validation: 0.2471498959792644]
	TIME [epoch: 2.75 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30180482025233824		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.30180482025233824 | validation: 0.34042975662308195]
	TIME [epoch: 2.72 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2965480423352694		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.2965480423352694 | validation: 0.2574252117281485]
	TIME [epoch: 2.72 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.296313986045429		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.296313986045429 | validation: 0.33355311893025696]
	TIME [epoch: 2.72 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29872434587457847		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.29872434587457847 | validation: 0.23107190257598706]
	TIME [epoch: 2.72 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3053641756400134		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.3053641756400134 | validation: 0.3863736959870268]
	TIME [epoch: 2.73 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3249316747536722		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.3249316747536722 | validation: 0.2687282300915842]
	TIME [epoch: 2.72 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2899551330601421		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.2899551330601421 | validation: 0.31021883932904426]
	TIME [epoch: 2.72 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29103708361754144		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.29103708361754144 | validation: 0.27438944919222313]
	TIME [epoch: 2.72 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2919607323668893		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.2919607323668893 | validation: 0.31286094081564847]
	TIME [epoch: 2.72 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2871763693241948		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.2871763693241948 | validation: 0.2621541757564093]
	TIME [epoch: 2.72 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2909381857378698		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.2909381857378698 | validation: 0.3757047778860631]
	TIME [epoch: 2.72 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31081345838730207		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.31081345838730207 | validation: 0.2161000557646673]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_838.pth
	Model improved!!!
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.317573607577599		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.317573607577599 | validation: 0.37936057473419543]
	TIME [epoch: 2.72 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3115519109198186		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.3115519109198186 | validation: 0.2661019057161894]
	TIME [epoch: 2.72 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2963500161716175		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.2963500161716175 | validation: 0.3275174332244318]
	TIME [epoch: 2.73 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2905783849214626		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.2905783849214626 | validation: 0.2673155657086303]
	TIME [epoch: 2.72 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29664561963224834		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.29664561963224834 | validation: 0.3796261299053696]
	TIME [epoch: 2.75 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3041864539947674		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.3041864539947674 | validation: 0.23978434457707404]
	TIME [epoch: 2.72 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3061109904309786		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.3061109904309786 | validation: 0.34087290292329353]
	TIME [epoch: 2.72 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29892330195281774		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.29892330195281774 | validation: 0.26327304439309346]
	TIME [epoch: 2.72 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2929031182026904		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.2929031182026904 | validation: 0.2912211354161773]
	TIME [epoch: 2.72 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28694353930857674		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.28694353930857674 | validation: 0.27959729039186665]
	TIME [epoch: 2.72 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2850411782168573		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.2850411782168573 | validation: 0.2930438262738279]
	TIME [epoch: 2.72 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28615144623276506		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.28615144623276506 | validation: 0.26865886599890976]
	TIME [epoch: 2.72 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28257447360735727		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.28257447360735727 | validation: 0.3138133215713071]
	TIME [epoch: 2.72 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28662613263314507		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.28662613263314507 | validation: 0.2847376893865835]
	TIME [epoch: 2.72 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28416344241757713		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.28416344241757713 | validation: 0.30265001622556076]
	TIME [epoch: 2.72 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2854205105587497		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.2854205105587497 | validation: 0.28504922056968257]
	TIME [epoch: 2.72 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28185260270917883		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.28185260270917883 | validation: 0.2951416013378758]
	TIME [epoch: 2.72 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2841788466348953		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.2841788466348953 | validation: 0.35892212535715823]
	TIME [epoch: 2.72 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31425424476938046		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.31425424476938046 | validation: 0.1945923134734065]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_857.pth
	Model improved!!!
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3497032086902425		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.3497032086902425 | validation: 0.3840412981581688]
	TIME [epoch: 2.71 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32560320545163857		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.32560320545163857 | validation: 0.29571413618495973]
	TIME [epoch: 2.72 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28666178011768284		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.28666178011768284 | validation: 0.2809600967688461]
	TIME [epoch: 2.71 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.280825189707893		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.280825189707893 | validation: 0.3401676832178877]
	TIME [epoch: 2.7 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2921067885392737		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.2921067885392737 | validation: 0.24294700298172286]
	TIME [epoch: 2.7 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30125221678965525		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.30125221678965525 | validation: 0.33831683708691357]
	TIME [epoch: 2.71 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30090576441514105		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.30090576441514105 | validation: 0.2564863861960425]
	TIME [epoch: 2.72 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2801704163044182		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.2801704163044182 | validation: 0.2682717088401206]
	TIME [epoch: 2.72 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2820940686657285		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.2820940686657285 | validation: 0.3497122984152715]
	TIME [epoch: 2.72 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29600552540617764		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.29600552540617764 | validation: 0.240285785290925]
	TIME [epoch: 2.72 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2957736781585814		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.2957736781585814 | validation: 0.34836820256678336]
	TIME [epoch: 2.72 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2912763313852583		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.2912763313852583 | validation: 0.25334746847961226]
	TIME [epoch: 2.72 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2798347992111		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.2798347992111 | validation: 0.3199292224739602]
	TIME [epoch: 2.72 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2874448843831519		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.2874448843831519 | validation: 0.25526450281682417]
	TIME [epoch: 2.72 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28880858067577314		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.28880858067577314 | validation: 0.35366996291134983]
	TIME [epoch: 2.72 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3027769767231863		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.3027769767231863 | validation: 0.25202361749777535]
	TIME [epoch: 2.72 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2865651018399278		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.2865651018399278 | validation: 0.3070446210166693]
	TIME [epoch: 2.72 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28144564065499855		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.28144564065499855 | validation: 0.28676039872975273]
	TIME [epoch: 2.72 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2776420999001069		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.2776420999001069 | validation: 0.30082976294086194]
	TIME [epoch: 2.71 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27679889165587446		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.27679889165587446 | validation: 0.27550803290083276]
	TIME [epoch: 2.73 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.278709282986867		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.278709282986867 | validation: 0.3206354181101999]
	TIME [epoch: 2.72 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28039965907755204		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.28039965907755204 | validation: 0.22353967169587374]
	TIME [epoch: 2.72 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30080047195205684		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.30080047195205684 | validation: 0.38699350975386615]
	TIME [epoch: 2.72 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3323773480727967		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.3323773480727967 | validation: 0.2518707470325843]
	TIME [epoch: 2.72 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27966270630837203		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.27966270630837203 | validation: 0.30264927914949813]
	TIME [epoch: 2.71 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27671810166993693		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.27671810166993693 | validation: 0.2905871854120154]
	TIME [epoch: 2.71 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27989932791700556		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.27989932791700556 | validation: 0.26572510552025214]
	TIME [epoch: 2.72 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2775888474850717		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.2775888474850717 | validation: 0.32367478851454207]
	TIME [epoch: 2.72 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28070648110137003		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.28070648110137003 | validation: 0.24189882560217524]
	TIME [epoch: 2.72 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28950815429214155		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.28950815429214155 | validation: 0.3632151442534823]
	TIME [epoch: 2.72 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.300975553880674		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.300975553880674 | validation: 0.23755408951897175]
	TIME [epoch: 2.72 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28690281660779643		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.28690281660779643 | validation: 0.31982278102120665]
	TIME [epoch: 2.72 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2815720092887905		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.2815720092887905 | validation: 0.27230564592456474]
	TIME [epoch: 2.72 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28090701017026165		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.28090701017026165 | validation: 0.33000778500271766]
	TIME [epoch: 2.72 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28626219538913883		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.28626219538913883 | validation: 0.24103848396147828]
	TIME [epoch: 2.72 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29058518732722355		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.29058518732722355 | validation: 0.34970378193195245]
	TIME [epoch: 2.72 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2925157424209009		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.2925157424209009 | validation: 0.24195180224380808]
	TIME [epoch: 2.74 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28372781826598475		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.28372781826598475 | validation: 0.3204816969169783]
	TIME [epoch: 2.72 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2857107791131702		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.2857107791131702 | validation: 0.25985811426417327]
	TIME [epoch: 2.72 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2784460773815888		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.2784460773815888 | validation: 0.26326760585677544]
	TIME [epoch: 2.72 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2783250036404447		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.2783250036404447 | validation: 0.328404829386674]
	TIME [epoch: 2.71 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28253755958905785		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.28253755958905785 | validation: 0.2252265556886425]
	TIME [epoch: 2.71 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29936179228894017		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.29936179228894017 | validation: 0.36054689150878155]
	TIME [epoch: 2.72 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2960543292174695		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.2960543292174695 | validation: 0.2525491432030408]
	TIME [epoch: 2.72 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28334950423862404		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.28334950423862404 | validation: 0.30180021375029065]
	TIME [epoch: 2.72 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27221433722038213		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.27221433722038213 | validation: 0.2918474967303503]
	TIME [epoch: 2.72 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2741714672607205		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.2741714672607205 | validation: 0.2706234155347479]
	TIME [epoch: 2.72 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27187958714703053		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.27187958714703053 | validation: 0.3131286469361172]
	TIME [epoch: 2.72 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.279956276552847		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.279956276552847 | validation: 0.24582503850164172]
	TIME [epoch: 2.72 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28216246976140513		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.28216246976140513 | validation: 0.3578910558855979]
	TIME [epoch: 2.73 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30160366578491077		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.30160366578491077 | validation: 0.23900297520639865]
	TIME [epoch: 2.73 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28775211103788645		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.28775211103788645 | validation: 0.3064231816981993]
	TIME [epoch: 2.72 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2711767410620856		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.2711767410620856 | validation: 0.26779429224501977]
	TIME [epoch: 2.71 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26978505082300763		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.26978505082300763 | validation: 0.26026820404754547]
	TIME [epoch: 2.73 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2704522757183718		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.2704522757183718 | validation: 0.31516344981923183]
	TIME [epoch: 2.73 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2784416685602653		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.2784416685602653 | validation: 0.22411369561540326]
	TIME [epoch: 2.73 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30187096136726654		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.30187096136726654 | validation: 0.3610536898418458]
	TIME [epoch: 2.73 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29951365420366605		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.29951365420366605 | validation: 0.26679507541023534]
	TIME [epoch: 2.71 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2731630995673827		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.2731630995673827 | validation: 0.27335656288099014]
	TIME [epoch: 2.71 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2681931612035325		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.2681931612035325 | validation: 0.2928990808249962]
	TIME [epoch: 2.71 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2709850486526636		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.2709850486526636 | validation: 0.2923612240663176]
	TIME [epoch: 2.71 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2733760433833921		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.2733760433833921 | validation: 0.25971944871846125]
	TIME [epoch: 2.71 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27418723412561624		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.27418723412561624 | validation: 0.31166778399983763]
	TIME [epoch: 2.71 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2751525151182095		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.2751525151182095 | validation: 0.24336464360885057]
	TIME [epoch: 2.73 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27905101354756395		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.27905101354756395 | validation: 0.3362121724526017]
	TIME [epoch: 2.72 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2855619726442174		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.2855619726442174 | validation: 0.22209990164266072]
	TIME [epoch: 2.72 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29792837429379576		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.29792837429379576 | validation: 0.3579491656725411]
	TIME [epoch: 2.72 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28329223893484284		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.28329223893484284 | validation: 0.26822646648478593]
	TIME [epoch: 2.72 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2743554999082686		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.2743554999082686 | validation: 0.269378062662729]
	TIME [epoch: 2.72 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2695166594988801		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.2695166594988801 | validation: 0.2829025462494346]
	TIME [epoch: 2.73 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2715303600595368		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.2715303600595368 | validation: 0.2859009459918043]
	TIME [epoch: 2.75 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2719089535160289		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.2719089535160289 | validation: 0.24702759713807618]
	TIME [epoch: 2.74 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27135387194757316		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.27135387194757316 | validation: 0.31558070422732176]
	TIME [epoch: 2.74 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2835805228011017		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.2835805228011017 | validation: 0.22766041452583266]
	TIME [epoch: 2.73 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29905668348819303		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.29905668348819303 | validation: 0.34369730419221894]
	TIME [epoch: 2.74 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.286682113813363		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.286682113813363 | validation: 0.2655665407372948]
	TIME [epoch: 2.73 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26725034972529743		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.26725034972529743 | validation: 0.2761755079792943]
	TIME [epoch: 2.74 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26876005281002896		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.26876005281002896 | validation: 0.2712951054433434]
	TIME [epoch: 2.73 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2734869771114752		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.2734869771114752 | validation: 0.26654193684737754]
	TIME [epoch: 2.74 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2635399265309045		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.2635399265309045 | validation: 0.26296604125640627]
	TIME [epoch: 2.74 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26822311905450713		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.26822311905450713 | validation: 0.2885141138231448]
	TIME [epoch: 2.73 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26759569211215567		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.26759569211215567 | validation: 0.28053430258102363]
	TIME [epoch: 2.74 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26560749181945015		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.26560749181945015 | validation: 0.3178047837802196]
	TIME [epoch: 2.74 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27962005107266225		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.27962005107266225 | validation: 0.2312823571998399]
	TIME [epoch: 2.73 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2905297050722206		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.2905297050722206 | validation: 0.35413879320464453]
	TIME [epoch: 2.73 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3024476320093554		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.3024476320093554 | validation: 0.25388635236534984]
	TIME [epoch: 2.73 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2709637306680011		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.2709637306680011 | validation: 0.2773063906414772]
	TIME [epoch: 2.73 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26631193654297974		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.26631193654297974 | validation: 0.28764173606480875]
	TIME [epoch: 2.72 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26766464055427086		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.26766464055427086 | validation: 0.25523152705064484]
	TIME [epoch: 2.72 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2740698731759935		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.2740698731759935 | validation: 0.32552901095439396]
	TIME [epoch: 2.72 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2834140360911504		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.2834140360911504 | validation: 0.22188033906297316]
	TIME [epoch: 2.72 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28503155603636693		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.28503155603636693 | validation: 0.31869373962631803]
	TIME [epoch: 2.72 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27926817392440517		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.27926817392440517 | validation: 0.25210535487044844]
	TIME [epoch: 2.73 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2737324710156272		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.2737324710156272 | validation: 0.2939077954891327]
	TIME [epoch: 2.72 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26914250664220696		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.26914250664220696 | validation: 0.2579891054633049]
	TIME [epoch: 2.72 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2725927527399002		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.2725927527399002 | validation: 0.2974474223502255]
	TIME [epoch: 2.72 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2675619603594242		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.2675619603594242 | validation: 0.2533046506087177]
	TIME [epoch: 2.72 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27454702888374555		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.27454702888374555 | validation: 0.29674408579522027]
	TIME [epoch: 2.72 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2674812463661228		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.2674812463661228 | validation: 0.2679997261333845]
	TIME [epoch: 2.72 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26759445501663354		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.26759445501663354 | validation: 0.27956097182399486]
	TIME [epoch: 2.72 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26641903647249315		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.26641903647249315 | validation: 0.2817933599235813]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_182153/states/model_phi1_4a_distortion_v2_4_v_mmd1_958.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 2348.744 seconds.
