Args:
Namespace(name='model_phi1_4a_distortion_v2_3_v_mmd1', outdir='out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_3/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_3/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2027285107

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.045598521228696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.045598521228696 | validation: 5.2331890075429515]
	TIME [epoch: 165 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.706945277608541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.706945277608541 | validation: 4.668533281914674]
	TIME [epoch: 0.755 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.523736963554509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.523736963554509 | validation: 3.6852482634433517]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008106618079474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.008106618079474 | validation: 3.4257431989529525]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.461146712239856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.461146712239856 | validation: 3.2130561023387645]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9366859604103923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9366859604103923 | validation: 2.56235336960456]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.192937782422674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.192937782422674 | validation: 1.971571306620463]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.94318312825431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.94318312825431 | validation: 1.7438900774743988]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7613074165365434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7613074165365434 | validation: 1.6516950193329727]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.598888004042051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.598888004042051 | validation: 1.5069165249125551]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.443003894433715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.443003894433715 | validation: 1.3714648456475926]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.341461079327163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.341461079327163 | validation: 1.3446642763347367]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2178258803923705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2178258803923705 | validation: 1.3779816304846166]
	TIME [epoch: 0.688 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1151807678882455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1151807678882455 | validation: 1.3844503884348527]
	TIME [epoch: 0.687 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0336141236212595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0336141236212595 | validation: 1.4055154366037594]
	TIME [epoch: 0.685 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9522367766467204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9522367766467204 | validation: 1.3726707645683915]
	TIME [epoch: 0.688 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.880608165255045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.880608165255045 | validation: 1.3206005299026422]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.808519352279322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.808519352279322 | validation: 1.260713388410835]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7601383881616641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7601383881616641 | validation: 1.2034820716644068]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.734897275696942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.734897275696942 | validation: 1.2518138952955948]
	TIME [epoch: 0.689 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7068152250719126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7068152250719126 | validation: 1.206907363841237]
	TIME [epoch: 0.689 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6880563377903246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6880563377903246 | validation: 1.2347927790723607]
	TIME [epoch: 0.694 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.668488826331605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.668488826331605 | validation: 1.234741833698472]
	TIME [epoch: 0.688 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6432001980467388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6432001980467388 | validation: 1.2251386143310474]
	TIME [epoch: 0.686 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6203718423508273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6203718423508273 | validation: 1.2582378602645896]
	TIME [epoch: 0.689 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6048386571730147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6048386571730147 | validation: 1.1568452057104162]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6023225609990284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6023225609990284 | validation: 1.477816686377017]
	TIME [epoch: 0.687 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.733398444123947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.733398444123947 | validation: 0.8982113211644226]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.786886034277315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.786886034277315 | validation: 1.2049348113769682]
	TIME [epoch: 0.687 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.559432983422222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.559432983422222 | validation: 1.3261275430618908]
	TIME [epoch: 0.684 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.594155448547742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.594155448547742 | validation: 1.1134876602316621]
	TIME [epoch: 0.686 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.558032991961507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.558032991961507 | validation: 1.1871960808779112]
	TIME [epoch: 0.686 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5319930497900094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5319930497900094 | validation: 1.2664000301727654]
	TIME [epoch: 0.684 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5292035427162898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5292035427162898 | validation: 1.173434019952901]
	TIME [epoch: 0.683 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5228653637481266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5228653637481266 | validation: 1.214516797217365]
	TIME [epoch: 0.685 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5057809617425297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5057809617425297 | validation: 1.2455902569560078]
	TIME [epoch: 0.686 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5091579157576696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5091579157576696 | validation: 1.1861833355748341]
	TIME [epoch: 0.684 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4924696554406935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4924696554406935 | validation: 1.2708924624309932]
	TIME [epoch: 0.682 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.483102219146099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.483102219146099 | validation: 1.1532391546527858]
	TIME [epoch: 0.684 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.497910684628977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.497910684628977 | validation: 1.3164238089756175]
	TIME [epoch: 0.687 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4941723207701676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4941723207701676 | validation: 1.0833386430398875]
	TIME [epoch: 0.686 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4964790195989497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4964790195989497 | validation: 1.3886604191021523]
	TIME [epoch: 0.685 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.506492172537274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.506492172537274 | validation: 1.0855080952824725]
	TIME [epoch: 0.684 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4943357836139828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4943357836139828 | validation: 1.3637227745403906]
	TIME [epoch: 0.685 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.492913572301149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.492913572301149 | validation: 1.1742633820772896]
	TIME [epoch: 0.685 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4681540371528679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4681540371528679 | validation: 1.274592091241448]
	TIME [epoch: 0.685 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4486321241137077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4486321241137077 | validation: 1.2233262813374264]
	TIME [epoch: 0.686 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4438063385818785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4438063385818785 | validation: 1.2851101541723804]
	TIME [epoch: 0.685 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4452165983585672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4452165983585672 | validation: 1.176952203286947]
	TIME [epoch: 0.686 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4376752712985823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4376752712985823 | validation: 1.304553078729573]
	TIME [epoch: 0.685 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4364366535696158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4364366535696158 | validation: 1.1322008220538995]
	TIME [epoch: 0.685 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4397443085881014		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.4397443085881014 | validation: 1.4449407640802103]
	TIME [epoch: 0.686 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4766682833437355		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.4766682833437355 | validation: 1.0232544413013682]
	TIME [epoch: 0.684 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4927136065778246		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.4927136065778246 | validation: 1.4330584421669676]
	TIME [epoch: 0.683 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.477459398905147		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.477459398905147 | validation: 1.2376530834004025]
	TIME [epoch: 0.685 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4120387877379899		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.4120387877379899 | validation: 1.1260765128207282]
	TIME [epoch: 0.685 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4315032077551757		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.4315032077551757 | validation: 1.3706101333205059]
	TIME [epoch: 0.685 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.435554673378492		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.435554673378492 | validation: 1.2167878379823505]
	TIME [epoch: 0.684 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.40359632986964		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.40359632986964 | validation: 1.224071269227254]
	TIME [epoch: 0.685 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4055712202581234		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.4055712202581234 | validation: 1.3027388879482342]
	TIME [epoch: 0.685 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.403241339369452		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.403241339369452 | validation: 1.1849223725492608]
	TIME [epoch: 0.684 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4038662104379005		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.4038662104379005 | validation: 1.3555449997355733]
	TIME [epoch: 0.686 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.409780121734886		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.409780121734886 | validation: 1.1514502249578766]
	TIME [epoch: 0.685 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4054254907013932		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.4054254907013932 | validation: 1.3878522153875612]
	TIME [epoch: 0.697 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4059214860751772		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.4059214860751772 | validation: 1.1351709200207003]
	TIME [epoch: 0.687 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4089663728404127		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.4089663728404127 | validation: 1.415801904753821]
	TIME [epoch: 0.685 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4250636081799208		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.4250636081799208 | validation: 1.1537275301257177]
	TIME [epoch: 0.684 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3884274854522471		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.3884274854522471 | validation: 1.2844926313254128]
	TIME [epoch: 0.684 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3730652308331595		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.3730652308331595 | validation: 1.2492167382976895]
	TIME [epoch: 0.686 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3658213518847802		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.3658213518847802 | validation: 1.2228373200897396]
	TIME [epoch: 0.685 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3612619512268485		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.3612619512268485 | validation: 1.2842927018233183]
	TIME [epoch: 0.686 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.355234547641264		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.355234547641264 | validation: 1.1685194943600956]
	TIME [epoch: 0.686 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3547559720817066		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.3547559720817066 | validation: 1.479890394159632]
	TIME [epoch: 0.687 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.417068158826255		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.417068158826255 | validation: 0.9319622234016127]
	TIME [epoch: 0.685 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.593746788337424		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.593746788337424 | validation: 1.4207335857830867]
	TIME [epoch: 0.685 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3932589914700353		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.3932589914700353 | validation: 1.3411133491248848]
	TIME [epoch: 0.685 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3617767058603585		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.3617767058603585 | validation: 1.0760562015787252]
	TIME [epoch: 0.685 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3639926550947974		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.3639926550947974 | validation: 1.2157698546393618]
	TIME [epoch: 0.685 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.325146286363822		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.325146286363822 | validation: 1.2757597279884068]
	TIME [epoch: 0.686 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.318772829652563		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.318772829652563 | validation: 1.1452837809093919]
	TIME [epoch: 0.684 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.313302703317292		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.313302703317292 | validation: 1.258031219605056]
	TIME [epoch: 0.686 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2890230292161107		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.2890230292161107 | validation: 1.1771440393497898]
	TIME [epoch: 0.684 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2704426184736761		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.2704426184736761 | validation: 1.2988147529928322]
	TIME [epoch: 0.684 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2523557489291535		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.2523557489291535 | validation: 0.9779758895886911]
	TIME [epoch: 0.686 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3383486070297657		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.3383486070297657 | validation: 1.7404032385652641]
	TIME [epoch: 0.687 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6891939766649904		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.6891939766649904 | validation: 1.0343859675884446]
	TIME [epoch: 0.686 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.208054532281152		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.208054532281152 | validation: 0.9520448620158897]
	TIME [epoch: 0.685 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2368097899013708		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.2368097899013708 | validation: 1.255911372834731]
	TIME [epoch: 0.685 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.219417776667671		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.219417776667671 | validation: 1.0667429780976154]
	TIME [epoch: 0.685 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1306831026586976		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.1306831026586976 | validation: 1.0078652815602218]
	TIME [epoch: 0.683 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0985513092875638		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.0985513092875638 | validation: 1.207096608846518]
	TIME [epoch: 0.686 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0914173521144412		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.0914173521144412 | validation: 0.8339299044718292]
	TIME [epoch: 0.684 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1548013657518494		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.1548013657518494 | validation: 1.279191914229538]
	TIME [epoch: 0.688 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2232119827222119		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.2232119827222119 | validation: 0.7680644907182457]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0963598667760255		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.0963598667760255 | validation: 0.9581770441850245]
	TIME [epoch: 0.69 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9054173664479729		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 0.9054173664479729 | validation: 0.941091338883788]
	TIME [epoch: 0.688 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8375339876328978		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.8375339876328978 | validation: 0.8073300724593611]
	TIME [epoch: 0.685 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8302081370110773		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.8302081370110773 | validation: 1.0922260889962099]
	TIME [epoch: 0.686 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9775246946804488		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.9775246946804488 | validation: 0.7956214501189441]
	TIME [epoch: 0.685 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2070206538170656		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.2070206538170656 | validation: 0.8818816109908563]
	TIME [epoch: 0.685 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7568490431769829		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.7568490431769829 | validation: 0.8917445779723802]
	TIME [epoch: 0.684 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7436203046734702		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.7436203046734702 | validation: 0.7101330275502085]
	TIME [epoch: 0.682 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8155407899497488		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.8155407899497488 | validation: 0.9285768496047135]
	TIME [epoch: 0.686 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7845520214087094		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.7845520214087094 | validation: 0.7010950231034508]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.724844267472742		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.724844267472742 | validation: 0.8267746565669966]
	TIME [epoch: 0.691 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.677088885516323		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 0.677088885516323 | validation: 0.674623136577936]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6531892626333762		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.6531892626333762 | validation: 0.8202493646842985]
	TIME [epoch: 0.685 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6791320351301756		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.6791320351301756 | validation: 0.6336852291916085]
	TIME [epoch: 0.681 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7260501355193941		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.7260501355193941 | validation: 0.9188820367435953]
	TIME [epoch: 0.683 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7757575172354869		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 0.7757575172354869 | validation: 0.6613439579007651]
	TIME [epoch: 0.681 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6517154298305823		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.6517154298305823 | validation: 0.6624540449479961]
	TIME [epoch: 0.681 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5655929676685657		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.5655929676685657 | validation: 0.6878106713152843]
	TIME [epoch: 0.679 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.564001400598042		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.564001400598042 | validation: 0.6027856010079554]
	TIME [epoch: 0.679 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5778076310384842		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.5778076310384842 | validation: 0.8956082313133527]
	TIME [epoch: 0.684 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7213016306323736		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.7213016306323736 | validation: 0.6194747270065677]
	TIME [epoch: 0.682 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6922925687484758		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.6922925687484758 | validation: 0.7317224938464589]
	TIME [epoch: 0.68 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5919388437086124		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.5919388437086124 | validation: 0.5361964528229707]
	TIME [epoch: 0.682 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.649664570497831		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.649664570497831 | validation: 0.927734938460225]
	TIME [epoch: 0.682 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7690839356198561		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.7690839356198561 | validation: 0.659730389568654]
	TIME [epoch: 0.68 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6349206472722223		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.6349206472722223 | validation: 0.5794326913180922]
	TIME [epoch: 0.683 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5435455015817003		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.5435455015817003 | validation: 0.6755347355207645]
	TIME [epoch: 0.684 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5685976710425846		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.5685976710425846 | validation: 0.5315603519441844]
	TIME [epoch: 0.685 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6351261922541778		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.6351261922541778 | validation: 0.8211968220030127]
	TIME [epoch: 0.687 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6870560810929112		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.6870560810929112 | validation: 0.6654520896587087]
	TIME [epoch: 0.682 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.618793496605925		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.618793496605925 | validation: 0.5982861895419398]
	TIME [epoch: 0.68 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5350755970512246		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.5350755970512246 | validation: 0.6051575728298019]
	TIME [epoch: 0.68 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5095956884294686		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.5095956884294686 | validation: 0.5423877485458236]
	TIME [epoch: 0.68 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5298130294454064		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.5298130294454064 | validation: 0.6828390950436551]
	TIME [epoch: 0.681 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5684392241836401		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.5684392241836401 | validation: 0.5541398357001064]
	TIME [epoch: 0.68 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5734690147357858		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.5734690147357858 | validation: 0.6303752423158778]
	TIME [epoch: 0.679 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5212837414260132		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.5212837414260132 | validation: 0.5030031970503107]
	TIME [epoch: 0.682 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5480807090870713		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.5480807090870713 | validation: 0.8333805298383347]
	TIME [epoch: 0.683 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6984360908714078		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.6984360908714078 | validation: 0.5768224269349905]
	TIME [epoch: 0.682 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.572698413618853		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.572698413618853 | validation: 0.5051584221884502]
	TIME [epoch: 0.682 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4908319609024279		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.4908319609024279 | validation: 0.674944024952755]
	TIME [epoch: 0.681 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5793366406052501		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.5793366406052501 | validation: 0.46445383027492193]
	TIME [epoch: 0.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6233315985918629		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.6233315985918629 | validation: 0.7691583826149803]
	TIME [epoch: 0.688 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.630892180023235		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.630892180023235 | validation: 0.592230661079708]
	TIME [epoch: 0.687 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5437447033003763		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.5437447033003763 | validation: 0.557634290513788]
	TIME [epoch: 0.687 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5018954619686967		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.5018954619686967 | validation: 0.515413496570751]
	TIME [epoch: 0.686 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4743819828632377		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.4743819828632377 | validation: 0.5817023199604148]
	TIME [epoch: 0.693 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48595312493546616		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.48595312493546616 | validation: 0.42772298640138406]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5211386602556798		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.5211386602556798 | validation: 0.75751501371104]
	TIME [epoch: 0.683 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6377670360394457		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.6377670360394457 | validation: 0.6333191513678209]
	TIME [epoch: 0.681 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5635325170208654		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.5635325170208654 | validation: 0.4771408886259833]
	TIME [epoch: 0.681 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48206738527182824		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.48206738527182824 | validation: 0.6351612725496828]
	TIME [epoch: 0.681 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5218192835368465		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.5218192835368465 | validation: 0.4575437702094995]
	TIME [epoch: 0.679 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5941580309141615		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.5941580309141615 | validation: 0.7435514182062655]
	TIME [epoch: 0.678 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6274173207126013		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.6274173207126013 | validation: 0.5561245847536777]
	TIME [epoch: 0.679 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5350229939113588		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.5350229939113588 | validation: 0.5321306354461232]
	TIME [epoch: 0.679 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4982660874628884		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.4982660874628884 | validation: 0.503013232970992]
	TIME [epoch: 0.679 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4493277798091722		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.4493277798091722 | validation: 0.5096926756913125]
	TIME [epoch: 0.678 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45272970929241624		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.45272970929241624 | validation: 0.49700028394163526]
	TIME [epoch: 0.679 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43937451604147837		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.43937451604147837 | validation: 0.4773158358920965]
	TIME [epoch: 0.679 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42909058284258883		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.42909058284258883 | validation: 0.46934051728022425]
	TIME [epoch: 0.679 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4246946073047		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.4246946073047 | validation: 0.452078778232281]
	TIME [epoch: 0.679 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43657083854617174		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.43657083854617174 | validation: 0.7543588451443768]
	TIME [epoch: 0.679 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6056099504998982		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.6056099504998982 | validation: 0.5365212559790985]
	TIME [epoch: 0.68 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6334784458559128		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.6334784458559128 | validation: 0.7326083884378366]
	TIME [epoch: 0.68 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6007578331508503		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.6007578331508503 | validation: 0.5366445374481207]
	TIME [epoch: 0.679 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5010750621255765		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.5010750621255765 | validation: 0.48653482999369263]
	TIME [epoch: 0.68 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47599673178908786		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.47599673178908786 | validation: 0.5430917710431779]
	TIME [epoch: 0.682 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4573778640084122		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.4573778640084122 | validation: 0.41556052620645617]
	TIME [epoch: 0.681 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48576979388682306		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.48576979388682306 | validation: 0.6327515964437929]
	TIME [epoch: 0.687 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.565552244448394		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.565552244448394 | validation: 0.4987147480848618]
	TIME [epoch: 0.685 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46734828268070416		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.46734828268070416 | validation: 0.43389184965465405]
	TIME [epoch: 0.683 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44616825986379044		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.44616825986379044 | validation: 0.4850106276648095]
	TIME [epoch: 0.683 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42133232597047565		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.42133232597047565 | validation: 0.40163728803707816]
	TIME [epoch: 0.683 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4344700879959577		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.4344700879959577 | validation: 0.6986896258268125]
	TIME [epoch: 0.69 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5765012284311515		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.5765012284311515 | validation: 0.526193868213544]
	TIME [epoch: 0.686 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.506179127749215		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.506179127749215 | validation: 0.4172269447116615]
	TIME [epoch: 0.685 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44343561709300006		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.44343561709300006 | validation: 0.5425130531636675]
	TIME [epoch: 0.685 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4677948127791897		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.4677948127791897 | validation: 0.40450732525181876]
	TIME [epoch: 0.684 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5154308677479175		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.5154308677479175 | validation: 0.7014519789310896]
	TIME [epoch: 0.684 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5779892623703742		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.5779892623703742 | validation: 0.48143218740449545]
	TIME [epoch: 0.684 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43387518027655975		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.43387518027655975 | validation: 0.38648543732119106]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43207019533405805		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.43207019533405805 | validation: 0.5220058498502452]
	TIME [epoch: 0.687 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45560926807330276		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.45560926807330276 | validation: 0.40346132369238896]
	TIME [epoch: 0.686 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41536946639881595		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.41536946639881595 | validation: 0.47346343570913585]
	TIME [epoch: 0.685 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41701303660983585		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.41701303660983585 | validation: 0.411225110990425]
	TIME [epoch: 0.684 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4033859792919148		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.4033859792919148 | validation: 0.46552550374716706]
	TIME [epoch: 0.695 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3956134593783967		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.3956134593783967 | validation: 0.3525031141182107]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4740589699834493		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.4740589699834493 | validation: 0.7439943502071132]
	TIME [epoch: 0.687 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6370217953988824		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.6370217953988824 | validation: 0.5034957452275821]
	TIME [epoch: 0.685 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47438694936901604		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.47438694936901604 | validation: 0.37679083208078656]
	TIME [epoch: 0.687 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4660053782348695		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.4660053782348695 | validation: 0.48859653870258024]
	TIME [epoch: 0.686 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4395053126268407		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.4395053126268407 | validation: 0.39791916865130744]
	TIME [epoch: 0.687 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39951241124456865		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.39951241124456865 | validation: 0.4130260941544694]
	TIME [epoch: 0.686 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3886761379276312		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.3886761379276312 | validation: 0.4236465755390258]
	TIME [epoch: 0.685 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38429456297949777		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.38429456297949777 | validation: 0.408208051833379]
	TIME [epoch: 0.685 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3949328021995872		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.3949328021995872 | validation: 0.5078317219548832]
	TIME [epoch: 0.684 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4376702130519043		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.4376702130519043 | validation: 0.6054745142558892]
	TIME [epoch: 0.684 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5073150180849948		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.5073150180849948 | validation: 0.4170582418486727]
	TIME [epoch: 0.684 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4332878002351546		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.4332878002351546 | validation: 0.539925334887967]
	TIME [epoch: 0.686 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4619489784149806		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.4619489784149806 | validation: 0.3844698984101121]
	TIME [epoch: 0.684 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41104812647820355		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.41104812647820355 | validation: 0.4079497454585177]
	TIME [epoch: 0.684 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39362531086364		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.39362531086364 | validation: 0.46463744077089164]
	TIME [epoch: 0.684 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40324004552324616		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.40324004552324616 | validation: 0.32451123969027607]
	TIME [epoch: 0.684 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4615593730528853		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.4615593730528853 | validation: 0.6958799239772044]
	TIME [epoch: 0.687 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6113388289112835		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.6113388289112835 | validation: 0.42846287092706464]
	TIME [epoch: 0.689 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4351808159997879		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.4351808159997879 | validation: 0.3690156923038377]
	TIME [epoch: 170 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4294156930571319		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.4294156930571319 | validation: 0.49895983811216277]
	TIME [epoch: 1.36 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42855678812214687		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.42855678812214687 | validation: 0.39695784228418046]
	TIME [epoch: 1.35 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38558254224880745		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.38558254224880745 | validation: 0.40799468185331683]
	TIME [epoch: 1.35 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38175535858183324		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.38175535858183324 | validation: 0.411836048791397]
	TIME [epoch: 1.35 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3832530387163971		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.3832530387163971 | validation: 0.39393634910564906]
	TIME [epoch: 1.35 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37828474140051		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.37828474140051 | validation: 0.4226473177690971]
	TIME [epoch: 1.35 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3779362536027556		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.3779362536027556 | validation: 0.40421766303635587]
	TIME [epoch: 1.35 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38240396960483636		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.38240396960483636 | validation: 0.4064946014746953]
	TIME [epoch: 1.35 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.410030454365661		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.410030454365661 | validation: 0.6024350968529857]
	TIME [epoch: 1.35 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.505002487558957		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.505002487558957 | validation: 0.34738851994393116]
	TIME [epoch: 1.35 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4218942493342603		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.4218942493342603 | validation: 0.4345925471269356]
	TIME [epoch: 1.35 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3815164965951148		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.3815164965951148 | validation: 0.37015850262961836]
	TIME [epoch: 1.34 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37854092394723465		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.37854092394723465 | validation: 0.4216662672136746]
	TIME [epoch: 1.34 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3856266689106314		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.3856266689106314 | validation: 0.38465256171104056]
	TIME [epoch: 1.34 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3821166887227574		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.3821166887227574 | validation: 0.4301575109186147]
	TIME [epoch: 1.34 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37584832197086415		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.37584832197086415 | validation: 0.33650171570507503]
	TIME [epoch: 1.34 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37947805643504257		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.37947805643504257 | validation: 0.4982797877311052]
	TIME [epoch: 1.34 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4288182353936973		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.4288182353936973 | validation: 0.298575415292114]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6121412245631291		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.6121412245631291 | validation: 0.6077155659055281]
	TIME [epoch: 1.35 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5235405518835001		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.5235405518835001 | validation: 0.48277209120699793]
	TIME [epoch: 1.35 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4044703591973338		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.4044703591973338 | validation: 0.3413058663665856]
	TIME [epoch: 1.34 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40784814089401267		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.40784814089401267 | validation: 0.4369769752742926]
	TIME [epoch: 1.34 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40117015544144174		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.40117015544144174 | validation: 0.411299359451734]
	TIME [epoch: 1.35 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3703488639358187		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.3703488639358187 | validation: 0.3389247393273711]
	TIME [epoch: 1.34 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36953649995781385		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.36953649995781385 | validation: 0.38756377340057274]
	TIME [epoch: 1.34 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.359355013322754		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.359355013322754 | validation: 0.3601936000043816]
	TIME [epoch: 1.34 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3528521160720549		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.3528521160720549 | validation: 0.38101509994777794]
	TIME [epoch: 1.34 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3531018974000125		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.3531018974000125 | validation: 0.3561579156161628]
	TIME [epoch: 1.34 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3610995246771945		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.3610995246771945 | validation: 0.48034340669099485]
	TIME [epoch: 1.34 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40470999208625347		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.40470999208625347 | validation: 0.44054687514193436]
	TIME [epoch: 1.36 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.516183410924538		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.516183410924538 | validation: 0.6686237096517486]
	TIME [epoch: 1.34 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5898677806956983		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.5898677806956983 | validation: 0.423181871534162]
	TIME [epoch: 1.34 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41312335064095257		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.41312335064095257 | validation: 0.41052731439715234]
	TIME [epoch: 1.34 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41496793300663415		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.41496793300663415 | validation: 0.3913645051618819]
	TIME [epoch: 1.34 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3493246197294311		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.3493246197294311 | validation: 0.3920396845756967]
	TIME [epoch: 1.34 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3636373049201876		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.3636373049201876 | validation: 0.3575092601334724]
	TIME [epoch: 1.34 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3576221694349754		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.3576221694349754 | validation: 0.35917916361075986]
	TIME [epoch: 1.35 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3433392578842914		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.3433392578842914 | validation: 0.3572593428204873]
	TIME [epoch: 1.34 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34127074257366447		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.34127074257366447 | validation: 0.331600870681346]
	TIME [epoch: 1.35 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3457438104590871		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.3457438104590871 | validation: 0.4084175632606133]
	TIME [epoch: 1.35 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35361329290975524		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.35361329290975524 | validation: 0.3403303169278402]
	TIME [epoch: 1.35 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4178808783205831		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.4178808783205831 | validation: 0.5924513360017537]
	TIME [epoch: 1.34 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4980150695255081		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.4980150695255081 | validation: 0.3341300274891695]
	TIME [epoch: 1.34 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36166844394734243		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.36166844394734243 | validation: 0.3275493424244079]
	TIME [epoch: 1.34 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3461797607500758		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.3461797607500758 | validation: 0.3797375752778843]
	TIME [epoch: 1.35 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3514760875535792		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.3514760875535792 | validation: 0.3506828065698492]
	TIME [epoch: 1.35 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35109810448161627		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.35109810448161627 | validation: 0.3705996897325622]
	TIME [epoch: 1.35 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3415033959436879		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.3415033959436879 | validation: 0.36176670392012]
	TIME [epoch: 1.35 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34745036449518396		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.34745036449518396 | validation: 0.34785917512394493]
	TIME [epoch: 1.34 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3766377347684613		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.3766377347684613 | validation: 0.568504562506182]
	TIME [epoch: 1.34 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4615332984302753		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.4615332984302753 | validation: 0.3254129924253766]
	TIME [epoch: 1.35 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.390573091744087		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.390573091744087 | validation: 0.3931180391929637]
	TIME [epoch: 1.35 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.348891081581133		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.348891081581133 | validation: 0.3275888100697279]
	TIME [epoch: 1.34 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.339768657590799		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.339768657590799 | validation: 0.3524228548194688]
	TIME [epoch: 1.34 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3457834528069095		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.3457834528069095 | validation: 0.33281627766595934]
	TIME [epoch: 1.34 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3391908275231893		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.3391908275231893 | validation: 0.38401857537661654]
	TIME [epoch: 1.35 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3477101329960429		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.3477101329960429 | validation: 0.29384371886359184]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3740017492585998		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.3740017492585998 | validation: 0.524579980801015]
	TIME [epoch: 1.35 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4417264439593806		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.4417264439593806 | validation: 0.3222228677861644]
	TIME [epoch: 1.34 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3726650980367825		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.3726650980367825 | validation: 0.3461420601236753]
	TIME [epoch: 1.34 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3488986831943469		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.3488986831943469 | validation: 0.46060976612728777]
	TIME [epoch: 1.34 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39172536514647815		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.39172536514647815 | validation: 0.3060500940060301]
	TIME [epoch: 1.35 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3807055832668509		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.3807055832668509 | validation: 0.4596224540339049]
	TIME [epoch: 1.34 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.395093811901643		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.395093811901643 | validation: 0.357253147154941]
	TIME [epoch: 1.35 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3401180877833062		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.3401180877833062 | validation: 0.29155473388300884]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34691876154306656		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.34691876154306656 | validation: 0.4168452690329101]
	TIME [epoch: 1.35 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36232496661202246		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.36232496661202246 | validation: 0.29483945200436834]
	TIME [epoch: 1.34 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33589023869311374		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.33589023869311374 | validation: 0.3310469011380967]
	TIME [epoch: 1.34 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3283845708081499		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.3283845708081499 | validation: 0.35714625152947743]
	TIME [epoch: 1.35 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33744640412643934		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.33744640412643934 | validation: 0.33955095358043463]
	TIME [epoch: 1.34 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39332581252173326		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.39332581252173326 | validation: 0.4999282989225094]
	TIME [epoch: 1.34 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39822874655491997		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.39822874655491997 | validation: 0.29625109005867095]
	TIME [epoch: 1.34 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34128651284183753		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.34128651284183753 | validation: 0.3689613014793669]
	TIME [epoch: 1.34 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33282606255931013		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.33282606255931013 | validation: 0.31203612991712115]
	TIME [epoch: 1.34 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34429549440349533		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.34429549440349533 | validation: 0.4403601275009704]
	TIME [epoch: 1.35 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3791566530336337		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.3791566530336337 | validation: 0.31704929581365393]
	TIME [epoch: 1.34 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38258582313570233		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.38258582313570233 | validation: 0.44304899448676355]
	TIME [epoch: 1.35 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3612190251678794		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.3612190251678794 | validation: 0.31608212002317243]
	TIME [epoch: 1.34 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33213296761906635		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.33213296761906635 | validation: 0.332933365897103]
	TIME [epoch: 1.34 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3246239258585402		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.3246239258585402 | validation: 0.34847621552305613]
	TIME [epoch: 1.34 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3278773295395122		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.3278773295395122 | validation: 0.33217138576091526]
	TIME [epoch: 1.35 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3284391278520398		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.3284391278520398 | validation: 0.3343352369762502]
	TIME [epoch: 1.34 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32798723675095015		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.32798723675095015 | validation: 0.3915710780582712]
	TIME [epoch: 1.34 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33739472200650666		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.33739472200650666 | validation: 0.32685711102565357]
	TIME [epoch: 1.35 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3530754122254915		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.3530754122254915 | validation: 0.46669461003113694]
	TIME [epoch: 1.34 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3891592925883727		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.3891592925883727 | validation: 0.27395542491835756]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_287.pth
	Model improved!!!
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38922731387856147		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.38922731387856147 | validation: 0.4460419769825354]
	TIME [epoch: 1.34 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3672590572549833		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.3672590572549833 | validation: 0.30747107877652075]
	TIME [epoch: 1.33 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33304651059007384		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.33304651059007384 | validation: 0.34367598898785084]
	TIME [epoch: 1.33 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33563933891738346		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.33563933891738346 | validation: 0.3683078168819348]
	TIME [epoch: 1.35 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3335708116436844		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.3335708116436844 | validation: 0.33175326575489905]
	TIME [epoch: 1.34 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31869437989581295		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.31869437989581295 | validation: 0.31896243856105616]
	TIME [epoch: 1.35 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31301625304547676		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.31301625304547676 | validation: 0.3613718737911266]
	TIME [epoch: 1.34 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.316910718929457		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.316910718929457 | validation: 0.3011961334390616]
	TIME [epoch: 1.35 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3122150586721196		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.3122150586721196 | validation: 0.3716574652016081]
	TIME [epoch: 1.35 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3229206921323205		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.3229206921323205 | validation: 0.2787240798619823]
	TIME [epoch: 1.35 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33960541595770966		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.33960541595770966 | validation: 0.5252036890891144]
	TIME [epoch: 1.33 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4295069749624239		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.4295069749624239 | validation: 0.30437132149518337]
	TIME [epoch: 1.35 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3665162121661399		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.3665162121661399 | validation: 0.37204820028834185]
	TIME [epoch: 1.33 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32525055086555493		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.32525055086555493 | validation: 0.3368823056534949]
	TIME [epoch: 1.34 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.311178129726873		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.311178129726873 | validation: 0.3067731689832984]
	TIME [epoch: 1.34 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.314302463941508		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.314302463941508 | validation: 0.38079692668493337]
	TIME [epoch: 1.33 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32038896149004836		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.32038896149004836 | validation: 0.31546400264224106]
	TIME [epoch: 1.34 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3284975455934417		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.3284975455934417 | validation: 0.39292192866832204]
	TIME [epoch: 1.35 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3354000765851052		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.3354000765851052 | validation: 0.38853922650836603]
	TIME [epoch: 1.35 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3527661176436429		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.3527661176436429 | validation: 0.4072708290926997]
	TIME [epoch: 1.34 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3468328309765563		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.3468328309765563 | validation: 0.3295969855544022]
	TIME [epoch: 1.35 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3243245268789506		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.3243245268789506 | validation: 0.3522470400054654]
	TIME [epoch: 1.34 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3157163251110125		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.3157163251110125 | validation: 0.2867651239993503]
	TIME [epoch: 1.34 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3101720253640191		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.3101720253640191 | validation: 0.3959532177625867]
	TIME [epoch: 1.34 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3292410368999146		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.3292410368999146 | validation: 0.2586370973984819]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3570977356984302		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.3570977356984302 | validation: 0.46073994132440055]
	TIME [epoch: 1.33 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3728983539331353		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.3728983539331353 | validation: 0.28850700264475276]
	TIME [epoch: 1.34 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31460348089009615		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.31460348089009615 | validation: 0.310806056757949]
	TIME [epoch: 1.34 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30783222018928846		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.30783222018928846 | validation: 0.3396885448004948]
	TIME [epoch: 1.35 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.306391537515841		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.306391537515841 | validation: 0.3200908674546141]
	TIME [epoch: 1.34 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3022199366973426		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.3022199366973426 | validation: 0.3453954873868792]
	TIME [epoch: 1.34 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30170780431061245		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.30170780431061245 | validation: 0.3142583621160487]
	TIME [epoch: 1.34 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30090888971039553		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.30090888971039553 | validation: 0.3594984223363545]
	TIME [epoch: 1.34 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3028398178321778		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.3028398178321778 | validation: 0.30162742882746474]
	TIME [epoch: 1.33 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30308883359796734		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.30308883359796734 | validation: 0.3965726768914818]
	TIME [epoch: 1.33 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3134964432909859		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.3134964432909859 | validation: 0.3028832133371409]
	TIME [epoch: 1.33 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3435658222100355		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.3435658222100355 | validation: 0.583248556466774]
	TIME [epoch: 1.33 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43930854066196234		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.43930854066196234 | validation: 0.3432556474696553]
	TIME [epoch: 1.33 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34462709736921		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.34462709736921 | validation: 0.31046141033570385]
	TIME [epoch: 1.33 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30565850016115803		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.30565850016115803 | validation: 0.3739953032488475]
	TIME [epoch: 1.33 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3080369613523682		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.3080369613523682 | validation: 0.27881959199460926]
	TIME [epoch: 1.34 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30969751257838773		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.30969751257838773 | validation: 0.3675172583078223]
	TIME [epoch: 1.33 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30698389853551855		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.30698389853551855 | validation: 0.2752373038838071]
	TIME [epoch: 1.33 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3159677980535291		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.3159677980535291 | validation: 0.3967173137047767]
	TIME [epoch: 1.33 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.327880424404309		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.327880424404309 | validation: 0.304580149770946]
	TIME [epoch: 1.33 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35569055438948755		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.35569055438948755 | validation: 0.41102685964915253]
	TIME [epoch: 1.33 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33230028362147346		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.33230028362147346 | validation: 0.3152091604157538]
	TIME [epoch: 1.33 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30097164793507164		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.30097164793507164 | validation: 0.3243361880082509]
	TIME [epoch: 1.33 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29274380992452465		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.29274380992452465 | validation: 0.3290486654584041]
	TIME [epoch: 1.33 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29547777108183104		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.29547777108183104 | validation: 0.3276655346008114]
	TIME [epoch: 1.33 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2961909625484212		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.2961909625484212 | validation: 0.30652399009819187]
	TIME [epoch: 1.33 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29788938099309664		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.29788938099309664 | validation: 0.38628271002332504]
	TIME [epoch: 1.33 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3096979550585682		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.3096979550585682 | validation: 0.2954110429998324]
	TIME [epoch: 1.33 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33757065579685086		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.33757065579685086 | validation: 0.46408697098246066]
	TIME [epoch: 1.33 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35927743144874313		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.35927743144874313 | validation: 0.3075777532823493]
	TIME [epoch: 1.33 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30936916202132125		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.30936916202132125 | validation: 0.3298793360373306]
	TIME [epoch: 1.33 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29494155502527997		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.29494155502527997 | validation: 0.33093676014318074]
	TIME [epoch: 1.33 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2936841009104474		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.2936841009104474 | validation: 0.31029259260440256]
	TIME [epoch: 1.33 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2839179613421605		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.2839179613421605 | validation: 0.3168748731983601]
	TIME [epoch: 1.33 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2898276924883213		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.2898276924883213 | validation: 0.3165071002506472]
	TIME [epoch: 1.33 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29393523328815574		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.29393523328815574 | validation: 0.3098309083896081]
	TIME [epoch: 1.33 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29083300010799906		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.29083300010799906 | validation: 0.34720509515006714]
	TIME [epoch: 1.33 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2914277937659548		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.2914277937659548 | validation: 0.3178119920432537]
	TIME [epoch: 1.35 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31999611779609377		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.31999611779609377 | validation: 0.4717107942293221]
	TIME [epoch: 1.35 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36512033122900206		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.36512033122900206 | validation: 0.28706184845595034]
	TIME [epoch: 1.34 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3436215704270572		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.3436215704270572 | validation: 0.41132675742501335]
	TIME [epoch: 1.35 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31450589850922056		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.31450589850922056 | validation: 0.3199812481185715]
	TIME [epoch: 1.35 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3197351538026555		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.3197351538026555 | validation: 0.3944154880898465]
	TIME [epoch: 1.35 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31342450824266904		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.31342450824266904 | validation: 0.30961011382212045]
	TIME [epoch: 1.35 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2886532058817756		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.2886532058817756 | validation: 0.3274929061810449]
	TIME [epoch: 1.35 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2878749730249579		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.2878749730249579 | validation: 0.3252273291091675]
	TIME [epoch: 1.35 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2839960291081301		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.2839960291081301 | validation: 0.2969185076333753]
	TIME [epoch: 1.35 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28342363316230945		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.28342363316230945 | validation: 0.34365532904882934]
	TIME [epoch: 1.34 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2824438162144498		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.2824438162144498 | validation: 0.2857144644736251]
	TIME [epoch: 1.35 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2881648571867356		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.2881648571867356 | validation: 0.38249369676283174]
	TIME [epoch: 1.34 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29522318079198134		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.29522318079198134 | validation: 0.29068322758423604]
	TIME [epoch: 1.34 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3067814402183584		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.3067814402183584 | validation: 0.4192518176770741]
	TIME [epoch: 1.35 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3278239964442672		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.3278239964442672 | validation: 0.2899811701090677]
	TIME [epoch: 1.35 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31367443520008986		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.31367443520008986 | validation: 0.3505234642201259]
	TIME [epoch: 1.35 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28417388046412523		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.28417388046412523 | validation: 0.2964551619141694]
	TIME [epoch: 1.35 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27830637994983926		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.27830637994983926 | validation: 0.35540394788456553]
	TIME [epoch: 1.36 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2873470774169139		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.2873470774169139 | validation: 0.29555712668257367]
	TIME [epoch: 1.34 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2944310175712477		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.2944310175712477 | validation: 0.3793361274628228]
	TIME [epoch: 1.34 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3046821409109086		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.3046821409109086 | validation: 0.2850989485225157]
	TIME [epoch: 1.34 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2875497709382165		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.2875497709382165 | validation: 0.3346700141213188]
	TIME [epoch: 1.35 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2743375557877514		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.2743375557877514 | validation: 0.30214789131983716]
	TIME [epoch: 1.34 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28139091423853135		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.28139091423853135 | validation: 0.3587607078473319]
	TIME [epoch: 1.34 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2901225529615888		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.2901225529615888 | validation: 0.2870085443230292]
	TIME [epoch: 1.34 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3055023654021255		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.3055023654021255 | validation: 0.4017108975332697]
	TIME [epoch: 1.34 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31688964943806974		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.31688964943806974 | validation: 0.27659378084360287]
	TIME [epoch: 1.35 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28787512498224777		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.28787512498224777 | validation: 0.33711633828998266]
	TIME [epoch: 1.34 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2763156326046516		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.2763156326046516 | validation: 0.3074916336378462]
	TIME [epoch: 1.35 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.275904866120666		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.275904866120666 | validation: 0.3025140561702469]
	TIME [epoch: 1.35 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26828222851652267		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.26828222851652267 | validation: 0.32043324852480803]
	TIME [epoch: 1.35 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2667499260768922		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.2667499260768922 | validation: 0.3018895598492425]
	TIME [epoch: 1.34 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2709074670050358		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.2709074670050358 | validation: 0.32800042066240986]
	TIME [epoch: 1.35 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26658504599591143		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.26658504599591143 | validation: 0.2631519382763254]
	TIME [epoch: 1.35 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2823152911899816		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.2823152911899816 | validation: 0.3874126981133992]
	TIME [epoch: 1.35 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3027199261058459		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.3027199261058459 | validation: 0.27679861005599427]
	TIME [epoch: 1.35 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3292919740560925		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.3292919740560925 | validation: 0.4108873813423386]
	TIME [epoch: 1.35 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32240696839472577		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.32240696839472577 | validation: 0.36096620945946284]
	TIME [epoch: 1.35 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2893572657897158		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.2893572657897158 | validation: 0.2605302712066592]
	TIME [epoch: 1.35 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27713019410021267		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.27713019410021267 | validation: 0.3620852412923392]
	TIME [epoch: 1.35 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27525060280436364		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.27525060280436364 | validation: 0.3120567525607596]
	TIME [epoch: 1.35 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26837327579033393		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.26837327579033393 | validation: 0.3016916676376734]
	TIME [epoch: 1.35 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26798529645405816		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.26798529645405816 | validation: 0.32406147723485196]
	TIME [epoch: 1.35 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2659362923433153		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.2659362923433153 | validation: 0.2926303765112674]
	TIME [epoch: 1.35 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26426562399193254		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.26426562399193254 | validation: 0.32693466517852404]
	TIME [epoch: 1.35 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2668341877277361		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.2668341877277361 | validation: 0.2887298573620557]
	TIME [epoch: 1.35 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2600477911364663		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.2600477911364663 | validation: 0.3242124794992891]
	TIME [epoch: 1.35 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.260055537980207		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.260055537980207 | validation: 0.2626915597071737]
	TIME [epoch: 1.35 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26249057221748556		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.26249057221748556 | validation: 0.3787513821175738]
	TIME [epoch: 1.35 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29133286178563866		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.29133286178563866 | validation: 0.2475484650373773]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_400.pth
	Model improved!!!
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31487150452617196		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.31487150452617196 | validation: 0.4122498627894638]
	TIME [epoch: 1.34 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3045963444510596		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.3045963444510596 | validation: 0.36145902915670003]
	TIME [epoch: 1.34 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28906334694683916		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.28906334694683916 | validation: 0.2857253635000649]
	TIME [epoch: 1.34 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25971454773127		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.25971454773127 | validation: 0.32374781463924834]
	TIME [epoch: 1.33 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2562514422257584		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.2562514422257584 | validation: 0.2969396489569685]
	TIME [epoch: 1.33 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2608772457637354		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.2608772457637354 | validation: 0.3282717597597655]
	TIME [epoch: 1.33 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2593800091543149		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.2593800091543149 | validation: 0.2974052958496758]
	TIME [epoch: 1.33 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25362284962961384		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.25362284962961384 | validation: 0.31625380851795654]
	TIME [epoch: 1.33 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2536104628458256		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.2536104628458256 | validation: 0.28326652236450006]
	TIME [epoch: 1.33 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2564007963537306		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.2564007963537306 | validation: 0.32885599813141536]
	TIME [epoch: 1.33 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2557875229737455		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.2557875229737455 | validation: 0.26003377026637214]
	TIME [epoch: 1.33 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2704163543999922		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.2704163543999922 | validation: 0.3800167019840894]
	TIME [epoch: 1.33 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2779962504240963		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.2779962504240963 | validation: 0.2657805661178116]
	TIME [epoch: 1.33 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27281350075381544		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.27281350075381544 | validation: 0.3507812610703742]
	TIME [epoch: 1.34 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25742469318981537		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.25742469318981537 | validation: 0.2971637784358602]
	TIME [epoch: 1.33 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24889977882692293		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.24889977882692293 | validation: 0.3032104476638091]
	TIME [epoch: 1.34 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2468527461867896		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.2468527461867896 | validation: 0.27329574030385534]
	TIME [epoch: 1.33 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2440698445515445		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.2440698445515445 | validation: 0.32578575500873796]
	TIME [epoch: 1.33 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2555238399071734		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.2555238399071734 | validation: 0.26566732537475224]
	TIME [epoch: 1.34 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2667668296278744		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.2667668296278744 | validation: 0.38269230122605696]
	TIME [epoch: 1.33 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28463656302815576		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.28463656302815576 | validation: 0.2803895702943943]
	TIME [epoch: 1.33 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2715575443053291		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.2715575443053291 | validation: 0.3248458838335424]
	TIME [epoch: 1.33 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24565761482538875		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.24565761482538875 | validation: 0.2805191299610736]
	TIME [epoch: 1.33 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23770564796529606		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.23770564796529606 | validation: 0.31115837128837776]
	TIME [epoch: 1.33 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2391325381446705		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.2391325381446705 | validation: 0.26143606279105347]
	TIME [epoch: 1.33 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24250888131620393		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.24250888131620393 | validation: 0.3096645980746154]
	TIME [epoch: 1.33 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23343528812152678		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.23343528812152678 | validation: 0.2799288359401964]
	TIME [epoch: 1.33 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2365419943639187		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.2365419943639187 | validation: 0.3006131575258271]
	TIME [epoch: 1.34 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23657570482275295		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.23657570482275295 | validation: 0.2935466466665927]
	TIME [epoch: 1.33 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2358101808049172		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.2358101808049172 | validation: 0.29578241060795724]
	TIME [epoch: 1.33 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23256829995885128		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.23256829995885128 | validation: 0.28274863312510784]
	TIME [epoch: 1.33 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23513738217722846		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.23513738217722846 | validation: 0.2958663602933892]
	TIME [epoch: 1.34 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23173952285651578		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.23173952285651578 | validation: 0.28797998732368396]
	TIME [epoch: 1.34 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.248380375410364		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.248380375410364 | validation: 0.318335367488578]
	TIME [epoch: 1.34 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2794296635337254		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.2794296635337254 | validation: 0.37184885772266796]
	TIME [epoch: 1.34 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29154529100722804		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.29154529100722804 | validation: 0.21606561704669627]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_436.pth
	Model improved!!!
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29519583075782513		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.29519583075782513 | validation: 0.3685727393853916]
	TIME [epoch: 1.34 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24933112725679735		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.24933112725679735 | validation: 0.296845598572758]
	TIME [epoch: 1.35 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22820230422018561		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.22820230422018561 | validation: 0.25626231022107987]
	TIME [epoch: 1.34 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22938855534671757		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.22938855534671757 | validation: 0.3111871953810876]
	TIME [epoch: 1.34 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23024815685289643		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.23024815685289643 | validation: 0.29302665254668375]
	TIME [epoch: 1.34 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2248052470255573		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.2248052470255573 | validation: 0.2931493679261794]
	TIME [epoch: 1.34 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2249333061927842		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.2249333061927842 | validation: 0.26020824316636537]
	TIME [epoch: 1.34 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22340672211831283		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.22340672211831283 | validation: 0.3020029913628099]
	TIME [epoch: 1.34 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22748528983051458		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.22748528983051458 | validation: 0.24805612860954943]
	TIME [epoch: 1.34 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22843352694051522		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.22843352694051522 | validation: 0.34732889421436836]
	TIME [epoch: 1.34 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24458611821833665		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.24458611821833665 | validation: 0.24131343482920986]
	TIME [epoch: 1.34 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2607201581834516		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.2607201581834516 | validation: 0.34841124706568416]
	TIME [epoch: 1.34 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24993459526701428		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.24993459526701428 | validation: 0.25834635850276433]
	TIME [epoch: 1.34 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.222474698005775		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.222474698005775 | validation: 0.2916476870388482]
	TIME [epoch: 1.34 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21763200136996183		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.21763200136996183 | validation: 0.2841719521040606]
	TIME [epoch: 1.34 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21338150837278724		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.21338150837278724 | validation: 0.28495895691366807]
	TIME [epoch: 1.34 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21531471682647826		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.21531471682647826 | validation: 0.29582858630726305]
	TIME [epoch: 1.34 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21999413124314832		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.21999413124314832 | validation: 0.27623665569866024]
	TIME [epoch: 1.34 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21909654013881905		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.21909654013881905 | validation: 0.31095956969752064]
	TIME [epoch: 1.34 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22732740897112502		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.22732740897112502 | validation: 0.2605690311107317]
	TIME [epoch: 1.34 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23696246160071505		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.23696246160071505 | validation: 0.3402629973569379]
	TIME [epoch: 1.34 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24881043201979394		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.24881043201979394 | validation: 0.21198027770780473]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_458.pth
	Model improved!!!
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.251647994963698		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.251647994963698 | validation: 0.34235457879396586]
	TIME [epoch: 1.34 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23016793311955105		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.23016793311955105 | validation: 0.2589440630808112]
	TIME [epoch: 1.35 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20948555091650684		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.20948555091650684 | validation: 0.25763025288924046]
	TIME [epoch: 1.35 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20465634953021294		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.20465634953021294 | validation: 0.2953597917992474]
	TIME [epoch: 1.34 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21357421082169686		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.21357421082169686 | validation: 0.25012236371095764]
	TIME [epoch: 1.35 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2086558182576625		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.2086558182576625 | validation: 0.29558489964333184]
	TIME [epoch: 1.34 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20683269772124185		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.20683269772124185 | validation: 0.25968349319501755]
	TIME [epoch: 1.35 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2125168560340792		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.2125168560340792 | validation: 0.2977448615899157]
	TIME [epoch: 1.34 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2206959537305709		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.2206959537305709 | validation: 0.26053431369080304]
	TIME [epoch: 1.34 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22790447244138698		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.22790447244138698 | validation: 0.3125773904628013]
	TIME [epoch: 1.34 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22991226897921557		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.22991226897921557 | validation: 0.22316829987933354]
	TIME [epoch: 1.35 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21636967242645955		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.21636967242645955 | validation: 0.32522561110771075]
	TIME [epoch: 1.34 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21313188681260783		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.21313188681260783 | validation: 0.21797266705896812]
	TIME [epoch: 1.35 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2183431503723941		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.2183431503723941 | validation: 0.30387380491062405]
	TIME [epoch: 1.35 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20444935907617803		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.20444935907617803 | validation: 0.26150646305657416]
	TIME [epoch: 1.34 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19821341496319175		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.19821341496319175 | validation: 0.274437411668707]
	TIME [epoch: 1.34 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19690815727039612		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.19690815727039612 | validation: 0.26215549080766054]
	TIME [epoch: 1.34 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20271816765234793		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.20271816765234793 | validation: 0.2889835137609081]
	TIME [epoch: 1.34 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2085435545764436		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.2085435545764436 | validation: 0.25487573052971924]
	TIME [epoch: 1.34 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22113783256388972		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.22113783256388972 | validation: 0.3207283679209079]
	TIME [epoch: 1.34 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22874352891762278		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.22874352891762278 | validation: 0.21521527378421335]
	TIME [epoch: 1.34 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2122829183328642		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.2122829183328642 | validation: 0.3210058116969646]
	TIME [epoch: 1.34 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2135684296140156		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.2135684296140156 | validation: 0.21298014509116864]
	TIME [epoch: 1.34 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20300243163882406		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.20300243163882406 | validation: 0.28922219570309204]
	TIME [epoch: 1.35 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1906639507568928		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.1906639507568928 | validation: 0.2502636096265643]
	TIME [epoch: 1.34 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18347260264586582		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.18347260264586582 | validation: 0.25276909658992447]
	TIME [epoch: 1.35 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1842476295272401		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.1842476295272401 | validation: 0.26224429370834956]
	TIME [epoch: 1.35 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18691808936898865		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.18691808936898865 | validation: 0.23611379926868828]
	TIME [epoch: 1.35 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1848689604800369		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.1848689604800369 | validation: 0.30680192147153557]
	TIME [epoch: 1.34 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1923226883592894		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.1923226883592894 | validation: 0.20898732179035387]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_488.pth
	Model improved!!!
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2103954062529381		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.2103954062529381 | validation: 0.3727534897603782]
	TIME [epoch: 1.34 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24684899795511914		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.24684899795511914 | validation: 0.23998301532053562]
	TIME [epoch: 1.34 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.227567287084795		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.227567287084795 | validation: 0.26769835013293763]
	TIME [epoch: 1.34 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20405356675417813		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.20405356675417813 | validation: 0.3046139596821201]
	TIME [epoch: 1.34 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1916654783505164		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.1916654783505164 | validation: 0.23822900652345258]
	TIME [epoch: 1.34 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18866868754287347		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.18866868754287347 | validation: 0.24926536176961803]
	TIME [epoch: 1.34 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1881348129783365		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.1881348129783365 | validation: 0.2571350725563167]
	TIME [epoch: 1.34 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18361412632422786		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.18361412632422786 | validation: 0.25245749768915754]
	TIME [epoch: 1.34 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1770880443178173		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.1770880443178173 | validation: 0.23596922831155168]
	TIME [epoch: 1.34 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17688062382744718		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.17688062382744718 | validation: 0.2704726385060539]
	TIME [epoch: 1.34 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17964474901173777		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.17964474901173777 | validation: 0.22713500662700803]
	TIME [epoch: 1.34 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17570607613536182		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.17570607613536182 | validation: 0.28867248976817]
	TIME [epoch: 1.34 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18760303679317994		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.18760303679317994 | validation: 0.20000563347817402]
	TIME [epoch: 172 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_501.pth
	Model improved!!!
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2156289704643989		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.2156289704643989 | validation: 0.3853937864785191]
	TIME [epoch: 2.67 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25069656161020193		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.25069656161020193 | validation: 0.2067797448098217]
	TIME [epoch: 2.66 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18870297763657465		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.18870297763657465 | validation: 0.22947222263950265]
	TIME [epoch: 2.69 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17338242715890878		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.17338242715890878 | validation: 0.2738791232656416]
	TIME [epoch: 2.66 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1740749280629283		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.1740749280629283 | validation: 0.2067548228086185]
	TIME [epoch: 2.67 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1750257356270898		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.1750257356270898 | validation: 0.26666516967872733]
	TIME [epoch: 2.66 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17227757590836823		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.17227757590836823 | validation: 0.23884658567481712]
	TIME [epoch: 2.67 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16857659470210706		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.16857659470210706 | validation: 0.2417819109716613]
	TIME [epoch: 2.66 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16460149585803582		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.16460149585803582 | validation: 0.2275579649810763]
	TIME [epoch: 2.66 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1669905029345097		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.1669905029345097 | validation: 0.23337481708929064]
	TIME [epoch: 2.66 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1751186171828381		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.1751186171828381 | validation: 0.26970289655253404]
	TIME [epoch: 2.66 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1930509734004271		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.1930509734004271 | validation: 0.29640929040900227]
	TIME [epoch: 2.66 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2252690297360985		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.2252690297360985 | validation: 0.23016692643774556]
	TIME [epoch: 2.66 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18266978645134835		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.18266978645134835 | validation: 0.2760270672452295]
	TIME [epoch: 2.66 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17217424064353865		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.17217424064353865 | validation: 0.19746367505816217]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_516.pth
	Model improved!!!
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17688513948818474		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.17688513948818474 | validation: 0.326074687127442]
	TIME [epoch: 2.67 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19724938087885702		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.19724938087885702 | validation: 0.21155528099400006]
	TIME [epoch: 2.66 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17216466949279166		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.17216466949279166 | validation: 0.24650606606315964]
	TIME [epoch: 2.66 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1726047589422209		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.1726047589422209 | validation: 0.23082350360768028]
	TIME [epoch: 2.66 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16100555764858193		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.16100555764858193 | validation: 0.23621419763029122]
	TIME [epoch: 2.67 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16258717530843456		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.16258717530843456 | validation: 0.22866133049795512]
	TIME [epoch: 2.66 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15774257992123034		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.15774257992123034 | validation: 0.22880529915851253]
	TIME [epoch: 2.66 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15486344817166203		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.15486344817166203 | validation: 0.23076413657457115]
	TIME [epoch: 2.66 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15523009886654715		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.15523009886654715 | validation: 0.2296684008663882]
	TIME [epoch: 2.66 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15106715586181504		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.15106715586181504 | validation: 0.1996959129967966]
	TIME [epoch: 2.66 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15888732252043464		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.15888732252043464 | validation: 0.3314998968185915]
	TIME [epoch: 2.66 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1923595044948924		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.1923595044948924 | validation: 0.19423916684874634]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_528.pth
	Model improved!!!
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2458124196006488		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.2458124196006488 | validation: 0.31104740143965326]
	TIME [epoch: 2.66 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1840817318416493		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.1840817318416493 | validation: 0.23007422608792094]
	TIME [epoch: 2.66 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15459562586262485		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.15459562586262485 | validation: 0.20562026861090313]
	TIME [epoch: 2.66 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1562517525864369		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.1562517525864369 | validation: 0.2813123712137084]
	TIME [epoch: 2.66 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1557484951481731		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.1557484951481731 | validation: 0.19845908064526951]
	TIME [epoch: 2.66 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1512663921823643		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.1512663921823643 | validation: 0.24426235027147528]
	TIME [epoch: 2.66 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1540090750690815		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.1540090750690815 | validation: 0.22154648301314142]
	TIME [epoch: 2.66 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1509236343223633		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.1509236343223633 | validation: 0.2216972174150354]
	TIME [epoch: 2.66 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15255773161570163		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.15255773161570163 | validation: 0.23970468604782652]
	TIME [epoch: 2.66 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17139178309989198		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.17139178309989198 | validation: 0.23590520614262986]
	TIME [epoch: 2.66 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1715968361338478		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.1715968361338478 | validation: 0.22280982057299542]
	TIME [epoch: 2.66 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1617398977053675		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.1617398977053675 | validation: 0.21609560452238552]
	TIME [epoch: 2.66 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14293204413024144		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.14293204413024144 | validation: 0.22895394217890855]
	TIME [epoch: 2.66 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14104985750692303		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.14104985750692303 | validation: 0.1948762880754374]
	TIME [epoch: 2.66 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1451170526122205		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.1451170526122205 | validation: 0.2578856703272625]
	TIME [epoch: 2.66 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1525807767147083		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.1525807767147083 | validation: 0.19141325016635743]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_544.pth
	Model improved!!!
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16021028359557352		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.16021028359557352 | validation: 0.3044181244481663]
	TIME [epoch: 2.65 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17792387936659498		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.17792387936659498 | validation: 0.18872488326955736]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_546.pth
	Model improved!!!
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1659072651036156		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.1659072651036156 | validation: 0.27817065925262535]
	TIME [epoch: 2.64 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18730856502278476		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.18730856502278476 | validation: 0.1934360430821391]
	TIME [epoch: 2.64 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15644147927986773		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.15644147927986773 | validation: 0.23538958006044086]
	TIME [epoch: 2.65 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14470510904544423		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.14470510904544423 | validation: 0.20999008179203027]
	TIME [epoch: 2.64 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14216719330805766		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.14216719330805766 | validation: 0.2262095236381151]
	TIME [epoch: 2.64 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14121267108940383		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.14121267108940383 | validation: 0.20023423983418215]
	TIME [epoch: 2.64 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13929068687716178		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.13929068687716178 | validation: 0.22772087139545716]
	TIME [epoch: 2.64 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14102976452968036		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.14102976452968036 | validation: 0.20157545267962168]
	TIME [epoch: 2.64 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14154300465092434		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.14154300465092434 | validation: 0.2514426014738235]
	TIME [epoch: 2.64 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14827366840788428		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.14827366840788428 | validation: 0.1863218543575282]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_556.pth
	Model improved!!!
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15158226017205115		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.15158226017205115 | validation: 0.2935397090838233]
	TIME [epoch: 2.66 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16885560092622712		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.16885560092622712 | validation: 0.18420539540276915]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_558.pth
	Model improved!!!
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1696061407127445		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.1696061407127445 | validation: 0.271726005743357]
	TIME [epoch: 2.65 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1794726678335071		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.1794726678335071 | validation: 0.2231048846470173]
	TIME [epoch: 2.66 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14431291080622252		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.14431291080622252 | validation: 0.19990876044702954]
	TIME [epoch: 2.65 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13237229711594817		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.13237229711594817 | validation: 0.226329820668408]
	TIME [epoch: 2.65 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13458234172174463		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.13458234172174463 | validation: 0.1931465396558504]
	TIME [epoch: 2.65 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13539941820867732		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.13539941820867732 | validation: 0.2521986341315192]
	TIME [epoch: 2.65 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1438222715635325		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.1438222715635325 | validation: 0.17644185420156014]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_565.pth
	Model improved!!!
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15163910695093866		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.15163910695093866 | validation: 0.29772807486392516]
	TIME [epoch: 2.66 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15608770986106393		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.15608770986106393 | validation: 0.19771519834455684]
	TIME [epoch: 2.64 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13258528824849422		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.13258528824849422 | validation: 0.21969151843893578]
	TIME [epoch: 2.64 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12961657166892784		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.12961657166892784 | validation: 0.18703615739744694]
	TIME [epoch: 2.63 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12851147837095547		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.12851147837095547 | validation: 0.23384867577418278]
	TIME [epoch: 2.63 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1373235327119952		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.1373235327119952 | validation: 0.18906825418877815]
	TIME [epoch: 2.64 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14325985913553937		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.14325985913553937 | validation: 0.24611262972745537]
	TIME [epoch: 2.64 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1483533646181961		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.1483533646181961 | validation: 0.19036137008178455]
	TIME [epoch: 2.64 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1405129539022119		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.1405129539022119 | validation: 0.22938429250662878]
	TIME [epoch: 2.63 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13650050396974586		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.13650050396974586 | validation: 0.19031164198836487]
	TIME [epoch: 2.64 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1246074848937644		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.1246074848937644 | validation: 0.21469560219090972]
	TIME [epoch: 2.64 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12274405533808129		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.12274405533808129 | validation: 0.18688844497329257]
	TIME [epoch: 2.64 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12376051331875818		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.12376051331875818 | validation: 0.22012915521933785]
	TIME [epoch: 2.64 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12620326069629126		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.12620326069629126 | validation: 0.17848991585094656]
	TIME [epoch: 2.64 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14286194883768236		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.14286194883768236 | validation: 0.3270029737524346]
	TIME [epoch: 2.64 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17965844042178478		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.17965844042178478 | validation: 0.2019950427114562]
	TIME [epoch: 2.64 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12159040894420182		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.12159040894420182 | validation: 0.19361975324175132]
	TIME [epoch: 2.64 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1385724881371695		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.1385724881371695 | validation: 0.2617795583672752]
	TIME [epoch: 2.64 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14657979379997285		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.14657979379997285 | validation: 0.21014157476683654]
	TIME [epoch: 2.64 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12195896359560361		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.12195896359560361 | validation: 0.17204384304029083]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_585.pth
	Model improved!!!
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1330796547284648		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.1330796547284648 | validation: 0.2865522594372251]
	TIME [epoch: 2.64 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15667116902325487		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.15667116902325487 | validation: 0.18960214254174856]
	TIME [epoch: 2.64 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14337080494164522		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.14337080494164522 | validation: 0.23279617125508445]
	TIME [epoch: 2.64 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1365040885857853		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.1365040885857853 | validation: 0.20993278583387545]
	TIME [epoch: 2.64 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13053419235166602		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.13053419235166602 | validation: 0.2112921331032027]
	TIME [epoch: 2.64 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1202655040137508		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.1202655040137508 | validation: 0.19618460287123374]
	TIME [epoch: 2.64 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1150407193712055		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.1150407193712055 | validation: 0.20945745696243373]
	TIME [epoch: 2.64 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11392573213627678		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.11392573213627678 | validation: 0.19309603427346458]
	TIME [epoch: 2.65 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11217844767200133		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.11217844767200133 | validation: 0.21480877044560295]
	TIME [epoch: 2.64 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11476415100792967		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.11476415100792967 | validation: 0.18771942160442556]
	TIME [epoch: 2.64 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1180239345012697		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.1180239345012697 | validation: 0.23436044415070112]
	TIME [epoch: 2.64 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13032291425581746		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.13032291425581746 | validation: 0.18481299038755794]
	TIME [epoch: 2.64 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12141574207687701		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.12141574207687701 | validation: 0.21695314816301875]
	TIME [epoch: 2.63 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12126687628909362		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.12126687628909362 | validation: 0.19088021459927562]
	TIME [epoch: 2.64 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11451702738371346		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.11451702738371346 | validation: 0.20932027474867929]
	TIME [epoch: 2.64 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11608927117785817		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.11608927117785817 | validation: 0.20110886757752766]
	TIME [epoch: 2.64 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11686642516530399		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.11686642516530399 | validation: 0.19931095831071496]
	TIME [epoch: 2.63 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12641347144885304		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.12641347144885304 | validation: 0.2398687270869027]
	TIME [epoch: 2.63 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1424456955181976		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.1424456955181976 | validation: 0.1794450493852687]
	TIME [epoch: 2.64 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14194785885966021		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.14194785885966021 | validation: 0.2804675674637636]
	TIME [epoch: 2.64 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14769094684749162		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.14769094684749162 | validation: 0.1691187843746274]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_606.pth
	Model improved!!!
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1195238438352098		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.1195238438352098 | validation: 0.2172929605778217]
	TIME [epoch: 2.64 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10789715949331495		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.10789715949331495 | validation: 0.19548556808714868]
	TIME [epoch: 2.64 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10695204949390652		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.10695204949390652 | validation: 0.19815662525898953]
	TIME [epoch: 2.64 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10596642006506674		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.10596642006506674 | validation: 0.18640714219890586]
	TIME [epoch: 2.63 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10446830929902781		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.10446830929902781 | validation: 0.21071906372663085]
	TIME [epoch: 2.64 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11007157496827748		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.11007157496827748 | validation: 0.1909224578899486]
	TIME [epoch: 2.63 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11445522492460772		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.11445522492460772 | validation: 0.22024855037854674]
	TIME [epoch: 2.63 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12319194003000966		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.12319194003000966 | validation: 0.18892221160241887]
	TIME [epoch: 2.63 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11866175530474843		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.11866175530474843 | validation: 0.21715049649973014]
	TIME [epoch: 2.63 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11307816590439519		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.11307816590439519 | validation: 0.18292956689512582]
	TIME [epoch: 2.64 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10526847741896321		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.10526847741896321 | validation: 0.218455783736842]
	TIME [epoch: 2.63 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1092776290115162		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.1092776290115162 | validation: 0.17445187365602183]
	TIME [epoch: 2.63 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12100573703363093		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.12100573703363093 | validation: 0.30023730133923693]
	TIME [epoch: 2.63 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16470608641363169		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.16470608641363169 | validation: 0.19286352061179227]
	TIME [epoch: 2.63 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10734621521204296		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.10734621521204296 | validation: 0.19551887477240815]
	TIME [epoch: 2.63 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11242197925819096		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.11242197925819096 | validation: 0.2593669126918056]
	TIME [epoch: 2.63 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14261887558435304		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.14261887558435304 | validation: 0.20163788786071166]
	TIME [epoch: 2.64 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10274015787690648		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.10274015787690648 | validation: 0.17931672649962094]
	TIME [epoch: 2.63 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11671973278690885		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.11671973278690885 | validation: 0.2454489619278702]
	TIME [epoch: 2.64 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1222412302231379		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.1222412302231379 | validation: 0.19478154625834299]
	TIME [epoch: 2.63 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10122836731566821		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.10122836731566821 | validation: 0.19116203722200079]
	TIME [epoch: 2.65 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10620274681526193		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.10620274681526193 | validation: 0.19417792989292534]
	TIME [epoch: 2.64 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10623563554156369		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.10623563554156369 | validation: 0.22411182105559369]
	TIME [epoch: 2.64 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11903658415007504		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.11903658415007504 | validation: 0.17848040388629088]
	TIME [epoch: 2.64 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11293823135972644		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.11293823135972644 | validation: 0.22077796948728756]
	TIME [epoch: 2.63 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10956858138452248		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.10956858138452248 | validation: 0.1771966230916891]
	TIME [epoch: 2.64 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10886324999918554		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.10886324999918554 | validation: 0.22188296181543593]
	TIME [epoch: 2.64 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10603170261138235		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.10603170261138235 | validation: 0.19069279888766763]
	TIME [epoch: 2.64 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10417623121601613		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.10417623121601613 | validation: 0.2184986781147476]
	TIME [epoch: 2.64 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10627846329892808		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.10627846329892808 | validation: 0.18230545981355017]
	TIME [epoch: 2.65 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09735687796961799		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.09735687796961799 | validation: 0.21014689923665597]
	TIME [epoch: 2.64 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09984532858365829		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.09984532858365829 | validation: 0.1904874033923556]
	TIME [epoch: 2.64 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10711811216625222		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.10711811216625222 | validation: 0.20457979245842994]
	TIME [epoch: 2.63 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12320324621622332		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.12320324621622332 | validation: 0.21941422747892592]
	TIME [epoch: 2.64 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1214384490738789		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.1214384490738789 | validation: 0.1913962909652958]
	TIME [epoch: 2.63 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0952640994113401		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.0952640994113401 | validation: 0.19340929952663385]
	TIME [epoch: 2.64 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09678616226568271		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.09678616226568271 | validation: 0.2033318379968351]
	TIME [epoch: 2.64 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09886505951136015		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.09886505951136015 | validation: 0.17680484213386233]
	TIME [epoch: 2.64 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10134008995991924		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.10134008995991924 | validation: 0.22143673927404503]
	TIME [epoch: 2.64 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10852226089134143		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.10852226089134143 | validation: 0.17358208778293713]
	TIME [epoch: 2.64 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10842431875549759		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.10842431875549759 | validation: 0.2258751415859274]
	TIME [epoch: 2.64 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1177007195047828		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.1177007195047828 | validation: 0.17111010091982826]
	TIME [epoch: 2.63 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10568431469701554		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.10568431469701554 | validation: 0.2047447177083117]
	TIME [epoch: 2.65 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09198979010290696		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.09198979010290696 | validation: 0.1803235506405971]
	TIME [epoch: 2.64 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08576874004095299		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.08576874004095299 | validation: 0.1965226098924917]
	TIME [epoch: 2.64 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08871462302582409		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.08871462302582409 | validation: 0.18364994231491605]
	TIME [epoch: 2.63 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09018149839510507		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.09018149839510507 | validation: 0.21408576701247117]
	TIME [epoch: 2.64 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09804841388870376		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.09804841388870376 | validation: 0.1748145148191973]
	TIME [epoch: 2.63 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10316692737947833		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.10316692737947833 | validation: 0.2427594236254887]
	TIME [epoch: 2.64 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12509717320852515		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.12509717320852515 | validation: 0.17701156735565296]
	TIME [epoch: 2.63 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0974134405682841		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.0974134405682841 | validation: 0.19651065837077236]
	TIME [epoch: 2.64 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10515514179466914		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.10515514179466914 | validation: 0.19793834718313047]
	TIME [epoch: 2.63 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10177636013185587		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.10177636013185587 | validation: 0.2035046334997369]
	TIME [epoch: 2.63 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0966736175431272		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.0966736175431272 | validation: 0.1706126534558235]
	TIME [epoch: 2.63 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09687827590078125		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.09687827590078125 | validation: 0.22206499007248387]
	TIME [epoch: 2.64 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1065851743554925		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.1065851743554925 | validation: 0.17291152279230215]
	TIME [epoch: 2.63 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0974488651051356		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.0974488651051356 | validation: 0.21074157227192766]
	TIME [epoch: 2.64 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09261280093071644		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.09261280093071644 | validation: 0.18402471069982435]
	TIME [epoch: 2.64 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08518194074837779		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.08518194074837779 | validation: 0.1881020953360102]
	TIME [epoch: 2.64 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08954193965790527		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.08954193965790527 | validation: 0.1918335359918887]
	TIME [epoch: 2.63 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09950202244512273		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.09950202244512273 | validation: 0.20445154155628803]
	TIME [epoch: 2.64 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09613059827872411		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.09613059827872411 | validation: 0.16671813134809066]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_668.pth
	Model improved!!!
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10047320228007985		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.10047320228007985 | validation: 0.22525642410839863]
	TIME [epoch: 2.64 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09989735274900859		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.09989735274900859 | validation: 0.17033591850179852]
	TIME [epoch: 2.64 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09303797829820905		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.09303797829820905 | validation: 0.20488171805130367]
	TIME [epoch: 2.64 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0851824809156116		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.0851824809156116 | validation: 0.16677060786859182]
	TIME [epoch: 2.64 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08900828996943864		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.08900828996943864 | validation: 0.19879813840784163]
	TIME [epoch: 2.64 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08858821077168468		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.08858821077168468 | validation: 0.17575099483919857]
	TIME [epoch: 2.64 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09049431543169043		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.09049431543169043 | validation: 0.19976279705499878]
	TIME [epoch: 2.63 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08998522277625202		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.08998522277625202 | validation: 0.17351105281170134]
	TIME [epoch: 2.64 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0924748171484259		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.0924748171484259 | validation: 0.19576622413188563]
	TIME [epoch: 2.63 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08898597854312465		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.08898597854312465 | validation: 0.1838794055264451]
	TIME [epoch: 2.64 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08382005364576718		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.08382005364576718 | validation: 0.17493201848766465]
	TIME [epoch: 2.64 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0908464067623901		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.0908464067623901 | validation: 0.2357773501942096]
	TIME [epoch: 2.64 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11627220737168666		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.11627220737168666 | validation: 0.1748382618635239]
	TIME [epoch: 2.63 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08310198684624517		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.08310198684624517 | validation: 0.1856377892374137]
	TIME [epoch: 2.63 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09280123670071991		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.09280123670071991 | validation: 0.22096836396678554]
	TIME [epoch: 2.64 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10424424986726624		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.10424424986726624 | validation: 0.17663744563258402]
	TIME [epoch: 2.63 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07875069256930796		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.07875069256930796 | validation: 0.18496507842341653]
	TIME [epoch: 2.64 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08344576999652797		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.08344576999652797 | validation: 0.18290307533847597]
	TIME [epoch: 2.63 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08732750485871786		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.08732750485871786 | validation: 0.19739231978297397]
	TIME [epoch: 2.63 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08292029986320756		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.08292029986320756 | validation: 0.16919081574236483]
	TIME [epoch: 2.64 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08380831855797535		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.08380831855797535 | validation: 0.21682249045648783]
	TIME [epoch: 2.64 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09539412394281195		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.09539412394281195 | validation: 0.15966243127010715]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_690.pth
	Model improved!!!
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10587478824291728		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.10587478824291728 | validation: 0.22250902169507608]
	TIME [epoch: 2.64 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09928900614411486		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.09928900614411486 | validation: 0.18200686222258758]
	TIME [epoch: 2.63 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0732508559031682		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.0732508559031682 | validation: 0.1777703246765977]
	TIME [epoch: 2.64 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07713770792218476		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.07713770792218476 | validation: 0.19065251231949765]
	TIME [epoch: 2.64 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0801055997236101		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.0801055997236101 | validation: 0.16866280258979485]
	TIME [epoch: 2.63 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08191502145478322		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.08191502145478322 | validation: 0.21137761224054505]
	TIME [epoch: 2.63 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09575569030788111		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.09575569030788111 | validation: 0.15991584835740358]
	TIME [epoch: 2.63 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09390747186521313		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.09390747186521313 | validation: 0.20417608438407941]
	TIME [epoch: 2.63 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08264857925003513		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.08264857925003513 | validation: 0.1748993232946643]
	TIME [epoch: 2.63 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0769510810497151		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.0769510810497151 | validation: 0.18479966247618412]
	TIME [epoch: 2.63 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0840227573427001		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.0840227573427001 | validation: 0.20404985441783163]
	TIME [epoch: 2.65 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09839609931807793		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.09839609931807793 | validation: 0.18118578888589884]
	TIME [epoch: 2.65 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09865228314237623		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.09865228314237623 | validation: 0.18162283716553795]
	TIME [epoch: 2.63 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07603502411216277		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.07603502411216277 | validation: 0.18439029984757982]
	TIME [epoch: 2.64 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07444340833597389		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.07444340833597389 | validation: 0.17470090777557623]
	TIME [epoch: 2.64 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07584379467163795		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.07584379467163795 | validation: 0.1966766560849408]
	TIME [epoch: 2.63 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07838931194450638		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.07838931194450638 | validation: 0.1675179684175362]
	TIME [epoch: 2.63 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07549757201439036		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.07549757201439036 | validation: 0.20132597205360653]
	TIME [epoch: 2.63 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07752696875234072		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.07752696875234072 | validation: 0.16520949959411724]
	TIME [epoch: 2.63 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09064819241498434		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.09064819241498434 | validation: 0.23162103939261397]
	TIME [epoch: 2.63 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12201083535941702		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.12201083535941702 | validation: 0.18026015778608756]
	TIME [epoch: 2.64 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0749497585533037		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.0749497585533037 | validation: 0.17713639295943984]
	TIME [epoch: 2.63 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10000267575477548		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.10000267575477548 | validation: 0.21342406994972915]
	TIME [epoch: 2.63 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09933532009304355		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.09933532009304355 | validation: 0.18089462513628024]
	TIME [epoch: 2.63 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07143518739847624		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.07143518739847624 | validation: 0.17386575065508955]
	TIME [epoch: 2.63 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07960383192315465		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.07960383192315465 | validation: 0.18466576647766228]
	TIME [epoch: 2.64 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07875974798760518		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.07875974798760518 | validation: 0.1832890039498526]
	TIME [epoch: 2.63 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07108132994174113		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.07108132994174113 | validation: 0.16988659952937218]
	TIME [epoch: 2.64 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07023839619989812		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.07023839619989812 | validation: 0.18653225913603944]
	TIME [epoch: 2.63 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07251163339748735		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.07251163339748735 | validation: 0.1485706441048215]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_720.pth
	Model improved!!!
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08861961438908246		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.08861961438908246 | validation: 0.2304559442820188]
	TIME [epoch: 2.65 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11249368687812304		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.11249368687812304 | validation: 0.1642748828535503]
	TIME [epoch: 2.65 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08213388607518551		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.08213388607518551 | validation: 0.17703533946409855]
	TIME [epoch: 2.65 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08204395841069864		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.08204395841069864 | validation: 0.21138140934419233]
	TIME [epoch: 2.65 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08969144553886105		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.08969144553886105 | validation: 0.18060730665947064]
	TIME [epoch: 2.65 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0711598777480008		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.0711598777480008 | validation: 0.17575594765074742]
	TIME [epoch: 2.63 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07397690523685845		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.07397690523685845 | validation: 0.1919572522091079]
	TIME [epoch: 2.64 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07631692868037405		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.07631692868037405 | validation: 0.17542663937771477]
	TIME [epoch: 2.64 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06856001234675117		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.06856001234675117 | validation: 0.18043302269449035]
	TIME [epoch: 2.64 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07004219475098035		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.07004219475098035 | validation: 0.1605513662616841]
	TIME [epoch: 2.64 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06548250955145729		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.06548250955145729 | validation: 0.18259754170882803]
	TIME [epoch: 2.64 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07037524809970783		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.07037524809970783 | validation: 0.1549585262613667]
	TIME [epoch: 2.63 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07118760636491299		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.07118760636491299 | validation: 0.20154434620421213]
	TIME [epoch: 2.64 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07936834453503205		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.07936834453503205 | validation: 0.15823399879408134]
	TIME [epoch: 2.64 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08903698018468377		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.08903698018468377 | validation: 0.20454202453694031]
	TIME [epoch: 2.63 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08979608445703498		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.08979608445703498 | validation: 0.1625959725366086]
	TIME [epoch: 2.63 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07436780493279853		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.07436780493279853 | validation: 0.17168329461643506]
	TIME [epoch: 2.63 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07389014721721551		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.07389014721721551 | validation: 0.1952921783176141]
	TIME [epoch: 2.63 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0850754878529435		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.0850754878529435 | validation: 0.16885110540358916]
	TIME [epoch: 2.64 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07111022349303892		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.07111022349303892 | validation: 0.18222780631412627]
	TIME [epoch: 2.64 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06828326354399283		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.06828326354399283 | validation: 0.16147586512379722]
	TIME [epoch: 2.64 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0657226498372398		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.0657226498372398 | validation: 0.17913106715378246]
	TIME [epoch: 2.63 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07283266087737672		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.07283266087737672 | validation: 0.16242596494664513]
	TIME [epoch: 2.65 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07976803576657081		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.07976803576657081 | validation: 0.19174759651934978]
	TIME [epoch: 2.64 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07733207409739393		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.07733207409739393 | validation: 0.16130101608458364]
	TIME [epoch: 2.64 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07187652134890384		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.07187652134890384 | validation: 0.18343800350227882]
	TIME [epoch: 2.63 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06650438055425471		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.06650438055425471 | validation: 0.15171940915636428]
	TIME [epoch: 2.64 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07341587655634393		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.07341587655634393 | validation: 0.20757206894915667]
	TIME [epoch: 2.63 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0888865406696991		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.0888865406696991 | validation: 0.15584505466363263]
	TIME [epoch: 2.64 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0751265465834729		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.0751265465834729 | validation: 0.17613325934974036]
	TIME [epoch: 2.64 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06796014853701461		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.06796014853701461 | validation: 0.15988642399109693]
	TIME [epoch: 2.64 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06359219033366773		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.06359219033366773 | validation: 0.16633866816141168]
	TIME [epoch: 2.63 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06414357064367955		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.06414357064367955 | validation: 0.17479130551217653]
	TIME [epoch: 2.64 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06258072104325		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.06258072104325 | validation: 0.15950283593523062]
	TIME [epoch: 2.63 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06259162322664923		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.06259162322664923 | validation: 0.17273678596187195]
	TIME [epoch: 2.64 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062024372530729226		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.062024372530729226 | validation: 0.15963087222129393]
	TIME [epoch: 2.64 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06923822606633818		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.06923822606633818 | validation: 0.20002226172315654]
	TIME [epoch: 2.64 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08559457072562524		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.08559457072562524 | validation: 0.15012332586247107]
	TIME [epoch: 2.64 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07938955618166675		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.07938955618166675 | validation: 0.1887461897999404]
	TIME [epoch: 2.64 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08681765641144383		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.08681765641144383 | validation: 0.17271305220972344]
	TIME [epoch: 2.63 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06386325151919174		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.06386325151919174 | validation: 0.15532736139818099]
	TIME [epoch: 2.64 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07426759913264791		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.07426759913264791 | validation: 0.1873880263676392]
	TIME [epoch: 2.64 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07517533845983775		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.07517533845983775 | validation: 0.1579995328911028]
	TIME [epoch: 2.64 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06580633732589179		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.06580633732589179 | validation: 0.16556290076975574]
	TIME [epoch: 2.63 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07081455189428996		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.07081455189428996 | validation: 0.1875894883721927]
	TIME [epoch: 2.64 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08230288518142281		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.08230288518142281 | validation: 0.15819109915915666]
	TIME [epoch: 2.63 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061984203871398864		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.061984203871398864 | validation: 0.16390841224430264]
	TIME [epoch: 2.64 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06416267728597984		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.06416267728597984 | validation: 0.1767783110183278]
	TIME [epoch: 2.64 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07083096976573869		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.07083096976573869 | validation: 0.16733247268999754]
	TIME [epoch: 2.64 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06980092040454398		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.06980092040454398 | validation: 0.1822369834780497]
	TIME [epoch: 2.64 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06759979799131405		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.06759979799131405 | validation: 0.15620037114175594]
	TIME [epoch: 2.64 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06300747577737675		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.06300747577737675 | validation: 0.1826708876140319]
	TIME [epoch: 2.63 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07062818286096605		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.07062818286096605 | validation: 0.14851409783663122]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_773.pth
	Model improved!!!
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06746011221495885		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.06746011221495885 | validation: 0.18568696289048583]
	TIME [epoch: 2.66 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0662346229173719		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.0662346229173719 | validation: 0.15546247403319904]
	TIME [epoch: 2.66 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06419619493569978		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.06419619493569978 | validation: 0.17504993202209862]
	TIME [epoch: 2.66 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06038768038972947		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.06038768038972947 | validation: 0.15298315794034767]
	TIME [epoch: 2.66 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060895704443336016		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.060895704443336016 | validation: 0.17764817180435533]
	TIME [epoch: 2.67 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06265549474042309		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.06265549474042309 | validation: 0.1491612935362395]
	TIME [epoch: 2.66 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06500379411066941		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.06500379411066941 | validation: 0.17571925650725959]
	TIME [epoch: 2.66 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05962767328446639		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.05962767328446639 | validation: 0.15419680612814457]
	TIME [epoch: 2.66 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062011075277356185		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.062011075277356185 | validation: 0.191348833069664]
	TIME [epoch: 2.66 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07723443580155698		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.07723443580155698 | validation: 0.1609333680812041]
	TIME [epoch: 2.67 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08013174823435348		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.08013174823435348 | validation: 0.17980781364463064]
	TIME [epoch: 2.66 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0817287106376655		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.0817287106376655 | validation: 0.17274617704952694]
	TIME [epoch: 2.66 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06166082919857756		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.06166082919857756 | validation: 0.1513873299818986]
	TIME [epoch: 2.66 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07357302874562761		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.07357302874562761 | validation: 0.19463341778441362]
	TIME [epoch: 2.66 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0853503264022282		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.0853503264022282 | validation: 0.15824224354432295]
	TIME [epoch: 2.66 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06052960468889284		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.06052960468889284 | validation: 0.17068713407871902]
	TIME [epoch: 2.66 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07691473107301498		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.07691473107301498 | validation: 0.16963862057116041]
	TIME [epoch: 2.66 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06807721521431619		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.06807721521431619 | validation: 0.17389525323892374]
	TIME [epoch: 2.66 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05799883139997343		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.05799883139997343 | validation: 0.15782423843727017]
	TIME [epoch: 2.66 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05968378856306194		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.05968378856306194 | validation: 0.16681956771287682]
	TIME [epoch: 2.66 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059793341416214285		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.059793341416214285 | validation: 0.15840703552860957]
	TIME [epoch: 2.66 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05680239591096022		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.05680239591096022 | validation: 0.15299546652191062]
	TIME [epoch: 2.66 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05715377007825736		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.05715377007825736 | validation: 0.1632562040128579]
	TIME [epoch: 2.66 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0572833863226558		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.0572833863226558 | validation: 0.16713786587178703]
	TIME [epoch: 2.66 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057279512271881876		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.057279512271881876 | validation: 0.15830507173908515]
	TIME [epoch: 2.66 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057210499189971585		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.057210499189971585 | validation: 0.17239763193681615]
	TIME [epoch: 2.66 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057198626741334635		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.057198626741334635 | validation: 0.14354273238144888]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_800.pth
	Model improved!!!
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061746177304452705		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.061746177304452705 | validation: 0.1930228970959044]
	TIME [epoch: 2.64 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0824597908230549		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.0824597908230549 | validation: 0.1493062173412897]
	TIME [epoch: 2.63 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06907507964046751		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.06907507964046751 | validation: 0.1747629435862414]
	TIME [epoch: 2.63 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059943300685864974		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.059943300685864974 | validation: 0.15094519116282357]
	TIME [epoch: 2.63 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05618156043689152		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.05618156043689152 | validation: 0.16075596483528282]
	TIME [epoch: 2.63 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05532742099162057		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.05532742099162057 | validation: 0.1572152087543947]
	TIME [epoch: 2.64 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054990865222239435		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.054990865222239435 | validation: 0.1595490286710881]
	TIME [epoch: 2.63 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06746656712112946		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.06746656712112946 | validation: 0.17000904656466514]
	TIME [epoch: 2.63 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07435280093445147		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.07435280093445147 | validation: 0.1512476664072232]
	TIME [epoch: 2.63 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060128980533396145		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.060128980533396145 | validation: 0.1591033540943847]
	TIME [epoch: 2.63 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05382055302132395		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.05382055302132395 | validation: 0.1585259784137748]
	TIME [epoch: 2.63 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05570430433181614		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.05570430433181614 | validation: 0.15386537478366677]
	TIME [epoch: 2.63 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05693927194757221		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.05693927194757221 | validation: 0.16354691101795005]
	TIME [epoch: 2.63 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060385223985650834		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.060385223985650834 | validation: 0.14865949933717487]
	TIME [epoch: 2.63 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06201255878506372		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.06201255878506372 | validation: 0.17083863962595067]
	TIME [epoch: 2.63 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06094779709903247		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.06094779709903247 | validation: 0.1558516385091976]
	TIME [epoch: 2.63 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05211440115927614		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.05211440115927614 | validation: 0.15515641852484408]
	TIME [epoch: 2.64 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053998495630955844		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.053998495630955844 | validation: 0.16463196614851935]
	TIME [epoch: 2.63 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05327469468384959		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.05327469468384959 | validation: 0.14698127783745188]
	TIME [epoch: 2.64 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053730427825008684		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.053730427825008684 | validation: 0.1939209131851242]
	TIME [epoch: 2.64 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07790781612055364		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.07790781612055364 | validation: 0.15055555711009683]
	TIME [epoch: 2.63 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07888959887426408		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.07888959887426408 | validation: 0.18245714358967735]
	TIME [epoch: 2.63 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07175751105488702		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.07175751105488702 | validation: 0.15402710947335554]
	TIME [epoch: 2.64 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051825703992071225		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.051825703992071225 | validation: 0.14108322766911957]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_824.pth
	Model improved!!!
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0699074803130398		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.0699074803130398 | validation: 0.18205912931901613]
	TIME [epoch: 2.67 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07250047588619532		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.07250047588619532 | validation: 0.16178007145643705]
	TIME [epoch: 2.66 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052857972697193195		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.052857972697193195 | validation: 0.1425183874112516]
	TIME [epoch: 2.66 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062489064936032776		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.062489064936032776 | validation: 0.16590391904462173]
	TIME [epoch: 2.64 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05586895158712907		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.05586895158712907 | validation: 0.16002886854594056]
	TIME [epoch: 2.64 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05110212812341935		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.05110212812341935 | validation: 0.14974811385932849]
	TIME [epoch: 2.63 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05873708031059695		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.05873708031059695 | validation: 0.16819649774953463]
	TIME [epoch: 2.63 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05615153249706822		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.05615153249706822 | validation: 0.14425742941432904]
	TIME [epoch: 2.63 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05637392399170982		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.05637392399170982 | validation: 0.1537824860896919]
	TIME [epoch: 2.63 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05578045738461235		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.05578045738461235 | validation: 0.15862375689004637]
	TIME [epoch: 2.63 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05631226773268772		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.05631226773268772 | validation: 0.15781307303924433]
	TIME [epoch: 2.64 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0591394557225493		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.0591394557225493 | validation: 0.15596340614781187]
	TIME [epoch: 2.63 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057890924502812656		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.057890924502812656 | validation: 0.1477648786775786]
	TIME [epoch: 2.63 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052228619185577065		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.052228619185577065 | validation: 0.14935078245002611]
	TIME [epoch: 2.63 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052529398229800135		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.052529398229800135 | validation: 0.1539592877487536]
	TIME [epoch: 2.64 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05249946709133778		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.05249946709133778 | validation: 0.14597008903533235]
	TIME [epoch: 2.64 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05619392146419396		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.05619392146419396 | validation: 0.17361902337801066]
	TIME [epoch: 2.63 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05634107678148897		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.05634107678148897 | validation: 0.1468061841893273]
	TIME [epoch: 2.63 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05386980079882276		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.05386980079882276 | validation: 0.16733857870913932]
	TIME [epoch: 2.63 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05891101253197308		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.05891101253197308 | validation: 0.13948644715645764]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_844.pth
	Model improved!!!
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05860132853570284		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.05860132853570284 | validation: 0.15856638618785426]
	TIME [epoch: 2.67 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05261611735416917		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.05261611735416917 | validation: 0.148063315785181]
	TIME [epoch: 2.66 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05170496831307759		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.05170496831307759 | validation: 0.14229709470881202]
	TIME [epoch: 2.66 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058448717554595415		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.058448717554595415 | validation: 0.1586590719109343]
	TIME [epoch: 2.66 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06192586340797158		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.06192586340797158 | validation: 0.14343146784096958]
	TIME [epoch: 2.66 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061311521037514576		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.061311521037514576 | validation: 0.15508267240931506]
	TIME [epoch: 2.66 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05115761304359655		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.05115761304359655 | validation: 0.14807769619470226]
	TIME [epoch: 2.66 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05005705549842288		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.05005705549842288 | validation: 0.1462656708155344]
	TIME [epoch: 2.66 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0575051042432756		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.0575051042432756 | validation: 0.1507706056482268]
	TIME [epoch: 2.66 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052228804778587684		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.052228804778587684 | validation: 0.15852997366568183]
	TIME [epoch: 2.66 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04942596353912644		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.04942596353912644 | validation: 0.14147258973970414]
	TIME [epoch: 2.63 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056226989549950694		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.056226989549950694 | validation: 0.16809318753109195]
	TIME [epoch: 2.64 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06413225507899599		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.06413225507899599 | validation: 0.14118236087763503]
	TIME [epoch: 2.63 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06250976380823023		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.06250976380823023 | validation: 0.16744659478884827]
	TIME [epoch: 2.63 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057502201061638156		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.057502201061638156 | validation: 0.1538781089088363]
	TIME [epoch: 2.64 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046128804449174526		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.046128804449174526 | validation: 0.14525531156451948]
	TIME [epoch: 2.64 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05036448736056112		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.05036448736056112 | validation: 0.15412933345834393]
	TIME [epoch: 2.66 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05063559676781891		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.05063559676781891 | validation: 0.14594842654723733]
	TIME [epoch: 2.64 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05237726281686282		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.05237726281686282 | validation: 0.15202959222272205]
	TIME [epoch: 2.65 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05568194744963325		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.05568194744963325 | validation: 0.15334381961146334]
	TIME [epoch: 2.65 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0517961365255867		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.0517961365255867 | validation: 0.14207011592113178]
	TIME [epoch: 2.65 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05211670195011066		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.05211670195011066 | validation: 0.16476588049446061]
	TIME [epoch: 2.64 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052016462454714904		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.052016462454714904 | validation: 0.14120062104920203]
	TIME [epoch: 2.65 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04928589704372754		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.04928589704372754 | validation: 0.1593487383337538]
	TIME [epoch: 2.65 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050440935672152624		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.050440935672152624 | validation: 0.1408792534905128]
	TIME [epoch: 2.64 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05900163836809529		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.05900163836809529 | validation: 0.17277644882557947]
	TIME [epoch: 2.63 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06220957915560301		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.06220957915560301 | validation: 0.1396611071341978]
	TIME [epoch: 2.64 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05326974041479732		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.05326974041479732 | validation: 0.15199932404967184]
	TIME [epoch: 2.63 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04918025945968973		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.04918025945968973 | validation: 0.15754316064967389]
	TIME [epoch: 2.65 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051567705869295		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.051567705869295 | validation: 0.15232823318244446]
	TIME [epoch: 2.63 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04963191216305699		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.04963191216305699 | validation: 0.15329069589431246]
	TIME [epoch: 2.64 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051915320908308235		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.051915320908308235 | validation: 0.14952810255991772]
	TIME [epoch: 2.64 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05434310298128693		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.05434310298128693 | validation: 0.154924506289075]
	TIME [epoch: 2.64 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052373162841587426		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.052373162841587426 | validation: 0.14508722470726002]
	TIME [epoch: 2.64 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04684250690725961		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.04684250690725961 | validation: 0.14818267175085972]
	TIME [epoch: 2.64 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05240909916134939		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.05240909916134939 | validation: 0.16175684596869444]
	TIME [epoch: 2.64 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061722792469127444		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.061722792469127444 | validation: 0.1438354758607101]
	TIME [epoch: 2.63 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051778360408318104		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.051778360408318104 | validation: 0.15867159501801265]
	TIME [epoch: 2.63 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057322264707223985		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.057322264707223985 | validation: 0.14369545396515457]
	TIME [epoch: 2.63 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04981246729697081		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.04981246729697081 | validation: 0.1502299177561482]
	TIME [epoch: 2.64 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04614466043336073		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.04614466043336073 | validation: 0.14415390012055712]
	TIME [epoch: 2.63 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047117485105907554		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.047117485105907554 | validation: 0.14594460307029178]
	TIME [epoch: 2.63 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04519338804109582		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.04519338804109582 | validation: 0.1493205331528805]
	TIME [epoch: 2.63 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0486979557300546		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.0486979557300546 | validation: 0.14423643596645808]
	TIME [epoch: 2.63 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045932187673745746		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.045932187673745746 | validation: 0.153346372654656]
	TIME [epoch: 2.63 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04855282898254414		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.04855282898254414 | validation: 0.140174883844949]
	TIME [epoch: 2.66 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048869730695931486		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.048869730695931486 | validation: 0.15805896283257992]
	TIME [epoch: 2.64 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05804643253859332		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.05804643253859332 | validation: 0.1421430976111731]
	TIME [epoch: 2.64 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05522247881710556		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.05522247881710556 | validation: 0.15617309303976581]
	TIME [epoch: 2.64 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05579739068805271		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.05579739068805271 | validation: 0.1426283123041486]
	TIME [epoch: 2.64 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04496324173017149		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.04496324173017149 | validation: 0.13899731575321117]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_895.pth
	Model improved!!!
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047066526450153336		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.047066526450153336 | validation: 0.15875482012565023]
	TIME [epoch: 2.65 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054765190100447385		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.054765190100447385 | validation: 0.14740865820538449]
	TIME [epoch: 2.65 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05068299614961823		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.05068299614961823 | validation: 0.154082191809743]
	TIME [epoch: 2.65 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05303060399811983		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.05303060399811983 | validation: 0.14066419047388343]
	TIME [epoch: 2.65 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04316411740641101		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.04316411740641101 | validation: 0.14796955099309547]
	TIME [epoch: 2.65 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0488113290847123		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.0488113290847123 | validation: 0.15174068898367032]
	TIME [epoch: 2.65 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05823140210107324		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.05823140210107324 | validation: 0.15248171457201298]
	TIME [epoch: 2.65 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04658227808487814		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.04658227808487814 | validation: 0.14952279025369442]
	TIME [epoch: 2.64 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04499111921434177		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.04499111921434177 | validation: 0.13222164241674927]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_904.pth
	Model improved!!!
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05136118552147929		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.05136118552147929 | validation: 0.16013627755746432]
	TIME [epoch: 2.65 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055624987457178816		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.055624987457178816 | validation: 0.13387365475534008]
	TIME [epoch: 2.66 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052042921193470204		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.052042921193470204 | validation: 0.15435664586382628]
	TIME [epoch: 2.64 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05190271220198561		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.05190271220198561 | validation: 0.14958868989111793]
	TIME [epoch: 2.64 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05056150578999536		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.05056150578999536 | validation: 0.13222485839264003]
	TIME [epoch: 2.64 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05047924303817657		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.05047924303817657 | validation: 0.14698240332499393]
	TIME [epoch: 2.64 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04529061925373067		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.04529061925373067 | validation: 0.14401969091565422]
	TIME [epoch: 2.64 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04493395839304403		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.04493395839304403 | validation: 0.15519682263051351]
	TIME [epoch: 2.65 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04553945860408416		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.04553945860408416 | validation: 0.14204815984160205]
	TIME [epoch: 2.64 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04323196275778973		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.04323196275778973 | validation: 0.15536859941523887]
	TIME [epoch: 2.64 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043281072061913416		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.043281072061913416 | validation: 0.14187005797582683]
	TIME [epoch: 2.64 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04244821557293088		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.04244821557293088 | validation: 0.13453905634932303]
	TIME [epoch: 2.64 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0487303042365838		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.0487303042365838 | validation: 0.15992600352683772]
	TIME [epoch: 2.65 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04979328232256186		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.04979328232256186 | validation: 0.14381331179526508]
	TIME [epoch: 2.64 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04588415189030719		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.04588415189030719 | validation: 0.15546282371386796]
	TIME [epoch: 2.64 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05026648874288608		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.05026648874288608 | validation: 0.14486949177410188]
	TIME [epoch: 2.64 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04710162833987222		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.04710162833987222 | validation: 0.14458516776572278]
	TIME [epoch: 2.64 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0504703831685292		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.0504703831685292 | validation: 0.13778119758279225]
	TIME [epoch: 2.65 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05207156383510686		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.05207156383510686 | validation: 0.15621430235838407]
	TIME [epoch: 2.65 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04953872393376672		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.04953872393376672 | validation: 0.14701591520920873]
	TIME [epoch: 2.65 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04255846483052128		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.04255846483052128 | validation: 0.1459823206603482]
	TIME [epoch: 2.64 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04527196361510894		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.04527196361510894 | validation: 0.15072716416071816]
	TIME [epoch: 2.64 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052633234968020706		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.052633234968020706 | validation: 0.1466829115026552]
	TIME [epoch: 2.64 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05211429540624197		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.05211429540624197 | validation: 0.1483833931249177]
	TIME [epoch: 2.65 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04781822349170099		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.04781822349170099 | validation: 0.1370854901640514]
	TIME [epoch: 2.64 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04047532299207112		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.04047532299207112 | validation: 0.14594801913459232]
	TIME [epoch: 2.64 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04342290817017313		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.04342290817017313 | validation: 0.14740597349811388]
	TIME [epoch: 2.64 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04873513785703288		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.04873513785703288 | validation: 0.14039300965876977]
	TIME [epoch: 2.64 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04344353538157197		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.04344353538157197 | validation: 0.1449136822857227]
	TIME [epoch: 2.64 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04266208014899614		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.04266208014899614 | validation: 0.1382904481436282]
	TIME [epoch: 2.65 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04385308164461606		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.04385308164461606 | validation: 0.14499049709141087]
	TIME [epoch: 2.65 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042624769856683835		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.042624769856683835 | validation: 0.13995009961865215]
	TIME [epoch: 2.65 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044134309839223854		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.044134309839223854 | validation: 0.14156858822638957]
	TIME [epoch: 2.64 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04187271605647377		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.04187271605647377 | validation: 0.13059183935788793]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_938.pth
	Model improved!!!
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05205901563261117		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.05205901563261117 | validation: 0.16565839312580743]
	TIME [epoch: 2.66 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057213141026274034		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.057213141026274034 | validation: 0.13824521058447378]
	TIME [epoch: 2.65 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044210417524638126		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.044210417524638126 | validation: 0.1414280978673137]
	TIME [epoch: 2.65 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040794813285741804		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.040794813285741804 | validation: 0.13346424937263765]
	TIME [epoch: 2.63 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04697621756189474		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.04697621756189474 | validation: 0.1548496384729636]
	TIME [epoch: 2.64 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0525083337619906		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.0525083337619906 | validation: 0.13365665601828058]
	TIME [epoch: 2.64 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0441351109865745		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.0441351109865745 | validation: 0.1379730135368285]
	TIME [epoch: 2.64 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04226143172841208		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.04226143172841208 | validation: 0.1519931055913582]
	TIME [epoch: 2.64 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045269527137384795		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.045269527137384795 | validation: 0.13540945789763012]
	TIME [epoch: 2.64 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04089912739276909		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.04089912739276909 | validation: 0.13711986434994392]
	TIME [epoch: 2.64 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04423739393375398		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.04423739393375398 | validation: 0.15723320425231235]
	TIME [epoch: 2.64 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04967386051479771		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.04967386051479771 | validation: 0.13681064921177716]
	TIME [epoch: 2.64 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05033041452036684		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.05033041452036684 | validation: 0.14967769896979696]
	TIME [epoch: 2.64 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05183912691450651		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.05183912691450651 | validation: 0.14507638858018684]
	TIME [epoch: 2.64 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048720059275634275		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.048720059275634275 | validation: 0.12981085913349352]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_953.pth
	Model improved!!!
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047667160884024756		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.047667160884024756 | validation: 0.14481275995561688]
	TIME [epoch: 2.65 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04184205427835039		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.04184205427835039 | validation: 0.14568648059555153]
	TIME [epoch: 2.65 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043358840236091725		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.043358840236091725 | validation: 0.13134844549964533]
	TIME [epoch: 2.65 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04877991980508539		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.04877991980508539 | validation: 0.13989626854185247]
	TIME [epoch: 2.65 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041933411195694034		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.041933411195694034 | validation: 0.14331923525648954]
	TIME [epoch: 2.65 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041103663486311934		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.041103663486311934 | validation: 0.13407767786251898]
	TIME [epoch: 2.65 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044302540855093955		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.044302540855093955 | validation: 0.1430122345512372]
	TIME [epoch: 2.64 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04498166152360067		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.04498166152360067 | validation: 0.14151147714952048]
	TIME [epoch: 2.65 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04339488404272451		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.04339488404272451 | validation: 0.148774089325604]
	TIME [epoch: 2.65 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04543647797178371		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.04543647797178371 | validation: 0.13609811115916146]
	TIME [epoch: 2.65 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04227109922858947		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.04227109922858947 | validation: 0.15098646859796638]
	TIME [epoch: 2.64 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04529725436532006		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.04529725436532006 | validation: 0.13564319152918122]
	TIME [epoch: 2.64 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04352665249580135		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.04352665249580135 | validation: 0.1406542088118727]
	TIME [epoch: 2.64 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04316104934512076		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.04316104934512076 | validation: 0.13159653558100745]
	TIME [epoch: 2.64 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04392119179072358		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.04392119179072358 | validation: 0.13847364783436927]
	TIME [epoch: 2.65 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04642552870425353		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.04642552870425353 | validation: 0.1456736967358506]
	TIME [epoch: 2.65 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04563305274915897		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.04563305274915897 | validation: 0.1354563730055877]
	TIME [epoch: 2.65 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03938568385430391		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.03938568385430391 | validation: 0.14400506917148312]
	TIME [epoch: 2.65 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040911334528946054		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.040911334528946054 | validation: 0.12956950153218622]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_972.pth
	Model improved!!!
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045061259384667644		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.045061259384667644 | validation: 0.13857005990423504]
	TIME [epoch: 2.66 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04365202977436359		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.04365202977436359 | validation: 0.13611795916576613]
	TIME [epoch: 2.65 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04090672340781305		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.04090672340781305 | validation: 0.14546509393161428]
	TIME [epoch: 2.65 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0410181674082346		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.0410181674082346 | validation: 0.1332576164578206]
	TIME [epoch: 2.65 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045826212101986404		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.045826212101986404 | validation: 0.13868348266051342]
	TIME [epoch: 2.65 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04578309780765003		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.04578309780765003 | validation: 0.14428367507044013]
	TIME [epoch: 2.65 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0432578312784969		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.0432578312784969 | validation: 0.12420390813098336]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_979.pth
	Model improved!!!
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04687120746656495		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.04687120746656495 | validation: 0.154078596715628]
	TIME [epoch: 2.65 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04507255916040279		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.04507255916040279 | validation: 0.12964974260275852]
	TIME [epoch: 2.66 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044178498211517585		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.044178498211517585 | validation: 0.12751852484994056]
	TIME [epoch: 2.66 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03891882120231342		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.03891882120231342 | validation: 0.14393305399961037]
	TIME [epoch: 2.67 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04215015594342046		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.04215015594342046 | validation: 0.13124995795939673]
	TIME [epoch: 2.66 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04260917535593949		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.04260917535593949 | validation: 0.1370320523138604]
	TIME [epoch: 2.66 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039484993122125486		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.039484993122125486 | validation: 0.14240836595864512]
	TIME [epoch: 2.66 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04193839827860371		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.04193839827860371 | validation: 0.13089388171006153]
	TIME [epoch: 2.66 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039910466372740594		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.039910466372740594 | validation: 0.12981240052934548]
	TIME [epoch: 2.66 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03955384012422196		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.03955384012422196 | validation: 0.14529675148058752]
	TIME [epoch: 2.66 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043102478331614055		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.043102478331614055 | validation: 0.1290840708729105]
	TIME [epoch: 2.66 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038066668981036396		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.038066668981036396 | validation: 0.13624753111912713]
	TIME [epoch: 2.66 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0395801079405068		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.0395801079405068 | validation: 0.13679748774955647]
	TIME [epoch: 2.66 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04069120894309177		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.04069120894309177 | validation: 0.13670692327973355]
	TIME [epoch: 2.66 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04752396466133381		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.04752396466133381 | validation: 0.14447081298083006]
	TIME [epoch: 2.67 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05112315531465092		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.05112315531465092 | validation: 0.13496368068881429]
	TIME [epoch: 2.66 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040502819421327135		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.040502819421327135 | validation: 0.13640762351513464]
	TIME [epoch: 2.66 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04841118153911591		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.04841118153911591 | validation: 0.1504479483844387]
	TIME [epoch: 2.66 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0505884102664643		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.0505884102664643 | validation: 0.1322192573785224]
	TIME [epoch: 2.66 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03969655831454381		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.03969655831454381 | validation: 0.1411396325914408]
	TIME [epoch: 2.66 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05011736186536028		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.05011736186536028 | validation: 0.13895929889292336]
	TIME [epoch: 2.66 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04261197213301621		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.04261197213301621 | validation: 0.14069420799710022]
	TIME [epoch: 173 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040046003422412435		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.040046003422412435 | validation: 0.13219082577818875]
	TIME [epoch: 5.71 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03832568697871373		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.03832568697871373 | validation: 0.13763185039642198]
	TIME [epoch: 5.7 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03826011930275938		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.03826011930275938 | validation: 0.13211030960713216]
	TIME [epoch: 5.7 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04212329580595069		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.04212329580595069 | validation: 0.1400545935627802]
	TIME [epoch: 5.71 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04320210996086633		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.04320210996086633 | validation: 0.13372639444179457]
	TIME [epoch: 5.7 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04085555307517789		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.04085555307517789 | validation: 0.12167450988081117]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_1007.pth
	Model improved!!!
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04593678195040221		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.04593678195040221 | validation: 0.13711930634702768]
	TIME [epoch: 5.7 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044221319316812994		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.044221319316812994 | validation: 0.12101791884225979]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_1009.pth
	Model improved!!!
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04036681186969391		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.04036681186969391 | validation: 0.13469771771541644]
	TIME [epoch: 5.71 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03792326779191372		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.03792326779191372 | validation: 0.13459442368506858]
	TIME [epoch: 5.71 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03792733495816646		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.03792733495816646 | validation: 0.13357748625432517]
	TIME [epoch: 5.7 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038746821390702665		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.038746821390702665 | validation: 0.13335773876813123]
	TIME [epoch: 5.71 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038294968859828776		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.038294968859828776 | validation: 0.1305373688633457]
	TIME [epoch: 5.7 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039700082331937864		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.039700082331937864 | validation: 0.1449091939792657]
	TIME [epoch: 5.71 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041761539415511834		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.041761539415511834 | validation: 0.13687589466812256]
	TIME [epoch: 5.7 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03885418547933767		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.03885418547933767 | validation: 0.13538067348093882]
	TIME [epoch: 5.7 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03662961901597806		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.03662961901597806 | validation: 0.1325961494216686]
	TIME [epoch: 5.7 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042327185441228064		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.042327185441228064 | validation: 0.1520460145579524]
	TIME [epoch: 5.7 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05692419914366669		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.05692419914366669 | validation: 0.13674843080117013]
	TIME [epoch: 5.69 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0381288087136311		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.0381288087136311 | validation: 0.13374028016434067]
	TIME [epoch: 5.7 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042006218603306486		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.042006218603306486 | validation: 0.12743415400215366]
	TIME [epoch: 5.69 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042186958508766546		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.042186958508766546 | validation: 0.14260639926015048]
	TIME [epoch: 5.7 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04170250448351893		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.04170250448351893 | validation: 0.13616321967885658]
	TIME [epoch: 5.69 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03985376445423792		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.03985376445423792 | validation: 0.1405187316972152]
	TIME [epoch: 5.69 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03882335853713962		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.03882335853713962 | validation: 0.13361371379170325]
	TIME [epoch: 5.7 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0362199956059583		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.0362199956059583 | validation: 0.12457178109851447]
	TIME [epoch: 5.69 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04113870968651853		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.04113870968651853 | validation: 0.1412585232051746]
	TIME [epoch: 5.69 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043628658061900104		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.043628658061900104 | validation: 0.12600521447687024]
	TIME [epoch: 5.7 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04028842862499008		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.04028842862499008 | validation: 0.14081213823394537]
	TIME [epoch: 5.7 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038683411759178325		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.038683411759178325 | validation: 0.1342699929650149]
	TIME [epoch: 5.7 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040533120794651385		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.040533120794651385 | validation: 0.14347695739799088]
	TIME [epoch: 5.7 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045600617870535966		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.045600617870535966 | validation: 0.1272335006738634]
	TIME [epoch: 5.7 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04063397617364158		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.04063397617364158 | validation: 0.1420914640809816]
	TIME [epoch: 5.7 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036851744258057435		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.036851744258057435 | validation: 0.13982863417583585]
	TIME [epoch: 5.69 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03823405917949014		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.03823405917949014 | validation: 0.13171768172940324]
	TIME [epoch: 5.7 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040306669497548686		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.040306669497548686 | validation: 0.13506795740955654]
	TIME [epoch: 5.7 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039863631361523634		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.039863631361523634 | validation: 0.1351351046073784]
	TIME [epoch: 5.69 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03769521666861313		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.03769521666861313 | validation: 0.13684362212696308]
	TIME [epoch: 5.71 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037980535941240345		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.037980535941240345 | validation: 0.13256182806535474]
	TIME [epoch: 5.69 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038926814214310344		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.038926814214310344 | validation: 0.1234086929180211]
	TIME [epoch: 5.7 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04218126160061589		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.04218126160061589 | validation: 0.1349933608930793]
	TIME [epoch: 5.71 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039763602504511435		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.039763602504511435 | validation: 0.1426317057056083]
	TIME [epoch: 5.69 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050775000505219346		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.050775000505219346 | validation: 0.12547945876661168]
	TIME [epoch: 5.7 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0400491496582614		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.0400491496582614 | validation: 0.13203391479384344]
	TIME [epoch: 5.7 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03839796434871741		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.03839796434871741 | validation: 0.14089949535699048]
	TIME [epoch: 5.69 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03912026534004616		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.03912026534004616 | validation: 0.13634937341167688]
	TIME [epoch: 5.7 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038478916285803626		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.038478916285803626 | validation: 0.12899215421115381]
	TIME [epoch: 5.7 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03754662878809772		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.03754662878809772 | validation: 0.14013751095163696]
	TIME [epoch: 5.7 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03829624784750701		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.03829624784750701 | validation: 0.13459747918491088]
	TIME [epoch: 5.7 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03749940354077895		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.03749940354077895 | validation: 0.11859559572437878]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_1051.pth
	Model improved!!!
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05547945940543299		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.05547945940543299 | validation: 0.1369087551044495]
	TIME [epoch: 5.7 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03735518975182512		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.03735518975182512 | validation: 0.14196505278400498]
	TIME [epoch: 5.7 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040362354777090784		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.040362354777090784 | validation: 0.12502472666513897]
	TIME [epoch: 5.69 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039243855196145906		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.039243855196145906 | validation: 0.12725212491801435]
	TIME [epoch: 5.7 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037745031083855796		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.037745031083855796 | validation: 0.14218703029583551]
	TIME [epoch: 5.71 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03943048584546375		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.03943048584546375 | validation: 0.13224811403826414]
	TIME [epoch: 5.71 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043451027053826315		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.043451027053826315 | validation: 0.12137985173541882]
	TIME [epoch: 5.7 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041664792512098855		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.041664792512098855 | validation: 0.1372104643053087]
	TIME [epoch: 5.7 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03761036359075669		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.03761036359075669 | validation: 0.12775313969594237]
	TIME [epoch: 5.7 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03772991726812342		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.03772991726812342 | validation: 0.13016630098553525]
	TIME [epoch: 5.7 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04306494295945813		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.04306494295945813 | validation: 0.14020976999553447]
	TIME [epoch: 5.7 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036989367992213106		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.036989367992213106 | validation: 0.13412111630328483]
	TIME [epoch: 5.7 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035127197106536216		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.035127197106536216 | validation: 0.12718419891855365]
	TIME [epoch: 5.69 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037216142693969244		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.037216142693969244 | validation: 0.13134242750484917]
	TIME [epoch: 5.71 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03498902701525867		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.03498902701525867 | validation: 0.1370654676310958]
	TIME [epoch: 5.7 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037427631546076805		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.037427631546076805 | validation: 0.12656737686751318]
	TIME [epoch: 5.7 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036446062316888586		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.036446062316888586 | validation: 0.13247282548866895]
	TIME [epoch: 5.71 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0360086501566961		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.0360086501566961 | validation: 0.13641261021530712]
	TIME [epoch: 5.7 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03794109333844846		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.03794109333844846 | validation: 0.13306776787722582]
	TIME [epoch: 5.69 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0349207961487151		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.0349207961487151 | validation: 0.1307748613658004]
	TIME [epoch: 5.71 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03803320097712774		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.03803320097712774 | validation: 0.13529618389310197]
	TIME [epoch: 5.71 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04287121478605627		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.04287121478605627 | validation: 0.12045050166923046]
	TIME [epoch: 5.72 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038841912931186655		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.038841912931186655 | validation: 0.11967956535841913]
	TIME [epoch: 5.72 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03709518570788782		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.03709518570788782 | validation: 0.13527596066819686]
	TIME [epoch: 5.71 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03816667543211555		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.03816667543211555 | validation: 0.1234468165093037]
	TIME [epoch: 5.71 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03772132538580714		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.03772132538580714 | validation: 0.1335312247441654]
	TIME [epoch: 5.69 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03602891195310426		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.03602891195310426 | validation: 0.13414387783056453]
	TIME [epoch: 5.71 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038038858206018385		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.038038858206018385 | validation: 0.12948480572478444]
	TIME [epoch: 5.69 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038962041984982314		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.038962041984982314 | validation: 0.13930342369780677]
	TIME [epoch: 5.69 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04409234749451172		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.04409234749451172 | validation: 0.1285067986999936]
	TIME [epoch: 5.69 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03557533577821858		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.03557533577821858 | validation: 0.1293675013250938]
	TIME [epoch: 5.7 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03570727650366351		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.03570727650366351 | validation: 0.13338894916544555]
	TIME [epoch: 5.7 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037056403222627335		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.037056403222627335 | validation: 0.13191039651380895]
	TIME [epoch: 5.7 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03477859623141205		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.03477859623141205 | validation: 0.1271440004677381]
	TIME [epoch: 5.71 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03828862206155772		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.03828862206155772 | validation: 0.13687930668120285]
	TIME [epoch: 5.72 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03897917174906398		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.03897917174906398 | validation: 0.12959052950722258]
	TIME [epoch: 5.72 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04133449776180639		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.04133449776180639 | validation: 0.12251354122169685]
	TIME [epoch: 5.72 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03594246565888315		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.03594246565888315 | validation: 0.1319334402642641]
	TIME [epoch: 5.71 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03659084712548984		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.03659084712548984 | validation: 0.124577048576686]
	TIME [epoch: 5.7 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036536517382463954		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.036536517382463954 | validation: 0.13016558357079006]
	TIME [epoch: 5.71 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036294052386307916		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.036294052386307916 | validation: 0.12566360291908252]
	TIME [epoch: 5.71 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03501486214451998		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.03501486214451998 | validation: 0.13322773782584404]
	TIME [epoch: 5.71 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03614016067230237		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.03614016067230237 | validation: 0.13458038894342272]
	TIME [epoch: 5.72 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03676421335349956		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.03676421335349956 | validation: 0.13037502050506125]
	TIME [epoch: 5.7 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03724063581863558		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.03724063581863558 | validation: 0.13293973744861307]
	TIME [epoch: 5.7 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03764962486685152		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.03764962486685152 | validation: 0.12944222957873794]
	TIME [epoch: 5.7 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03562541453280092		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.03562541453280092 | validation: 0.1247269905200395]
	TIME [epoch: 5.69 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03565298734511926		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.03565298734511926 | validation: 0.12492536509364993]
	TIME [epoch: 5.7 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03705532948270846		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.03705532948270846 | validation: 0.12771171760794317]
	TIME [epoch: 5.69 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03719188413113028		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.03719188413113028 | validation: 0.12854199742786523]
	TIME [epoch: 5.7 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036690153557610156		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.036690153557610156 | validation: 0.12592495667846162]
	TIME [epoch: 5.69 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03597868721005533		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.03597868721005533 | validation: 0.13722973023138996]
	TIME [epoch: 5.69 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03768378898041688		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.03768378898041688 | validation: 0.12702337319493126]
	TIME [epoch: 5.7 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038859057393772614		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.038859057393772614 | validation: 0.13393729221246736]
	TIME [epoch: 5.7 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038007737482830234		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.038007737482830234 | validation: 0.12576840921588855]
	TIME [epoch: 5.69 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035772606317089076		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.035772606317089076 | validation: 0.13101885389340903]
	TIME [epoch: 5.69 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03740278524501265		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.03740278524501265 | validation: 0.13051929569693027]
	TIME [epoch: 5.69 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03515688481385754		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.03515688481385754 | validation: 0.12540219896465624]
	TIME [epoch: 5.7 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038126612759718057		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.038126612759718057 | validation: 0.13163043380308628]
	TIME [epoch: 5.69 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03874213826262357		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.03874213826262357 | validation: 0.12180924837491247]
	TIME [epoch: 5.69 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03813798769473414		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.03813798769473414 | validation: 0.12705331340023138]
	TIME [epoch: 5.69 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0373439263339012		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.0373439263339012 | validation: 0.1303422153272744]
	TIME [epoch: 5.7 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036402225396866626		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.036402225396866626 | validation: 0.11683776886508226]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_1114.pth
	Model improved!!!
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050468412956692785		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.050468412956692785 | validation: 0.13250947894880533]
	TIME [epoch: 5.67 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033937613232205385		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.033937613232205385 | validation: 0.13564522495119263]
	TIME [epoch: 5.66 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03967313856244917		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.03967313856244917 | validation: 0.1295865785240403]
	TIME [epoch: 5.66 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03416750564192837		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.03416750564192837 | validation: 0.12421740215011173]
	TIME [epoch: 5.67 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03553885527995101		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.03553885527995101 | validation: 0.13110911497215152]
	TIME [epoch: 5.66 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03414644040803282		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.03414644040803282 | validation: 0.13454709466584935]
	TIME [epoch: 5.67 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036875728270319985		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.036875728270319985 | validation: 0.13057271984019142]
	TIME [epoch: 5.69 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03626733356067306		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.03626733356067306 | validation: 0.12689492847362424]
	TIME [epoch: 5.69 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03502701024072379		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.03502701024072379 | validation: 0.12904586796202908]
	TIME [epoch: 5.69 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035463012139059585		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.035463012139059585 | validation: 0.12782101655616704]
	TIME [epoch: 5.7 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034426998241482945		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.034426998241482945 | validation: 0.12964920911057762]
	TIME [epoch: 5.69 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03721699529032081		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.03721699529032081 | validation: 0.1269003526716026]
	TIME [epoch: 5.69 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035396398262405634		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.035396398262405634 | validation: 0.12133001962446002]
	TIME [epoch: 5.69 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03700697282294407		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.03700697282294407 | validation: 0.13206739721271568]
	TIME [epoch: 5.69 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033650797067324704		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.033650797067324704 | validation: 0.13355511320834107]
	TIME [epoch: 5.69 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03565719162692052		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.03565719162692052 | validation: 0.12741700040827394]
	TIME [epoch: 5.69 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03944241845375458		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.03944241845375458 | validation: 0.12605365782223274]
	TIME [epoch: 5.69 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03608931763770994		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.03608931763770994 | validation: 0.12646430729820765]
	TIME [epoch: 5.69 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035574958893584696		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.035574958893584696 | validation: 0.13418117742937963]
	TIME [epoch: 5.7 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03438690296220411		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.03438690296220411 | validation: 0.13262440552779783]
	TIME [epoch: 5.69 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0344139973804388		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.0344139973804388 | validation: 0.12823507454847635]
	TIME [epoch: 5.7 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033924418348382		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.033924418348382 | validation: 0.12456494866067461]
	TIME [epoch: 5.69 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03348263244421001		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.03348263244421001 | validation: 0.1320603809872712]
	TIME [epoch: 5.69 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03620809383691964		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.03620809383691964 | validation: 0.12855332729937594]
	TIME [epoch: 5.68 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03945263578582126		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.03945263578582126 | validation: 0.12886240971094684]
	TIME [epoch: 5.7 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03560783180262971		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.03560783180262971 | validation: 0.1333548741793967]
	TIME [epoch: 5.69 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03668605005629618		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.03668605005629618 | validation: 0.12461377308387744]
	TIME [epoch: 5.7 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0339632137108944		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.0339632137108944 | validation: 0.12425824896021509]
	TIME [epoch: 5.69 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0352183961930143		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.0352183961930143 | validation: 0.13166674955369673]
	TIME [epoch: 5.69 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035044582026246295		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.035044582026246295 | validation: 0.12290326316478849]
	TIME [epoch: 5.7 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03469973941668621		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.03469973941668621 | validation: 0.12215706490778735]
	TIME [epoch: 5.69 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03471239359582067		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.03471239359582067 | validation: 0.12599967356796704]
	TIME [epoch: 5.7 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03349475755563682		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.03349475755563682 | validation: 0.12910014012873972]
	TIME [epoch: 5.69 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03413990390179383		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.03413990390179383 | validation: 0.1258982971068423]
	TIME [epoch: 5.69 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032045240444547235		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.032045240444547235 | validation: 0.12898970591054676]
	TIME [epoch: 5.69 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03647895275169288		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.03647895275169288 | validation: 0.12821214973146663]
	TIME [epoch: 5.69 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03413361833996628		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.03413361833996628 | validation: 0.1306838189328429]
	TIME [epoch: 5.7 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03584794787104237		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.03584794787104237 | validation: 0.11895092402360428]
	TIME [epoch: 5.69 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038016552797390364		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.038016552797390364 | validation: 0.13105351454226485]
	TIME [epoch: 5.69 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034631099439540305		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.034631099439540305 | validation: 0.12208133524518144]
	TIME [epoch: 5.69 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03694925697767452		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.03694925697767452 | validation: 0.13112885656729728]
	TIME [epoch: 5.7 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04136047238155356		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.04136047238155356 | validation: 0.1271946122475612]
	TIME [epoch: 5.7 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03598303598966637		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.03598303598966637 | validation: 0.1119048684751472]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_1157.pth
	Model improved!!!
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0392118411164749		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.0392118411164749 | validation: 0.12528063970360256]
	TIME [epoch: 5.67 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033627210769092634		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.033627210769092634 | validation: 0.13358693649114267]
	TIME [epoch: 5.67 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035993896710403775		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.035993896710403775 | validation: 0.12106778901912643]
	TIME [epoch: 5.68 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035498775053649026		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.035498775053649026 | validation: 0.1194320760063603]
	TIME [epoch: 5.66 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038685395564173816		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.038685395564173816 | validation: 0.1288176524384093]
	TIME [epoch: 5.68 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03502454688483871		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.03502454688483871 | validation: 0.13444990785224378]
	TIME [epoch: 5.66 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03672815850130902		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.03672815850130902 | validation: 0.1287281584458782]
	TIME [epoch: 5.66 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03346294088327334		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.03346294088327334 | validation: 0.12107560274903562]
	TIME [epoch: 5.66 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03349658915391166		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.03349658915391166 | validation: 0.12205341594543831]
	TIME [epoch: 5.67 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034021305533575665		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.034021305533575665 | validation: 0.1270315008504119]
	TIME [epoch: 5.66 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03445053246212332		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.03445053246212332 | validation: 0.13140339260331124]
	TIME [epoch: 5.67 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03489831889816726		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.03489831889816726 | validation: 0.11637200355197186]
	TIME [epoch: 5.66 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033620586808777365		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.033620586808777365 | validation: 0.12353733104670087]
	TIME [epoch: 5.67 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034148231186800375		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.034148231186800375 | validation: 0.12190347497197758]
	TIME [epoch: 5.67 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03367226526087896		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.03367226526087896 | validation: 0.12468494902711719]
	TIME [epoch: 5.67 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03435980525700288		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.03435980525700288 | validation: 0.12868986635346957]
	TIME [epoch: 5.68 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034908915780496486		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.034908915780496486 | validation: 0.1301454067536396]
	TIME [epoch: 5.66 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03550868380188602		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.03550868380188602 | validation: 0.11784879856942042]
	TIME [epoch: 5.66 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03407317972319938		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.03407317972319938 | validation: 0.13219269447477516]
	TIME [epoch: 5.66 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0338059248131682		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.0338059248131682 | validation: 0.1242423400599956]
	TIME [epoch: 5.67 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03640689471149822		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.03640689471149822 | validation: 0.12410697683648286]
	TIME [epoch: 5.66 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03593794437012613		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.03593794437012613 | validation: 0.12598563003121513]
	TIME [epoch: 5.66 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033971941910981664		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.033971941910981664 | validation: 0.12757403391938235]
	TIME [epoch: 5.67 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03237312431369283		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.03237312431369283 | validation: 0.12358944084668018]
	TIME [epoch: 5.66 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03477999991983234		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.03477999991983234 | validation: 0.12249778298433874]
	TIME [epoch: 5.67 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03363745734273386		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.03363745734273386 | validation: 0.12733744150708898]
	TIME [epoch: 5.67 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03494908501299082		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.03494908501299082 | validation: 0.12312564846326315]
	TIME [epoch: 5.67 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03419101966341617		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.03419101966341617 | validation: 0.12235319961059128]
	TIME [epoch: 5.66 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03421318558497975		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.03421318558497975 | validation: 0.12207747054437075]
	TIME [epoch: 5.66 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032061613467024117		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.032061613467024117 | validation: 0.1255749308104066]
	TIME [epoch: 5.67 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03246338914090447		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.03246338914090447 | validation: 0.12334418428654846]
	TIME [epoch: 5.67 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03200798947888432		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.03200798947888432 | validation: 0.1276615120383172]
	TIME [epoch: 5.67 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03352863251825434		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.03352863251825434 | validation: 0.12426568111580716]
	TIME [epoch: 5.67 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032978849813141496		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.032978849813141496 | validation: 0.13229025526371838]
	TIME [epoch: 5.67 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03297443259615795		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.03297443259615795 | validation: 0.10955609592533606]
	TIME [epoch: 5.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_1192.pth
	Model improved!!!
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03858284310260883		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.03858284310260883 | validation: 0.1292512282625571]
	TIME [epoch: 5.64 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032752390884157705		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.032752390884157705 | validation: 0.12907630397939415]
	TIME [epoch: 5.63 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03624572218363094		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.03624572218363094 | validation: 0.1316906092784421]
	TIME [epoch: 5.62 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035732098779718405		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.035732098779718405 | validation: 0.11148769719219283]
	TIME [epoch: 5.63 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040199458016974624		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.040199458016974624 | validation: 0.1189881262783687]
	TIME [epoch: 5.63 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03466584138551174		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.03466584138551174 | validation: 0.13320503141176704]
	TIME [epoch: 5.64 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03677080818896771		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.03677080818896771 | validation: 0.127594171272811]
	TIME [epoch: 5.63 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033445122721296995		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.033445122721296995 | validation: 0.1265350820074291]
	TIME [epoch: 5.63 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03214690946649296		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.03214690946649296 | validation: 0.1251417191778988]
	TIME [epoch: 5.63 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032703061267717014		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.032703061267717014 | validation: 0.12904274452881306]
	TIME [epoch: 5.63 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032765009245612424		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.032765009245612424 | validation: 0.12751764843498276]
	TIME [epoch: 5.63 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033997859710888874		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.033997859710888874 | validation: 0.12473742736733025]
	TIME [epoch: 5.64 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03056405468446502		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.03056405468446502 | validation: 0.11878404198738168]
	TIME [epoch: 5.63 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03367624044942776		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.03367624044942776 | validation: 0.1322549207237428]
	TIME [epoch: 5.63 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03418635068456019		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.03418635068456019 | validation: 0.12980555443565608]
	TIME [epoch: 5.63 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03336059634605485		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.03336059634605485 | validation: 0.12533283521215208]
	TIME [epoch: 5.63 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03227755695995997		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.03227755695995997 | validation: 0.1261519878128333]
	TIME [epoch: 5.64 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03612008937064846		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.03612008937064846 | validation: 0.1299765488678009]
	TIME [epoch: 5.64 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031638298875805125		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.031638298875805125 | validation: 0.12154133273262327]
	TIME [epoch: 5.63 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03165239804574663		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.03165239804574663 | validation: 0.12376895375733739]
	TIME [epoch: 5.64 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032186533210240786		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.032186533210240786 | validation: 0.125859030878863]
	TIME [epoch: 5.63 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033606354921226295		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.033606354921226295 | validation: 0.12218726995135239]
	TIME [epoch: 5.65 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033464666809080446		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.033464666809080446 | validation: 0.11685598183041308]
	TIME [epoch: 5.63 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03277845948909442		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.03277845948909442 | validation: 0.12906023493123517]
	TIME [epoch: 5.64 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03292386133110366		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.03292386133110366 | validation: 0.11812231378062488]
	TIME [epoch: 5.63 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033164312634960245		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.033164312634960245 | validation: 0.12416889161189215]
	TIME [epoch: 5.64 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03438964642007822		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.03438964642007822 | validation: 0.12724824581152502]
	TIME [epoch: 5.64 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03340301326635173		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.03340301326635173 | validation: 0.12370374860980662]
	TIME [epoch: 5.65 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032590580862890434		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.032590580862890434 | validation: 0.11683060276285535]
	TIME [epoch: 5.66 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03470068899421524		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.03470068899421524 | validation: 0.12792736540142222]
	TIME [epoch: 5.64 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03422120893311666		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.03422120893311666 | validation: 0.12511971759724502]
	TIME [epoch: 5.64 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03417082122457831		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.03417082122457831 | validation: 0.12122679565514703]
	TIME [epoch: 5.64 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031208281524151244		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.031208281524151244 | validation: 0.12306123890315522]
	TIME [epoch: 5.64 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03065054864758053		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.03065054864758053 | validation: 0.11208393553875913]
	TIME [epoch: 5.64 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03641465370557992		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.03641465370557992 | validation: 0.1246709446124899]
	TIME [epoch: 5.64 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031705848567197166		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.031705848567197166 | validation: 0.12345247768345131]
	TIME [epoch: 5.63 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03294188397174798		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.03294188397174798 | validation: 0.12319498452881952]
	TIME [epoch: 5.67 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03142075574118107		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.03142075574118107 | validation: 0.12203619568768063]
	TIME [epoch: 5.67 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03289582331517415		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.03289582331517415 | validation: 0.12417193314708524]
	TIME [epoch: 5.66 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03465941601408395		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.03465941601408395 | validation: 0.12258712466812478]
	TIME [epoch: 5.66 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032729323398395724		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.032729323398395724 | validation: 0.12497962062683238]
	TIME [epoch: 5.68 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03072528460900406		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.03072528460900406 | validation: 0.12535953727031923]
	TIME [epoch: 5.68 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033286614473111154		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.033286614473111154 | validation: 0.1259487847994975]
	TIME [epoch: 5.68 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036012220740274804		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.036012220740274804 | validation: 0.1225223308689309]
	TIME [epoch: 5.68 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032261354871297054		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.032261354871297054 | validation: 0.12371305659470547]
	TIME [epoch: 5.64 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03141978020715026		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.03141978020715026 | validation: 0.1242579883326862]
	TIME [epoch: 5.63 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03399514842801253		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.03399514842801253 | validation: 0.1235294358363875]
	TIME [epoch: 5.63 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03501577887518242		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.03501577887518242 | validation: 0.12829697681930194]
	TIME [epoch: 5.63 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033437407323812875		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.033437407323812875 | validation: 0.12825229377341782]
	TIME [epoch: 5.65 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035029309842700335		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.035029309842700335 | validation: 0.11864379453664818]
	TIME [epoch: 5.63 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03272983718057964		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.03272983718057964 | validation: 0.11776756227492845]
	TIME [epoch: 5.63 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03173274487606767		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.03173274487606767 | validation: 0.11770662843963481]
	TIME [epoch: 5.63 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03157713479079094		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.03157713479079094 | validation: 0.12361513521886872]
	TIME [epoch: 5.63 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03198301608744963		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.03198301608744963 | validation: 0.12470440797317339]
	TIME [epoch: 5.7 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0315780832175709		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.0315780832175709 | validation: 0.1256311374737192]
	TIME [epoch: 5.7 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029833485666796916		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.029833485666796916 | validation: 0.1214063856048075]
	TIME [epoch: 5.69 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03312021228063488		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.03312021228063488 | validation: 0.11545305766758768]
	TIME [epoch: 5.67 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032400459423024996		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.032400459423024996 | validation: 0.12749297910615]
	TIME [epoch: 5.67 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03199771304366796		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.03199771304366796 | validation: 0.1265032464150091]
	TIME [epoch: 5.68 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03292010186232487		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.03292010186232487 | validation: 0.11341894684292333]
	TIME [epoch: 5.67 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036290800599714866		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.036290800599714866 | validation: 0.12393072379011068]
	TIME [epoch: 5.67 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03074033899283653		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.03074033899283653 | validation: 0.13364332381095956]
	TIME [epoch: 5.67 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032637786009016886		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.032637786009016886 | validation: 0.11494757790625004]
	TIME [epoch: 5.67 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03310640049764719		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.03310640049764719 | validation: 0.11927952128094971]
	TIME [epoch: 5.68 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03319163745837025		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.03319163745837025 | validation: 0.12782361176217036]
	TIME [epoch: 5.67 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031275930648040545		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.031275930648040545 | validation: 0.12444974728761476]
	TIME [epoch: 5.67 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03343803864088108		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.03343803864088108 | validation: 0.12159790258503388]
	TIME [epoch: 5.66 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034263217331463584		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.034263217331463584 | validation: 0.11686335607092141]
	TIME [epoch: 5.63 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03242118469211423		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.03242118469211423 | validation: 0.12693870897073856]
	TIME [epoch: 5.63 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03201827619835474		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.03201827619835474 | validation: 0.1228398014940872]
	TIME [epoch: 5.64 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031349087697330746		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.031349087697330746 | validation: 0.12042102665804202]
	TIME [epoch: 5.64 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03157210262081035		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.03157210262081035 | validation: 0.12118792347843216]
	TIME [epoch: 5.63 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031100365980571264		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.031100365980571264 | validation: 0.12645956469055827]
	TIME [epoch: 5.64 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030572001093471492		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.030572001093471492 | validation: 0.12288780275691824]
	TIME [epoch: 5.64 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03334516667234324		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.03334516667234324 | validation: 0.11442032614728291]
	TIME [epoch: 5.64 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03304061567174198		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.03304061567174198 | validation: 0.12234001729381255]
	TIME [epoch: 5.69 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030746608023676302		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.030746608023676302 | validation: 0.12832291880442237]
	TIME [epoch: 5.69 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032278569238098805		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.032278569238098805 | validation: 0.12217524802939415]
	TIME [epoch: 5.69 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03166404891901414		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.03166404891901414 | validation: 0.11831934545595309]
	TIME [epoch: 5.69 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030988597864507063		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.030988597864507063 | validation: 0.11982093044451725]
	TIME [epoch: 5.7 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03280067752078953		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.03280067752078953 | validation: 0.1232937589982258]
	TIME [epoch: 5.68 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030340010611685113		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.030340010611685113 | validation: 0.12422020971097003]
	TIME [epoch: 5.69 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032004559142804125		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.032004559142804125 | validation: 0.12329756358495786]
	TIME [epoch: 5.7 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032323036617320444		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.032323036617320444 | validation: 0.12390013533036813]
	TIME [epoch: 5.69 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03161546234675368		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.03161546234675368 | validation: 0.1187856388241634]
	TIME [epoch: 5.69 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03115760109182305		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.03115760109182305 | validation: 0.12681802103393008]
	TIME [epoch: 5.69 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0323545424831334		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.0323545424831334 | validation: 0.11796292766695107]
	TIME [epoch: 5.69 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029967998542427626		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.029967998542427626 | validation: 0.11663445132024573]
	TIME [epoch: 5.69 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03136874446940367		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.03136874446940367 | validation: 0.12556347080894703]
	TIME [epoch: 5.69 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03256684145307846		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.03256684145307846 | validation: 0.12480189482188875]
	TIME [epoch: 5.69 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033845485992577924		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.033845485992577924 | validation: 0.12029440065302784]
	TIME [epoch: 5.7 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03113487733862149		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.03113487733862149 | validation: 0.11837448508464161]
	TIME [epoch: 5.69 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03168575124434827		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.03168575124434827 | validation: 0.12731825069709135]
	TIME [epoch: 5.69 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03185859245509444		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.03185859245509444 | validation: 0.12098436204409352]
	TIME [epoch: 5.68 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031153419654673515		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.031153419654673515 | validation: 0.10969950550316661]
	TIME [epoch: 5.68 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03407310995610638		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.03407310995610638 | validation: 0.12488829363069427]
	TIME [epoch: 5.69 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03265628801972231		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.03265628801972231 | validation: 0.12744273890842098]
	TIME [epoch: 5.68 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031109877609673018		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.031109877609673018 | validation: 0.11271034677855984]
	TIME [epoch: 5.69 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04064944210200743		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.04064944210200743 | validation: 0.11520675591389827]
	TIME [epoch: 5.69 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03502136929469432		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.03502136929469432 | validation: 0.1210678291455464]
	TIME [epoch: 5.69 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03309831164993093		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.03309831164993093 | validation: 0.1270666410959509]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_3_v_mmd1_1293.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 4273.424 seconds.
