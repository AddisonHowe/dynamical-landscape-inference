Args:
Namespace(name='model_phi1_4a_distortion_v2_5_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_5/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_5/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.02510099858045578, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2495637888

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.694468300645646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.694468300645646 | validation: 7.119336510576592]
	TIME [epoch: 162 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.425336406603483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.425336406603483 | validation: 7.346253575721987]
	TIME [epoch: 0.817 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.714711158651251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.714711158651251 | validation: 7.391332768212468]
	TIME [epoch: 0.71 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.874365651956082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.874365651956082 | validation: 7.279784199010139]
	TIME [epoch: 0.711 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.675677620595638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.675677620595638 | validation: 6.960688428392604]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.59559477873567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.59559477873567 | validation: 7.162721799131167]
	TIME [epoch: 0.711 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.688650724709659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.688650724709659 | validation: 7.118841764819334]
	TIME [epoch: 0.712 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.635191010986696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.635191010986696 | validation: 7.162623674905529]
	TIME [epoch: 0.711 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.279974465951065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.279974465951065 | validation: 6.701420357621787]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.907083651818807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.907083651818807 | validation: 7.019849451071912]
	TIME [epoch: 0.713 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.185441401500068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.185441401500068 | validation: 6.9919659325384576]
	TIME [epoch: 0.709 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.105014862135877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.105014862135877 | validation: 6.856605957103122]
	TIME [epoch: 0.71 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.89876083071744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.89876083071744 | validation: 6.794289154164787]
	TIME [epoch: 0.712 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.825231727259078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.825231727259078 | validation: 6.755496072084561]
	TIME [epoch: 0.712 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.735674705864445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.735674705864445 | validation: 6.503107973609037]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.6935920491701895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6935920491701895 | validation: 6.623725522020829]
	TIME [epoch: 0.71 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.651331353982421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.651331353982421 | validation: 6.387293677263958]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.49157261362007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.49157261362007 | validation: 6.412104831037194]
	TIME [epoch: 0.714 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.396605443029708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.396605443029708 | validation: 6.203256542697362]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.346162055720291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.346162055720291 | validation: 6.268341909864484]
	TIME [epoch: 0.709 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.335707593833088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.335707593833088 | validation: 6.007544994654366]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.293727948668458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.293727948668458 | validation: 6.06514751297907]
	TIME [epoch: 0.713 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.167404955247749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.167404955247749 | validation: 5.915573831161751]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.027520961737454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.027520961737454 | validation: 5.842573649870395]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.950141376443113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.950141376443113 | validation: 5.752291111755665]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.865651504250042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.865651504250042 | validation: 5.685161134929851]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.800926690475651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.800926690475651 | validation: 5.5734884416774335]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7486688544927107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7486688544927107 | validation: 5.586393390824776]
	TIME [epoch: 0.714 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.792182102716129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.792182102716129 | validation: 5.457747948887381]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.753549589282063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.753549589282063 | validation: 5.457896190171609]
	TIME [epoch: 0.717 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.626417766755162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.626417766755162 | validation: 5.3426058989810485]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.554023743407567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.554023743407567 | validation: 5.328090911524568]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4963853365304467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4963853365304467 | validation: 5.263735826918043]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4550136032313805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4550136032313805 | validation: 5.224538339160089]
	TIME [epoch: 0.719 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4199692251494116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4199692251494116 | validation: 5.1574073570784895]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.386515446866891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.386515446866891 | validation: 5.09487618745476]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3588749998686382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3588749998686382 | validation: 5.062679890978861]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.325117868189985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.325117868189985 | validation: 4.862478798238491]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.253273413678207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.253273413678207 | validation: 4.29551047536016]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9260676044524576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9260676044524576 | validation: 3.7679069887658665]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1588158176558303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1588158176558303 | validation: 4.546977160545227]
	TIME [epoch: 0.714 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.249596307433459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.249596307433459 | validation: 3.9946489545224235]
	TIME [epoch: 0.71 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.67971290268193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.67971290268193 | validation: 3.6706182749362357]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.628383473405411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.628383473405411 | validation: 3.6175218003838783]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.362568063122705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.362568063122705 | validation: 3.3423387325038507]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.28621947822922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.28621947822922 | validation: 3.1552019850251987]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2578732532740555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2578732532740555 | validation: 3.154480550147996]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.189437701515405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.189437701515405 | validation: 2.742639272948451]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5924358790615334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5924358790615334 | validation: 4.136642698538217]
	TIME [epoch: 0.712 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.931909888963673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.931909888963673 | validation: 3.0617392827548136]
	TIME [epoch: 0.711 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.266566815082702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.266566815082702 | validation: 2.8435453770124726]
	TIME [epoch: 0.71 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2887495380570106		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 2.2887495380570106 | validation: 2.762397716028592]
	TIME [epoch: 0.711 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.18036355459388		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 2.18036355459388 | validation: 2.855151396597915]
	TIME [epoch: 0.71 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1602772103060817		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 2.1602772103060817 | validation: 2.698485461221246]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.108854589760466		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 2.108854589760466 | validation: 2.554963723204029]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.083496011789508		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 2.083496011789508 | validation: 2.517733223175162]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0610976516037876		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 2.0610976516037876 | validation: 2.491466660649732]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.031763428758517		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 2.031763428758517 | validation: 2.365757298001285]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.013109488084674		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 2.013109488084674 | validation: 2.4793262226752084]
	TIME [epoch: 0.711 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0166580339284548		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 2.0166580339284548 | validation: 2.223808402655724]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.156295281691373		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 2.156295281691373 | validation: 3.561737183739938]
	TIME [epoch: 0.717 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6708154265945216		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 2.6708154265945216 | validation: 2.322182496282918]
	TIME [epoch: 0.707 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.093445296397909		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 2.093445296397909 | validation: 2.144706232965172]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.135533340847441		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 2.135533340847441 | validation: 2.318731323473107]
	TIME [epoch: 0.713 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0254300399863827		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 2.0254300399863827 | validation: 2.2352950236540963]
	TIME [epoch: 0.71 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9785203920343555		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.9785203920343555 | validation: 2.0844190018150783]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.961141375613267		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.961141375613267 | validation: 2.1608700556856135]
	TIME [epoch: 0.712 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.946499823343037		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.946499823343037 | validation: 1.9958438267363818]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9412121162027602		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.9412121162027602 | validation: 2.3633039235646507]
	TIME [epoch: 0.711 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9711208624118444		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.9711208624118444 | validation: 2.0178464630601334]
	TIME [epoch: 0.709 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1044060289367885		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 2.1044060289367885 | validation: 2.6462458560255415]
	TIME [epoch: 0.707 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.128516178199494		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 2.128516178199494 | validation: 2.086727166891992]
	TIME [epoch: 0.706 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9610256735340414		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.9610256735340414 | validation: 1.9594096914718193]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9536367505287382		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.9536367505287382 | validation: 2.2649468510371213]
	TIME [epoch: 0.711 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9523564402810802		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.9523564402810802 | validation: 1.9524809654942403]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9620285363367957		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.9620285363367957 | validation: 2.2491145148264216]
	TIME [epoch: 0.708 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9399350654731513		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.9399350654731513 | validation: 1.9137299461508677]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.946047811613958		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.946047811613958 | validation: 2.2011115881003684]
	TIME [epoch: 0.708 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9256250419596908		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.9256250419596908 | validation: 1.9584546229484905]
	TIME [epoch: 0.706 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9031211130093284		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.9031211130093284 | validation: 2.084564631589754]
	TIME [epoch: 0.705 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8888228847444988		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.8888228847444988 | validation: 1.962587998213973]
	TIME [epoch: 0.704 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8969150418528784		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.8969150418528784 | validation: 2.307246868605502]
	TIME [epoch: 0.704 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9537737101018728		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.9537737101018728 | validation: 1.9209562964827027]
	TIME [epoch: 0.704 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.090088583171113		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 2.090088583171113 | validation: 2.5130937856003466]
	TIME [epoch: 0.704 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.043131068989133		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 2.043131068989133 | validation: 2.081309593281759]
	TIME [epoch: 0.706 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9096878083699687		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.9096878083699687 | validation: 1.8780164154686887]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8950836788452217		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.8950836788452217 | validation: 2.2027252699400726]
	TIME [epoch: 0.712 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9345709396633326		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.9345709396633326 | validation: 1.9269418816864932]
	TIME [epoch: 0.706 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9965744932749123		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.9965744932749123 | validation: 2.2479828304058076]
	TIME [epoch: 0.705 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9261222112379353		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.9261222112379353 | validation: 1.9910384977249427]
	TIME [epoch: 0.706 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8546323763169488		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.8546323763169488 | validation: 1.902042712255193]
	TIME [epoch: 0.704 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8668198504091964		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.8668198504091964 | validation: 2.2209426781450876]
	TIME [epoch: 0.704 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9251956852919008		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.9251956852919008 | validation: 1.8730871606167896]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0186826400294966		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 2.0186826400294966 | validation: 2.212994773926686]
	TIME [epoch: 0.709 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.914031396895079		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.914031396895079 | validation: 2.0490710998424904]
	TIME [epoch: 0.706 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8321026050147373		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.8321026050147373 | validation: 1.8751521399141582]
	TIME [epoch: 0.711 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8837803089316285		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.8837803089316285 | validation: 2.2397821199418066]
	TIME [epoch: 0.706 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9342914919945786		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.9342914919945786 | validation: 1.8557284325446588]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9815019169361137		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.9815019169361137 | validation: 1.9719662622812864]
	TIME [epoch: 0.708 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8392582261685813		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.8392582261685813 | validation: 2.1164195033050466]
	TIME [epoch: 0.706 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8660175948030469		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.8660175948030469 | validation: 1.8677313013851782]
	TIME [epoch: 0.708 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.929686734069397		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.929686734069397 | validation: 2.203034966294696]
	TIME [epoch: 0.706 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8773234093574211		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.8773234093574211 | validation: 1.8699648059533545]
	TIME [epoch: 0.705 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8239323503493383		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.8239323503493383 | validation: 1.8832993362153694]
	TIME [epoch: 0.707 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.785817799997513		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.785817799997513 | validation: 1.9301350264360189]
	TIME [epoch: 0.705 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7750287882554634		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.7750287882554634 | validation: 1.794316722866929]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.799834020380302		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.799834020380302 | validation: 2.197077127319747]
	TIME [epoch: 0.712 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8704548562302272		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.8704548562302272 | validation: 1.8307512561082344]
	TIME [epoch: 0.707 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0263987302280566		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 2.0263987302280566 | validation: 2.1934072265182047]
	TIME [epoch: 0.705 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8520611934633584		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.8520611934633584 | validation: 1.9346219309385675]
	TIME [epoch: 0.706 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7486907534985727		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.7486907534985727 | validation: 1.7762652672692103]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8145762080952346		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.8145762080952346 | validation: 2.202322074101361]
	TIME [epoch: 0.712 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.873123202660808		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.873123202660808 | validation: 1.908252856233478]
	TIME [epoch: 0.712 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9445028714439472		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.9445028714439472 | validation: 1.7340542664188257]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8339243626435875		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.8339243626435875 | validation: 2.0574828590089447]
	TIME [epoch: 0.711 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8792951773138555		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.8792951773138555 | validation: 1.8182760497403625]
	TIME [epoch: 0.71 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8228828991419805		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.8228828991419805 | validation: 1.7151158162511095]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.742354496763863		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.742354496763863 | validation: 1.8496748571929058]
	TIME [epoch: 0.709 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.732267457725143		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.732267457725143 | validation: 1.7006876643256708]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7491839611822178		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.7491839611822178 | validation: 1.9201787058449342]
	TIME [epoch: 0.709 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7253564878938061		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.7253564878938061 | validation: 1.6602939297023356]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.677298367154781		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.677298367154781 | validation: 1.8050944192389986]
	TIME [epoch: 0.711 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6723479131730534		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.6723479131730534 | validation: 1.6156659177907262]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7077652611294594		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.7077652611294594 | validation: 2.0967771712903316]
	TIME [epoch: 0.709 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7940559480315397		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.7940559480315397 | validation: 1.593363939129629]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8064600664304447		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.8064600664304447 | validation: 1.8346520506524073]
	TIME [epoch: 0.71 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.659022383888202		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.659022383888202 | validation: 1.680766285090539]
	TIME [epoch: 0.708 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.642522670229393		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.642522670229393 | validation: 1.5847007260615555]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.809445070257174		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.809445070257174 | validation: 2.193982377279051]
	TIME [epoch: 0.715 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9594601285150715		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.9594601285150715 | validation: 1.8005971588707077]
	TIME [epoch: 0.709 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7036464486547913		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.7036464486547913 | validation: 1.5544560125743148]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9270431428576718		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.9270431428576718 | validation: 1.9269171997543937]
	TIME [epoch: 0.712 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.762095022615801		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.762095022615801 | validation: 1.9329180120007246]
	TIME [epoch: 0.712 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7371569926731918		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.7371569926731918 | validation: 1.6087579405091188]
	TIME [epoch: 0.712 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.678450674134248		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.678450674134248 | validation: 1.6147583321096626]
	TIME [epoch: 0.711 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5752895040953128		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.5752895040953128 | validation: 1.683849469140791]
	TIME [epoch: 0.711 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.57008528734074		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.57008528734074 | validation: 1.55394094824371]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5780341943091611		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.5780341943091611 | validation: 1.6317854121676225]
	TIME [epoch: 0.716 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5587050700993283		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.5587050700993283 | validation: 1.5596337914515326]
	TIME [epoch: 0.713 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5642594427642484		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.5642594427642484 | validation: 1.5811637149196498]
	TIME [epoch: 0.712 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5782128959829669		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.5782128959829669 | validation: 1.64917083792138]
	TIME [epoch: 0.711 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6156896967524923		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.6156896967524923 | validation: 1.581809667857297]
	TIME [epoch: 0.709 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6153597953988268		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.6153597953988268 | validation: 1.6654590136312004]
	TIME [epoch: 0.709 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5881826170347941		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.5881826170347941 | validation: 1.5066949201763464]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_144.pth
	Model improved!!!
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5158515356845244		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.5158515356845244 | validation: 1.6093213723488722]
	TIME [epoch: 0.712 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5429324648054583		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.5429324648054583 | validation: 1.482010788278263]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6546105034156684		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.6546105034156684 | validation: 1.543767395193326]
	TIME [epoch: 0.71 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4886640381827465		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.4886640381827465 | validation: 1.4374043634738582]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5172042180481		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.5172042180481 | validation: 2.0541521711255917]
	TIME [epoch: 0.708 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7518385482931995		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.7518385482931995 | validation: 1.3908564007384443]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_150.pth
	Model improved!!!
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5429221170897136		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.5429221170897136 | validation: 1.6711490765041697]
	TIME [epoch: 0.712 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6032440255270644		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.6032440255270644 | validation: 1.4903167452835895]
	TIME [epoch: 0.711 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5326715388931382		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.5326715388931382 | validation: 1.449876359149428]
	TIME [epoch: 0.711 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4488875230619473		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.4488875230619473 | validation: 1.4794544541210866]
	TIME [epoch: 0.709 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5103333776281807		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.5103333776281807 | validation: 1.5788278363647343]
	TIME [epoch: 0.709 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.560190546511421		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.560190546511421 | validation: 1.5378999143014624]
	TIME [epoch: 0.709 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6052837824087782		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.6052837824087782 | validation: 1.4340359733520944]
	TIME [epoch: 0.71 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4191212733481695		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.4191212733481695 | validation: 1.406437849385781]
	TIME [epoch: 0.706 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4114572261140752		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.4114572261140752 | validation: 1.4973687658135688]
	TIME [epoch: 0.705 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.453063008675053		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.453063008675053 | validation: 1.4627603937271976]
	TIME [epoch: 0.707 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5141860131892597		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.5141860131892597 | validation: 1.7330093890869158]
	TIME [epoch: 0.705 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5876587652464065		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.5876587652464065 | validation: 1.343410102722203]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5473927877351907		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.5473927877351907 | validation: 1.6184262766614084]
	TIME [epoch: 0.715 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.56160209727176		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.56160209727176 | validation: 1.3650857160085048]
	TIME [epoch: 0.711 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.467708878248621		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.467708878248621 | validation: 1.3680429640883096]
	TIME [epoch: 0.716 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4211405988898225		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.4211405988898225 | validation: 1.5232737081062138]
	TIME [epoch: 0.71 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.428960051867064		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.428960051867064 | validation: 1.3712337921704647]
	TIME [epoch: 0.709 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4031457874828954		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.4031457874828954 | validation: 1.4940334655185055]
	TIME [epoch: 0.708 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3960350161682111		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.3960350161682111 | validation: 1.4286118459305142]
	TIME [epoch: 0.707 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.408188282283659		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.408188282283659 | validation: 1.4868251525658658]
	TIME [epoch: 0.705 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.454308744594015		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.454308744594015 | validation: 1.6660062335870156]
	TIME [epoch: 0.707 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5838066824087835		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.5838066824087835 | validation: 1.3825571782143826]
	TIME [epoch: 0.705 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5060258048506043		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.5060258048506043 | validation: 1.4800994950885196]
	TIME [epoch: 0.706 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4163899711497288		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.4163899711497288 | validation: 1.3142915154551702]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3695033257995408		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.3695033257995408 | validation: 1.4366767325881138]
	TIME [epoch: 0.713 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4071243352056386		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.4071243352056386 | validation: 1.3700150580197255]
	TIME [epoch: 0.71 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4869638791139597		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.4869638791139597 | validation: 1.3504843493838428]
	TIME [epoch: 0.711 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3436644557247603		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.3436644557247603 | validation: 1.350725304535295]
	TIME [epoch: 0.714 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3317700202786653		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.3317700202786653 | validation: 1.2591186117216155]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3415978024879625		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.3415978024879625 | validation: 1.4805169774937086]
	TIME [epoch: 0.715 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3766060754281022		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.3766060754281022 | validation: 1.2807928450269224]
	TIME [epoch: 0.709 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4165495541077702		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.4165495541077702 | validation: 1.692044249652915]
	TIME [epoch: 0.708 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5684038903413937		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.5684038903413937 | validation: 1.46452516140189]
	TIME [epoch: 0.708 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4740784796422506		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.4740784796422506 | validation: 1.354297270601492]
	TIME [epoch: 0.711 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3865664772516655		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.3865664772516655 | validation: 1.3377518937317228]
	TIME [epoch: 0.707 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3104130516891332		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.3104130516891332 | validation: 1.2537943071152928]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3082347947598714		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.3082347947598714 | validation: 1.3681922514163205]
	TIME [epoch: 0.711 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.338503489084795		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.338503489084795 | validation: 1.2177748610567194]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.368635020855096		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.368635020855096 | validation: 1.3633188893596384]
	TIME [epoch: 0.708 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3689611415378966		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.3689611415378966 | validation: 1.3536726032602104]
	TIME [epoch: 0.708 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3885929230197394		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.3885929230197394 | validation: 1.2587644412967152]
	TIME [epoch: 0.707 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3047209616630762		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.3047209616630762 | validation: 1.350724741817319]
	TIME [epoch: 0.705 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3018802673520202		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.3018802673520202 | validation: 1.3000394551913348]
	TIME [epoch: 0.705 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3780159538673267		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.3780159538673267 | validation: 1.61480319174769]
	TIME [epoch: 0.704 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.521825262957124		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.521825262957124 | validation: 1.2975775748513154]
	TIME [epoch: 0.705 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.341544102457961		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.341544102457961 | validation: 1.2814817943588905]
	TIME [epoch: 0.705 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2623877656084022		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.2623877656084022 | validation: 1.2177726223741736]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_197.pth
	Model improved!!!
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2496991832124582		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.2496991832124582 | validation: 1.228099764702695]
	TIME [epoch: 0.709 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2431435203406307		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.2431435203406307 | validation: 1.2226330446980327]
	TIME [epoch: 0.707 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2460235339511334		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.2460235339511334 | validation: 1.244937165658463]
	TIME [epoch: 0.706 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2735245782029407		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.2735245782029407 | validation: 1.3553806007328455]
	TIME [epoch: 168 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3850952703417831		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.3850952703417831 | validation: 1.3613577994193826]
	TIME [epoch: 1.4 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3922759401072602		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.3922759401072602 | validation: 1.2280434027329485]
	TIME [epoch: 1.38 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2247407284500296		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.2247407284500296 | validation: 1.1790590084574075]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_204.pth
	Model improved!!!
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2279950299984845		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.2279950299984845 | validation: 1.3107457963105664]
	TIME [epoch: 1.38 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.274531995295002		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.274531995295002 | validation: 1.3533895765716975]
	TIME [epoch: 1.38 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3222282419181863		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.3222282419181863 | validation: 1.3999155023777352]
	TIME [epoch: 1.38 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4943075323881123		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.4943075323881123 | validation: 1.676896139915538]
	TIME [epoch: 1.38 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5642339176351425		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.5642339176351425 | validation: 1.2281468723769353]
	TIME [epoch: 1.38 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.229714982182036		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.229714982182036 | validation: 1.1219669989272352]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_210.pth
	Model improved!!!
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3287766314819422		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.3287766314819422 | validation: 1.312855779233577]
	TIME [epoch: 1.38 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2666780070737484		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.2666780070737484 | validation: 1.1538374510303597]
	TIME [epoch: 1.39 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2082644530395488		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 1.2082644530395488 | validation: 1.1283887965416948]
	TIME [epoch: 1.38 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2324218148679702		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.2324218148679702 | validation: 1.2014791548912733]
	TIME [epoch: 1.38 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.226941550831729		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 1.226941550831729 | validation: 1.1987614609013382]
	TIME [epoch: 1.38 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2233048836001168		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.2233048836001168 | validation: 1.1714504857050714]
	TIME [epoch: 1.39 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2206667856609763		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.2206667856609763 | validation: 1.2781901981850243]
	TIME [epoch: 1.38 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2289881757841488		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.2289881757841488 | validation: 1.1029220262305928]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.191813837060983		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.191813837060983 | validation: 1.1533777626315704]
	TIME [epoch: 1.38 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2084334309472535		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.2084334309472535 | validation: 1.2522287982691376]
	TIME [epoch: 1.38 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2678225704716644		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 1.2678225704716644 | validation: 1.3933243574348317]
	TIME [epoch: 1.38 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4050133641172824		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.4050133641172824 | validation: 1.192040200355084]
	TIME [epoch: 1.38 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2538409268482682		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 1.2538409268482682 | validation: 1.26641078827686]
	TIME [epoch: 1.38 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2157213933602176		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.2157213933602176 | validation: 1.033779812308428]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_224.pth
	Model improved!!!
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1677781591027059		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 1.1677781591027059 | validation: 1.1058893829119643]
	TIME [epoch: 1.38 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1424810352818913		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 1.1424810352818913 | validation: 1.142867052608876]
	TIME [epoch: 1.38 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1561699437675168		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.1561699437675168 | validation: 1.132030836060869]
	TIME [epoch: 1.38 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1896165060907353		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.1896165060907353 | validation: 1.247280240155513]
	TIME [epoch: 1.39 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.220718277741075		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 1.220718277741075 | validation: 1.10912240520908]
	TIME [epoch: 1.38 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.190173204162542		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.190173204162542 | validation: 1.1448837018068503]
	TIME [epoch: 1.38 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1806618613571036		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 1.1806618613571036 | validation: 1.2067567016059537]
	TIME [epoch: 1.39 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.225275059187934		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.225275059187934 | validation: 1.292610159784148]
	TIME [epoch: 1.38 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3126522358932473		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 1.3126522358932473 | validation: 1.1604482413083956]
	TIME [epoch: 1.38 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1838319015891137		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.1838319015891137 | validation: 1.1084817749746765]
	TIME [epoch: 1.38 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1345523476087536		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 1.1345523476087536 | validation: 1.0429737066336224]
	TIME [epoch: 1.38 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.125938677961752		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.125938677961752 | validation: 1.1217503783916758]
	TIME [epoch: 1.38 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1468583014131244		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 1.1468583014131244 | validation: 1.0535338308320987]
	TIME [epoch: 1.38 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1844381963297395		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 1.1844381963297395 | validation: 1.0908681930277453]
	TIME [epoch: 1.38 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.157041982368611		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 1.157041982368611 | validation: 1.1645697346939876]
	TIME [epoch: 1.38 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1466961708497743		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 1.1466961708497743 | validation: 1.0031491750380788]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1377374143033694		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 1.1377374143033694 | validation: 1.181807434378472]
	TIME [epoch: 1.39 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1402665141786832		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 1.1402665141786832 | validation: 0.9840456118917592]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1144657145612558		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 1.1144657145612558 | validation: 1.1352383189579978]
	TIME [epoch: 1.38 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.122431028862695		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 1.122431028862695 | validation: 1.0549273136358341]
	TIME [epoch: 1.38 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1870449736425042		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 1.1870449736425042 | validation: 1.4658419562491607]
	TIME [epoch: 1.38 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.423426396886979		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.423426396886979 | validation: 1.2033030605926878]
	TIME [epoch: 1.38 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1868418461077166		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 1.1868418461077166 | validation: 1.006655177416576]
	TIME [epoch: 1.38 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0936882608156218		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 1.0936882608156218 | validation: 1.0266064827493486]
	TIME [epoch: 1.38 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.084533724542989		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 1.084533724542989 | validation: 1.004775572430568]
	TIME [epoch: 1.38 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.08483302878097		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 1.08483302878097 | validation: 1.0455547402151788]
	TIME [epoch: 1.38 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1058383627546373		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 1.1058383627546373 | validation: 1.174771328104292]
	TIME [epoch: 1.39 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1621262494910933		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 1.1621262494910933 | validation: 1.0771004547216152]
	TIME [epoch: 1.38 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2081214282906523		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 1.2081214282906523 | validation: 1.0522315170547452]
	TIME [epoch: 1.38 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1081285373526382		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 1.1081285373526382 | validation: 1.132530963756214]
	TIME [epoch: 1.38 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1133187349512303		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 1.1133187349512303 | validation: 0.9597353657392717]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_255.pth
	Model improved!!!
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1466244868791255		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 1.1466244868791255 | validation: 1.1160814727805695]
	TIME [epoch: 1.39 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1246159202746497		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 1.1246159202746497 | validation: 1.075035336165457]
	TIME [epoch: 1.38 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1226969024924356		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 1.1226969024924356 | validation: 1.0109929193167069]
	TIME [epoch: 1.38 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.119926204809235		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 1.119926204809235 | validation: 1.233530926102509]
	TIME [epoch: 1.38 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1365886045137128		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 1.1365886045137128 | validation: 0.9589702633883276]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_260.pth
	Model improved!!!
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.079690028292577		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 1.079690028292577 | validation: 1.0489801651595732]
	TIME [epoch: 1.38 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0877234306272168		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 1.0877234306272168 | validation: 1.0633419609035417]
	TIME [epoch: 1.38 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1225965564009592		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 1.1225965564009592 | validation: 1.1479388889695208]
	TIME [epoch: 1.38 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1913440445449681		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 1.1913440445449681 | validation: 1.07611481025149]
	TIME [epoch: 1.38 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.123488050447016		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 1.123488050447016 | validation: 1.037460340753125]
	TIME [epoch: 1.38 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.091617665324974		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 1.091617665324974 | validation: 1.0425372392411478]
	TIME [epoch: 1.38 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.064185559578361		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 1.064185559578361 | validation: 0.9789942945387815]
	TIME [epoch: 1.38 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0610710905332812		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 1.0610710905332812 | validation: 1.1270948418904034]
	TIME [epoch: 1.38 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0846674103955232		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 1.0846674103955232 | validation: 0.9363993758272126]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_269.pth
	Model improved!!!
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.11605892485743		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 1.11605892485743 | validation: 1.1887618925106362]
	TIME [epoch: 1.38 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.129304548073186		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 1.129304548073186 | validation: 1.0672006239258942]
	TIME [epoch: 1.38 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0970786709967635		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 1.0970786709967635 | validation: 0.8939905876998413]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_272.pth
	Model improved!!!
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1182345073553637		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 1.1182345073553637 | validation: 1.065916463418648]
	TIME [epoch: 1.39 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.069562822901802		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 1.069562822901802 | validation: 0.9878098425897719]
	TIME [epoch: 1.39 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0546985720655826		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 1.0546985720655826 | validation: 0.9597536664811867]
	TIME [epoch: 1.38 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0553863798399805		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 1.0553863798399805 | validation: 1.0722187580376292]
	TIME [epoch: 1.38 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0758732247890221		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 1.0758732247890221 | validation: 0.9615249990726114]
	TIME [epoch: 1.38 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0923508071873114		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 1.0923508071873114 | validation: 1.1503600740125854]
	TIME [epoch: 1.38 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1165782214343338		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 1.1165782214343338 | validation: 0.9975594650594254]
	TIME [epoch: 1.38 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1145025150392098		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 1.1145025150392098 | validation: 1.2865844754298257]
	TIME [epoch: 1.38 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.262057065861728		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 1.262057065861728 | validation: 1.1164471939120222]
	TIME [epoch: 1.38 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1444329572363352		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 1.1444329572363352 | validation: 0.9801888134419277]
	TIME [epoch: 1.38 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0494377534211317		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 1.0494377534211317 | validation: 0.9406835516705002]
	TIME [epoch: 1.38 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.037937094021174		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 1.037937094021174 | validation: 0.9646901390142576]
	TIME [epoch: 1.38 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0263549557450313		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 1.0263549557450313 | validation: 0.9747993293173007]
	TIME [epoch: 1.38 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0314722059234984		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 1.0314722059234984 | validation: 0.9966348027163623]
	TIME [epoch: 1.38 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0589477736191102		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 1.0589477736191102 | validation: 1.0127540357138771]
	TIME [epoch: 1.38 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.120471096489168		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 1.120471096489168 | validation: 1.1235101217629964]
	TIME [epoch: 1.38 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1288981773747364		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 1.1288981773747364 | validation: 0.9709930864520513]
	TIME [epoch: 1.38 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0933061061401892		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 1.0933061061401892 | validation: 1.081342081768463]
	TIME [epoch: 1.38 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0708289522763002		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 1.0708289522763002 | validation: 1.0303240040386108]
	TIME [epoch: 1.38 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0989298823278817		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 1.0989298823278817 | validation: 1.1484367136436375]
	TIME [epoch: 1.38 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.119967397232804		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 1.119967397232804 | validation: 0.981103503976377]
	TIME [epoch: 1.39 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0542330175348145		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 1.0542330175348145 | validation: 0.8964871480018513]
	TIME [epoch: 1.38 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0444252516235548		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 1.0444252516235548 | validation: 1.028989888731215]
	TIME [epoch: 1.38 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.025743694400075		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 1.025743694400075 | validation: 0.8692237602618289]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0407298039778439		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 1.0407298039778439 | validation: 1.0811342358942786]
	TIME [epoch: 1.39 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0463171334241776		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 1.0463171334241776 | validation: 0.9299262615462944]
	TIME [epoch: 1.4 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.078759863864287		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 1.078759863864287 | validation: 1.2132811298663047]
	TIME [epoch: 1.39 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.146864169608514		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 1.146864169608514 | validation: 1.019646803275865]
	TIME [epoch: 1.38 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0956819671579128		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 1.0956819671579128 | validation: 0.9479884420638971]
	TIME [epoch: 1.4 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0574249798941553		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 1.0574249798941553 | validation: 1.0808703844600305]
	TIME [epoch: 1.39 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0445193346269037		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 1.0445193346269037 | validation: 0.8895214065419609]
	TIME [epoch: 1.39 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0475294914653486		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 1.0475294914653486 | validation: 1.0551427835756344]
	TIME [epoch: 1.39 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.054874647679533		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 1.054874647679533 | validation: 0.9894637382111064]
	TIME [epoch: 1.39 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0954892101186864		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 1.0954892101186864 | validation: 1.082992698663924]
	TIME [epoch: 1.39 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.097935586189561		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 1.097935586189561 | validation: 0.995257292599273]
	TIME [epoch: 1.39 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0403548447900648		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 1.0403548447900648 | validation: 0.9227835437387741]
	TIME [epoch: 1.39 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0246836220360465		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 1.0246836220360465 | validation: 0.9623597361031836]
	TIME [epoch: 1.39 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.02610392928039		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 1.02610392928039 | validation: 0.973391257536939]
	TIME [epoch: 1.39 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0222947778104494		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 1.0222947778104494 | validation: 0.9547207805350336]
	TIME [epoch: 1.39 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.050011719256159		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 1.050011719256159 | validation: 1.1068645761252174]
	TIME [epoch: 1.39 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1078242211714149		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 1.1078242211714149 | validation: 0.9848043322955068]
	TIME [epoch: 1.39 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0774093896197696		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 1.0774093896197696 | validation: 1.044605268303319]
	TIME [epoch: 1.39 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.10523759182808		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 1.10523759182808 | validation: 1.1591684825421493]
	TIME [epoch: 1.39 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1141236567128996		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 1.1141236567128996 | validation: 0.9312403094340187]
	TIME [epoch: 1.39 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.054408466194708		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 1.054408466194708 | validation: 1.0347409618324124]
	TIME [epoch: 1.39 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0320745886929374		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 1.0320745886929374 | validation: 0.8991012188457432]
	TIME [epoch: 1.39 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0156106648197527		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 1.0156106648197527 | validation: 0.9833169105527535]
	TIME [epoch: 1.4 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0206845701966893		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 1.0206845701966893 | validation: 0.9020973398274699]
	TIME [epoch: 1.39 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0076605933856533		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 1.0076605933856533 | validation: 0.9807924437809823]
	TIME [epoch: 1.39 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0118659287508869		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 1.0118659287508869 | validation: 0.8610006011384057]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_322.pth
	Model improved!!!
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0282805815606477		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 1.0282805815606477 | validation: 1.1409969363690555]
	TIME [epoch: 1.38 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0796764951034208		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 1.0796764951034208 | validation: 1.0071425196624628]
	TIME [epoch: 1.38 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1158981229624174		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 1.1158981229624174 | validation: 1.0134601569197235]
	TIME [epoch: 1.38 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0881919151065396		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 1.0881919151065396 | validation: 1.1052137568201246]
	TIME [epoch: 1.38 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0544670957770652		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 1.0544670957770652 | validation: 0.8479976906027802]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_327.pth
	Model improved!!!
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0689234931158007		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 1.0689234931158007 | validation: 1.1054149243273277]
	TIME [epoch: 1.39 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.044772455457794		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 1.044772455457794 | validation: 0.898076190490701]
	TIME [epoch: 1.39 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0199100223932143		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 1.0199100223932143 | validation: 0.9318274197917875]
	TIME [epoch: 1.39 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0149493464617703		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 1.0149493464617703 | validation: 0.9755483584167379]
	TIME [epoch: 1.39 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.019182816413285		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 1.019182816413285 | validation: 0.9514745031839967]
	TIME [epoch: 1.39 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0365282993523066		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 1.0365282993523066 | validation: 1.0229758220242453]
	TIME [epoch: 1.39 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0667731022428828		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 1.0667731022428828 | validation: 1.0278609767468467]
	TIME [epoch: 1.39 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0834078416384982		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 1.0834078416384982 | validation: 0.9971046572457871]
	TIME [epoch: 1.38 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.035584592880999		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 1.035584592880999 | validation: 0.9563178482801459]
	TIME [epoch: 1.39 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.027822201204874		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 1.027822201204874 | validation: 0.9769335276849077]
	TIME [epoch: 1.38 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0359472602402129		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 1.0359472602402129 | validation: 1.0114918406265776]
	TIME [epoch: 1.38 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.05156058000414		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 1.05156058000414 | validation: 1.0032895751455075]
	TIME [epoch: 1.38 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0392073207151447		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 1.0392073207151447 | validation: 0.9303232385123739]
	TIME [epoch: 1.39 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0332321703630778		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 1.0332321703630778 | validation: 1.029175300827557]
	TIME [epoch: 1.39 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0256472951824858		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 1.0256472951824858 | validation: 0.8435907899106447]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_342.pth
	Model improved!!!
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.040367980219295		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 1.040367980219295 | validation: 1.107229839367062]
	TIME [epoch: 1.39 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0489594168056526		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 1.0489594168056526 | validation: 0.9092945325459841]
	TIME [epoch: 1.39 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0201800347389476		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 1.0201800347389476 | validation: 0.9732423842848755]
	TIME [epoch: 1.38 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0211291153231234		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 1.0211291153231234 | validation: 1.027059551736263]
	TIME [epoch: 1.38 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0359560308194098		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 1.0359560308194098 | validation: 0.9293612970174965]
	TIME [epoch: 1.38 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0499122212955456		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 1.0499122212955456 | validation: 1.0409228331876554]
	TIME [epoch: 1.38 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0309351052107738		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 1.0309351052107738 | validation: 0.8880617549771644]
	TIME [epoch: 1.38 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0064357557725472		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 1.0064357557725472 | validation: 0.9562887617914071]
	TIME [epoch: 1.38 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9943789587616161		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.9943789587616161 | validation: 0.9403130724563504]
	TIME [epoch: 1.38 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.012903121706935		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 1.012903121706935 | validation: 1.081985860055301]
	TIME [epoch: 1.38 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0862341234007524		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 1.0862341234007524 | validation: 1.0679264080433502]
	TIME [epoch: 1.38 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.092172877227186		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 1.092172877227186 | validation: 0.9254055311884205]
	TIME [epoch: 1.38 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0333122018049297		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 1.0333122018049297 | validation: 0.9544251271828351]
	TIME [epoch: 1.39 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9940996240788534		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.9940996240788534 | validation: 0.8871224706847678]
	TIME [epoch: 1.38 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0061110810231113		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 1.0061110810231113 | validation: 1.078782909702437]
	TIME [epoch: 1.38 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0357960442257836		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 1.0357960442257836 | validation: 0.8984686823209277]
	TIME [epoch: 1.38 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0451985572739526		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 1.0451985572739526 | validation: 1.009689210246949]
	TIME [epoch: 1.38 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.019153801465534		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 1.019153801465534 | validation: 0.9260201157232928]
	TIME [epoch: 1.38 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9974577572096138		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.9974577572096138 | validation: 0.9177927754683592]
	TIME [epoch: 1.39 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9969062867831586		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.9969062867831586 | validation: 0.946399671787771]
	TIME [epoch: 1.38 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9922573757372114		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.9922573757372114 | validation: 0.8832392394805404]
	TIME [epoch: 1.38 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0118898793244315		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 1.0118898793244315 | validation: 1.088989951685248]
	TIME [epoch: 1.38 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0533985311057885		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 1.0533985311057885 | validation: 0.9258834813782922]
	TIME [epoch: 1.39 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0423309335710655		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 1.0423309335710655 | validation: 0.9696897684119051]
	TIME [epoch: 1.38 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0152111194482003		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 1.0152111194482003 | validation: 0.9428954279658985]
	TIME [epoch: 1.38 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9986115302645083		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.9986115302645083 | validation: 0.8989091076648372]
	TIME [epoch: 1.38 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0356195968069972		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 1.0356195968069972 | validation: 1.0594496906144364]
	TIME [epoch: 1.38 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0530765804256095		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 1.0530765804256095 | validation: 0.9047554754033179]
	TIME [epoch: 1.38 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0252626445605995		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 1.0252626445605995 | validation: 0.8934273471168743]
	TIME [epoch: 1.38 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9927050551056792		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.9927050551056792 | validation: 0.9618850273119925]
	TIME [epoch: 1.38 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9966547893736771		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.9966547893736771 | validation: 0.8927426932320884]
	TIME [epoch: 1.38 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0006587893031602		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 1.0006587893031602 | validation: 0.9309808027535408]
	TIME [epoch: 1.38 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.009392614884218		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 1.009392614884218 | validation: 1.1044301784621684]
	TIME [epoch: 1.38 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.047710302105549		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 1.047710302105549 | validation: 0.8889877374028903]
	TIME [epoch: 1.39 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0638204110700875		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 1.0638204110700875 | validation: 1.085713684091199]
	TIME [epoch: 1.38 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0220487778486855		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 1.0220487778486855 | validation: 0.8561858126216783]
	TIME [epoch: 1.38 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9866642894527846		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.9866642894527846 | validation: 0.9215373931898875]
	TIME [epoch: 1.38 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9796440539735218		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.9796440539735218 | validation: 0.9528953789440525]
	TIME [epoch: 1.38 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9839590127534217		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.9839590127534217 | validation: 0.8881734577500318]
	TIME [epoch: 1.38 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9969914207066944		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.9969914207066944 | validation: 1.0056822542813193]
	TIME [epoch: 1.39 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0153858655434689		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 1.0153858655434689 | validation: 0.9654947345654108]
	TIME [epoch: 1.39 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0446529070968986		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 1.0446529070968986 | validation: 0.9860955700049152]
	TIME [epoch: 1.38 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0466921342264137		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 1.0466921342264137 | validation: 1.0206780740209696]
	TIME [epoch: 1.39 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.041115888947345		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 1.041115888947345 | validation: 0.8968015636460559]
	TIME [epoch: 1.39 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9990934679591736		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.9990934679591736 | validation: 0.8959640815512483]
	TIME [epoch: 1.39 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9893842168459815		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.9893842168459815 | validation: 0.9534720137444364]
	TIME [epoch: 1.39 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0038779404641702		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 1.0038779404641702 | validation: 0.9253718499860182]
	TIME [epoch: 1.38 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9979341951989752		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.9979341951989752 | validation: 0.930961272028188]
	TIME [epoch: 1.38 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.000006497988977		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 1.000006497988977 | validation: 0.9007180161854991]
	TIME [epoch: 1.38 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9992791584431534		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.9992791584431534 | validation: 0.9170303700635348]
	TIME [epoch: 1.38 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9847221072361575		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.9847221072361575 | validation: 0.9508235626134255]
	TIME [epoch: 1.38 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9949563751745032		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.9949563751745032 | validation: 0.8668462434160672]
	TIME [epoch: 1.38 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0036595101969892		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 1.0036595101969892 | validation: 1.0954930266617138]
	TIME [epoch: 1.38 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0428781481943386		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 1.0428781481943386 | validation: 0.8581450774537409]
	TIME [epoch: 1.38 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.054401416321516		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 1.054401416321516 | validation: 1.028868964664656]
	TIME [epoch: 1.38 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.007821271649123		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 1.007821271649123 | validation: 0.9405201621411818]
	TIME [epoch: 1.39 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9914215672825418		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.9914215672825418 | validation: 0.8639430437534314]
	TIME [epoch: 1.38 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0002504488798838		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 1.0002504488798838 | validation: 1.0698181721286741]
	TIME [epoch: 1.38 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.017678191406453		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 1.017678191406453 | validation: 0.8602711000646185]
	TIME [epoch: 1.39 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9982159155536201		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.9982159155536201 | validation: 0.918961538349172]
	TIME [epoch: 1.39 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9824050942995678		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.9824050942995678 | validation: 0.9468553961004901]
	TIME [epoch: 1.39 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9781343650601501		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.9781343650601501 | validation: 0.9037861330353714]
	TIME [epoch: 1.39 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9862828333009185		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.9862828333009185 | validation: 0.990845786238463]
	TIME [epoch: 1.38 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.008865928704449		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 1.008865928704449 | validation: 0.9221352032488183]
	TIME [epoch: 1.38 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0076070971056132		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 1.0076070971056132 | validation: 0.966011956726116]
	TIME [epoch: 1.38 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9936625086831581		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.9936625086831581 | validation: 0.8813296150926977]
	TIME [epoch: 1.38 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9822264205195679		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.9822264205195679 | validation: 0.932487299428005]
	TIME [epoch: 1.38 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9779174666418823		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.9779174666418823 | validation: 0.881083214489154]
	TIME [epoch: 1.38 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9756525801661589		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.9756525801661589 | validation: 0.9401211745535868]
	TIME [epoch: 1.38 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9857516099227819		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.9857516099227819 | validation: 0.9521197299118036]
	TIME [epoch: 1.38 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.996298848194664		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.996298848194664 | validation: 0.9148794116206272]
	TIME [epoch: 1.38 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0096097636291648		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 1.0096097636291648 | validation: 0.9525058286518036]
	TIME [epoch: 1.39 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.984704173112291		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.984704173112291 | validation: 0.8848797076136755]
	TIME [epoch: 1.38 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9810390102688711		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.9810390102688711 | validation: 0.8907843233365494]
	TIME [epoch: 1.38 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9866632461059311		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.9866632461059311 | validation: 1.0867093181544776]
	TIME [epoch: 1.39 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0276073852547574		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 1.0276073852547574 | validation: 0.8744171969347294]
	TIME [epoch: 1.38 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0037423630166475		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 1.0037423630166475 | validation: 0.9411726136455819]
	TIME [epoch: 1.39 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9822294806728371		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.9822294806728371 | validation: 0.9060291273456191]
	TIME [epoch: 1.38 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9650314573643308		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.9650314573643308 | validation: 0.8505124884296997]
	TIME [epoch: 1.38 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9845842317402655		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.9845842317402655 | validation: 1.104152087537471]
	TIME [epoch: 1.38 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0314664182082702		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 1.0314664182082702 | validation: 0.8410898305322523]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_423.pth
	Model improved!!!
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0253308350955028		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 1.0253308350955028 | validation: 0.9209766449886789]
	TIME [epoch: 1.39 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9696590678781135		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.9696590678781135 | validation: 0.9309433472912011]
	TIME [epoch: 1.39 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.964700827421533		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.964700827421533 | validation: 0.8240937722673852]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_426.pth
	Model improved!!!
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9718434597980314		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.9718434597980314 | validation: 0.9275649042482546]
	TIME [epoch: 1.39 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9593128348545557		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.9593128348545557 | validation: 0.856689663489603]
	TIME [epoch: 1.39 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9550198091824124		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.9550198091824124 | validation: 0.8943886275432521]
	TIME [epoch: 1.39 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9519970550471717		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.9519970550471717 | validation: 0.9199993640782559]
	TIME [epoch: 1.39 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9878167206060894		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.9878167206060894 | validation: 0.9520837777943634]
	TIME [epoch: 1.39 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0141825507898943		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 1.0141825507898943 | validation: 0.9749232770829791]
	TIME [epoch: 1.38 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0094875039318905		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 1.0094875039318905 | validation: 0.8919595742637917]
	TIME [epoch: 1.38 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9895752332300838		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.9895752332300838 | validation: 0.9024605990674953]
	TIME [epoch: 1.38 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9724519192815957		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.9724519192815957 | validation: 0.9246181992460237]
	TIME [epoch: 1.38 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9785283128788916		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.9785283128788916 | validation: 0.8793883664711171]
	TIME [epoch: 1.38 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9806053544419078		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.9806053544419078 | validation: 1.0182856810691785]
	TIME [epoch: 1.38 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9891217265256614		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.9891217265256614 | validation: 0.8087573284392348]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_438.pth
	Model improved!!!
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9855111817065166		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.9855111817065166 | validation: 0.9886279220464614]
	TIME [epoch: 1.39 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9740058512172113		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.9740058512172113 | validation: 0.8142191285928414]
	TIME [epoch: 1.39 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.959701882507229		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.959701882507229 | validation: 0.9072792048788888]
	TIME [epoch: 1.38 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9758469389083707		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.9758469389083707 | validation: 0.9460773278411088]
	TIME [epoch: 1.38 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9795151963669888		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.9795151963669888 | validation: 0.9385676523575985]
	TIME [epoch: 1.38 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9868135076078737		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.9868135076078737 | validation: 0.9387186450496232]
	TIME [epoch: 1.39 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9779904509464828		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.9779904509464828 | validation: 0.9079977353930451]
	TIME [epoch: 1.39 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9686120414523544		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.9686120414523544 | validation: 0.8565209494129769]
	TIME [epoch: 1.38 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9537013211160175		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.9537013211160175 | validation: 0.9308214580078396]
	TIME [epoch: 1.38 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.956481813064599		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.956481813064599 | validation: 0.8361183518471507]
	TIME [epoch: 1.38 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9660901270789707		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.9660901270789707 | validation: 1.0564523252992704]
	TIME [epoch: 1.38 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9860499367174572		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.9860499367174572 | validation: 0.8226519820437315]
	TIME [epoch: 1.38 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.976675629861802		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.976675629861802 | validation: 0.9620335960598679]
	TIME [epoch: 1.38 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9609771948455111		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.9609771948455111 | validation: 0.8529913438542263]
	TIME [epoch: 1.38 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9592264233933483		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.9592264233933483 | validation: 0.8954298995319321]
	TIME [epoch: 1.38 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9730757160953268		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.9730757160953268 | validation: 1.0058884919293]
	TIME [epoch: 1.38 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9924371314732643		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.9924371314732643 | validation: 0.9033093708006616]
	TIME [epoch: 1.38 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9676958133045888		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.9676958133045888 | validation: 0.8892433197263383]
	TIME [epoch: 1.38 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9500097884476212		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.9500097884476212 | validation: 0.911838521052223]
	TIME [epoch: 1.38 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9446383670930084		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.9446383670930084 | validation: 0.8064709997070869]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_458.pth
	Model improved!!!
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9480535107137346		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.9480535107137346 | validation: 0.9344752241704128]
	TIME [epoch: 1.39 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9565624385364896		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.9565624385364896 | validation: 0.812925474272846]
	TIME [epoch: 1.39 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9601320351516267		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.9601320351516267 | validation: 0.9798018769037333]
	TIME [epoch: 1.38 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9733377002637742		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.9733377002637742 | validation: 0.8923400579814071]
	TIME [epoch: 1.38 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9746760421428513		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.9746760421428513 | validation: 0.9544386532253104]
	TIME [epoch: 1.38 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9696297921674595		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.9696297921674595 | validation: 0.9232453767675509]
	TIME [epoch: 1.38 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9638240469661048		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.9638240469661048 | validation: 0.8548385173691596]
	TIME [epoch: 1.38 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9425427754120961		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.9425427754120961 | validation: 0.9168365860437974]
	TIME [epoch: 1.39 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9549286050189555		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.9549286050189555 | validation: 0.8220721189563028]
	TIME [epoch: 1.39 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.949718569078167		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.949718569078167 | validation: 1.032050546702838]
	TIME [epoch: 1.39 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9706955025003111		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.9706955025003111 | validation: 0.851664587201274]
	TIME [epoch: 1.39 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9642515962400683		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.9642515962400683 | validation: 0.8552795941425058]
	TIME [epoch: 1.38 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9553528265024244		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.9553528265024244 | validation: 0.9559212805957128]
	TIME [epoch: 1.38 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.960173171559283		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.960173171559283 | validation: 0.816042388472872]
	TIME [epoch: 1.39 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9562998727314854		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.9562998727314854 | validation: 0.9520526120489002]
	TIME [epoch: 1.39 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9529032564430507		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.9529032564430507 | validation: 0.9385159833034447]
	TIME [epoch: 1.39 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9748209313755442		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.9748209313755442 | validation: 0.896386098864784]
	TIME [epoch: 1.39 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9501529342415882		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.9501529342415882 | validation: 0.8491746918414219]
	TIME [epoch: 1.39 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9323551571674727		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.9323551571674727 | validation: 0.9117894411312304]
	TIME [epoch: 1.39 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9258645547210337		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.9258645547210337 | validation: 0.852752395513408]
	TIME [epoch: 1.39 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9334067216908445		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.9334067216908445 | validation: 0.8718837270189582]
	TIME [epoch: 1.39 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9428515112517581		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.9428515112517581 | validation: 0.8641058053407967]
	TIME [epoch: 1.38 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.950351271242911		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.950351271242911 | validation: 0.9210175340764621]
	TIME [epoch: 1.39 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9641909388860532		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.9641909388860532 | validation: 0.9316089243079486]
	TIME [epoch: 1.39 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9717637702768543		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.9717637702768543 | validation: 0.9089241867082509]
	TIME [epoch: 1.39 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9576762297287686		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.9576762297287686 | validation: 0.8952398924689423]
	TIME [epoch: 1.39 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9448221185018914		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.9448221185018914 | validation: 0.9087230231162521]
	TIME [epoch: 1.39 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.931531403647865		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.931531403647865 | validation: 0.7954561225252814]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_486.pth
	Model improved!!!
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9299783855957903		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.9299783855957903 | validation: 1.0527879612195972]
	TIME [epoch: 1.39 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9687182395279496		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.9687182395279496 | validation: 0.8052493621943763]
	TIME [epoch: 1.39 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9672653638309625		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.9672653638309625 | validation: 0.9216232381684469]
	TIME [epoch: 1.39 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9312830690665831		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.9312830690665831 | validation: 0.839214644349866]
	TIME [epoch: 1.39 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.918870330506211		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.918870330506211 | validation: 0.8525336774871395]
	TIME [epoch: 1.39 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9293950719692012		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.9293950719692012 | validation: 0.8903129181491848]
	TIME [epoch: 1.39 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9281018916064777		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.9281018916064777 | validation: 0.8650321407944201]
	TIME [epoch: 1.39 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9343477658666206		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.9343477658666206 | validation: 0.9105208342273583]
	TIME [epoch: 1.39 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9602205477151609		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.9602205477151609 | validation: 0.9439944011044885]
	TIME [epoch: 1.39 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9605789320442857		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.9605789320442857 | validation: 0.8413015812102057]
	TIME [epoch: 1.39 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9438818571950728		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.9438818571950728 | validation: 0.9242498505709436]
	TIME [epoch: 1.39 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9333945786096299		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.9333945786096299 | validation: 0.8216657388132758]
	TIME [epoch: 1.39 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9379192417926197		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.9379192417926197 | validation: 0.92695888911929]
	TIME [epoch: 1.39 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9259865828448978		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.9259865828448978 | validation: 0.7965250270276913]
	TIME [epoch: 1.39 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9249760786521202		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.9249760786521202 | validation: 0.9102917406540902]
	TIME [epoch: 172 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9183049811748204		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.9183049811748204 | validation: 0.8364351691910253]
	TIME [epoch: 2.75 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9063746134560674		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.9063746134560674 | validation: 0.8259134383433987]
	TIME [epoch: 2.74 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9029013417375706		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.9029013417375706 | validation: 0.8769343482165056]
	TIME [epoch: 2.74 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9127523883546925		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.9127523883546925 | validation: 0.8089634652212349]
	TIME [epoch: 2.75 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9157283686111559		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.9157283686111559 | validation: 0.9312103758336712]
	TIME [epoch: 2.74 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9481012213462034		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.9481012213462034 | validation: 0.8754776976852967]
	TIME [epoch: 2.74 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9864386130266232		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.9864386130266232 | validation: 0.9442059263932013]
	TIME [epoch: 2.74 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.95025451709372		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.95025451709372 | validation: 0.830038377556297]
	TIME [epoch: 2.74 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9057328071793063		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.9057328071793063 | validation: 0.8036613695637027]
	TIME [epoch: 2.74 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9006763823176858		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.9006763823176858 | validation: 0.9536924191640227]
	TIME [epoch: 2.74 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.923541793138337		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.923541793138337 | validation: 0.7813395721449109]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_512.pth
	Model improved!!!
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9512007683942965		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.9512007683942965 | validation: 0.9427439565876186]
	TIME [epoch: 2.75 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9268771885595766		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.9268771885595766 | validation: 0.8514382662991576]
	TIME [epoch: 2.74 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9108057381396748		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.9108057381396748 | validation: 0.8055184375052111]
	TIME [epoch: 2.74 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9197558293580552		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.9197558293580552 | validation: 0.9332716725003088]
	TIME [epoch: 2.75 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9221366621960395		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.9221366621960395 | validation: 0.8320327258672301]
	TIME [epoch: 2.75 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9236370014639442		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.9236370014639442 | validation: 0.8464700101648049]
	TIME [epoch: 2.74 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9051949595528049		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.9051949595528049 | validation: 0.8816278803618165]
	TIME [epoch: 2.74 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9033074429892005		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.9033074429892005 | validation: 0.8107341075333623]
	TIME [epoch: 2.74 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9131560120022094		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.9131560120022094 | validation: 0.8984319496144167]
	TIME [epoch: 2.74 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9088389127925289		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.9088389127925289 | validation: 0.8249427543310772]
	TIME [epoch: 2.74 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9096780563744064		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.9096780563744064 | validation: 0.8555377081705976]
	TIME [epoch: 2.74 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.917558616078895		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.917558616078895 | validation: 0.8875774194507646]
	TIME [epoch: 2.74 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9083687295554549		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.9083687295554549 | validation: 0.7800692420154713]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_525.pth
	Model improved!!!
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9112511647187642		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.9112511647187642 | validation: 0.9087278539989385]
	TIME [epoch: 2.74 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9045906251249359		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.9045906251249359 | validation: 0.8123954813577623]
	TIME [epoch: 2.74 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.922064274139739		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.922064274139739 | validation: 1.0063520169372842]
	TIME [epoch: 2.74 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9486064915115102		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.9486064915115102 | validation: 0.8025937541457151]
	TIME [epoch: 2.74 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9110313837491301		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.9110313837491301 | validation: 0.8379857757963747]
	TIME [epoch: 2.74 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8793483276413545		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.8793483276413545 | validation: 0.8347070751133464]
	TIME [epoch: 2.74 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8885913935667235		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.8885913935667235 | validation: 0.818589398118397]
	TIME [epoch: 2.74 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8777246518458495		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.8777246518458495 | validation: 0.8727816893615096]
	TIME [epoch: 2.74 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8944109932795193		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.8944109932795193 | validation: 0.8103814101637874]
	TIME [epoch: 2.74 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9133871749110694		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.9133871749110694 | validation: 0.9887974302899836]
	TIME [epoch: 2.74 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9391562725804946		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.9391562725804946 | validation: 0.8401275096188883]
	TIME [epoch: 2.74 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9077342144843441		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.9077342144843441 | validation: 0.7602665464803029]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_537.pth
	Model improved!!!
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9026501546176402		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.9026501546176402 | validation: 0.9668746716203236]
	TIME [epoch: 2.74 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9200072540822977		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.9200072540822977 | validation: 0.7571064942790603]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_539.pth
	Model improved!!!
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9150187282929635		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.9150187282929635 | validation: 0.8921063582545226]
	TIME [epoch: 2.74 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8999613604637793		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.8999613604637793 | validation: 0.820783116617779]
	TIME [epoch: 2.74 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8780897033171823		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.8780897033171823 | validation: 0.8110423619213185]
	TIME [epoch: 2.74 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8774186903376309		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.8774186903376309 | validation: 0.8207566476222248]
	TIME [epoch: 2.74 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8711686871285584		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.8711686871285584 | validation: 0.793623685427686]
	TIME [epoch: 2.74 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8750143791942466		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.8750143791942466 | validation: 0.8846271588386305]
	TIME [epoch: 2.74 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8769871030686842		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.8769871030686842 | validation: 0.7717663270265898]
	TIME [epoch: 2.74 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8915179392909787		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.8915179392909787 | validation: 0.9739235287118815]
	TIME [epoch: 2.74 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9172624365761525		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.9172624365761525 | validation: 0.7838143752192045]
	TIME [epoch: 2.75 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9202486841627965		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.9202486841627965 | validation: 0.8955707038507668]
	TIME [epoch: 2.74 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9080183694159175		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.9080183694159175 | validation: 0.8644170366133184]
	TIME [epoch: 2.74 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9105114898463417		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.9105114898463417 | validation: 0.8173392034023131]
	TIME [epoch: 2.74 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9016724905173485		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.9016724905173485 | validation: 0.8848479855741496]
	TIME [epoch: 2.75 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8843562025396908		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.8843562025396908 | validation: 0.7975269909131154]
	TIME [epoch: 2.74 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8637493621477161		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.8637493621477161 | validation: 0.8052982245449454]
	TIME [epoch: 2.74 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8677509774179802		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.8677509774179802 | validation: 0.8220825702861303]
	TIME [epoch: 2.74 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8657480457119496		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.8657480457119496 | validation: 0.79788433977486]
	TIME [epoch: 2.74 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8703577298694148		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.8703577298694148 | validation: 0.8342981687566633]
	TIME [epoch: 2.74 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8707895085819312		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.8707895085819312 | validation: 0.8283994217321203]
	TIME [epoch: 2.74 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8719212724326973		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.8719212724326973 | validation: 0.8640246823915038]
	TIME [epoch: 2.74 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8860178876749691		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.8860178876749691 | validation: 0.7813957611218337]
	TIME [epoch: 2.74 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8962120978114078		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.8962120978114078 | validation: 0.9396389935669242]
	TIME [epoch: 2.74 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8890816110172997		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.8890816110172997 | validation: 0.7507683007715835]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_562.pth
	Model improved!!!
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.877453567841786		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.877453567841786 | validation: 0.8437078348927601]
	TIME [epoch: 2.74 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8693128714135065		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.8693128714135065 | validation: 0.7921900750772896]
	TIME [epoch: 2.76 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8617952363974187		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.8617952363974187 | validation: 0.8345235137469008]
	TIME [epoch: 2.74 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8672453159215051		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.8672453159215051 | validation: 0.8245900388175337]
	TIME [epoch: 2.74 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.870362216754894		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.870362216754894 | validation: 0.8458554495917657]
	TIME [epoch: 2.74 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.86397480194964		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.86397480194964 | validation: 0.7827826766693751]
	TIME [epoch: 2.74 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8625696104085347		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.8625696104085347 | validation: 0.8136117861639335]
	TIME [epoch: 2.74 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8569853562956314		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.8569853562956314 | validation: 0.9084067568117068]
	TIME [epoch: 2.74 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9249243916777752		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.9249243916777752 | validation: 0.8096792757439947]
	TIME [epoch: 2.74 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0019566947220089		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 1.0019566947220089 | validation: 0.8826213166981951]
	TIME [epoch: 2.74 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8717429272349887		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.8717429272349887 | validation: 0.8171864029964477]
	TIME [epoch: 2.74 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.86109953174947		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.86109953174947 | validation: 0.8693593936370695]
	TIME [epoch: 2.74 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8719724047310721		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.8719724047310721 | validation: 0.7716106924829904]
	TIME [epoch: 2.74 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8607137713180731		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.8607137713180731 | validation: 0.8439710598488145]
	TIME [epoch: 2.74 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8531329801105951		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.8531329801105951 | validation: 0.7630620127126347]
	TIME [epoch: 2.74 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.84870185325093		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.84870185325093 | validation: 0.8350468254325973]
	TIME [epoch: 2.74 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8478120795977785		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.8478120795977785 | validation: 0.7773905611039673]
	TIME [epoch: 2.74 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8560504214920748		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.8560504214920748 | validation: 0.8893151266218718]
	TIME [epoch: 2.74 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8645331855367943		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.8645331855367943 | validation: 0.7354521829799069]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_581.pth
	Model improved!!!
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8710309644910321		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.8710309644910321 | validation: 0.8951660178887849]
	TIME [epoch: 2.74 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8580115680334645		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.8580115680334645 | validation: 0.7579898775382574]
	TIME [epoch: 2.74 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8538445948248894		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.8538445948248894 | validation: 0.8404708150558289]
	TIME [epoch: 2.74 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8409997015286987		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.8409997015286987 | validation: 0.7782232576294584]
	TIME [epoch: 2.74 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.841003270849412		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.841003270849412 | validation: 0.8183339859655434]
	TIME [epoch: 2.74 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8490817359779006		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.8490817359779006 | validation: 0.7833034948891465]
	TIME [epoch: 2.74 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.871873126465454		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.871873126465454 | validation: 0.9042593636486184]
	TIME [epoch: 2.74 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8915389092457449		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.8915389092457449 | validation: 0.82865559854277]
	TIME [epoch: 2.74 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8701391041073177		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.8701391041073177 | validation: 0.7542943824080033]
	TIME [epoch: 2.74 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8504779315177464		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.8504779315177464 | validation: 0.9071073101671363]
	TIME [epoch: 2.74 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8677223266521878		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.8677223266521878 | validation: 0.7302363551700135]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_592.pth
	Model improved!!!
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.874235837692766		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.874235837692766 | validation: 0.871085614600554]
	TIME [epoch: 2.74 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.847453760171919		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.847453760171919 | validation: 0.7558760366784789]
	TIME [epoch: 2.74 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8337203942443295		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.8337203942443295 | validation: 0.7774819251341497]
	TIME [epoch: 2.74 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8374924133376896		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.8374924133376896 | validation: 0.7923696911866878]
	TIME [epoch: 2.74 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8290154385563406		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.8290154385563406 | validation: 0.7649449344292387]
	TIME [epoch: 2.74 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.842120896655743		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.842120896655743 | validation: 0.8696574826433638]
	TIME [epoch: 2.74 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8519300912911578		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.8519300912911578 | validation: 0.7483259635080087]
	TIME [epoch: 2.74 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8782329668232153		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.8782329668232153 | validation: 0.8724432692024382]
	TIME [epoch: 2.74 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8593116843681868		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.8593116843681868 | validation: 0.7955101519679912]
	TIME [epoch: 2.75 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8401643730229031		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.8401643730229031 | validation: 0.7584645583920602]
	TIME [epoch: 2.74 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8275097599708184		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.8275097599708184 | validation: 0.8718397413055547]
	TIME [epoch: 2.74 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8431198608856993		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.8431198608856993 | validation: 0.717002829398882]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_604.pth
	Model improved!!!
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8591730217069997		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.8591730217069997 | validation: 0.9177021124586903]
	TIME [epoch: 2.74 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8556971915064933		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.8556971915064933 | validation: 0.7434021319435866]
	TIME [epoch: 2.74 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8335120616268671		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.8335120616268671 | validation: 0.7908575340010193]
	TIME [epoch: 2.74 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8245061928501406		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.8245061928501406 | validation: 0.7970168390816612]
	TIME [epoch: 2.74 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8158018525179307		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.8158018525179307 | validation: 0.7452421816273174]
	TIME [epoch: 2.74 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8174384117205582		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.8174384117205582 | validation: 0.8444640005442935]
	TIME [epoch: 2.74 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8336053364073583		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.8336053364073583 | validation: 0.7198507376609228]
	TIME [epoch: 2.74 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8484811551796565		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.8484811551796565 | validation: 0.8826226904044827]
	TIME [epoch: 2.75 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8655059350697544		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.8655059350697544 | validation: 0.799114051062627]
	TIME [epoch: 2.74 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8431638365388271		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.8431638365388271 | validation: 0.7874786453354345]
	TIME [epoch: 2.74 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.827442009376237		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.827442009376237 | validation: 0.857305335179793]
	TIME [epoch: 2.74 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8341978209516429		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.8341978209516429 | validation: 0.7403360998711053]
	TIME [epoch: 2.74 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8329520379208928		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.8329520379208928 | validation: 0.829536003239341]
	TIME [epoch: 2.74 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8229870633545455		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.8229870633545455 | validation: 0.7716237707167148]
	TIME [epoch: 2.74 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.819864991908556		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.819864991908556 | validation: 0.7611186892384345]
	TIME [epoch: 2.74 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8097432008671411		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.8097432008671411 | validation: 0.8195103995571146]
	TIME [epoch: 2.74 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8109669963490787		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.8109669963490787 | validation: 0.729964209601197]
	TIME [epoch: 2.74 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8119634352050018		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.8119634352050018 | validation: 0.8288463141868638]
	TIME [epoch: 2.74 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8220054261312028		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.8220054261312028 | validation: 0.7241366943709586]
	TIME [epoch: 2.74 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8278669618637738		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.8278669618637738 | validation: 0.8821070770247316]
	TIME [epoch: 2.74 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8471660016791556		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.8471660016791556 | validation: 0.708498827506708]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_625.pth
	Model improved!!!
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8565968665286097		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.8565968665286097 | validation: 0.8479378170391215]
	TIME [epoch: 2.74 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8299911964036641		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.8299911964036641 | validation: 0.7836913911800304]
	TIME [epoch: 2.74 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8063633985277617		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.8063633985277617 | validation: 0.7340743854890693]
	TIME [epoch: 2.74 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8277763740626465		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.8277763740626465 | validation: 0.8897543776731616]
	TIME [epoch: 2.74 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8337660377039217		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.8337660377039217 | validation: 0.7445737610591325]
	TIME [epoch: 2.74 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.817396477124444		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.817396477124444 | validation: 0.7960383886019776]
	TIME [epoch: 2.74 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8051974926599494		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.8051974926599494 | validation: 0.7781597463636056]
	TIME [epoch: 2.74 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7939084087181231		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.7939084087181231 | validation: 0.7419430954206174]
	TIME [epoch: 2.75 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7999983394495152		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.7999983394495152 | validation: 0.8016972789614606]
	TIME [epoch: 2.74 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8042752206402889		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.8042752206402889 | validation: 0.7706883421745663]
	TIME [epoch: 2.74 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8070106734036488		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.8070106734036488 | validation: 0.7917623023788221]
	TIME [epoch: 2.74 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8150694305455615		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.8150694305455615 | validation: 0.7652711454665752]
	TIME [epoch: 2.74 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.828931168793302		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.828931168793302 | validation: 0.8137546475492187]
	TIME [epoch: 2.75 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8193408001653326		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.8193408001653326 | validation: 0.7448353233597622]
	TIME [epoch: 2.74 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8048592488151366		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.8048592488151366 | validation: 0.7949183875021435]
	TIME [epoch: 2.74 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8002096036840063		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.8002096036840063 | validation: 0.7282747481745497]
	TIME [epoch: 2.74 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7899570878159762		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.7899570878159762 | validation: 0.8467904083085545]
	TIME [epoch: 2.74 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.81229320327358		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.81229320327358 | validation: 0.699950963889215]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_643.pth
	Model improved!!!
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8390337956030922		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.8390337956030922 | validation: 0.9102870005219361]
	TIME [epoch: 2.74 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8339203682254595		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.8339203682254595 | validation: 0.7243171361856909]
	TIME [epoch: 2.74 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8037685415549832		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.8037685415549832 | validation: 0.7914358268600266]
	TIME [epoch: 2.75 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.791116448801012		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.791116448801012 | validation: 0.7892472703273906]
	TIME [epoch: 2.75 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7906876996117808		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.7906876996117808 | validation: 0.7427098350395931]
	TIME [epoch: 2.74 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8008517976995626		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.8008517976995626 | validation: 0.8635205927428367]
	TIME [epoch: 2.74 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8090139988860969		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.8090139988860969 | validation: 0.7370950949436068]
	TIME [epoch: 2.74 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7871928082239705		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.7871928082239705 | validation: 0.767040927002586]
	TIME [epoch: 2.73 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7893754243085445		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.7893754243085445 | validation: 0.7767986028542059]
	TIME [epoch: 2.73 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.786148293467418		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.786148293467418 | validation: 0.7414078904690864]
	TIME [epoch: 2.73 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7859631257156644		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.7859631257156644 | validation: 0.8281397304319893]
	TIME [epoch: 2.74 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8006664134517729		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.8006664134517729 | validation: 0.6939580745962785]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_655.pth
	Model improved!!!
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8413279044846254		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.8413279044846254 | validation: 0.8763274549286907]
	TIME [epoch: 2.75 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8154989535946985		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.8154989535946985 | validation: 0.7190674831375038]
	TIME [epoch: 2.75 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7891751216512912		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.7891751216512912 | validation: 0.783401975932255]
	TIME [epoch: 2.75 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7816735463061062		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.7816735463061062 | validation: 0.7580497096998269]
	TIME [epoch: 2.75 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7804889065182485		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.7804889065182485 | validation: 0.7968055196852255]
	TIME [epoch: 2.75 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.792397543319637		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.792397543319637 | validation: 0.755909170650892]
	TIME [epoch: 2.73 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7991693386724057		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.7991693386724057 | validation: 0.7954730215131325]
	TIME [epoch: 2.73 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7932724166390992		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.7932724166390992 | validation: 0.743717711633616]
	TIME [epoch: 2.73 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7761224572774263		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.7761224572774263 | validation: 0.7603217809811853]
	TIME [epoch: 2.73 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7722891201539872		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.7722891201539872 | validation: 0.7564477514015382]
	TIME [epoch: 2.74 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7653669669484094		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.7653669669484094 | validation: 0.7278728603289069]
	TIME [epoch: 2.73 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7625400188500341		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.7625400188500341 | validation: 0.7744233083607949]
	TIME [epoch: 2.73 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.766735750382594		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.766735750382594 | validation: 0.686145654197128]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_668.pth
	Model improved!!!
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7811739104159768		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.7811739104159768 | validation: 0.8938104883140895]
	TIME [epoch: 2.74 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8150708392940501		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.8150708392940501 | validation: 0.6863408984181774]
	TIME [epoch: 2.73 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.837926387377219		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.837926387377219 | validation: 0.8285523497308681]
	TIME [epoch: 2.73 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.789419161157071		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.789419161157071 | validation: 0.7412869280518386]
	TIME [epoch: 2.73 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7563106565783414		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.7563106565783414 | validation: 0.7173587688085756]
	TIME [epoch: 2.73 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7632472517331661		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.7632472517331661 | validation: 0.781867785298468]
	TIME [epoch: 2.73 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7618935613735479		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.7618935613735479 | validation: 0.7318534501309666]
	TIME [epoch: 2.73 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7657983096919462		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.7657983096919462 | validation: 0.8168994627212065]
	TIME [epoch: 2.74 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7729345996000426		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.7729345996000426 | validation: 0.7746920717497295]
	TIME [epoch: 2.73 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7944231808025799		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.7944231808025799 | validation: 0.8232443058383174]
	TIME [epoch: 2.73 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7875100393149288		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.7875100393149288 | validation: 0.7418390388672091]
	TIME [epoch: 2.74 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7631922802280449		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.7631922802280449 | validation: 0.7261552750077801]
	TIME [epoch: 2.74 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7617985660930147		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.7617985660930147 | validation: 0.7908864647668725]
	TIME [epoch: 2.73 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.755120821139003		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.755120821139003 | validation: 0.6728798590434414]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_682.pth
	Model improved!!!
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7671344801620217		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.7671344801620217 | validation: 0.876967241844492]
	TIME [epoch: 2.74 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8001442879177052		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.8001442879177052 | validation: 0.6779435415746484]
	TIME [epoch: 2.73 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7928596954898347		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.7928596954898347 | validation: 0.7741992989116976]
	TIME [epoch: 2.73 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7580050575822095		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.7580050575822095 | validation: 0.7604894961421398]
	TIME [epoch: 2.73 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7465134410634798		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.7465134410634798 | validation: 0.7055389280490814]
	TIME [epoch: 2.74 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7564245978166071		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.7564245978166071 | validation: 0.7755183486331652]
	TIME [epoch: 2.74 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7480349720811649		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.7480349720811649 | validation: 0.702302509844622]
	TIME [epoch: 2.74 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7530271398041647		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.7530271398041647 | validation: 0.7904045846597509]
	TIME [epoch: 2.73 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7459388111193775		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.7459388111193775 | validation: 0.7057384523508956]
	TIME [epoch: 2.74 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7467708793059474		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.7467708793059474 | validation: 0.7684434773156718]
	TIME [epoch: 2.73 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7460316219251611		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.7460316219251611 | validation: 0.6782723032360425]
	TIME [epoch: 2.74 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7526056876488556		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.7526056876488556 | validation: 0.8436538222031174]
	TIME [epoch: 2.73 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7664938138531663		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.7664938138531663 | validation: 0.6896530929642819]
	TIME [epoch: 2.73 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7711909235633911		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.7711909235633911 | validation: 0.7873105152637563]
	TIME [epoch: 2.73 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7609211886858154		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.7609211886858154 | validation: 0.7537020983071341]
	TIME [epoch: 2.74 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7598037136749397		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.7598037136749397 | validation: 0.717405004934701]
	TIME [epoch: 2.73 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7543392395012003		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.7543392395012003 | validation: 0.8196502565082885]
	TIME [epoch: 2.73 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.75758074809194		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.75758074809194 | validation: 0.6901442416721153]
	TIME [epoch: 2.74 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.741913779256833		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.741913779256833 | validation: 0.7479353650321441]
	TIME [epoch: 2.73 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7303886439016138		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.7303886439016138 | validation: 0.708798495562213]
	TIME [epoch: 2.73 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7342151326290063		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.7342151326290063 | validation: 0.7223626346626217]
	TIME [epoch: 2.73 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7325839445090415		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.7325839445090415 | validation: 0.7185266612553475]
	TIME [epoch: 2.74 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7321077205193522		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.7321077205193522 | validation: 0.7269950809123531]
	TIME [epoch: 2.74 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7385621697437186		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.7385621697437186 | validation: 0.7825712019631741]
	TIME [epoch: 2.73 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.748739406347689		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.748739406347689 | validation: 0.7301561983807047]
	TIME [epoch: 2.74 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7380969627168859		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.7380969627168859 | validation: 0.7520816824755279]
	TIME [epoch: 2.74 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7428798258379844		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.7428798258379844 | validation: 0.7476831859405784]
	TIME [epoch: 2.74 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7318191203946498		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.7318191203946498 | validation: 0.6992768192513724]
	TIME [epoch: 2.73 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7283934690556184		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.7283934690556184 | validation: 0.7956047933021102]
	TIME [epoch: 2.74 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7404491920425547		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.7404491920425547 | validation: 0.6481674895958935]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_712.pth
	Model improved!!!
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7738357771597145		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.7738357771597145 | validation: 0.8318661619854485]
	TIME [epoch: 2.73 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7608005541057592		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.7608005541057592 | validation: 0.6673450417077689]
	TIME [epoch: 2.73 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7308286423485996		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.7308286423485996 | validation: 0.7317980631819352]
	TIME [epoch: 2.73 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7141287848121581		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.7141287848121581 | validation: 0.7260480797119375]
	TIME [epoch: 2.73 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7167373504362066		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.7167373504362066 | validation: 0.7215911909416809]
	TIME [epoch: 2.73 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7165871090087467		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.7165871090087467 | validation: 0.7144427429242659]
	TIME [epoch: 2.73 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7170912642731304		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.7170912642731304 | validation: 0.7349513139170747]
	TIME [epoch: 2.74 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7179070600972014		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.7179070600972014 | validation: 0.7010328135468138]
	TIME [epoch: 2.73 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7319315030922057		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.7319315030922057 | validation: 0.7814622849715787]
	TIME [epoch: 2.74 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7365858335310559		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.7365858335310559 | validation: 0.7281491187438224]
	TIME [epoch: 2.74 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7299364890286498		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.7299364890286498 | validation: 0.7157589832392558]
	TIME [epoch: 2.74 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7166916058336688		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.7166916058336688 | validation: 0.72339507671993]
	TIME [epoch: 2.73 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7135826777655874		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.7135826777655874 | validation: 0.6786402009295887]
	TIME [epoch: 2.73 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7044803324006387		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.7044803324006387 | validation: 0.7820255537880416]
	TIME [epoch: 2.74 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7243607766361804		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.7243607766361804 | validation: 0.6109094304172958]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_727.pth
	Model improved!!!
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7606346184258959		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.7606346184258959 | validation: 0.8154491338112351]
	TIME [epoch: 2.73 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7426484935500325		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.7426484935500325 | validation: 0.6527584369754162]
	TIME [epoch: 2.74 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7088148317863302		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.7088148317863302 | validation: 0.7192121795076014]
	TIME [epoch: 2.73 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7023329577885045		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.7023329577885045 | validation: 0.7339628796285401]
	TIME [epoch: 2.73 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6956133584117979		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.6956133584117979 | validation: 0.686952648668128]
	TIME [epoch: 2.73 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.691525184324989		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.691525184324989 | validation: 0.739841938152892]
	TIME [epoch: 2.73 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7024329820706169		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.7024329820706169 | validation: 0.6819821267112864]
	TIME [epoch: 2.73 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7048981021123554		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.7048981021123554 | validation: 0.74616676597192]
	TIME [epoch: 2.73 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7008327669288101		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.7008327669288101 | validation: 0.6726450215564314]
	TIME [epoch: 2.73 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7009843887923147		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.7009843887923147 | validation: 0.7416389596742143]
	TIME [epoch: 2.73 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7106301184298991		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.7106301184298991 | validation: 0.7412775573388842]
	TIME [epoch: 2.73 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7312915284021162		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.7312915284021162 | validation: 0.6900148422104412]
	TIME [epoch: 2.73 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7179310707177051		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.7179310707177051 | validation: 0.7561411554500812]
	TIME [epoch: 2.74 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7025968937670254		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.7025968937670254 | validation: 0.6184417902553769]
	TIME [epoch: 2.73 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7058580694292073		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.7058580694292073 | validation: 0.7902391592260868]
	TIME [epoch: 2.73 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7085221626967382		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.7085221626967382 | validation: 0.6432802948012246]
	TIME [epoch: 2.74 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7001124203075486		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.7001124203075486 | validation: 0.7109956832446288]
	TIME [epoch: 2.73 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.687262925148346		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.687262925148346 | validation: 0.6751850827674863]
	TIME [epoch: 2.74 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6780671159117561		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.6780671159117561 | validation: 0.6966925697291515]
	TIME [epoch: 2.73 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6777203868610819		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.6777203868610819 | validation: 0.682385479215881]
	TIME [epoch: 2.73 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6915589573099351		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.6915589573099351 | validation: 0.7261224362107406]
	TIME [epoch: 2.73 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6893367627664823		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.6893367627664823 | validation: 0.6581229461006151]
	TIME [epoch: 2.73 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.698909103432596		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.698909103432596 | validation: 0.7505252783824014]
	TIME [epoch: 2.73 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6953187128591681		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.6953187128591681 | validation: 0.6481921536219449]
	TIME [epoch: 2.74 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6861657289278432		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.6861657289278432 | validation: 0.7328712664014481]
	TIME [epoch: 2.73 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6848006129436872		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.6848006129436872 | validation: 0.6271570961883263]
	TIME [epoch: 2.73 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6896849975069593		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.6896849975069593 | validation: 0.7584037051546888]
	TIME [epoch: 2.74 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6929374721952684		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.6929374721952684 | validation: 0.6327883479856973]
	TIME [epoch: 2.74 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7085644889540721		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.7085644889540721 | validation: 0.774126256262886]
	TIME [epoch: 2.73 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6885254554454722		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.6885254554454722 | validation: 0.6819918126162403]
	TIME [epoch: 2.73 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6644898662405353		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.6644898662405353 | validation: 0.6475448028799287]
	TIME [epoch: 2.73 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6660022372248122		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.6660022372248122 | validation: 0.7304236428305515]
	TIME [epoch: 2.73 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6716345614191692		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.6716345614191692 | validation: 0.6022574618780077]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_760.pth
	Model improved!!!
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6804912758591855		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.6804912758591855 | validation: 0.7250918515068956]
	TIME [epoch: 2.73 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6749711980341274		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.6749711980341274 | validation: 0.6377408219073539]
	TIME [epoch: 2.74 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6712799195819116		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.6712799195819116 | validation: 0.7000916873094334]
	TIME [epoch: 2.74 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6575523367676823		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.6575523367676823 | validation: 0.6917557819683773]
	TIME [epoch: 2.73 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6676159056310766		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.6676159056310766 | validation: 0.6479755652406084]
	TIME [epoch: 2.74 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6583355536645199		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.6583355536645199 | validation: 0.6864803451487861]
	TIME [epoch: 2.74 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6602403693447244		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.6602403693447244 | validation: 0.6626922613384445]
	TIME [epoch: 2.73 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6553053163444769		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.6553053163444769 | validation: 0.7200078643598219]
	TIME [epoch: 2.73 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6552744852961996		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.6552744852961996 | validation: 0.6357510566335354]
	TIME [epoch: 2.73 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6621971737527695		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.6621971737527695 | validation: 0.7350343809359208]
	TIME [epoch: 2.73 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6623704046628208		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.6623704046628208 | validation: 0.6336203708516193]
	TIME [epoch: 2.73 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6478977671908911		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.6478977671908911 | validation: 0.7116163755316687]
	TIME [epoch: 2.73 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6529792090997104		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.6529792090997104 | validation: 0.5993891075128256]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_773.pth
	Model improved!!!
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6696100088934489		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.6696100088934489 | validation: 0.760035802184535]
	TIME [epoch: 2.75 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.687456016460374		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.687456016460374 | validation: 0.6149987667922963]
	TIME [epoch: 2.75 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6615660331563089		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.6615660331563089 | validation: 0.6755838348549159]
	TIME [epoch: 2.75 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6416343991675137		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.6416343991675137 | validation: 0.707567439704703]
	TIME [epoch: 2.75 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6403781560796147		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.6403781560796147 | validation: 0.5959345938640376]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_778.pth
	Model improved!!!
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6475248408444426		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.6475248408444426 | validation: 0.714632356686979]
	TIME [epoch: 2.75 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6523375770715718		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.6523375770715718 | validation: 0.6237553885935974]
	TIME [epoch: 2.75 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6364562330142812		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.6364562330142812 | validation: 0.6584303814428667]
	TIME [epoch: 2.75 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6390490145127844		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.6390490145127844 | validation: 0.6586288026738839]
	TIME [epoch: 2.75 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6341656413501849		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.6341656413501849 | validation: 0.6667480790705348]
	TIME [epoch: 2.75 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6347454212030773		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.6347454212030773 | validation: 0.6757050382303106]
	TIME [epoch: 2.75 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6548676525877395		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.6548676525877395 | validation: 0.6475329242056991]
	TIME [epoch: 2.75 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6472244185392174		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.6472244185392174 | validation: 0.668985111125506]
	TIME [epoch: 2.75 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6391249152018027		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.6391249152018027 | validation: 0.6135030689155156]
	TIME [epoch: 2.75 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6324991013111775		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.6324991013111775 | validation: 0.6620882690832587]
	TIME [epoch: 2.75 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6241832680418571		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.6241832680418571 | validation: 0.5884103937432548]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_789.pth
	Model improved!!!
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6246490050801028		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.6246490050801028 | validation: 0.705336622632212]
	TIME [epoch: 2.74 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6337695834813413		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.6337695834813413 | validation: 0.5730928804785712]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_791.pth
	Model improved!!!
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6353841187956389		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.6353841187956389 | validation: 0.7252137637380166]
	TIME [epoch: 2.74 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6438513808680737		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.6438513808680737 | validation: 0.5890535795294927]
	TIME [epoch: 2.75 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6292875878458754		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.6292875878458754 | validation: 0.6732393669048272]
	TIME [epoch: 2.74 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6168380109494244		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.6168380109494244 | validation: 0.617649622685163]
	TIME [epoch: 2.74 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6049521992276339		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.6049521992276339 | validation: 0.6311240719371495]
	TIME [epoch: 2.75 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6068006752539722		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.6068006752539722 | validation: 0.6431351089999343]
	TIME [epoch: 2.75 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.614324305563567		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.614324305563567 | validation: 0.5893078236155562]
	TIME [epoch: 2.74 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6350616398817184		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.6350616398817184 | validation: 0.7114621759189863]
	TIME [epoch: 2.74 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6418657164671357		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.6418657164671357 | validation: 0.598712821839641]
	TIME [epoch: 2.74 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6193013974252066		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.6193013974252066 | validation: 0.6268318147724052]
	TIME [epoch: 2.74 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6056201153925744		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.6056201153925744 | validation: 0.6554793574350692]
	TIME [epoch: 2.74 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.600563497852857		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.600563497852857 | validation: 0.5778453207705007]
	TIME [epoch: 2.74 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5988716432992773		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.5988716432992773 | validation: 0.6814807858798618]
	TIME [epoch: 2.75 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6084051792654512		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.6084051792654512 | validation: 0.5649611011610162]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_805.pth
	Model improved!!!
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6154133088495245		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.6154133088495245 | validation: 0.7086512065977248]
	TIME [epoch: 2.74 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6169116428091241		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.6169116428091241 | validation: 0.5675352157003017]
	TIME [epoch: 2.74 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.604254464401672		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.604254464401672 | validation: 0.6416104698530702]
	TIME [epoch: 2.74 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5901918484601378		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.5901918484601378 | validation: 0.6014243771984122]
	TIME [epoch: 2.74 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5919754614687234		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.5919754614687234 | validation: 0.6052811669849053]
	TIME [epoch: 2.74 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5843429898762885		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.5843429898762885 | validation: 0.6436213820534347]
	TIME [epoch: 2.74 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.595497393783535		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.595497393783535 | validation: 0.589226889384139]
	TIME [epoch: 2.74 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6189166856532341		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.6189166856532341 | validation: 0.6663501952829723]
	TIME [epoch: 2.74 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6208520364378546		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.6208520364378546 | validation: 0.5820633540245165]
	TIME [epoch: 2.73 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5835061413629639		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.5835061413629639 | validation: 0.6077280905926234]
	TIME [epoch: 2.74 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5764513196384086		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.5764513196384086 | validation: 0.6025295936556916]
	TIME [epoch: 2.74 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5731095901872658		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.5731095901872658 | validation: 0.5859971674642879]
	TIME [epoch: 2.74 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5798935385246855		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.5798935385246855 | validation: 0.6204105237861004]
	TIME [epoch: 2.74 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5709535888492488		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.5709535888492488 | validation: 0.5857236457418188]
	TIME [epoch: 2.74 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5785627398747842		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.5785627398747842 | validation: 0.6272131856075712]
	TIME [epoch: 2.74 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5848374931374738		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.5848374931374738 | validation: 0.5336629664969449]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_821.pth
	Model improved!!!
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6112687244160104		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.6112687244160104 | validation: 0.7432439921505719]
	TIME [epoch: 2.75 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6188430904620507		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.6188430904620507 | validation: 0.5486077496097455]
	TIME [epoch: 2.75 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.568691130585568		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.568691130585568 | validation: 0.5820858639539866]
	TIME [epoch: 2.75 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5697057460713411		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.5697057460713411 | validation: 0.6066934872600693]
	TIME [epoch: 2.75 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5715416306371863		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.5715416306371863 | validation: 0.6226205804279412]
	TIME [epoch: 2.75 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5890645881307088		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.5890645881307088 | validation: 0.5618100399163642]
	TIME [epoch: 2.75 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5818625097752768		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.5818625097752768 | validation: 0.6195940761432456]
	TIME [epoch: 2.75 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5645218457796325		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.5645218457796325 | validation: 0.5637088370956679]
	TIME [epoch: 2.74 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5612947158857006		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.5612947158857006 | validation: 0.6143346309056004]
	TIME [epoch: 2.75 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5608154224612859		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.5608154224612859 | validation: 0.5427254333638152]
	TIME [epoch: 2.74 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5554322577663856		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.5554322577663856 | validation: 0.6046703001300138]
	TIME [epoch: 2.74 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5553890458143761		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.5553890458143761 | validation: 0.5338552281563145]
	TIME [epoch: 2.74 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5567643119190779		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.5567643119190779 | validation: 0.6366281084883108]
	TIME [epoch: 2.74 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.56110626698582		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.56110626698582 | validation: 0.4953071473154538]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_835.pth
	Model improved!!!
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5707907981519657		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.5707907981519657 | validation: 0.635903363528436]
	TIME [epoch: 2.75 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5614329220745825		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.5614329220745825 | validation: 0.5180871018538967]
	TIME [epoch: 2.75 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.551650824373921		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.551650824373921 | validation: 0.6092727920705721]
	TIME [epoch: 2.74 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5391147454885368		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.5391147454885368 | validation: 0.5255559710286272]
	TIME [epoch: 2.75 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5400228124875526		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.5400228124875526 | validation: 0.6245425142963145]
	TIME [epoch: 2.75 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5581483885554758		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.5581483885554758 | validation: 0.5422640308787177]
	TIME [epoch: 2.75 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5659032972735057		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.5659032972735057 | validation: 0.6053886006918258]
	TIME [epoch: 2.75 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5557743735771234		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.5557743735771234 | validation: 0.5724794536705006]
	TIME [epoch: 2.74 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5372797860971321		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.5372797860971321 | validation: 0.5270764926574663]
	TIME [epoch: 2.74 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5318222360005634		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.5318222360005634 | validation: 0.6207517198674712]
	TIME [epoch: 2.74 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.536610301243586		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.536610301243586 | validation: 0.48145077211193377]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_846.pth
	Model improved!!!
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5401991806387443		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.5401991806387443 | validation: 0.6448329035246515]
	TIME [epoch: 2.75 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5444298496130782		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.5444298496130782 | validation: 0.48565336037315004]
	TIME [epoch: 2.74 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5427699774572731		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.5427699774572731 | validation: 0.5867926569337123]
	TIME [epoch: 2.75 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5346194745499083		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.5346194745499083 | validation: 0.5429156964248476]
	TIME [epoch: 2.75 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5304941879329919		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.5304941879329919 | validation: 0.5412823311480497]
	TIME [epoch: 2.75 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5229936921521514		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.5229936921521514 | validation: 0.5932158375049361]
	TIME [epoch: 2.74 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5276014469824964		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.5276014469824964 | validation: 0.5017273930812868]
	TIME [epoch: 2.75 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5266868498404285		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.5266868498404285 | validation: 0.5515493941569893]
	TIME [epoch: 2.74 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5167010588533711		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.5167010588533711 | validation: 0.5514886169024289]
	TIME [epoch: 2.75 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5110653388749274		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.5110653388749274 | validation: 0.5276991241971374]
	TIME [epoch: 2.74 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5099347894660899		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.5099347894660899 | validation: 0.5263556762231375]
	TIME [epoch: 2.75 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5117738028294953		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.5117738028294953 | validation: 0.5507758701836767]
	TIME [epoch: 2.74 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5024871330836802		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.5024871330836802 | validation: 0.5475168905942269]
	TIME [epoch: 2.75 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5213965136972096		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.5213965136972096 | validation: 0.5388352049254614]
	TIME [epoch: 2.74 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5364444872307466		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.5364444872307466 | validation: 0.5836112180999591]
	TIME [epoch: 2.75 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5341343195438087		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.5341343195438087 | validation: 0.4994398603573153]
	TIME [epoch: 2.75 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5104471696924432		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.5104471696924432 | validation: 0.5765440338472463]
	TIME [epoch: 2.75 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5045218228056622		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.5045218228056622 | validation: 0.46167724235437807]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_864.pth
	Model improved!!!
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5100129482966116		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.5100129482966116 | validation: 0.633188908813173]
	TIME [epoch: 2.73 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5165717776865774		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.5165717776865774 | validation: 0.4502144489538414]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_866.pth
	Model improved!!!
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5108466768507429		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.5108466768507429 | validation: 0.5791743186704909]
	TIME [epoch: 2.74 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5072344568904201		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.5072344568904201 | validation: 0.49108237568070834]
	TIME [epoch: 2.74 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5021980228531032		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.5021980228531032 | validation: 0.567580108480465]
	TIME [epoch: 2.74 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49714098237127974		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.49714098237127974 | validation: 0.5148467963876248]
	TIME [epoch: 2.74 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4958692872640483		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.4958692872640483 | validation: 0.5295612210242285]
	TIME [epoch: 2.74 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4991600888775497		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.4991600888775497 | validation: 0.5269708374805832]
	TIME [epoch: 2.74 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49437395713019044		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.49437395713019044 | validation: 0.509916257623311]
	TIME [epoch: 2.73 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4843620596554119		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.4843620596554119 | validation: 0.5365974626326049]
	TIME [epoch: 2.74 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48668852742944463		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.48668852742944463 | validation: 0.49616990570793906]
	TIME [epoch: 2.73 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4815813078329893		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.4815813078329893 | validation: 0.5394140850512746]
	TIME [epoch: 2.73 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47751224344623666		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.47751224344623666 | validation: 0.47158613563491525]
	TIME [epoch: 2.73 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4760090990121877		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.4760090990121877 | validation: 0.5654530364992191]
	TIME [epoch: 2.74 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48373954470573494		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.48373954470573494 | validation: 0.44352275785478634]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_879.pth
	Model improved!!!
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49296355792886715		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.49296355792886715 | validation: 0.5947491695942568]
	TIME [epoch: 2.75 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4984713049963307		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.4984713049963307 | validation: 0.4355376496164607]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_881.pth
	Model improved!!!
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4894493539032535		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.4894493539032535 | validation: 0.5764230179947004]
	TIME [epoch: 2.74 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4998636586021478		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.4998636586021478 | validation: 0.4929514297866293]
	TIME [epoch: 2.73 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49152169482692104		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.49152169482692104 | validation: 0.4997458616110982]
	TIME [epoch: 2.73 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4676501393420329		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.4676501393420329 | validation: 0.4932553175937054]
	TIME [epoch: 2.73 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45940256331273616		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.45940256331273616 | validation: 0.4718542106725462]
	TIME [epoch: 2.73 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4602593853757801		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.4602593853757801 | validation: 0.5210820388310188]
	TIME [epoch: 2.73 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4663458744985851		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.4663458744985851 | validation: 0.45434976577816094]
	TIME [epoch: 2.73 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4689723429806418		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.4689723429806418 | validation: 0.510807861113768]
	TIME [epoch: 2.74 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4599485371208125		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.4599485371208125 | validation: 0.4532212322537701]
	TIME [epoch: 2.73 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4652469183698993		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.4652469183698993 | validation: 0.5211759364326829]
	TIME [epoch: 3.6 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4635830241680857		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.4635830241680857 | validation: 0.4363708942236983]
	TIME [epoch: 2.75 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4591927648539354		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.4591927648539354 | validation: 0.559216813891929]
	TIME [epoch: 2.74 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46952462649968296		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.46952462649968296 | validation: 0.40330228131580387]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_894.pth
	Model improved!!!
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4707898983932542		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.4707898983932542 | validation: 0.562615957990826]
	TIME [epoch: 2.74 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46533808702998886		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.46533808702998886 | validation: 0.45414887230820744]
	TIME [epoch: 2.74 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4463858622901099		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.4463858622901099 | validation: 0.4565029749264546]
	TIME [epoch: 2.74 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44542933042099536		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.44542933042099536 | validation: 0.5113376155383432]
	TIME [epoch: 2.74 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45238618364169014		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.45238618364169014 | validation: 0.4375516519782061]
	TIME [epoch: 2.75 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44743533500913707		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.44743533500913707 | validation: 0.5159209210063794]
	TIME [epoch: 2.74 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44809179915033953		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.44809179915033953 | validation: 0.4363244065263918]
	TIME [epoch: 2.74 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4423927051401229		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.4423927051401229 | validation: 0.501054337561261]
	TIME [epoch: 2.74 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4390626467566967		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.4390626467566967 | validation: 0.44594474099075776]
	TIME [epoch: 2.74 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4368634325307378		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.4368634325307378 | validation: 0.5106337152375788]
	TIME [epoch: 2.74 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43834820414884973		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.43834820414884973 | validation: 0.4577853295868204]
	TIME [epoch: 2.74 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44698569879846406		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.44698569879846406 | validation: 0.49544176933051054]
	TIME [epoch: 2.74 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44645383265527744		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.44645383265527744 | validation: 0.5070001623778337]
	TIME [epoch: 2.74 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4484631525462396		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.4484631525462396 | validation: 0.42015110387202015]
	TIME [epoch: 2.74 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45450058908963137		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.45450058908963137 | validation: 0.5369028341745116]
	TIME [epoch: 2.74 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4370944230666231		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.4370944230666231 | validation: 0.4148503208879022]
	TIME [epoch: 2.75 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43806963949829153		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.43806963949829153 | validation: 0.5028740551922113]
	TIME [epoch: 2.74 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4332240823001831		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.4332240823001831 | validation: 0.43020193765121517]
	TIME [epoch: 2.74 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4307182378805172		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.4307182378805172 | validation: 0.4786067626467361]
	TIME [epoch: 2.74 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41795099203593		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.41795099203593 | validation: 0.4125194460208689]
	TIME [epoch: 2.74 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4218088630349405		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.4218088630349405 | validation: 0.4925040427725099]
	TIME [epoch: 2.74 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4224079155447595		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.4224079155447595 | validation: 0.4119394772194598]
	TIME [epoch: 2.74 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42506324127094425		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.42506324127094425 | validation: 0.5221873308524584]
	TIME [epoch: 2.75 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4289813988202292		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.4289813988202292 | validation: 0.38197421549459115]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_918.pth
	Model improved!!!
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4299756558262527		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.4299756558262527 | validation: 0.5081269960085151]
	TIME [epoch: 2.75 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41823760358384776		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.41823760358384776 | validation: 0.3980485648285142]
	TIME [epoch: 2.75 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41027623281071346		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.41027623281071346 | validation: 0.4823171945839735]
	TIME [epoch: 2.75 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41123146590995896		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.41123146590995896 | validation: 0.4050341375831746]
	TIME [epoch: 2.75 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4185914823127236		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.4185914823127236 | validation: 0.4790966163757358]
	TIME [epoch: 2.74 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4298781799825504		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.4298781799825504 | validation: 0.4248033124930708]
	TIME [epoch: 2.74 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42409948350867227		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.42409948350867227 | validation: 0.4630943205933619]
	TIME [epoch: 2.74 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4124413009151746		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.4124413009151746 | validation: 0.44795555250134866]
	TIME [epoch: 2.74 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39521056707733015		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.39521056707733015 | validation: 0.4215506610981762]
	TIME [epoch: 2.75 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3954445712607087		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.3954445712607087 | validation: 0.4689644200553767]
	TIME [epoch: 2.74 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4013847085266329		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.4013847085266329 | validation: 0.37204728987321517]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_929.pth
	Model improved!!!
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4117056161863649		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.4117056161863649 | validation: 0.54943019277421]
	TIME [epoch: 2.75 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4321114166054764		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.4321114166054764 | validation: 0.3835245945487313]
	TIME [epoch: 2.75 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4052362505844728		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.4052362505844728 | validation: 0.464374460529137]
	TIME [epoch: 2.74 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4006586595251564		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.4006586595251564 | validation: 0.433466490374883]
	TIME [epoch: 2.75 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39184524399107845		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.39184524399107845 | validation: 0.41810612443519396]
	TIME [epoch: 2.75 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3926931834490667		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.3926931834490667 | validation: 0.4351588352548541]
	TIME [epoch: 2.74 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39387897528841764		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.39387897528841764 | validation: 0.4428765080381097]
	TIME [epoch: 2.74 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4007200357941978		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.4007200357941978 | validation: 0.41175195194108155]
	TIME [epoch: 2.74 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4057595746113546		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.4057595746113546 | validation: 0.4801341707385577]
	TIME [epoch: 2.74 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4053952462115025		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.4053952462115025 | validation: 0.36847143431547813]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_939.pth
	Model improved!!!
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40327521494462804		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.40327521494462804 | validation: 0.48004060425831646]
	TIME [epoch: 2.74 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.384586713047441		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.384586713047441 | validation: 0.39384048628816287]
	TIME [epoch: 2.74 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3846024430649817		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.3846024430649817 | validation: 0.41748465039774985]
	TIME [epoch: 2.74 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38248407972257936		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.38248407972257936 | validation: 0.4170871476399217]
	TIME [epoch: 2.74 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37693238885530295		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.37693238885530295 | validation: 0.39698268931962344]
	TIME [epoch: 2.74 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3750876662582908		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.3750876662582908 | validation: 0.4516213541082934]
	TIME [epoch: 2.74 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3835876915705944		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.3835876915705944 | validation: 0.35810936423278367]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_946.pth
	Model improved!!!
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38844366210506764		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.38844366210506764 | validation: 0.46331620345579305]
	TIME [epoch: 2.74 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.397982092188372		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.397982092188372 | validation: 0.3859354906415171]
	TIME [epoch: 2.73 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3800900170107191		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.3800900170107191 | validation: 0.43009403820820397]
	TIME [epoch: 2.74 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3695933400255636		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.3695933400255636 | validation: 0.38229099807837474]
	TIME [epoch: 2.74 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3675891388589521		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.3675891388589521 | validation: 0.4458436987225827]
	TIME [epoch: 2.74 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3684620186570728		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.3684620186570728 | validation: 0.3552399056500125]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_952.pth
	Model improved!!!
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38647763712509664		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.38647763712509664 | validation: 0.5062864759268522]
	TIME [epoch: 2.74 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4043914948803581		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.4043914948803581 | validation: 0.39547595837728733]
	TIME [epoch: 2.74 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3711204796740726		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.3711204796740726 | validation: 0.38077725157466424]
	TIME [epoch: 2.74 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3693939947275877		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.3693939947275877 | validation: 0.4639276880836695]
	TIME [epoch: 2.74 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3759548988637998		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.3759548988637998 | validation: 0.35108785543141574]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_957.pth
	Model improved!!!
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3840704574680764		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.3840704574680764 | validation: 0.46190882592061816]
	TIME [epoch: 2.74 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.367991307654016		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.367991307654016 | validation: 0.3789048791455198]
	TIME [epoch: 2.74 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35414623415619134		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.35414623415619134 | validation: 0.39687117432158114]
	TIME [epoch: 2.74 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35229881851414646		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.35229881851414646 | validation: 0.4198735886982018]
	TIME [epoch: 2.74 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3571199029324481		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.3571199029324481 | validation: 0.36959962652049116]
	TIME [epoch: 2.75 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3508957225415361		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.3508957225415361 | validation: 0.4288657042152636]
	TIME [epoch: 2.74 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34929416056158347		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.34929416056158347 | validation: 0.36136563359479357]
	TIME [epoch: 2.74 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35943147841935874		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.35943147841935874 | validation: 0.4664180640692288]
	TIME [epoch: 2.74 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36759427435470843		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.36759427435470843 | validation: 0.343529629759179]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_966.pth
	Model improved!!!
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3664647371582903		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.3664647371582903 | validation: 0.4539422993664959]
	TIME [epoch: 2.74 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3649489403827515		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.3649489403827515 | validation: 0.40136515191133476]
	TIME [epoch: 2.74 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36919730422770797		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.36919730422770797 | validation: 0.38899848481154414]
	TIME [epoch: 2.74 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3631849661556004		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.3631849661556004 | validation: 0.42213084892333735]
	TIME [epoch: 2.74 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3526382309036323		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.3526382309036323 | validation: 0.3729207052024754]
	TIME [epoch: 2.74 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3417800007717969		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.3417800007717969 | validation: 0.39622073675218156]
	TIME [epoch: 2.74 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3376866233320745		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.3376866233320745 | validation: 0.39014359638508367]
	TIME [epoch: 2.74 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34369391973107766		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.34369391973107766 | validation: 0.39903246679256493]
	TIME [epoch: 2.74 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34663948447542736		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.34663948447542736 | validation: 0.35694749215378174]
	TIME [epoch: 2.74 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35006161335124303		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.35006161335124303 | validation: 0.42956359218513107]
	TIME [epoch: 2.73 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35478581625527605		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.35478581625527605 | validation: 0.34311627183794946]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_977.pth
	Model improved!!!
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3416713503672661		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.3416713503672661 | validation: 0.4146693507720494]
	TIME [epoch: 2.75 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3401290638443746		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.3401290638443746 | validation: 0.34360749503282845]
	TIME [epoch: 2.75 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33515064298209885		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.33515064298209885 | validation: 0.41626992949172703]
	TIME [epoch: 2.75 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33702557742817546		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.33702557742817546 | validation: 0.3332196365085941]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_981.pth
	Model improved!!!
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.334552277580155		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.334552277580155 | validation: 0.4052501725541062]
	TIME [epoch: 2.75 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33177562381433084		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.33177562381433084 | validation: 0.36070236087843943]
	TIME [epoch: 2.75 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33168587877570654		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.33168587877570654 | validation: 0.42546577231004284]
	TIME [epoch: 2.75 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33342749286437284		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.33342749286437284 | validation: 0.32811389352371906]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_985.pth
	Model improved!!!
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3350613807221913		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.3350613807221913 | validation: 0.43256036305768886]
	TIME [epoch: 2.73 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33886803169750523		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.33886803169750523 | validation: 0.33866061489127947]
	TIME [epoch: 2.74 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33141403542043935		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.33141403542043935 | validation: 0.3940883350969945]
	TIME [epoch: 2.74 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3375710828484418		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.3375710828484418 | validation: 0.36925134959277756]
	TIME [epoch: 2.74 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34268345682113577		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.34268345682113577 | validation: 0.39385144785348236]
	TIME [epoch: 2.73 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3340716984744734		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.3340716984744734 | validation: 0.38087842895068624]
	TIME [epoch: 2.74 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3406042613275788		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.3406042613275788 | validation: 0.35908747772382216]
	TIME [epoch: 2.75 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32256575777001717		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.32256575777001717 | validation: 0.3943270406936457]
	TIME [epoch: 2.75 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3159156308423225		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.3159156308423225 | validation: 0.33107500509777027]
	TIME [epoch: 2.75 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3183563418747635		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.3183563418747635 | validation: 0.42353999071461684]
	TIME [epoch: 2.74 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3265930954345309		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.3265930954345309 | validation: 0.3243208639898742]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_996.pth
	Model improved!!!
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3329184484221582		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.3329184484221582 | validation: 0.4071794601910245]
	TIME [epoch: 2.73 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3270152342493943		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.3270152342493943 | validation: 0.323706323345173]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_998.pth
	Model improved!!!
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31902026990760135		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.31902026990760135 | validation: 0.388582794498338]
	TIME [epoch: 2.75 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31010302527942163		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.31010302527942163 | validation: 0.33677599834304006]
	TIME [epoch: 2.75 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3062204304076448		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.3062204304076448 | validation: 0.3722246936017141]
	TIME [epoch: 176 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3059371396960795		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.3059371396960795 | validation: 0.35487240497060757]
	TIME [epoch: 5.87 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3004517339299712		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.3004517339299712 | validation: 0.37466531206791603]
	TIME [epoch: 5.87 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3051479381085014		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.3051479381085014 | validation: 0.3298440875571343]
	TIME [epoch: 5.87 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30870630390842746		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.30870630390842746 | validation: 0.3989246064190214]
	TIME [epoch: 5.86 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3204324857607304		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.3204324857607304 | validation: 0.34166764283489476]
	TIME [epoch: 5.87 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3444680492686097		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.3444680492686097 | validation: 0.3877318517429265]
	TIME [epoch: 5.87 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32131645186465746		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.32131645186465746 | validation: 0.3650963814494366]
	TIME [epoch: 5.87 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29904498907226396		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.29904498907226396 | validation: 0.33006772636847925]
	TIME [epoch: 5.88 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30191920815243684		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.30191920815243684 | validation: 0.4066144625848856]
	TIME [epoch: 5.86 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30204805313886307		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.30204805313886307 | validation: 0.30231347818702936]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_1011.pth
	Model improved!!!
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3178218662422812		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.3178218662422812 | validation: 0.42018798029678345]
	TIME [epoch: 5.86 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3103231291439952		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.3103231291439952 | validation: 0.3142929602607392]
	TIME [epoch: 5.86 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29531694927076735		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.29531694927076735 | validation: 0.3758803167849443]
	TIME [epoch: 5.86 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2929555689495214		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.2929555689495214 | validation: 0.34627658147117363]
	TIME [epoch: 5.86 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28908093009278707		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.28908093009278707 | validation: 0.3451872163762728]
	TIME [epoch: 5.86 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2869656751246259		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.2869656751246259 | validation: 0.3494706571384503]
	TIME [epoch: 5.87 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2938707435293155		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.2938707435293155 | validation: 0.3841964650252786]
	TIME [epoch: 5.86 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30356194465036185		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.30356194465036185 | validation: 0.3571193218952904]
	TIME [epoch: 5.87 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31356985688275774		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.31356985688275774 | validation: 0.3861062300221857]
	TIME [epoch: 5.86 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30254083739940335		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.30254083739940335 | validation: 0.33824207553677277]
	TIME [epoch: 5.87 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2872054285493246		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.2872054285493246 | validation: 0.3549361362707738]
	TIME [epoch: 5.87 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2847942576636586		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.2847942576636586 | validation: 0.345733058060243]
	TIME [epoch: 5.86 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28833760922242857		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.28833760922242857 | validation: 0.34932005621413265]
	TIME [epoch: 5.87 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2853229518807014		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.2853229518807014 | validation: 0.3604289640082236]
	TIME [epoch: 5.86 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29302747545416624		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.29302747545416624 | validation: 0.3305100970369009]
	TIME [epoch: 5.86 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.286014119526459		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.286014119526459 | validation: 0.4239518996436239]
	TIME [epoch: 5.86 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3089575161413395		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.3089575161413395 | validation: 0.29004460976693036]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_1028.pth
	Model improved!!!
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.313016028897895		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.313016028897895 | validation: 0.4103506828888909]
	TIME [epoch: 5.87 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2923521286569497		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.2923521286569497 | validation: 0.3474541237996366]
	TIME [epoch: 5.86 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2804073400759363		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.2804073400759363 | validation: 0.32906450139334337]
	TIME [epoch: 5.87 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2755011348938429		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.2755011348938429 | validation: 0.37736019739110654]
	TIME [epoch: 5.86 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28139895755216243		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.28139895755216243 | validation: 0.31342064695801874]
	TIME [epoch: 5.87 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2909239384915718		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.2909239384915718 | validation: 0.3950703397125098]
	TIME [epoch: 5.87 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29034758805592203		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.29034758805592203 | validation: 0.32106173004595107]
	TIME [epoch: 5.87 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2910347967491464		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.2910347967491464 | validation: 0.37600302443423567]
	TIME [epoch: 5.86 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28317427761907094		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.28317427761907094 | validation: 0.332998838344005]
	TIME [epoch: 5.87 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27417885617251286		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.27417885617251286 | validation: 0.3384329994815903]
	TIME [epoch: 5.86 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27629800132170196		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.27629800132170196 | validation: 0.36277579460830733]
	TIME [epoch: 5.86 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2730662772566281		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.2730662772566281 | validation: 0.3215018975896799]
	TIME [epoch: 5.86 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.273192873061047		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.273192873061047 | validation: 0.36854103769265767]
	TIME [epoch: 5.86 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2726836728411412		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.2726836728411412 | validation: 0.3055532162966452]
	TIME [epoch: 5.87 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2809663790696954		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.2809663790696954 | validation: 0.38232496576097486]
	TIME [epoch: 5.89 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27401728334015024		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.27401728334015024 | validation: 0.3119206784293167]
	TIME [epoch: 5.87 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27489724912632085		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.27489724912632085 | validation: 0.368863171065938]
	TIME [epoch: 5.87 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27359684281410723		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.27359684281410723 | validation: 0.3221524904510209]
	TIME [epoch: 5.88 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2740481222164442		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.2740481222164442 | validation: 0.3776223267483084]
	TIME [epoch: 5.88 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28811642220249134		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.28811642220249134 | validation: 0.32548460713352056]
	TIME [epoch: 5.88 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28740257759111953		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.28740257759111953 | validation: 0.3635083794895111]
	TIME [epoch: 5.87 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26947252238354774		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.26947252238354774 | validation: 0.3232493568957847]
	TIME [epoch: 5.87 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25992465050848856		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.25992465050848856 | validation: 0.3599109398508358]
	TIME [epoch: 5.87 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2627356410433471		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.2627356410433471 | validation: 0.33251197664734083]
	TIME [epoch: 5.86 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2624576285140975		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.2624576285140975 | validation: 0.3484747253667102]
	TIME [epoch: 5.86 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26170655941556925		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.26170655941556925 | validation: 0.30044991714247526]
	TIME [epoch: 5.86 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2627020684646395		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.2627020684646395 | validation: 0.377236625388333]
	TIME [epoch: 5.87 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2618688606784461		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.2618688606784461 | validation: 0.29005190617446647]
	TIME [epoch: 5.87 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2800604157647623		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.2800604157647623 | validation: 0.3829386839325216]
	TIME [epoch: 5.87 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26919467262265445		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.26919467262265445 | validation: 0.3214336285334475]
	TIME [epoch: 5.86 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2557983418920858		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.2557983418920858 | validation: 0.347666771292087]
	TIME [epoch: 5.87 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2553241698853839		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.2553241698853839 | validation: 0.36131572218682395]
	TIME [epoch: 5.87 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2687975173709509		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.2687975173709509 | validation: 0.34203291469934927]
	TIME [epoch: 5.87 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27523808989055987		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.27523808989055987 | validation: 0.33059041543442946]
	TIME [epoch: 5.86 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2572133836276705		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.2572133836276705 | validation: 0.3450904388667111]
	TIME [epoch: 5.88 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2529741420043291		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.2529741420043291 | validation: 0.3106913752069569]
	TIME [epoch: 5.87 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25316212286415485		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.25316212286415485 | validation: 0.3586245190604687]
	TIME [epoch: 5.87 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2566474003055674		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.2566474003055674 | validation: 0.30847554162618146]
	TIME [epoch: 5.87 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2530001252682788		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.2530001252682788 | validation: 0.37087798473422495]
	TIME [epoch: 5.87 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26665267341554516		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.26665267341554516 | validation: 0.2960591162351795]
	TIME [epoch: 5.87 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.269335431861245		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.269335431861245 | validation: 0.3483814667764391]
	TIME [epoch: 5.87 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25447583321981415		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.25447583321981415 | validation: 0.3104306758361028]
	TIME [epoch: 5.89 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24790632926921102		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.24790632926921102 | validation: 0.3412523570202006]
	TIME [epoch: 5.87 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24208090832816076		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.24208090832816076 | validation: 0.3199733627819392]
	TIME [epoch: 5.87 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24484738312564142		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.24484738312564142 | validation: 0.33031861019250314]
	TIME [epoch: 5.87 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2423891131622979		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.2423891131622979 | validation: 0.3109646850696275]
	TIME [epoch: 5.87 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24037663833943324		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.24037663833943324 | validation: 0.31803561104392575]
	TIME [epoch: 5.88 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24122211869376364		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.24122211869376364 | validation: 0.32670294869320476]
	TIME [epoch: 5.87 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24308599746998194		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.24308599746998194 | validation: 0.3043642583966275]
	TIME [epoch: 5.87 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2515068896501261		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.2515068896501261 | validation: 0.396931507983295]
	TIME [epoch: 5.88 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2656039298270912		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.2656039298270912 | validation: 0.30014343546111627]
	TIME [epoch: 5.87 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25310563518996754		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.25310563518996754 | validation: 0.35975709356911767]
	TIME [epoch: 5.87 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2411805488071709		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.2411805488071709 | validation: 0.30816526060081134]
	TIME [epoch: 5.87 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.266708158186874		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.266708158186874 | validation: 0.3641229729278956]
	TIME [epoch: 5.87 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25772083477756313		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.25772083477756313 | validation: 0.3003652740374116]
	TIME [epoch: 5.87 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24354491245334559		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.24354491245334559 | validation: 0.3432675131279104]
	TIME [epoch: 5.87 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23437322290449808		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.23437322290449808 | validation: 0.30395818811662845]
	TIME [epoch: 5.87 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23544259931795136		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.23544259931795136 | validation: 0.33934484668397147]
	TIME [epoch: 5.87 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23896651584776699		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.23896651584776699 | validation: 0.33443907127058453]
	TIME [epoch: 5.87 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23655969352077535		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.23655969352077535 | validation: 0.31770197719350207]
	TIME [epoch: 5.87 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2430358149280761		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.2430358149280761 | validation: 0.3456737414464179]
	TIME [epoch: 5.87 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24097882258918674		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.24097882258918674 | validation: 0.28705886647233425]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_1090.pth
	Model improved!!!
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.251691799514695		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.251691799514695 | validation: 0.3508339671401191]
	TIME [epoch: 5.86 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24825897700867516		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.24825897700867516 | validation: 0.30429317979404713]
	TIME [epoch: 5.87 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23696601781360918		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.23696601781360918 | validation: 0.32691396932808386]
	TIME [epoch: 5.87 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23119878381105768		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.23119878381105768 | validation: 0.33257362818161185]
	TIME [epoch: 5.87 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23061906592484732		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.23061906592484732 | validation: 0.310616557195347]
	TIME [epoch: 5.87 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23607952129102988		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.23607952129102988 | validation: 0.3354288473628253]
	TIME [epoch: 5.87 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2333735125169482		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.2333735125169482 | validation: 0.28961501835224945]
	TIME [epoch: 5.87 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23573369135423805		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.23573369135423805 | validation: 0.3661273017560722]
	TIME [epoch: 5.87 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24580332639588925		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.24580332639588925 | validation: 0.2941126837607881]
	TIME [epoch: 5.86 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2404956708904232		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.2404956708904232 | validation: 0.35461700256573647]
	TIME [epoch: 5.88 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23617898608763388		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.23617898608763388 | validation: 0.2990769549426722]
	TIME [epoch: 5.87 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23318387300472837		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.23318387300472837 | validation: 0.3420515311178103]
	TIME [epoch: 5.88 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23321643576518838		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.23321643576518838 | validation: 0.3009616735139621]
	TIME [epoch: 5.87 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23102903058871754		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.23102903058871754 | validation: 0.35042392067062816]
	TIME [epoch: 5.87 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23024493218098113		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.23024493218098113 | validation: 0.2872608524906596]
	TIME [epoch: 5.87 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22906037324933398		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.22906037324933398 | validation: 0.33390705375664825]
	TIME [epoch: 5.87 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2306561876676461		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.2306561876676461 | validation: 0.2947415350212714]
	TIME [epoch: 5.87 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24307767654255497		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.24307767654255497 | validation: 0.35009775118863196]
	TIME [epoch: 5.87 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24438828057468556		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.24438828057468556 | validation: 0.3210023055439548]
	TIME [epoch: 5.87 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22624973985568952		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.22624973985568952 | validation: 0.31525649494818997]
	TIME [epoch: 5.88 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2218051889125731		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.2218051889125731 | validation: 0.3309774550003025]
	TIME [epoch: 5.86 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22537941784864082		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.22537941784864082 | validation: 0.2977086842705339]
	TIME [epoch: 5.87 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22818291708569194		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.22818291708569194 | validation: 0.3488837368040498]
	TIME [epoch: 5.86 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22538311661307886		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.22538311661307886 | validation: 0.2950976548318886]
	TIME [epoch: 5.87 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2252690954197806		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.2252690954197806 | validation: 0.34257239711349974]
	TIME [epoch: 5.87 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22244030287751282		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.22244030287751282 | validation: 0.3198692070441521]
	TIME [epoch: 5.86 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2193764791756257		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.2193764791756257 | validation: 0.3137767076108026]
	TIME [epoch: 5.87 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21614049568043592		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.21614049568043592 | validation: 0.3189902965022027]
	TIME [epoch: 5.87 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2158231138844227		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.2158231138844227 | validation: 0.27583849305224417]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_1119.pth
	Model improved!!!
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22195330913408318		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.22195330913408318 | validation: 0.35271370056948576]
	TIME [epoch: 5.86 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22595237622060754		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.22595237622060754 | validation: 0.2786766899615323]
	TIME [epoch: 5.86 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23118914968742524		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.23118914968742524 | validation: 0.3390114370374313]
	TIME [epoch: 5.87 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2273003456503709		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.2273003456503709 | validation: 0.30380237753632233]
	TIME [epoch: 5.86 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21954609398592262		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.21954609398592262 | validation: 0.31863139161253273]
	TIME [epoch: 5.87 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21260741832475064		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.21260741832475064 | validation: 0.3310996423035773]
	TIME [epoch: 5.87 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21745483831548462		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.21745483831548462 | validation: 0.2916212940554769]
	TIME [epoch: 5.87 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2186249085308245		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.2186249085308245 | validation: 0.33511012918509775]
	TIME [epoch: 5.86 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21860483719765206		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.21860483719765206 | validation: 0.2999824653263654]
	TIME [epoch: 5.87 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21511324151861522		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.21511324151861522 | validation: 0.3081036016524714]
	TIME [epoch: 5.87 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2192259394878218		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.2192259394878218 | validation: 0.30944682534143475]
	TIME [epoch: 5.86 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.217632073165965		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.217632073165965 | validation: 0.3351784732697673]
	TIME [epoch: 5.87 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21338100737170607		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.21338100737170607 | validation: 0.3267806897100014]
	TIME [epoch: 5.86 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21833756436834384		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.21833756436834384 | validation: 0.2884019950998294]
	TIME [epoch: 5.87 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22411529282008325		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.22411529282008325 | validation: 0.3616710556664411]
	TIME [epoch: 5.86 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2268327685931692		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.2268327685931692 | validation: 0.2820069675483176]
	TIME [epoch: 5.86 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21730299236273845		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.21730299236273845 | validation: 0.3325067243001947]
	TIME [epoch: 5.87 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20817885669107047		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.20817885669107047 | validation: 0.31535818633999635]
	TIME [epoch: 5.86 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21143110671845378		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.21143110671845378 | validation: 0.3115572364097577]
	TIME [epoch: 5.86 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2081349005857409		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.2081349005857409 | validation: 0.33588402351466684]
	TIME [epoch: 5.86 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20970247645504486		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.20970247645504486 | validation: 0.29852666083373847]
	TIME [epoch: 5.86 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20723634254223425		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.20723634254223425 | validation: 0.3241581631535871]
	TIME [epoch: 5.87 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20348557625730437		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.20348557625730437 | validation: 0.29378927904711905]
	TIME [epoch: 5.86 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20725750568867007		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.20725750568867007 | validation: 0.3393890282456843]
	TIME [epoch: 5.87 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21521908198808248		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.21521908198808248 | validation: 0.268070115306263]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_1144.pth
	Model improved!!!
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22957940456967352		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.22957940456967352 | validation: 0.33890612341777226]
	TIME [epoch: 5.86 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22044419879865756		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.22044419879865756 | validation: 0.29862106041260666]
	TIME [epoch: 5.87 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20527562841698374		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.20527562841698374 | validation: 0.30117603771166285]
	TIME [epoch: 5.86 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2048402078515407		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.2048402078515407 | validation: 0.30889218073059893]
	TIME [epoch: 5.86 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20463630772158317		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.20463630772158317 | validation: 0.29463826667368354]
	TIME [epoch: 5.86 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20215461220950268		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.20215461220950268 | validation: 0.35007675771357855]
	TIME [epoch: 5.86 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21326354957222457		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.21326354957222457 | validation: 0.28098420232849713]
	TIME [epoch: 5.87 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21238067308703337		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.21238067308703337 | validation: 0.32973158996058266]
	TIME [epoch: 5.86 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20577260794682217		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.20577260794682217 | validation: 0.2850592948746783]
	TIME [epoch: 5.87 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20947372116387122		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.20947372116387122 | validation: 0.3279586215783118]
	TIME [epoch: 5.86 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19699390259751792		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.19699390259751792 | validation: 0.28689165549914225]
	TIME [epoch: 5.86 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20302775637684847		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.20302775637684847 | validation: 0.3227302552829369]
	TIME [epoch: 5.87 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2037944514541568		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.2037944514541568 | validation: 0.29890742897308564]
	TIME [epoch: 5.86 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19898268151590212		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.19898268151590212 | validation: 0.30300295401281085]
	TIME [epoch: 5.86 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2006726523655019		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.2006726523655019 | validation: 0.2899836955803724]
	TIME [epoch: 5.86 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20407203690413045		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.20407203690413045 | validation: 0.3141463872435373]
	TIME [epoch: 5.86 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20395612349852588		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.20395612349852588 | validation: 0.2837734183606229]
	TIME [epoch: 5.87 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21312557302721347		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.21312557302721347 | validation: 0.3181426791788786]
	TIME [epoch: 5.86 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19716984873771792		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.19716984873771792 | validation: 0.29340755534805124]
	TIME [epoch: 5.86 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20068261652942632		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.20068261652942632 | validation: 0.317302058935222]
	TIME [epoch: 5.86 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19981447075393674		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.19981447075393674 | validation: 0.29195220907493247]
	TIME [epoch: 5.86 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19743220894391303		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.19743220894391303 | validation: 0.3134589178808585]
	TIME [epoch: 5.87 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19810557675730756		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.19810557675730756 | validation: 0.2943506638326386]
	TIME [epoch: 5.86 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19124170519494216		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.19124170519494216 | validation: 0.31231082357841594]
	TIME [epoch: 5.86 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1932688282779833		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.1932688282779833 | validation: 0.2975220800752961]
	TIME [epoch: 5.86 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19240953988086248		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.19240953988086248 | validation: 0.3035928836950072]
	TIME [epoch: 5.86 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19331207444585147		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.19331207444585147 | validation: 0.33092211650328895]
	TIME [epoch: 5.87 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19761444344717916		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.19761444344717916 | validation: 0.2848622794774627]
	TIME [epoch: 5.86 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21280148688481418		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.21280148688481418 | validation: 0.31532388922935695]
	TIME [epoch: 5.86 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20049192015243228		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.20049192015243228 | validation: 0.30980364767013263]
	TIME [epoch: 5.86 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19393222947118027		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.19393222947118027 | validation: 0.2642640624711346]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_1175.pth
	Model improved!!!
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20689067367992717		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.20689067367992717 | validation: 0.3410103239320023]
	TIME [epoch: 5.89 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20543707930966734		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.20543707930966734 | validation: 0.2857013145243776]
	TIME [epoch: 5.88 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1928589990241882		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.1928589990241882 | validation: 0.31011328781316644]
	TIME [epoch: 5.89 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1893806314166301		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.1893806314166301 | validation: 0.28602291358230536]
	TIME [epoch: 5.88 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19007480037617272		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.19007480037617272 | validation: 0.3046175088743612]
	TIME [epoch: 5.89 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1894921847440068		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.1894921847440068 | validation: 0.279909783379554]
	TIME [epoch: 5.89 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19201738579029054		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.19201738579029054 | validation: 0.32462478370589726]
	TIME [epoch: 5.88 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.197718945387053		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.197718945387053 | validation: 0.28392962657886733]
	TIME [epoch: 5.89 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20218810097539705		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.20218810097539705 | validation: 0.3267701800568529]
	TIME [epoch: 5.88 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1935239520498838		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.1935239520498838 | validation: 0.2768777591929517]
	TIME [epoch: 5.88 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19128173411358504		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.19128173411358504 | validation: 0.318371094966736]
	TIME [epoch: 5.89 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18933946259566337		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.18933946259566337 | validation: 0.2902677449309782]
	TIME [epoch: 5.88 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18839006262355937		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.18839006262355937 | validation: 0.30782297377752377]
	TIME [epoch: 5.89 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18694821617204976		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.18694821617204976 | validation: 0.29732810957310324]
	TIME [epoch: 5.88 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1843168827248407		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.1843168827248407 | validation: 0.29838806821128633]
	TIME [epoch: 5.88 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18539221642649256		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.18539221642649256 | validation: 0.30731925608531985]
	TIME [epoch: 5.89 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18598706254323744		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.18598706254323744 | validation: 0.3073809277584363]
	TIME [epoch: 5.89 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18561404746784987		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.18561404746784987 | validation: 0.3004571209832751]
	TIME [epoch: 5.89 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18188119004883455		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.18188119004883455 | validation: 0.31322892958768955]
	TIME [epoch: 5.92 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18676574630035858		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.18676574630035858 | validation: 0.25761535564587157]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_1195.pth
	Model improved!!!
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1948116940051601		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.1948116940051601 | validation: 0.3290966812049817]
	TIME [epoch: 5.85 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20380983170208755		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.20380983170208755 | validation: 0.26628137337909624]
	TIME [epoch: 5.85 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19498906094411744		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.19498906094411744 | validation: 0.3022631759948753]
	TIME [epoch: 5.87 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18262914591812887		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.18262914591812887 | validation: 0.2939087160271365]
	TIME [epoch: 5.86 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18688550759655645		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.18688550759655645 | validation: 0.277665229944718]
	TIME [epoch: 5.89 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18583718199817312		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.18583718199817312 | validation: 0.3245072106287844]
	TIME [epoch: 5.86 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19624278793908867		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.19624278793908867 | validation: 0.2650864111127131]
	TIME [epoch: 5.85 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.197204629896767		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.197204629896767 | validation: 0.30094438924185973]
	TIME [epoch: 5.85 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18421121692375408		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.18421121692375408 | validation: 0.2939636122848432]
	TIME [epoch: 5.85 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1809644865679421		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.1809644865679421 | validation: 0.29589924184206184]
	TIME [epoch: 5.85 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18168481850979715		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.18168481850979715 | validation: 0.29546574440384304]
	TIME [epoch: 5.86 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17993379849810354		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.17993379849810354 | validation: 0.3010563503177459]
	TIME [epoch: 5.85 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17820154967500224		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.17820154967500224 | validation: 0.29639633174724717]
	TIME [epoch: 5.86 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1846635567091885		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.1846635567091885 | validation: 0.30565202945122844]
	TIME [epoch: 5.85 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18268662565912233		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.18268662565912233 | validation: 0.2981931849761867]
	TIME [epoch: 5.85 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1819573444832651		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.1819573444832651 | validation: 0.2987649689340559]
	TIME [epoch: 5.86 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1824580948698673		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.1824580948698673 | validation: 0.2971883919679168]
	TIME [epoch: 5.85 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1776184906133269		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.1776184906133269 | validation: 0.31305795544147214]
	TIME [epoch: 5.86 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17966496717944316		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.17966496717944316 | validation: 0.2831063462835454]
	TIME [epoch: 5.86 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1841614443856053		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.1841614443856053 | validation: 0.33855880822409434]
	TIME [epoch: 5.85 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19749404254342134		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.19749404254342134 | validation: 0.2621796120724212]
	TIME [epoch: 5.87 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19900124851861592		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.19900124851861592 | validation: 0.30535250321857615]
	TIME [epoch: 5.85 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18737600471216304		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.18737600471216304 | validation: 0.3102863491445356]
	TIME [epoch: 5.86 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18294778251203503		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.18294778251203503 | validation: 0.28030696392984195]
	TIME [epoch: 5.85 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17713484625616424		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.17713484625616424 | validation: 0.30577934496608217]
	TIME [epoch: 5.85 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17495953474919212		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.17495953474919212 | validation: 0.3004962573728688]
	TIME [epoch: 5.85 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1754277878968814		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.1754277878968814 | validation: 0.3120919293587693]
	TIME [epoch: 5.86 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1780753132994429		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.1780753132994429 | validation: 0.29267911592748386]
	TIME [epoch: 5.85 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17785678673968008		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.17785678673968008 | validation: 0.3016086812942043]
	TIME [epoch: 5.86 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17462958100686515		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.17462958100686515 | validation: 0.28588791735407487]
	TIME [epoch: 5.85 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17868462983213662		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.17868462983213662 | validation: 0.31728494431712695]
	TIME [epoch: 5.86 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18330465888364803		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.18330465888364803 | validation: 0.3107835643960868]
	TIME [epoch: 5.85 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17760916448017608		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.17760916448017608 | validation: 0.28813739339615074]
	TIME [epoch: 5.86 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17330018971903363		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.17330018971903363 | validation: 0.32635985476534446]
	TIME [epoch: 5.85 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18216959212343412		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.18216959212343412 | validation: 0.26501039064639315]
	TIME [epoch: 5.85 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18714028689908485		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.18714028689908485 | validation: 0.3230968578795892]
	TIME [epoch: 5.85 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17733380984852226		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.17733380984852226 | validation: 0.2832007062015761]
	TIME [epoch: 5.86 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17145837348841636		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.17145837348841636 | validation: 0.304731781693949]
	TIME [epoch: 5.85 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17520825812561197		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.17520825812561197 | validation: 0.2944679969853204]
	TIME [epoch: 5.86 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17768098713428163		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.17768098713428163 | validation: 0.297033394601376]
	TIME [epoch: 5.85 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1790625502053092		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.1790625502053092 | validation: 0.30313121071557697]
	TIME [epoch: 5.86 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.176130696735148		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.176130696735148 | validation: 0.28613885939450157]
	TIME [epoch: 5.86 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16943115916118143		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.16943115916118143 | validation: 0.2928296555976761]
	TIME [epoch: 5.85 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16693771434416735		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.16693771434416735 | validation: 0.29240546767150993]
	TIME [epoch: 5.85 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1739558355251865		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.1739558355251865 | validation: 0.29837473380434926]
	TIME [epoch: 5.85 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16878555224981953		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.16878555224981953 | validation: 0.2772781025195575]
	TIME [epoch: 5.85 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17754099096902276		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.17754099096902276 | validation: 0.33709022077138057]
	TIME [epoch: 5.85 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17696014780976022		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.17696014780976022 | validation: 0.282863610200638]
	TIME [epoch: 5.84 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17830280016152444		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.17830280016152444 | validation: 0.3093936022884618]
	TIME [epoch: 5.86 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17398114724438946		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.17398114724438946 | validation: 0.30166811464033244]
	TIME [epoch: 5.85 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1704717917862306		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.1704717917862306 | validation: 0.2912613983288302]
	TIME [epoch: 5.86 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1687716244685964		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.1687716244685964 | validation: 0.289233780984988]
	TIME [epoch: 5.85 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16564062109980746		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.16564062109980746 | validation: 0.29968738456946076]
	TIME [epoch: 5.86 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1732338499097353		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.1732338499097353 | validation: 0.2926885568454986]
	TIME [epoch: 5.84 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17257625926551773		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.17257625926551773 | validation: 0.28849533625767715]
	TIME [epoch: 5.86 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17193587160605348		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.17193587160605348 | validation: 0.30315190560538063]
	TIME [epoch: 5.85 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17209075626853454		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.17209075626853454 | validation: 0.2897881381246588]
	TIME [epoch: 5.86 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17148364100312605		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.17148364100312605 | validation: 0.32567712196316595]
	TIME [epoch: 5.85 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1754065737171404		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.1754065737171404 | validation: 0.265273717971571]
	TIME [epoch: 5.86 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1828076953067482		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.1828076953067482 | validation: 0.31051620735405516]
	TIME [epoch: 5.85 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1678359913597118		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.1678359913597118 | validation: 0.2987917129383156]
	TIME [epoch: 5.85 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1686872117922627		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.1686872117922627 | validation: 0.28998028202756515]
	TIME [epoch: 5.86 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16651349978084126		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.16651349978084126 | validation: 0.2945683892085592]
	TIME [epoch: 5.85 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16806720573770295		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.16806720573770295 | validation: 0.29531579741443553]
	TIME [epoch: 5.85 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17170334792786712		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.17170334792786712 | validation: 0.27136552668553016]
	TIME [epoch: 5.86 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1675617770694839		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.1675617770694839 | validation: 0.3065803092709864]
	TIME [epoch: 5.85 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1666706304440337		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.1666706304440337 | validation: 0.2641555857587856]
	TIME [epoch: 5.86 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17719669394199464		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.17719669394199464 | validation: 0.3033318483417805]
	TIME [epoch: 5.87 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17085311719264973		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.17085311719264973 | validation: 0.2848296812058835]
	TIME [epoch: 5.88 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16750915993747062		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.16750915993747062 | validation: 0.29161994521704787]
	TIME [epoch: 5.87 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16206796666642986		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.16206796666642986 | validation: 0.3176276191986605]
	TIME [epoch: 5.87 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17115257413845025		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.17115257413845025 | validation: 0.27191988278579765]
	TIME [epoch: 5.88 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1749511947850707		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.1749511947850707 | validation: 0.3030038999805699]
	TIME [epoch: 5.87 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16353467809960137		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.16353467809960137 | validation: 0.2916606834348651]
	TIME [epoch: 5.88 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.162908145547487		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.162908145547487 | validation: 0.2882349790070694]
	TIME [epoch: 5.88 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16763016038247028		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.16763016038247028 | validation: 0.30630800175418554]
	TIME [epoch: 5.87 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1730186427792543		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.1730186427792543 | validation: 0.2903146836377219]
	TIME [epoch: 5.86 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16638009057629416		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.16638009057629416 | validation: 0.3004043624472887]
	TIME [epoch: 5.85 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1602876736959887		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.1602876736959887 | validation: 0.29304172954732527]
	TIME [epoch: 5.85 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16436750183906113		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.16436750183906113 | validation: 0.27777166211774657]
	TIME [epoch: 5.85 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1635565513855138		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.1635565513855138 | validation: 0.3064301083393176]
	TIME [epoch: 5.85 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16785667995827247		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.16785667995827247 | validation: 0.2637880619529371]
	TIME [epoch: 5.85 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1655194161095821		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.1655194161095821 | validation: 0.2997051865821911]
	TIME [epoch: 5.85 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16460131372362596		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.16460131372362596 | validation: 0.2866215209389446]
	TIME [epoch: 5.88 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16237427674410299		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.16237427674410299 | validation: 0.2794622971079734]
	TIME [epoch: 5.85 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15817377828466708		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.15817377828466708 | validation: 0.27786579093395275]
	TIME [epoch: 5.85 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1609247941118545		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.1609247941118545 | validation: 0.3136893062359943]
	TIME [epoch: 5.85 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1698555655263045		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.1698555655263045 | validation: 0.25823226858911397]
	TIME [epoch: 5.85 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17687476412279332		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.17687476412279332 | validation: 0.29234287758478555]
	TIME [epoch: 5.86 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1660086493944867		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.1660086493944867 | validation: 0.2852123335454438]
	TIME [epoch: 5.85 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1614925740674032		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.1614925740674032 | validation: 0.26522721563489293]
	TIME [epoch: 5.85 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16002143179800066		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.16002143179800066 | validation: 0.3125464526723399]
	TIME [epoch: 5.85 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16361246783964262		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.16361246783964262 | validation: 0.28567904283514445]
	TIME [epoch: 5.87 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16371875864832497		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.16371875864832497 | validation: 0.302822416223537]
	TIME [epoch: 5.86 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16188362592776329		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.16188362592776329 | validation: 0.27450023567231224]
	TIME [epoch: 5.87 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16458410211835156		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.16458410211835156 | validation: 0.30014857889874125]
	TIME [epoch: 5.86 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17859422014597726		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.17859422014597726 | validation: 0.29706191514375163]
	TIME [epoch: 5.86 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1649129917234044		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.1649129917234044 | validation: 0.27057422331227854]
	TIME [epoch: 5.86 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16268423932989173		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.16268423932989173 | validation: 0.28794861351140977]
	TIME [epoch: 5.86 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1638328033909595		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.1638328033909595 | validation: 0.29650410995050996]
	TIME [epoch: 5.87 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16272165653689696		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.16272165653689696 | validation: 0.2684654116523276]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_5_v_mmd4_1296.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 4449.303 seconds.
