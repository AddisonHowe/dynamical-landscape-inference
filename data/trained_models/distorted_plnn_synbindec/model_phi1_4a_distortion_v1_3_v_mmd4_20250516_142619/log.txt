Args:
Namespace(name='model_phi1_4a_distortion_v1_3_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_3/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_3/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.048453342, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 589727581

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.775088308900491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.775088308900491 | validation: 6.08609866584542]
	TIME [epoch: 164 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.506736197736752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.506736197736752 | validation: 6.555735496391939]
	TIME [epoch: 0.764 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.515210741417918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.515210741417918 | validation: 6.22338282341245]
	TIME [epoch: 0.693 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.904001048477373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.904001048477373 | validation: 5.716837560501386]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.271444959262303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.271444959262303 | validation: 5.550852620021164]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.312072987758392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.312072987758392 | validation: 5.79454288118084]
	TIME [epoch: 0.694 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.149123341348229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.149123341348229 | validation: 5.693127863766591]
	TIME [epoch: 0.692 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9729678217555673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9729678217555673 | validation: 5.529591396806044]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9620635454880397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9620635454880397 | validation: 5.579827239914007]
	TIME [epoch: 0.694 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8216301411277813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8216301411277813 | validation: 5.439731501118962]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6840193404251487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6840193404251487 | validation: 5.374346864092455]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.653145027053339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.653145027053339 | validation: 5.4349062161462545]
	TIME [epoch: 0.691 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.61182015859797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.61182015859797 | validation: 5.340644350108502]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4785335377975763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4785335377975763 | validation: 5.307419395360967]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4398743275110117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4398743275110117 | validation: 5.331039966499195]
	TIME [epoch: 0.695 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4210348351257456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4210348351257456 | validation: 5.314384245940087]
	TIME [epoch: 0.693 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.376359517893601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.376359517893601 | validation: 5.282751836887758]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3454379068223057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3454379068223057 | validation: 5.246214095994411]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3120057650720005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3120057650720005 | validation: 5.2343654874022185]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.288292072032354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.288292072032354 | validation: 5.20968021246625]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2693105220621423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2693105220621423 | validation: 5.187057775656432]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2462809914295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2462809914295 | validation: 5.164397099567811]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.230449156418824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.230449156418824 | validation: 5.151566294735286]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2196689740878344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2196689740878344 | validation: 5.127543018352125]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.209948698432989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.209948698432989 | validation: 5.104368258798172]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1935372857447875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1935372857447875 | validation: 5.108975960745596]
	TIME [epoch: 0.69 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2166097078226734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2166097078226734 | validation: 5.076119267829484]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.225170729170062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.225170729170062 | validation: 5.091476240465976]
	TIME [epoch: 0.692 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2725953710432094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2725953710432094 | validation: 5.045391596103158]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1466122118272937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1466122118272937 | validation: 5.011447395127583]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.155865779037753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.155865779037753 | validation: 5.007630445971678]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.170405586230362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.170405586230362 | validation: 4.9935594195884825]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.113893646695022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.113893646695022 | validation: 4.940496566759903]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1160738687813048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1160738687813048 | validation: 4.950231055667118]
	TIME [epoch: 0.695 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.093507720891925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.093507720891925 | validation: 4.908240244747469]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0782162137391635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0782162137391635 | validation: 4.886411338910769]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0588047403248106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0588047403248106 | validation: 4.856297158324431]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0606570259768553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0606570259768553 | validation: 4.916971987517916]
	TIME [epoch: 0.695 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0979462017406685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0979462017406685 | validation: 4.887873132779291]
	TIME [epoch: 0.693 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2125157610706445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2125157610706445 | validation: 4.793661402834036]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.029478552916153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.029478552916153 | validation: 4.850093412486604]
	TIME [epoch: 0.695 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.079370466718079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.079370466718079 | validation: 4.762056328088412]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.062296513086177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.062296513086177 | validation: 4.732398029368047]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0124022250913174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0124022250913174 | validation: 4.763080704219735]
	TIME [epoch: 0.695 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0212108464057215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0212108464057215 | validation: 4.686225543411671]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0069547043887486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0069547043887486 | validation: 4.697565604918276]
	TIME [epoch: 0.693 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.983552441319958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.983552441319958 | validation: 4.689349594679219]
	TIME [epoch: 0.693 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9735277880376594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9735277880376594 | validation: 4.65628508149875]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9644241098299604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9644241098299604 | validation: 4.654428909685599]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9762123863309764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9762123863309764 | validation: 4.63540564178155]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0147133621824596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0147133621824596 | validation: 4.6328976336756]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.982322661527053		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 2.982322661527053 | validation: 4.567340283472549]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9788325020067816		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 2.9788325020067816 | validation: 4.606309809001036]
	TIME [epoch: 0.698 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9633305271227153		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 2.9633305271227153 | validation: 4.526562190850337]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.072771892956231		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 3.072771892956231 | validation: 4.538457776466285]
	TIME [epoch: 0.694 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.933293743514446		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 2.933293743514446 | validation: 4.5393778270624185]
	TIME [epoch: 0.694 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.913080065151909		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 2.913080065151909 | validation: 4.486154243142074]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9317317208920866		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 2.9317317208920866 | validation: 4.479149618786571]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8987604074041395		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 2.8987604074041395 | validation: 4.440870617186663]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9015541430340774		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 2.9015541430340774 | validation: 4.4446063650709595]
	TIME [epoch: 0.693 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8869939359778223		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 2.8869939359778223 | validation: 4.3921803776738]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9366576021389803		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 2.9366576021389803 | validation: 4.438037534926659]
	TIME [epoch: 0.695 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.94823446265891		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 2.94823446265891 | validation: 4.344197537328863]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.940112622558024		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 2.940112622558024 | validation: 4.302521355081426]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8495821400204413		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 2.8495821400204413 | validation: 4.318466307294331]
	TIME [epoch: 0.695 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8555327101258805		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 2.8555327101258805 | validation: 4.279144029581515]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8622243513402146		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 2.8622243513402146 | validation: 4.253481914248701]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.831968522565112		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 2.831968522565112 | validation: 4.202022139937048]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.816239349061476		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 2.816239349061476 | validation: 4.182189963617831]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8010547078874213		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 2.8010547078874213 | validation: 4.132373937868968]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8303510938488388		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 2.8303510938488388 | validation: 4.524590697548987]
	TIME [epoch: 0.694 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1871971925425977		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 3.1871971925425977 | validation: 4.156744423293489]
	TIME [epoch: 0.693 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.942854691402673		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 2.942854691402673 | validation: 4.118792128844784]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8400953119326426		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 2.8400953119326426 | validation: 4.148850808630439]
	TIME [epoch: 0.695 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.820381602803967		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 2.820381602803967 | validation: 4.089026529546779]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7849828302473294		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 2.7849828302473294 | validation: 4.0406401717848475]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7882407740099984		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 2.7882407740099984 | validation: 4.0439277577508355]
	TIME [epoch: 0.696 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7603772943482507		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 2.7603772943482507 | validation: 4.004579372720653]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7556788756147177		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 2.7556788756147177 | validation: 3.9623513653640416]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7485609615647877		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 2.7485609615647877 | validation: 3.938993754619847]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7365392105376447		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 2.7365392105376447 | validation: 3.9044433379884795]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7172670120601583		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 2.7172670120601583 | validation: 3.850203427027203]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.721494676637139		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 2.721494676637139 | validation: 3.889128924799994]
	TIME [epoch: 0.696 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.737591016099251		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 2.737591016099251 | validation: 3.8102645612527946]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.732392861725407		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 2.732392861725407 | validation: 3.941830512664298]
	TIME [epoch: 0.696 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.886896113310282		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 2.886896113310282 | validation: 4.401175630295728]
	TIME [epoch: 0.697 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.157942007146278		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 3.157942007146278 | validation: 3.8677148880706698]
	TIME [epoch: 0.695 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7453252318861683		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 2.7453252318861683 | validation: 3.808652613268934]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7962306621254958		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 2.7962306621254958 | validation: 3.804231787784832]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7026325047097486		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 2.7026325047097486 | validation: 3.7878902577331477]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.712357807470363		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 2.712357807470363 | validation: 3.7486936917685343]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7094915967328244		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 2.7094915967328244 | validation: 3.7594908906558744]
	TIME [epoch: 0.695 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.687097846367444		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 2.687097846367444 | validation: 3.7607817422683554]
	TIME [epoch: 0.696 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6763489059851486		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 2.6763489059851486 | validation: 3.719288009474802]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6861814267677966		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 2.6861814267677966 | validation: 3.7340059394755283]
	TIME [epoch: 0.695 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6802937356143697		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 2.6802937356143697 | validation: 3.694905338721317]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6745850425002278		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 2.6745850425002278 | validation: 3.756756316104425]
	TIME [epoch: 0.696 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6837995537014754		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 2.6837995537014754 | validation: 3.670623321974363]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.70272893894091		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 2.70272893894091 | validation: 3.8128611207873777]
	TIME [epoch: 0.693 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.722385461662888		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 2.722385461662888 | validation: 3.6653333261787893]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7204882933659995		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 2.7204882933659995 | validation: 3.7372295185795106]
	TIME [epoch: 0.695 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6824040663295334		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 2.6824040663295334 | validation: 3.6378160627555234]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.66686153407943		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 2.66686153407943 | validation: 3.636779959070319]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6438071531091896		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 2.6438071531091896 | validation: 3.5805773523741418]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.637996292731855		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 2.637996292731855 | validation: 3.618028683275351]
	TIME [epoch: 0.695 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6344681223778394		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 2.6344681223778394 | validation: 3.5499620183900156]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.638704265189599		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 2.638704265189599 | validation: 3.595558709448861]
	TIME [epoch: 0.696 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.630284312780493		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 2.630284312780493 | validation: 3.4587232538150197]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6715336435422334		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 2.6715336435422334 | validation: 3.911054315234126]
	TIME [epoch: 0.693 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.851163717739279		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 2.851163717739279 | validation: 3.514887523584892]
	TIME [epoch: 0.692 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6987363302213634		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 2.6987363302213634 | validation: 3.5065565205504132]
	TIME [epoch: 0.692 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6043011647198817		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 2.6043011647198817 | validation: 3.5024297955528283]
	TIME [epoch: 0.691 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.625603713978033		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 2.625603713978033 | validation: 3.3020970040278614]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.606143187281172		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 2.606143187281172 | validation: 3.5463574534952516]
	TIME [epoch: 0.693 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.597149938308015		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 2.597149938308015 | validation: 3.2757363469473675]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5450695569285013		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 2.5450695569285013 | validation: 3.26481348088681]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4954050968974433		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 2.4954050968974433 | validation: 3.136069169773994]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4594979195079207		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 2.4594979195079207 | validation: 3.026325321125247]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.41446118549653		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 2.41446118549653 | validation: 2.847284148540075]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3138084204678866		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 2.3138084204678866 | validation: 2.512168235157922]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4036265054227988		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 2.4036265054227988 | validation: 3.2723661758289113]
	TIME [epoch: 0.697 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.912507789541627		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 2.912507789541627 | validation: 2.6646494473363407]
	TIME [epoch: 0.694 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2479643853089657		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 2.2479643853089657 | validation: 1.8092605750502102]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.501301581965233		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 2.501301581965233 | validation: 2.528063533077385]
	TIME [epoch: 0.697 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.097676415907215		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 2.097676415907215 | validation: 2.2704206338841995]
	TIME [epoch: 0.695 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.041832261264339		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 2.041832261264339 | validation: 1.8550077253735324]
	TIME [epoch: 0.694 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7519743061802482		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.7519743061802482 | validation: 1.6363046308258846]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7571131982100436		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.7571131982100436 | validation: 2.1339137177483596]
	TIME [epoch: 0.696 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.860265491354402		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.860265491354402 | validation: 1.8996648457235659]
	TIME [epoch: 0.694 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.811945777600363		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.811945777600363 | validation: 1.7679388651277472]
	TIME [epoch: 0.694 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6928401006964862		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.6928401006964862 | validation: 1.6433572719516147]
	TIME [epoch: 0.694 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6523703697452572		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.6523703697452572 | validation: 1.8147088856561162]
	TIME [epoch: 0.694 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6611168777942198		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.6611168777942198 | validation: 1.7136514271695593]
	TIME [epoch: 0.694 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6840101417733007		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.6840101417733007 | validation: 1.8386850196376003]
	TIME [epoch: 0.694 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6757822822568278		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.6757822822568278 | validation: 1.6528793635061647]
	TIME [epoch: 0.695 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6148113023729949		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.6148113023729949 | validation: 1.6959701050932232]
	TIME [epoch: 0.694 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5795672112504635		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.5795672112504635 | validation: 1.6028818790951602]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.574398649706277		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.574398649706277 | validation: 1.7616693569925712]
	TIME [epoch: 0.695 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5985963625560962		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.5985963625560962 | validation: 1.6676396936081368]
	TIME [epoch: 0.694 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6069816974483409		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.6069816974483409 | validation: 1.6128021776689807]
	TIME [epoch: 0.693 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.569776781178846		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.569776781178846 | validation: 1.6934977295894376]
	TIME [epoch: 0.697 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5784380189763974		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.5784380189763974 | validation: 1.468759278100341]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6411354989363383		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.6411354989363383 | validation: 1.6394997222867902]
	TIME [epoch: 0.694 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5522233331812942		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.5522233331812942 | validation: 1.484669730360253]
	TIME [epoch: 0.694 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4543434902859014		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.4543434902859014 | validation: 1.4019071212484464]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.480631711990463		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.480631711990463 | validation: 1.7906718464159388]
	TIME [epoch: 0.695 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5803643715431286		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.5803643715431286 | validation: 1.5684327207875803]
	TIME [epoch: 0.693 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.622461193677719		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.622461193677719 | validation: 1.5599810172753044]
	TIME [epoch: 0.695 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4864579570893426		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.4864579570893426 | validation: 1.4320090348406938]
	TIME [epoch: 0.692 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.446297630106679		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.446297630106679 | validation: 1.4933744206524293]
	TIME [epoch: 0.693 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.431849152287027		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.431849152287027 | validation: 1.487050202069132]
	TIME [epoch: 0.695 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4630054305301505		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.4630054305301505 | validation: 1.417308363591383]
	TIME [epoch: 0.694 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4876503317333711		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.4876503317333711 | validation: 1.5889197265311314]
	TIME [epoch: 0.693 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5062096924073098		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.5062096924073098 | validation: 1.2985719765697263]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.405852230280279		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.405852230280279 | validation: 1.3672928392044155]
	TIME [epoch: 0.695 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3611522168780505		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.3611522168780505 | validation: 1.3524966060405141]
	TIME [epoch: 0.692 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3650739423885379		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.3650739423885379 | validation: 1.3759488146775347]
	TIME [epoch: 0.692 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.39646376141433		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.39646376141433 | validation: 1.485960751019018]
	TIME [epoch: 0.692 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.403753327932438		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.403753327932438 | validation: 1.4298899883595215]
	TIME [epoch: 0.692 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5211174331473984		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.5211174331473984 | validation: 1.6878138217102507]
	TIME [epoch: 0.692 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5824384885783196		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.5824384885783196 | validation: 1.197026326520032]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_161.pth
	Model improved!!!
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.410747933950811		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.410747933950811 | validation: 1.4488814891824073]
	TIME [epoch: 0.698 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3458604469956037		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.3458604469956037 | validation: 1.2297200240394812]
	TIME [epoch: 0.695 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2895071988842433		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.2895071988842433 | validation: 1.219702869986928]
	TIME [epoch: 0.695 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3349490951752867		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.3349490951752867 | validation: 1.6527325906180057]
	TIME [epoch: 0.695 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4658884607949585		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.4658884607949585 | validation: 1.2708151597763875]
	TIME [epoch: 0.695 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.320550357311663		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.320550357311663 | validation: 1.2512571107025494]
	TIME [epoch: 0.694 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.308349953636139		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.308349953636139 | validation: 1.3691612308766694]
	TIME [epoch: 0.695 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2875846491003173		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.2875846491003173 | validation: 1.2171217897950986]
	TIME [epoch: 0.692 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3414765908838908		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.3414765908838908 | validation: 1.5323554217829358]
	TIME [epoch: 0.691 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.41327817549533		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.41327817549533 | validation: 1.2300442632209014]
	TIME [epoch: 0.692 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4633663867567623		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.4633663867567623 | validation: 1.3181176845962543]
	TIME [epoch: 0.69 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2626306936070666		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.2626306936070666 | validation: 1.2005891289035882]
	TIME [epoch: 0.692 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2468449338248662		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.2468449338248662 | validation: 1.2359229690167013]
	TIME [epoch: 0.691 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2825191142679035		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.2825191142679035 | validation: 1.4852766950363927]
	TIME [epoch: 0.692 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3715332301844843		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.3715332301844843 | validation: 1.1002691634084234]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3132579527253245		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.3132579527253245 | validation: 1.4306185740761543]
	TIME [epoch: 0.697 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3154704123128569		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.3154704123128569 | validation: 1.1079783688530915]
	TIME [epoch: 1.08 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2233948144808222		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.2233948144808222 | validation: 1.1463062301370586]
	TIME [epoch: 0.692 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2161591505780553		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.2161591505780553 | validation: 1.335268231504176]
	TIME [epoch: 0.694 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.257594263490408		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.257594263490408 | validation: 1.1859013755088792]
	TIME [epoch: 0.694 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2697335153529448		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.2697335153529448 | validation: 1.498237774367918]
	TIME [epoch: 0.693 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3820710471111273		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.3820710471111273 | validation: 1.212506456577134]
	TIME [epoch: 0.694 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5409528289392476		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.5409528289392476 | validation: 1.2987038418805625]
	TIME [epoch: 0.694 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2239066621824384		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.2239066621824384 | validation: 1.1843238249658283]
	TIME [epoch: 0.694 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2103300732977662		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.2103300732977662 | validation: 1.116003497625711]
	TIME [epoch: 0.693 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2385337033830472		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.2385337033830472 | validation: 1.3499907482642959]
	TIME [epoch: 0.693 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2580438998521564		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.2580438998521564 | validation: 1.083460947490534]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.23761285168446		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.23761285168446 | validation: 1.3612408988873677]
	TIME [epoch: 0.695 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2471960675479152		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.2471960675479152 | validation: 1.1259084370653685]
	TIME [epoch: 0.694 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1803482051654286		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.1803482051654286 | validation: 1.1211607184211203]
	TIME [epoch: 0.695 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1908514624233055		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.1908514624233055 | validation: 1.3625614837661848]
	TIME [epoch: 0.696 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2486371158202243		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.2486371158202243 | validation: 1.107424354328001]
	TIME [epoch: 0.693 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3499816106884521		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.3499816106884521 | validation: 1.4170255378091792]
	TIME [epoch: 0.694 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.294151994899036		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.294151994899036 | validation: 0.9755470911948159]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.192428952084014		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.192428952084014 | validation: 1.176495194243536]
	TIME [epoch: 0.695 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1375425602548175		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.1375425602548175 | validation: 1.0965961179091115]
	TIME [epoch: 0.692 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.155205921097011		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.155205921097011 | validation: 1.151559452939839]
	TIME [epoch: 0.693 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1862136465709978		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.1862136465709978 | validation: 1.3471443876351998]
	TIME [epoch: 0.694 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2220991627012694		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.2220991627012694 | validation: 0.9825809040520387]
	TIME [epoch: 0.692 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2063990057266647		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.2063990057266647 | validation: 1.402723073750801]
	TIME [epoch: 168 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2503245882753342		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.2503245882753342 | validation: 1.0330797333854922]
	TIME [epoch: 1.37 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1679112569828982		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.1679112569828982 | validation: 1.1392551194033438]
	TIME [epoch: 1.36 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1816429812482596		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.1816429812482596 | validation: 0.9755746994572014]
	TIME [epoch: 1.35 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1861629998881427		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.1861629998881427 | validation: 1.1314004123851948]
	TIME [epoch: 1.35 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.139711306772541		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.139711306772541 | validation: 0.8647754689928647]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_206.pth
	Model improved!!!
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.13495108177938		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.13495108177938 | validation: 1.2193176517478461]
	TIME [epoch: 1.36 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1337384494569371		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.1337384494569371 | validation: 0.9708176521961314]
	TIME [epoch: 1.35 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1256868266343936		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.1256868266343936 | validation: 1.2638235791021104]
	TIME [epoch: 1.36 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1803809475305158		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.1803809475305158 | validation: 1.2319265295053992]
	TIME [epoch: 1.35 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1979105706674629		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.1979105706674629 | validation: 1.2107286739725058]
	TIME [epoch: 1.36 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3225857306840254		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.3225857306840254 | validation: 1.5183640547904016]
	TIME [epoch: 1.35 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3941315301624007		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 1.3941315301624007 | validation: 0.8825801461646101]
	TIME [epoch: 1.35 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.205896861200839		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.205896861200839 | validation: 1.3075458356673384]
	TIME [epoch: 1.35 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1464232922590925		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 1.1464232922590925 | validation: 0.9980203575010095]
	TIME [epoch: 1.35 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0515178084266164		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.0515178084266164 | validation: 0.9057926504952744]
	TIME [epoch: 1.35 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0873860484786706		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.0873860484786706 | validation: 1.2956584780598472]
	TIME [epoch: 1.36 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1646742337593925		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.1646742337593925 | validation: 1.0298453013244235]
	TIME [epoch: 1.35 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.086254255870214		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.086254255870214 | validation: 1.033769769574217]
	TIME [epoch: 1.35 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.107760140692087		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.107760140692087 | validation: 1.2241384132987023]
	TIME [epoch: 1.35 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1403668638191116		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 1.1403668638191116 | validation: 0.9395087152088138]
	TIME [epoch: 1.35 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2308064503868672		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.2308064503868672 | validation: 1.3094816273918728]
	TIME [epoch: 1.35 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2265904343398173		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 1.2265904343398173 | validation: 0.9199220141414041]
	TIME [epoch: 1.35 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1093613083272826		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.1093613083272826 | validation: 1.0772455052290748]
	TIME [epoch: 1.35 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0454005764342997		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 1.0454005764342997 | validation: 1.0005002654985824]
	TIME [epoch: 1.36 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0579240022008518		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 1.0579240022008518 | validation: 0.9745782490438826]
	TIME [epoch: 1.36 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0669479095624264		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.0669479095624264 | validation: 1.1485165782405078]
	TIME [epoch: 1.35 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0853938845926603		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.0853938845926603 | validation: 0.88512189749753]
	TIME [epoch: 1.35 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1032472950668706		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 1.1032472950668706 | validation: 1.3965509461058496]
	TIME [epoch: 1.35 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1985601288016443		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.1985601288016443 | validation: 0.8961742828266206]
	TIME [epoch: 1.35 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9955212268199781		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.9955212268199781 | validation: 0.8625554309503333]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.01416512664773		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.01416512664773 | validation: 1.0754393896940713]
	TIME [epoch: 1.36 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.032091045655254		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 1.032091045655254 | validation: 0.8919782310221237]
	TIME [epoch: 1.35 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0614139210424731		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.0614139210424731 | validation: 0.8414893129173383]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_234.pth
	Model improved!!!
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0604803936626408		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 1.0604803936626408 | validation: 1.3114753859478356]
	TIME [epoch: 1.36 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2326229162535522		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.2326229162535522 | validation: 0.8020405789764287]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.26851287770857		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 1.26851287770857 | validation: 1.475434336773195]
	TIME [epoch: 1.36 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.225150104090122		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 1.225150104090122 | validation: 1.0511372813358217]
	TIME [epoch: 1.36 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.028611549618665		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 1.028611549618665 | validation: 0.8459823809927919]
	TIME [epoch: 1.35 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0355125087117316		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 1.0355125087117316 | validation: 1.162434096055606]
	TIME [epoch: 1.35 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0673357011289974		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 1.0673357011289974 | validation: 0.9308626039833116]
	TIME [epoch: 1.35 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0085527103119865		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 1.0085527103119865 | validation: 0.928168044495124]
	TIME [epoch: 1.35 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9892307883478643		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.9892307883478643 | validation: 1.0074795008118176]
	TIME [epoch: 1.35 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.993961635549386		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.993961635549386 | validation: 0.8709128646802292]
	TIME [epoch: 1.35 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0223272743713545		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 1.0223272743713545 | validation: 1.392096924645818]
	TIME [epoch: 1.36 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.246021530197018		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.246021530197018 | validation: 0.9272203842278466]
	TIME [epoch: 1.35 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2491391848171103		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 1.2491391848171103 | validation: 0.9817666035590341]
	TIME [epoch: 1.35 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9856982263163644		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.9856982263163644 | validation: 0.9255543571507575]
	TIME [epoch: 1.39 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9570457754551882		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.9570457754551882 | validation: 0.8275125670020723]
	TIME [epoch: 1.35 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9730784062147285		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.9730784062147285 | validation: 1.106027173924525]
	TIME [epoch: 1.35 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0313646285914435		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 1.0313646285914435 | validation: 0.7957843338064144]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_251.pth
	Model improved!!!
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0109878903749852		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 1.0109878903749852 | validation: 1.1517549153859465]
	TIME [epoch: 1.36 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0417401607295886		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 1.0417401607295886 | validation: 0.7702724196226965]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9215142082672705		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.9215142082672705 | validation: 0.8434706347553388]
	TIME [epoch: 1.36 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9001942183919299		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.9001942183919299 | validation: 0.8468306350776328]
	TIME [epoch: 1.36 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9052211763426182		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.9052211763426182 | validation: 0.9338486643197186]
	TIME [epoch: 1.35 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9370717241620429		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.9370717241620429 | validation: 1.1665320297698851]
	TIME [epoch: 1.36 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1400385707241874		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 1.1400385707241874 | validation: 1.1521634044586844]
	TIME [epoch: 1.35 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.46174746074545		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 1.46174746074545 | validation: 1.1013898596844318]
	TIME [epoch: 1.36 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0409451158070548		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 1.0409451158070548 | validation: 0.7097342568589835]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_260.pth
	Model improved!!!
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0096265388629113		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 1.0096265388629113 | validation: 1.1896490890035973]
	TIME [epoch: 1.36 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.057028918644929		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 1.057028918644929 | validation: 0.7729216742165019]
	TIME [epoch: 1.36 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8979771789572948		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.8979771789572948 | validation: 0.8867292570335267]
	TIME [epoch: 1.35 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9136451438706914		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.9136451438706914 | validation: 0.9601956859247676]
	TIME [epoch: 1.35 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9579368935012886		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.9579368935012886 | validation: 0.9224117486600838]
	TIME [epoch: 1.35 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9447474370457032		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.9447474370457032 | validation: 0.852135063542959]
	TIME [epoch: 1.36 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.966860757922452		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.966860757922452 | validation: 1.1289270891764274]
	TIME [epoch: 1.36 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.096552864573866		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 1.096552864573866 | validation: 0.7611865291731228]
	TIME [epoch: 1.35 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.147901363469377		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 1.147901363469377 | validation: 1.1808904426335238]
	TIME [epoch: 1.36 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0293276656240364		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 1.0293276656240364 | validation: 0.806659762174283]
	TIME [epoch: 1.35 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8728566175839818		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.8728566175839818 | validation: 0.7005749479470089]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_271.pth
	Model improved!!!
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9253802158338903		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.9253802158338903 | validation: 1.0794609055996565]
	TIME [epoch: 1.36 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9884108286910835		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.9884108286910835 | validation: 0.7223855630850915]
	TIME [epoch: 1.36 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8929543799709643		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.8929543799709643 | validation: 0.8535836455141598]
	TIME [epoch: 1.35 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8853657265624076		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.8853657265624076 | validation: 0.7857171589672305]
	TIME [epoch: 1.36 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9504096878747736		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.9504096878747736 | validation: 1.1367523612780683]
	TIME [epoch: 1.35 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.046264574902979		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 1.046264574902979 | validation: 0.8781426438245377]
	TIME [epoch: 1.36 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9897854602170069		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.9897854602170069 | validation: 0.9160738882401248]
	TIME [epoch: 1.35 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8983556256155714		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.8983556256155714 | validation: 0.7840975132024142]
	TIME [epoch: 1.36 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8643467331392651		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.8643467331392651 | validation: 0.8268520747400475]
	TIME [epoch: 1.36 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8469168998401456		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.8469168998401456 | validation: 0.7131761679413027]
	TIME [epoch: 1.36 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9046205889212519		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.9046205889212519 | validation: 1.1661080741933212]
	TIME [epoch: 1.35 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0376057912290295		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 1.0376057912290295 | validation: 0.6927346789124955]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_283.pth
	Model improved!!!
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0479593878391587		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 1.0479593878391587 | validation: 1.0144748298853887]
	TIME [epoch: 1.35 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9679919602521602		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.9679919602521602 | validation: 0.719399482841166]
	TIME [epoch: 1.35 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8273820846225859		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.8273820846225859 | validation: 0.7798113388037109]
	TIME [epoch: 1.35 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.817068305570966		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.817068305570966 | validation: 0.7870936045247559]
	TIME [epoch: 1.36 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8315590206442833		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.8315590206442833 | validation: 0.825520985513504]
	TIME [epoch: 1.35 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9132547323568487		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.9132547323568487 | validation: 0.8944216916820302]
	TIME [epoch: 1.35 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9688400457484585		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.9688400457484585 | validation: 0.8080284538257276]
	TIME [epoch: 1.35 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9459344437627317		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.9459344437627317 | validation: 0.7341753146080467]
	TIME [epoch: 1.35 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8067109638005068		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.8067109638005068 | validation: 0.8329433119711855]
	TIME [epoch: 1.35 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8010015037175775		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.8010015037175775 | validation: 0.6773817081083124]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_293.pth
	Model improved!!!
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8890298366579097		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.8890298366579097 | validation: 1.2501611086386333]
	TIME [epoch: 1.35 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0962498225795962		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 1.0962498225795962 | validation: 0.8744251451077661]
	TIME [epoch: 1.36 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1075466260803661		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 1.1075466260803661 | validation: 0.9801932976371023]
	TIME [epoch: 1.35 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1184055180838512		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 1.1184055180838512 | validation: 0.9099119230356593]
	TIME [epoch: 1.35 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.887260677112348		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.887260677112348 | validation: 0.7972143350916175]
	TIME [epoch: 1.35 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9604652732395104		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.9604652732395104 | validation: 0.8262590384716472]
	TIME [epoch: 1.35 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8437785172631672		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.8437785172631672 | validation: 0.8022820202217176]
	TIME [epoch: 1.35 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.829460215753655		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.829460215753655 | validation: 0.8854998739027278]
	TIME [epoch: 1.36 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8280672816263711		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.8280672816263711 | validation: 0.6648764521534144]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_302.pth
	Model improved!!!
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8088000057594232		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.8088000057594232 | validation: 0.9190040127869346]
	TIME [epoch: 1.36 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8693883547970045		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.8693883547970045 | validation: 0.6787601320682042]
	TIME [epoch: 1.36 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.990129355188042		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.990129355188042 | validation: 0.9683997365899674]
	TIME [epoch: 1.36 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9228387325051982		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.9228387325051982 | validation: 0.6643199524683859]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_306.pth
	Model improved!!!
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8745100948717791		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.8745100948717791 | validation: 0.8736589564566959]
	TIME [epoch: 1.36 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8253929482126344		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.8253929482126344 | validation: 0.6531126973160112]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7970779746280003		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.7970779746280003 | validation: 0.7774412514201068]
	TIME [epoch: 1.35 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7872766715550864		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.7872766715550864 | validation: 0.6463437041421456]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_310.pth
	Model improved!!!
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8312669844743528		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.8312669844743528 | validation: 0.974961691340253]
	TIME [epoch: 1.35 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.912636759329165		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.912636759329165 | validation: 0.6839107732303487]
	TIME [epoch: 1.35 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9439691246710326		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.9439691246710326 | validation: 0.9037515409125091]
	TIME [epoch: 1.35 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8508278690544324		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.8508278690544324 | validation: 0.7263612693009588]
	TIME [epoch: 1.35 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8134259106937911		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.8134259106937911 | validation: 0.801163150526082]
	TIME [epoch: 1.35 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8234008262210912		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.8234008262210912 | validation: 0.8432482714498131]
	TIME [epoch: 1.36 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8380040310661503		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.8380040310661503 | validation: 0.7359828123545002]
	TIME [epoch: 1.36 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8598167012724036		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.8598167012724036 | validation: 0.8421192019969143]
	TIME [epoch: 1.36 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8045651161186951		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.8045651161186951 | validation: 0.6308973165480487]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_319.pth
	Model improved!!!
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7524184162665206		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.7524184162665206 | validation: 0.8193764070263008]
	TIME [epoch: 1.36 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7910743681822294		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.7910743681822294 | validation: 0.6412735537501808]
	TIME [epoch: 1.35 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9706769750956684		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.9706769750956684 | validation: 1.0536232056613528]
	TIME [epoch: 1.35 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.007119061175426		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 1.007119061175426 | validation: 0.678426703178292]
	TIME [epoch: 1.35 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8401564063714102		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.8401564063714102 | validation: 0.7669028821220945]
	TIME [epoch: 1.35 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7816969280806523		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.7816969280806523 | validation: 0.6917355975687004]
	TIME [epoch: 1.35 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7582655578200337		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.7582655578200337 | validation: 0.7248390162771616]
	TIME [epoch: 1.35 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.75563786563619		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.75563786563619 | validation: 0.6931974668335007]
	TIME [epoch: 1.35 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7688697955444633		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.7688697955444633 | validation: 0.8785158123626143]
	TIME [epoch: 1.35 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.791344733548178		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.791344733548178 | validation: 0.6423066586740219]
	TIME [epoch: 1.36 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8638391491708284		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.8638391491708284 | validation: 1.029400115902968]
	TIME [epoch: 1.35 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9645624477426493		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.9645624477426493 | validation: 0.6297285670514348]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_331.pth
	Model improved!!!
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8278532711652892		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.8278532711652892 | validation: 0.7302139678671733]
	TIME [epoch: 1.35 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.747736278680857		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.747736278680857 | validation: 0.6338388179670986]
	TIME [epoch: 1.35 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7353256107124821		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.7353256107124821 | validation: 0.717176277492117]
	TIME [epoch: 1.36 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7412698986365015		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.7412698986365015 | validation: 0.7021944357298642]
	TIME [epoch: 1.35 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8013819209510024		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.8013819209510024 | validation: 0.818200344884094]
	TIME [epoch: 1.35 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8714087963195256		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.8714087963195256 | validation: 0.8702245676906378]
	TIME [epoch: 1.35 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8269395434158144		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.8269395434158144 | validation: 0.6133416551705473]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_338.pth
	Model improved!!!
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7246411169010796		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.7246411169010796 | validation: 0.7268710822911988]
	TIME [epoch: 1.35 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7221463144676074		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.7221463144676074 | validation: 0.6000113553079532]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_340.pth
	Model improved!!!
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8562494552452795		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.8562494552452795 | validation: 1.0631842033352457]
	TIME [epoch: 1.36 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0069641877045536		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 1.0069641877045536 | validation: 0.6404267696228851]
	TIME [epoch: 1.35 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8338792094375476		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.8338792094375476 | validation: 0.7220376402142276]
	TIME [epoch: 1.36 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7231000142788816		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.7231000142788816 | validation: 0.6625239181236581]
	TIME [epoch: 1.36 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7033644731389478		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.7033644731389478 | validation: 0.6738208048188333]
	TIME [epoch: 1.35 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6901107517863261		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.6901107517863261 | validation: 0.5968016055617807]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_346.pth
	Model improved!!!
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6955395915065692		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.6955395915065692 | validation: 0.8075414691057562]
	TIME [epoch: 1.35 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7677488413068563		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.7677488413068563 | validation: 0.6556980316683362]
	TIME [epoch: 1.35 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9897659913148116		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.9897659913148116 | validation: 0.9757555107753909]
	TIME [epoch: 1.36 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9038799264335107		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.9038799264335107 | validation: 0.5956123166906666]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_350.pth
	Model improved!!!
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7129936593444454		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.7129936593444454 | validation: 0.6404013119247551]
	TIME [epoch: 1.35 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6996899240982546		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.6996899240982546 | validation: 0.7327081452360009]
	TIME [epoch: 1.36 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7564914151247873		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.7564914151247873 | validation: 0.6884029540293874]
	TIME [epoch: 1.36 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7712091625656072		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.7712091625656072 | validation: 0.640760325986502]
	TIME [epoch: 1.35 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.734773803353328		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.734773803353328 | validation: 0.7629494735049729]
	TIME [epoch: 1.35 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7535553876260354		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.7535553876260354 | validation: 0.6050766525247919]
	TIME [epoch: 1.35 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8778232363832095		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.8778232363832095 | validation: 0.987204117579667]
	TIME [epoch: 1.35 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8837475480351782		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.8837475480351782 | validation: 0.5987909867676681]
	TIME [epoch: 1.35 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.696509062571063		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.696509062571063 | validation: 0.6709200866971301]
	TIME [epoch: 1.35 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6940378045114542		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.6940378045114542 | validation: 0.6489158552890143]
	TIME [epoch: 1.35 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7357674695973486		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.7357674695973486 | validation: 0.766741254312921]
	TIME [epoch: 1.35 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.777973467121785		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.777973467121785 | validation: 0.691543002738654]
	TIME [epoch: 1.36 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7639941343228484		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.7639941343228484 | validation: 0.7439126600521544]
	TIME [epoch: 1.35 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7257971324264593		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.7257971324264593 | validation: 0.5834266630623856]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_364.pth
	Model improved!!!
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7321478581620326		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.7321478581620326 | validation: 0.8774524133600059]
	TIME [epoch: 1.35 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8466168710783126		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.8466168710783126 | validation: 0.58268256914221]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_366.pth
	Model improved!!!
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8230273872251354		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.8230273872251354 | validation: 0.7226981083878666]
	TIME [epoch: 1.35 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7059145963523962		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.7059145963523962 | validation: 0.5799123970473085]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6979942147736097		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.6979942147736097 | validation: 0.7356904755241362]
	TIME [epoch: 1.36 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.701436133474049		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.701436133474049 | validation: 0.5778488465794454]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_370.pth
	Model improved!!!
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7148493946372376		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.7148493946372376 | validation: 0.8043411327711381]
	TIME [epoch: 1.36 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7558430728135749		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.7558430728135749 | validation: 0.5692009421130659]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_372.pth
	Model improved!!!
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7504974084156913		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.7504974084156913 | validation: 0.7708433580245924]
	TIME [epoch: 1.36 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7782196225550798		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.7782196225550798 | validation: 0.566096536342967]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_374.pth
	Model improved!!!
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7801379480891726		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.7801379480891726 | validation: 0.6962613105622385]
	TIME [epoch: 1.36 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6957442616971222		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.6957442616971222 | validation: 0.5537083469787973]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_376.pth
	Model improved!!!
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6776365498628562		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.6776365498628562 | validation: 0.6886158800806121]
	TIME [epoch: 1.35 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.679088548271215		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.679088548271215 | validation: 0.5822025596388917]
	TIME [epoch: 1.35 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7155316174165859		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.7155316174165859 | validation: 0.8591180861305251]
	TIME [epoch: 1.35 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7829382208541721		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.7829382208541721 | validation: 0.576153625148283]
	TIME [epoch: 1.35 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7767840411194289		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.7767840411194289 | validation: 0.7956022829246017]
	TIME [epoch: 1.36 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7657140566545201		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.7657140566545201 | validation: 0.5644306180014856]
	TIME [epoch: 1.35 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6710946459939436		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.6710946459939436 | validation: 0.679137693794867]
	TIME [epoch: 1.35 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6757309421692227		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.6757309421692227 | validation: 0.5752278128105727]
	TIME [epoch: 1.35 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6678880894512077		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.6678880894512077 | validation: 0.6605907717686926]
	TIME [epoch: 1.35 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6808855661464049		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.6808855661464049 | validation: 0.625264106600601]
	TIME [epoch: 1.35 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6787249642539736		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.6787249642539736 | validation: 0.6005695125872222]
	TIME [epoch: 1.35 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.736895261988795		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.736895261988795 | validation: 0.8546166364258565]
	TIME [epoch: 1.35 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9099586638544022		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.9099586638544022 | validation: 0.5582576040401401]
	TIME [epoch: 1.35 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7972643657088923		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.7972643657088923 | validation: 0.8397912787829949]
	TIME [epoch: 1.35 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7521377630167402		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.7521377630167402 | validation: 0.5962650408549228]
	TIME [epoch: 1.35 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6439543593950109		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.6439543593950109 | validation: 0.5347716832416352]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_392.pth
	Model improved!!!
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6835769600239356		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.6835769600239356 | validation: 0.7984175504096414]
	TIME [epoch: 1.35 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7670667984206277		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.7670667984206277 | validation: 0.5668057906270022]
	TIME [epoch: 1.35 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8222935527076711		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.8222935527076711 | validation: 0.7230764789417059]
	TIME [epoch: 1.35 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6840250082437095		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.6840250082437095 | validation: 0.5632960056110384]
	TIME [epoch: 1.35 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.649342391325693		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.649342391325693 | validation: 0.6108740919829707]
	TIME [epoch: 1.35 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6431217911215208		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.6431217911215208 | validation: 0.5924512960097545]
	TIME [epoch: 1.35 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6543134783341661		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.6543134783341661 | validation: 0.6655874736243693]
	TIME [epoch: 1.36 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6879744997755423		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.6879744997755423 | validation: 0.5534765999136148]
	TIME [epoch: 1.35 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7637698158943357		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.7637698158943357 | validation: 0.7059058297368499]
	TIME [epoch: 1.35 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.733094222752969		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.733094222752969 | validation: 0.5488857872743286]
	TIME [epoch: 1.36 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6688916115649847		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.6688916115649847 | validation: 0.6662819074627291]
	TIME [epoch: 1.35 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6491196010029481		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.6491196010029481 | validation: 0.5496859233823234]
	TIME [epoch: 1.35 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7085342359572531		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.7085342359572531 | validation: 0.9037203516188976]
	TIME [epoch: 1.35 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8291665712878407		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.8291665712878407 | validation: 0.5431137993949595]
	TIME [epoch: 1.35 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6603627669439954		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.6603627669439954 | validation: 0.6137836558801614]
	TIME [epoch: 1.35 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.722398951435568		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.722398951435568 | validation: 0.6032045017568683]
	TIME [epoch: 1.35 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6579756451360163		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.6579756451360163 | validation: 0.6169605692216814]
	TIME [epoch: 1.35 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6266984616207798		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.6266984616207798 | validation: 0.542021148257373]
	TIME [epoch: 1.35 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6931968173502439		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.6931968173502439 | validation: 0.8499920010321284]
	TIME [epoch: 1.35 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7856479773595529		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.7856479773595529 | validation: 0.5333654938962885]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_412.pth
	Model improved!!!
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7247389997638066		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.7247389997638066 | validation: 0.6081602226999872]
	TIME [epoch: 1.35 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6695225175693296		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.6695225175693296 | validation: 0.6100293819646292]
	TIME [epoch: 1.36 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6481765281102816		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.6481765281102816 | validation: 0.5404969944980866]
	TIME [epoch: 1.36 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6134654075412128		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.6134654075412128 | validation: 0.6001598214990644]
	TIME [epoch: 1.36 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6087702646687894		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.6087702646687894 | validation: 0.5424657450582867]
	TIME [epoch: 1.36 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6665025522514548		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.6665025522514548 | validation: 0.7821201266926552]
	TIME [epoch: 1.36 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7691750333805452		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.7691750333805452 | validation: 0.526111595537784]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_419.pth
	Model improved!!!
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7548501338106627		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.7548501338106627 | validation: 0.7277052248701699]
	TIME [epoch: 1.36 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6839595034575322		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.6839595034575322 | validation: 0.5250573366884946]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_421.pth
	Model improved!!!
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6373239337137979		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.6373239337137979 | validation: 0.5998487970191961]
	TIME [epoch: 1.37 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.616642410275159		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.616642410275159 | validation: 0.5174440030183773]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_423.pth
	Model improved!!!
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6247136320572091		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.6247136320572091 | validation: 0.6604306977743026]
	TIME [epoch: 1.36 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6516570598989594		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.6516570598989594 | validation: 0.5196264974539567]
	TIME [epoch: 1.36 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7169969123924983		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.7169969123924983 | validation: 0.729124448794557]
	TIME [epoch: 1.36 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6837150488932734		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.6837150488932734 | validation: 0.5850164646557087]
	TIME [epoch: 1.36 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8572345764979984		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.8572345764979984 | validation: 0.6349639905213091]
	TIME [epoch: 1.36 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6400781385012122		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.6400781385012122 | validation: 0.5641883895153487]
	TIME [epoch: 1.36 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6056780058210669		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.6056780058210669 | validation: 0.5426272011336937]
	TIME [epoch: 1.36 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6138388252247827		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.6138388252247827 | validation: 0.6505828752331971]
	TIME [epoch: 1.36 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6345453452042152		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.6345453452042152 | validation: 0.4958442992234293]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_432.pth
	Model improved!!!
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.60347852573333		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.60347852573333 | validation: 0.592169914304267]
	TIME [epoch: 1.36 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6315933302062637		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.6315933302062637 | validation: 0.5297893264229789]
	TIME [epoch: 1.36 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7383044997795882		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.7383044997795882 | validation: 0.703112519478957]
	TIME [epoch: 1.36 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7062275053629989		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.7062275053629989 | validation: 0.5105661990073275]
	TIME [epoch: 1.36 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.61883366003476		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.61883366003476 | validation: 0.6130583085271323]
	TIME [epoch: 1.36 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6056195941405924		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.6056195941405924 | validation: 0.5216659433484704]
	TIME [epoch: 1.36 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6566421721473833		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.6566421721473833 | validation: 0.6924839442560722]
	TIME [epoch: 1.36 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6751228739706205		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.6751228739706205 | validation: 0.4900555417653092]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_440.pth
	Model improved!!!
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6486221329931968		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.6486221329931968 | validation: 0.6021579899542397]
	TIME [epoch: 1.36 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.622825533853085		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.622825533853085 | validation: 0.49944337056203275]
	TIME [epoch: 1.36 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6193546181358545		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.6193546181358545 | validation: 0.6153790998757069]
	TIME [epoch: 1.36 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6149535335548607		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.6149535335548607 | validation: 0.49922699772027457]
	TIME [epoch: 1.36 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6270217097478923		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.6270217097478923 | validation: 0.6804337247075689]
	TIME [epoch: 1.36 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6605141091964856		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.6605141091964856 | validation: 0.5079637236877108]
	TIME [epoch: 1.36 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6463845212980824		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.6463845212980824 | validation: 0.6107915785531134]
	TIME [epoch: 1.36 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6216437798815901		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.6216437798815901 | validation: 0.4918439878034926]
	TIME [epoch: 1.36 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6273711935408112		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.6273711935408112 | validation: 0.6212303542102636]
	TIME [epoch: 1.36 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6223087025930782		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.6223087025930782 | validation: 0.48824454783832255]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_450.pth
	Model improved!!!
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6112939794347186		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.6112939794347186 | validation: 0.645946284872692]
	TIME [epoch: 1.36 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6329799018603255		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.6329799018603255 | validation: 0.509558406955242]
	TIME [epoch: 1.36 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6788585095852679		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.6788585095852679 | validation: 0.6676530682726493]
	TIME [epoch: 1.36 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6361590381252207		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.6361590381252207 | validation: 0.48618119431862433]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_454.pth
	Model improved!!!
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5883356116622683		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.5883356116622683 | validation: 0.5422933678517916]
	TIME [epoch: 1.36 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5704014444742717		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.5704014444742717 | validation: 0.4774762664506432]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_456.pth
	Model improved!!!
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5587397245194989		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.5587397245194989 | validation: 0.5678969711300131]
	TIME [epoch: 1.35 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5911059358076498		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.5911059358076498 | validation: 0.4872732868717831]
	TIME [epoch: 1.35 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7062744341363342		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.7062744341363342 | validation: 0.6455563110778354]
	TIME [epoch: 1.35 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6494006939920225		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.6494006939920225 | validation: 0.5092190355318763]
	TIME [epoch: 1.35 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6234567901281496		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.6234567901281496 | validation: 0.589195648934604]
	TIME [epoch: 1.35 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5989321722546725		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.5989321722546725 | validation: 0.499242128219768]
	TIME [epoch: 1.36 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5811139170030711		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.5811139170030711 | validation: 0.5718982220073194]
	TIME [epoch: 1.36 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6049086853260062		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.6049086853260062 | validation: 0.5910435245945488]
	TIME [epoch: 1.36 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6336043249692476		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.6336043249692476 | validation: 0.5390091726216427]
	TIME [epoch: 1.36 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6017914112364212		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.6017914112364212 | validation: 0.5334224536644515]
	TIME [epoch: 1.36 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.580867268995831		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.580867268995831 | validation: 0.5124545785604823]
	TIME [epoch: 1.36 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5536227651457656		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.5536227651457656 | validation: 0.5832109179598568]
	TIME [epoch: 1.36 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5889019283635814		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.5889019283635814 | validation: 0.4706258403797685]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_469.pth
	Model improved!!!
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5928808232278405		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.5928808232278405 | validation: 0.6897829062966103]
	TIME [epoch: 1.35 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6987682825923132		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.6987682825923132 | validation: 0.5378399348116635]
	TIME [epoch: 1.35 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8097558414607923		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.8097558414607923 | validation: 0.5703578301735387]
	TIME [epoch: 1.35 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5729716196417708		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.5729716196417708 | validation: 0.541489144566864]
	TIME [epoch: 1.35 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5709040740275846		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.5709040740275846 | validation: 0.47550694235938257]
	TIME [epoch: 1.36 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6335696422189543		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.6335696422189543 | validation: 0.6349122101705698]
	TIME [epoch: 1.36 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6012643481897798		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.6012643481897798 | validation: 0.45827165349857557]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_476.pth
	Model improved!!!
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5655271831333999		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.5655271831333999 | validation: 0.5479426682767843]
	TIME [epoch: 1.35 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5707778169865224		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.5707778169865224 | validation: 0.45821494525163936]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_478.pth
	Model improved!!!
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5859822485076687		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.5859822485076687 | validation: 0.5959585765130446]
	TIME [epoch: 1.36 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5969806811217646		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.5969806811217646 | validation: 0.460340894206142]
	TIME [epoch: 1.36 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.600539342850076		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.600539342850076 | validation: 0.5632577351368782]
	TIME [epoch: 1.36 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5681953277836316		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.5681953277836316 | validation: 0.4511943289604801]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_482.pth
	Model improved!!!
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5468802108314228		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.5468802108314228 | validation: 0.5300975030988949]
	TIME [epoch: 1.36 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5420777452565989		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.5420777452565989 | validation: 0.4656673246931722]
	TIME [epoch: 1.35 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.564052647658955		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.564052647658955 | validation: 0.586363005758657]
	TIME [epoch: 1.35 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5877725039379575		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.5877725039379575 | validation: 0.48818130385267705]
	TIME [epoch: 1.35 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6377956214522823		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.6377956214522823 | validation: 0.741612427115963]
	TIME [epoch: 1.35 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6804626537727303		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.6804626537727303 | validation: 0.4611941965498376]
	TIME [epoch: 1.35 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5212574351592285		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.5212574351592285 | validation: 0.4574396888371163]
	TIME [epoch: 1.35 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5376076317923973		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.5376076317923973 | validation: 0.6063646135847183]
	TIME [epoch: 1.35 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5788111745681487		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.5788111745681487 | validation: 0.44622054336786815]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_491.pth
	Model improved!!!
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5282021265417334		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.5282021265417334 | validation: 0.5447384347961867]
	TIME [epoch: 1.36 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5518604351699004		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.5518604351699004 | validation: 0.4534967503657057]
	TIME [epoch: 1.35 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6272460009220605		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.6272460009220605 | validation: 0.6093693692939788]
	TIME [epoch: 1.35 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6339277314992632		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.6339277314992632 | validation: 0.545768548586597]
	TIME [epoch: 1.36 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6046169858996583		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.6046169858996583 | validation: 0.5088243962423098]
	TIME [epoch: 1.35 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5395223314253077		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.5395223314253077 | validation: 0.4589503490017844]
	TIME [epoch: 1.35 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5131386400013652		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.5131386400013652 | validation: 0.5027360117473342]
	TIME [epoch: 1.35 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.518049050313048		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.518049050313048 | validation: 0.44881694565361796]
	TIME [epoch: 1.35 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5869894716503632		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.5869894716503632 | validation: 0.7104510881858612]
	TIME [epoch: 1.35 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6630587175961091		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.6630587175961091 | validation: 0.43524137829513293]
	TIME [epoch: 175 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_501.pth
	Model improved!!!
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5836846701695736		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.5836846701695736 | validation: 0.5243541309533509]
	TIME [epoch: 2.68 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.557372914361978		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.557372914361978 | validation: 0.473464127119149]
	TIME [epoch: 2.67 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5388829364672079		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.5388829364672079 | validation: 0.46601484029799983]
	TIME [epoch: 2.67 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5231433777405037		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.5231433777405037 | validation: 0.43352377931118613]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_505.pth
	Model improved!!!
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5060341203464609		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.5060341203464609 | validation: 0.4906883686535623]
	TIME [epoch: 2.67 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5147173991284003		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.5147173991284003 | validation: 0.4245052941367024]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_507.pth
	Model improved!!!
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5833330633724916		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.5833330633724916 | validation: 0.6751505207782187]
	TIME [epoch: 2.68 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.636954055853308		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.636954055853308 | validation: 0.458315615909722]
	TIME [epoch: 2.68 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5831692549428223		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.5831692549428223 | validation: 0.49627523880556956]
	TIME [epoch: 2.67 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5108103290552172		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.5108103290552172 | validation: 0.43173763338150306]
	TIME [epoch: 2.67 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49749369825385453		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.49749369825385453 | validation: 0.48965171092981774]
	TIME [epoch: 2.67 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5070653827287296		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.5070653827287296 | validation: 0.45338444921141047]
	TIME [epoch: 2.67 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5271687118527008		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.5271687118527008 | validation: 0.5085598988721488]
	TIME [epoch: 2.67 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.558325520216547		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.558325520216547 | validation: 0.454527261030111]
	TIME [epoch: 2.67 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5599932964420444		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.5599932964420444 | validation: 0.5342136199929278]
	TIME [epoch: 2.68 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5381047300378846		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.5381047300378846 | validation: 0.435809075125996]
	TIME [epoch: 2.67 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.543761723033734		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.543761723033734 | validation: 0.5474842693760945]
	TIME [epoch: 2.67 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5369834636561092		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.5369834636561092 | validation: 0.4261004524135439]
	TIME [epoch: 2.67 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5268816951512005		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.5268816951512005 | validation: 0.5755653547213818]
	TIME [epoch: 2.67 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5612671635651362		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.5612671635651362 | validation: 0.41246937351542245]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_521.pth
	Model improved!!!
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.532067110582061		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.532067110582061 | validation: 0.48113814588344045]
	TIME [epoch: 2.67 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5141200092781757		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.5141200092781757 | validation: 0.44162438947565835]
	TIME [epoch: 2.67 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5183122557441263		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.5183122557441263 | validation: 0.5235153547606358]
	TIME [epoch: 2.68 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5196944200430336		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.5196944200430336 | validation: 0.4282915020441802]
	TIME [epoch: 2.67 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5112572467687195		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.5112572467687195 | validation: 0.522192716703406]
	TIME [epoch: 2.68 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5106909348378709		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.5106909348378709 | validation: 0.4391753296086135]
	TIME [epoch: 2.67 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.53431363960876		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.53431363960876 | validation: 0.6299555572548678]
	TIME [epoch: 2.67 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5794383907528188		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.5794383907528188 | validation: 0.4310548254710456]
	TIME [epoch: 2.67 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49656027785104884		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.49656027785104884 | validation: 0.4589063816438336]
	TIME [epoch: 2.67 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49515816087274916		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.49515816087274916 | validation: 0.42830770724549583]
	TIME [epoch: 2.67 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5374694563799414		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.5374694563799414 | validation: 0.5834873149659462]
	TIME [epoch: 2.67 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5595074781176289		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.5595074781176289 | validation: 0.41882481771032315]
	TIME [epoch: 2.67 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48789175398351503		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.48789175398351503 | validation: 0.4619829470370711]
	TIME [epoch: 2.68 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4731781428154259		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.4731781428154259 | validation: 0.39759310729162645]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_535.pth
	Model improved!!!
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48465770620630244		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.48465770620630244 | validation: 0.5189216057531034]
	TIME [epoch: 2.67 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5634710711772737		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.5634710711772737 | validation: 0.46778970764349065]
	TIME [epoch: 2.68 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5400199354210505		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.5400199354210505 | validation: 0.47331873108605244]
	TIME [epoch: 2.68 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49959707803718306		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.49959707803718306 | validation: 0.4188416609913313]
	TIME [epoch: 2.67 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45444833255053896		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.45444833255053896 | validation: 0.4200368091706054]
	TIME [epoch: 2.67 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45141838991182115		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.45141838991182115 | validation: 0.4178957737505701]
	TIME [epoch: 2.67 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45816471339784587		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.45816471339784587 | validation: 0.5183913957390749]
	TIME [epoch: 2.67 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6512105753604464		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.6512105753604464 | validation: 0.5356199356930142]
	TIME [epoch: 2.67 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5193323083074007		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.5193323083074007 | validation: 0.43624389285194987]
	TIME [epoch: 2.67 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6100264750574889		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.6100264750574889 | validation: 0.4981059969490087]
	TIME [epoch: 2.67 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.491752552858716		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.491752552858716 | validation: 0.39460734105000506]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_546.pth
	Model improved!!!
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.451865955061601		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.451865955061601 | validation: 0.4231374773432375]
	TIME [epoch: 2.67 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44307568073657977		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.44307568073657977 | validation: 0.3844166215689844]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_548.pth
	Model improved!!!
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44233201730854993		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.44233201730854993 | validation: 0.4263408675297591]
	TIME [epoch: 2.67 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44948417528228024		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.44948417528228024 | validation: 0.39952639569455484]
	TIME [epoch: 2.67 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47070822113675165		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.47070822113675165 | validation: 0.5502344802853099]
	TIME [epoch: 2.67 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5480553442352154		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.5480553442352154 | validation: 0.4400790361822846]
	TIME [epoch: 2.67 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5888319155220122		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.5888319155220122 | validation: 0.4966737716374571]
	TIME [epoch: 2.67 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49211303113370053		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.49211303113370053 | validation: 0.4008926140443089]
	TIME [epoch: 2.67 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4420115427153273		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.4420115427153273 | validation: 0.39193492886139847]
	TIME [epoch: 2.67 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42847765098778373		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.42847765098778373 | validation: 0.41310116910730577]
	TIME [epoch: 2.67 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42702038596265046		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.42702038596265046 | validation: 0.38950260680503607]
	TIME [epoch: 2.68 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4382048644078349		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.4382048644078349 | validation: 0.4827956979150444]
	TIME [epoch: 2.67 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5018552601018638		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.5018552601018638 | validation: 0.40664205749999915]
	TIME [epoch: 2.68 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.56806024279064		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.56806024279064 | validation: 0.4507508104240298]
	TIME [epoch: 2.67 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4588156141110465		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.4588156141110465 | validation: 0.3875613393697533]
	TIME [epoch: 2.67 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4223613769882217		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.4223613769882217 | validation: 0.4088461613847047]
	TIME [epoch: 2.67 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41911153804379325		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.41911153804379325 | validation: 0.4013229367897122]
	TIME [epoch: 2.67 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4407555146259513		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.4407555146259513 | validation: 0.5564023171008173]
	TIME [epoch: 2.67 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5109355117092659		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.5109355117092659 | validation: 0.4403646811950517]
	TIME [epoch: 2.67 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5574890829174494		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.5574890829174494 | validation: 0.5054064542044661]
	TIME [epoch: 2.67 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4923909694432827		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.4923909694432827 | validation: 0.37609797064371014]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_567.pth
	Model improved!!!
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46531796238837986		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.46531796238837986 | validation: 0.4657504868775677]
	TIME [epoch: 2.69 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47887519032414927		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.47887519032414927 | validation: 0.4144382506939008]
	TIME [epoch: 2.69 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45654660306285805		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.45654660306285805 | validation: 0.42040308836175594]
	TIME [epoch: 2.69 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.443992959575931		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.443992959575931 | validation: 0.368773689601152]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_571.pth
	Model improved!!!
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4336033644754623		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.4336033644754623 | validation: 0.4335927419298778]
	TIME [epoch: 2.68 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45314739396563963		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.45314739396563963 | validation: 0.38444016639187223]
	TIME [epoch: 2.69 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47855538825861693		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.47855538825861693 | validation: 0.47690378548592716]
	TIME [epoch: 2.69 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4789827905774481		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.4789827905774481 | validation: 0.3818324981891779]
	TIME [epoch: 2.69 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4340989969838572		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.4340989969838572 | validation: 0.40826020962217663]
	TIME [epoch: 2.69 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43344800958797924		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.43344800958797924 | validation: 0.4072866346143094]
	TIME [epoch: 2.67 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4740742284264132		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.4740742284264132 | validation: 0.5506406084653457]
	TIME [epoch: 2.67 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5068312315405057		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.5068312315405057 | validation: 0.3674849833372278]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_579.pth
	Model improved!!!
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42351874834311204		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.42351874834311204 | validation: 0.4174633925090345]
	TIME [epoch: 2.69 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42814272863021957		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.42814272863021957 | validation: 0.42750176014516905]
	TIME [epoch: 2.69 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48312282596764333		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.48312282596764333 | validation: 0.4453627371200692]
	TIME [epoch: 2.69 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46296632231360335		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.46296632231360335 | validation: 0.3511927468609015]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_583.pth
	Model improved!!!
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.429274845366472		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.429274845366472 | validation: 0.4020453764156158]
	TIME [epoch: 2.69 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4168171724542459		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.4168171724542459 | validation: 0.35911625975823763]
	TIME [epoch: 2.69 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4262003797507372		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.4262003797507372 | validation: 0.47595899010644555]
	TIME [epoch: 2.69 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4701431780855991		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.4701431780855991 | validation: 0.3859866107886747]
	TIME [epoch: 2.67 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44659145317952387		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.44659145317952387 | validation: 0.40912690662910056]
	TIME [epoch: 2.67 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4289498724882247		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.4289498724882247 | validation: 0.36052199349456454]
	TIME [epoch: 2.67 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4199850006416502		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.4199850006416502 | validation: 0.40497332510294887]
	TIME [epoch: 2.67 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4169928418179525		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.4169928418179525 | validation: 0.36315511391729266]
	TIME [epoch: 2.68 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42126041135773246		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.42126041135773246 | validation: 0.4059573337496161]
	TIME [epoch: 2.68 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4172984800040743		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.4172984800040743 | validation: 0.3495131370610624]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_593.pth
	Model improved!!!
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4177092346392164		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.4177092346392164 | validation: 0.4206323130837426]
	TIME [epoch: 2.68 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4277837763694232		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.4277837763694232 | validation: 0.4141048716115592]
	TIME [epoch: 2.69 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45050106226392983		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.45050106226392983 | validation: 0.40796352703067895]
	TIME [epoch: 2.68 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4179698416948637		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.4179698416948637 | validation: 0.35187957901149175]
	TIME [epoch: 2.69 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3825825395515861		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.3825825395515861 | validation: 0.37287378001978394]
	TIME [epoch: 2.69 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3828732455215594		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.3828732455215594 | validation: 0.3461823966846237]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_599.pth
	Model improved!!!
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39591358676100435		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.39591358676100435 | validation: 0.5128004069777852]
	TIME [epoch: 2.68 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4967914308230958		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.4967914308230958 | validation: 0.4158868739704132]
	TIME [epoch: 2.68 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4874986219916627		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.4874986219916627 | validation: 0.38199474254415794]
	TIME [epoch: 2.68 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.402575122639895		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.402575122639895 | validation: 0.33715018551006093]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_603.pth
	Model improved!!!
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38070048971650905		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.38070048971650905 | validation: 0.3760114461447132]
	TIME [epoch: 2.67 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.378604177414717		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.378604177414717 | validation: 0.3387667881830272]
	TIME [epoch: 2.67 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40395205636988485		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.40395205636988485 | validation: 0.40354588914569106]
	TIME [epoch: 2.68 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4307206637173043		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.4307206637173043 | validation: 0.34887742829329477]
	TIME [epoch: 2.68 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4170213417169056		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.4170213417169056 | validation: 0.3750293760755363]
	TIME [epoch: 2.67 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3873681686353018		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.3873681686353018 | validation: 0.33987897323308974]
	TIME [epoch: 2.69 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3766604584819393		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.3766604584819393 | validation: 0.3695997659096225]
	TIME [epoch: 2.69 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38017220420436937		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.38017220420436937 | validation: 0.3792791890444077]
	TIME [epoch: 2.69 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41389525432271557		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.41389525432271557 | validation: 0.44798632671143485]
	TIME [epoch: 2.69 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43539008978636257		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.43539008978636257 | validation: 0.359469322139196]
	TIME [epoch: 2.69 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3796158578485991		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.3796158578485991 | validation: 0.3457249257554074]
	TIME [epoch: 2.68 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35992316138717534		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.35992316138717534 | validation: 0.3729623659322213]
	TIME [epoch: 2.69 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4417777219775445		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.4417777219775445 | validation: 0.6106107512485673]
	TIME [epoch: 2.69 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5413546388690934		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.5413546388690934 | validation: 0.3227723076549765]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_617.pth
	Model improved!!!
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37350479128640857		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.37350479128640857 | validation: 0.41740395886886356]
	TIME [epoch: 2.67 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4036692169513469		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.4036692169513469 | validation: 0.360512350478057]
	TIME [epoch: 2.68 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3775639414940164		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.3775639414940164 | validation: 0.3565149511361274]
	TIME [epoch: 2.67 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3681850769388763		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.3681850769388763 | validation: 0.30888845603625326]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_621.pth
	Model improved!!!
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3820482439251464		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.3820482439251464 | validation: 0.4221802290426099]
	TIME [epoch: 2.67 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41744941763743015		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.41744941763743015 | validation: 0.3514163045663427]
	TIME [epoch: 2.68 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3986065741576821		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.3986065741576821 | validation: 0.3735029913580212]
	TIME [epoch: 2.67 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37512464320292865		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.37512464320292865 | validation: 0.30414787261973103]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_625.pth
	Model improved!!!
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.366651709732895		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.366651709732895 | validation: 0.4273700497981021]
	TIME [epoch: 2.69 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.503358637060924		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.503358637060924 | validation: 0.45482196809560493]
	TIME [epoch: 2.69 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4039975369267527		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.4039975369267527 | validation: 0.32318451240278323]
	TIME [epoch: 2.68 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3629316955485959		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.3629316955485959 | validation: 0.4156174407075841]
	TIME [epoch: 2.68 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40417386647220965		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.40417386647220965 | validation: 0.3257541496208223]
	TIME [epoch: 2.68 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37158234290779185		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.37158234290779185 | validation: 0.3509209798563756]
	TIME [epoch: 2.67 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37230432275915276		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.37230432275915276 | validation: 0.3625927130003778]
	TIME [epoch: 2.67 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35425984682823314		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.35425984682823314 | validation: 0.35787210220475185]
	TIME [epoch: 2.68 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3844553839019491		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.3844553839019491 | validation: 0.42066088921045036]
	TIME [epoch: 2.68 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4211502973653705		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.4211502973653705 | validation: 0.31201489480417954]
	TIME [epoch: 2.67 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3933623543085858		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.3933623543085858 | validation: 0.3394179576548945]
	TIME [epoch: 2.67 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3491052554572934		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.3491052554572934 | validation: 0.3111293878146897]
	TIME [epoch: 2.68 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33631238424291576		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.33631238424291576 | validation: 0.3194287909757151]
	TIME [epoch: 2.68 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3275279150983087		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.3275279150983087 | validation: 0.3163385440550768]
	TIME [epoch: 2.68 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3390909201091912		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.3390909201091912 | validation: 0.3778504267489766]
	TIME [epoch: 2.68 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38334143737369875		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.38334143737369875 | validation: 0.3591341989080348]
	TIME [epoch: 2.68 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4132109644613274		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.4132109644613274 | validation: 0.3638974549283627]
	TIME [epoch: 2.68 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3731885767663349		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.3731885767663349 | validation: 0.3531900066556389]
	TIME [epoch: 2.68 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37298361703006633		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.37298361703006633 | validation: 0.37532912118424644]
	TIME [epoch: 2.68 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36232714812790634		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.36232714812790634 | validation: 0.2966237708851094]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_645.pth
	Model improved!!!
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33691132937001184		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.33691132937001184 | validation: 0.3543277531550311]
	TIME [epoch: 2.68 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3518048025714164		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.3518048025714164 | validation: 0.3422180875554752]
	TIME [epoch: 2.69 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36612564584965895		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.36612564584965895 | validation: 0.3573907395828472]
	TIME [epoch: 2.68 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3567562389998408		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.3567562389998408 | validation: 0.3100721785784083]
	TIME [epoch: 2.68 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3298171864040117		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.3298171864040117 | validation: 0.3269661847420895]
	TIME [epoch: 2.68 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3356908146171598		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.3356908146171598 | validation: 0.2879103293562239]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_651.pth
	Model improved!!!
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34937512097726886		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.34937512097726886 | validation: 0.3825973717220875]
	TIME [epoch: 2.69 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37887374869660634		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.37887374869660634 | validation: 0.3654727343823077]
	TIME [epoch: 2.68 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.357324836823281		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.357324836823281 | validation: 0.3281805544943544]
	TIME [epoch: 2.69 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32449610284150626		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.32449610284150626 | validation: 0.3078258678744473]
	TIME [epoch: 2.68 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3288961084769139		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.3288961084769139 | validation: 0.29400956390730953]
	TIME [epoch: 2.69 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.341776475488476		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.341776475488476 | validation: 0.3625867313449131]
	TIME [epoch: 2.68 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3844576463333788		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.3844576463333788 | validation: 0.32088986714966955]
	TIME [epoch: 2.69 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36785672157582594		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.36785672157582594 | validation: 0.3364438604511241]
	TIME [epoch: 2.69 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3454533616915777		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.3454533616915777 | validation: 0.28648660277608456]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_660.pth
	Model improved!!!
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3193991326873939		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.3193991326873939 | validation: 0.30202682287669946]
	TIME [epoch: 2.69 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3163337131772706		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.3163337131772706 | validation: 0.292553108222276]
	TIME [epoch: 2.68 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33296258031800663		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.33296258031800663 | validation: 0.3357232748094477]
	TIME [epoch: 2.69 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3434626754096634		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.3434626754096634 | validation: 0.2930696466327266]
	TIME [epoch: 2.69 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3334195102959581		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.3334195102959581 | validation: 0.3302478812573041]
	TIME [epoch: 2.69 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3297055403441418		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.3297055403441418 | validation: 0.2848793305096163]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_666.pth
	Model improved!!!
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.315502394354067		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.315502394354067 | validation: 0.3279067962356478]
	TIME [epoch: 2.68 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3226709790856833		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.3226709790856833 | validation: 0.33403612795014337]
	TIME [epoch: 2.67 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3592039590736922		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.3592039590736922 | validation: 0.3957985588633286]
	TIME [epoch: 2.68 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36619431034450567		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.36619431034450567 | validation: 0.284094026362991]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_670.pth
	Model improved!!!
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29917818266453644		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.29917818266453644 | validation: 0.27028586016962697]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_671.pth
	Model improved!!!
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27754664348800456		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.27754664348800456 | validation: 0.28271420099555344]
	TIME [epoch: 2.67 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28321167147666304		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.28321167147666304 | validation: 0.36407345516115713]
	TIME [epoch: 2.67 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.365772440940863		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.365772440940863 | validation: 0.42540102531261925]
	TIME [epoch: 2.67 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3817421338280235		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.3817421338280235 | validation: 0.2809179149408538]
	TIME [epoch: 2.67 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32590728027957094		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.32590728027957094 | validation: 0.4131285642166862]
	TIME [epoch: 2.67 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3978363064542566		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.3978363064542566 | validation: 0.300230148529709]
	TIME [epoch: 2.68 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32881729338171783		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.32881729338171783 | validation: 0.3132067348268964]
	TIME [epoch: 2.67 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3014588678075575		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.3014588678075575 | validation: 0.27483453344114067]
	TIME [epoch: 2.67 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30143408778597314		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.30143408778597314 | validation: 0.32232249943143876]
	TIME [epoch: 2.67 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3177455812321198		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.3177455812321198 | validation: 0.2929209470076281]
	TIME [epoch: 2.67 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33255304155436527		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.33255304155436527 | validation: 0.31785010037997036]
	TIME [epoch: 2.67 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3133359685710168		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.3133359685710168 | validation: 0.2632170710262584]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_683.pth
	Model improved!!!
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3023960274377208		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.3023960274377208 | validation: 0.28941555420075177]
	TIME [epoch: 2.67 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2895274900163771		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.2895274900163771 | validation: 0.25559659579152]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_685.pth
	Model improved!!!
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28452596618633186		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.28452596618633186 | validation: 0.28544973486592723]
	TIME [epoch: 2.67 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2919154897587434		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.2919154897587434 | validation: 0.28252557019932045]
	TIME [epoch: 2.68 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30520981767908373		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.30520981767908373 | validation: 0.30718446229685]
	TIME [epoch: 2.68 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3085687448193088		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.3085687448193088 | validation: 0.28908458693503003]
	TIME [epoch: 2.67 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30630506047309874		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.30630506047309874 | validation: 0.3471672690265698]
	TIME [epoch: 2.67 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3379484946038887		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.3379484946038887 | validation: 0.30376512632802316]
	TIME [epoch: 2.67 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29926176379527875		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.29926176379527875 | validation: 0.27703703167108495]
	TIME [epoch: 2.67 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27911851543157595		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.27911851543157595 | validation: 0.2452992166751276]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_693.pth
	Model improved!!!
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2814618119678938		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.2814618119678938 | validation: 0.3037784910964756]
	TIME [epoch: 2.67 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3069938574366926		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.3069938574366926 | validation: 0.26022174034732537]
	TIME [epoch: 2.67 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3076688694281148		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.3076688694281148 | validation: 0.2969793399616237]
	TIME [epoch: 2.67 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2944062185653414		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.2944062185653414 | validation: 0.24999114167904646]
	TIME [epoch: 2.67 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2700981978158898		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.2700981978158898 | validation: 0.25997704003573985]
	TIME [epoch: 2.68 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26837423855101783		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.26837423855101783 | validation: 0.24570276978163286]
	TIME [epoch: 2.67 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2762326274723281		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.2762326274723281 | validation: 0.3045495577470713]
	TIME [epoch: 2.67 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3024205159932937		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.3024205159932937 | validation: 0.318258054222343]
	TIME [epoch: 2.67 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32622144075942444		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.32622144075942444 | validation: 0.33801734461399274]
	TIME [epoch: 2.67 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32443247763785643		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.32443247763785643 | validation: 0.26014765473272466]
	TIME [epoch: 2.67 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26326920327622455		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.26326920327622455 | validation: 0.26744368341366437]
	TIME [epoch: 2.67 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2650455309452011		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.2650455309452011 | validation: 0.25368635796803285]
	TIME [epoch: 2.67 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26120822149179407		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.26120822149179407 | validation: 0.2979449987268342]
	TIME [epoch: 2.67 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30085853994335543		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.30085853994335543 | validation: 0.2940177352673878]
	TIME [epoch: 2.67 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31356248673099407		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.31356248673099407 | validation: 0.2964359010775874]
	TIME [epoch: 2.67 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3014370376935309		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.3014370376935309 | validation: 0.2412621684737455]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_709.pth
	Model improved!!!
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28476307957279373		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.28476307957279373 | validation: 0.3070054701915503]
	TIME [epoch: 2.67 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29474284038626075		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.29474284038626075 | validation: 0.2850485343613169]
	TIME [epoch: 2.67 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2896785994578214		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.2896785994578214 | validation: 0.3046771112337714]
	TIME [epoch: 2.67 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28136552171220425		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.28136552171220425 | validation: 0.25110467929924996]
	TIME [epoch: 2.67 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26301146727781993		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.26301146727781993 | validation: 0.2716016925363329]
	TIME [epoch: 2.67 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2614143599676419		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.2614143599676419 | validation: 0.23024906873163295]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_715.pth
	Model improved!!!
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2621327106566183		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.2621327106566183 | validation: 0.29281530236018494]
	TIME [epoch: 2.68 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27426804130052285		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.27426804130052285 | validation: 0.2575542981830849]
	TIME [epoch: 2.68 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2703203000953556		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.2703203000953556 | validation: 0.31967132621803307]
	TIME [epoch: 2.68 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29613405904789997		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.29613405904789997 | validation: 0.28078569854300234]
	TIME [epoch: 2.68 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26921837714785046		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.26921837714785046 | validation: 0.23523527675074313]
	TIME [epoch: 2.68 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2612675693959464		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.2612675693959464 | validation: 0.2916971291345853]
	TIME [epoch: 2.67 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3010370161948452		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.3010370161948452 | validation: 0.2535964289319152]
	TIME [epoch: 2.67 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2838769586527874		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.2838769586527874 | validation: 0.2770394581954239]
	TIME [epoch: 2.67 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25289874126270406		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.25289874126270406 | validation: 0.36900467669890386]
	TIME [epoch: 2.67 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4353638957494691		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.4353638957494691 | validation: 0.3036882302506542]
	TIME [epoch: 2.67 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27545699084542113		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.27545699084542113 | validation: 0.22780158110308762]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_726.pth
	Model improved!!!
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2512565922222859		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.2512565922222859 | validation: 0.24800409061004755]
	TIME [epoch: 2.69 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25326026119627193		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.25326026119627193 | validation: 0.2374024659283665]
	TIME [epoch: 2.68 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2634171013885925		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.2634171013885925 | validation: 0.2691547943109448]
	TIME [epoch: 2.68 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2551629687250787		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.2551629687250787 | validation: 0.2515023949645285]
	TIME [epoch: 2.69 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2543761061377596		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.2543761061377596 | validation: 0.30291906138604996]
	TIME [epoch: 2.68 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2800235550221625		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.2800235550221625 | validation: 0.2509791518789618]
	TIME [epoch: 2.68 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2629883806089147		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.2629883806089147 | validation: 0.24601754805232467]
	TIME [epoch: 2.68 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2469432313344844		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.2469432313344844 | validation: 0.21527233385616232]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_734.pth
	Model improved!!!
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2490230852849166		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.2490230852849166 | validation: 0.2524388566161314]
	TIME [epoch: 2.68 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24825158395960178		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.24825158395960178 | validation: 0.22994842877359783]
	TIME [epoch: 2.69 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2555091911839051		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.2555091911839051 | validation: 0.24294151474602962]
	TIME [epoch: 2.68 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2509089958745656		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.2509089958745656 | validation: 0.23550862757638794]
	TIME [epoch: 2.69 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25434237244140795		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.25434237244140795 | validation: 0.25828095372826393]
	TIME [epoch: 2.68 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2467752193456547		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.2467752193456547 | validation: 0.2879364960447016]
	TIME [epoch: 2.69 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28476836923741206		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.28476836923741206 | validation: 0.2930254385850763]
	TIME [epoch: 2.69 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2710983418035376		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.2710983418035376 | validation: 0.22248157410547656]
	TIME [epoch: 2.68 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23637916626764777		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.23637916626764777 | validation: 0.23151984897111977]
	TIME [epoch: 2.67 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23099184048347085		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.23099184048347085 | validation: 0.21865083889940157]
	TIME [epoch: 2.67 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24568799990894233		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.24568799990894233 | validation: 0.26690402347295134]
	TIME [epoch: 2.67 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26855556267475794		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.26855556267475794 | validation: 0.22987795100306166]
	TIME [epoch: 2.67 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2506146071166828		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.2506146071166828 | validation: 0.2795356778448171]
	TIME [epoch: 2.69 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2551635920450157		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.2551635920450157 | validation: 0.2390538517020933]
	TIME [epoch: 2.67 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2399806654031629		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.2399806654031629 | validation: 0.2552950628076336]
	TIME [epoch: 2.68 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2337493450172854		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.2337493450172854 | validation: 0.2372310886821945]
	TIME [epoch: 2.67 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.228766915859855		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.228766915859855 | validation: 0.23581038958227962]
	TIME [epoch: 2.68 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.228762889777278		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.228762889777278 | validation: 0.21692380447768028]
	TIME [epoch: 2.68 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23445851751957755		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.23445851751957755 | validation: 0.2550436746950977]
	TIME [epoch: 2.68 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2503478106358959		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.2503478106358959 | validation: 0.22751879692234597]
	TIME [epoch: 2.67 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24224284045365024		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.24224284045365024 | validation: 0.2563016218949719]
	TIME [epoch: 2.67 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24234918484817208		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.24234918484817208 | validation: 0.24805844311150063]
	TIME [epoch: 2.67 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23831278642063283		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.23831278642063283 | validation: 0.23300985721333936]
	TIME [epoch: 2.67 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22192164046960663		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.22192164046960663 | validation: 0.20850528467900872]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_758.pth
	Model improved!!!
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21892472404301672		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.21892472404301672 | validation: 0.21805555414343358]
	TIME [epoch: 2.69 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20853765183823797		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.20853765183823797 | validation: 0.2244175563653034]
	TIME [epoch: 2.69 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22072168695709518		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.22072168695709518 | validation: 0.32979560459747087]
	TIME [epoch: 2.68 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3158221506240355		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.3158221506240355 | validation: 0.27050625355342617]
	TIME [epoch: 2.69 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28509836388768733		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.28509836388768733 | validation: 0.2229148048584759]
	TIME [epoch: 2.69 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21751833534008497		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.21751833534008497 | validation: 0.21617740720961712]
	TIME [epoch: 2.69 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20642493900363668		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.20642493900363668 | validation: 0.2224483792672347]
	TIME [epoch: 2.68 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21322921775164255		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.21322921775164255 | validation: 0.21953502552480111]
	TIME [epoch: 2.68 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2328844289650076		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.2328844289650076 | validation: 0.25753635988151674]
	TIME [epoch: 2.68 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24979130022795626		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.24979130022795626 | validation: 0.22621618521530565]
	TIME [epoch: 2.69 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23159211794606172		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.23159211794606172 | validation: 0.23090496455698464]
	TIME [epoch: 2.68 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2329015677594306		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.2329015677594306 | validation: 0.21596968175607528]
	TIME [epoch: 2.69 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2147639145288367		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.2147639145288367 | validation: 0.22453605903969132]
	TIME [epoch: 2.68 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21319939738106883		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.21319939738106883 | validation: 0.20303067331138613]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_772.pth
	Model improved!!!
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23316558845591367		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.23316558845591367 | validation: 0.2543524119407157]
	TIME [epoch: 2.67 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25080829208957495		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.25080829208957495 | validation: 0.2160007376103403]
	TIME [epoch: 2.67 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22910949135026165		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.22910949135026165 | validation: 0.24801144626614724]
	TIME [epoch: 2.67 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21418738362811546		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.21418738362811546 | validation: 0.22099228575826146]
	TIME [epoch: 2.68 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21383227578296723		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.21383227578296723 | validation: 0.22081621454214362]
	TIME [epoch: 2.67 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20368315642931614		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.20368315642931614 | validation: 0.20338039267233465]
	TIME [epoch: 2.68 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20438962490094373		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.20438962490094373 | validation: 0.21676987083216132]
	TIME [epoch: 2.67 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2176069839524901		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.2176069839524901 | validation: 0.2610149441606312]
	TIME [epoch: 2.68 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2635229849276395		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.2635229849276395 | validation: 0.23544811998074905]
	TIME [epoch: 2.67 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24969965746489262		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.24969965746489262 | validation: 0.21509292345336928]
	TIME [epoch: 2.69 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2083599327714611		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.2083599327714611 | validation: 0.2156019708507705]
	TIME [epoch: 2.68 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20772700603686808		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.20772700603686808 | validation: 0.2184582197757372]
	TIME [epoch: 2.68 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20588899850075387		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.20588899850075387 | validation: 0.19615536719442148]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_785.pth
	Model improved!!!
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2102325219090969		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.2102325219090969 | validation: 0.22667500005429428]
	TIME [epoch: 2.67 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22773574114602751		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.22773574114602751 | validation: 0.19620705455680695]
	TIME [epoch: 2.67 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2199468419685449		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.2199468419685449 | validation: 0.2174908862259514]
	TIME [epoch: 2.67 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20423623884092826		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.20423623884092826 | validation: 0.19060828356298784]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_789.pth
	Model improved!!!
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1936251363822594		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.1936251363822594 | validation: 0.20322380993882938]
	TIME [epoch: 2.67 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19291561218895958		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.19291561218895958 | validation: 0.19559863376467002]
	TIME [epoch: 2.67 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2031770336496432		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.2031770336496432 | validation: 0.23833100177706618]
	TIME [epoch: 2.67 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22262064477352345		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.22262064477352345 | validation: 0.2384935095664365]
	TIME [epoch: 2.67 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23255305964078632		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.23255305964078632 | validation: 0.24126454464015668]
	TIME [epoch: 2.68 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22497482831502197		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.22497482831502197 | validation: 0.20413855498432537]
	TIME [epoch: 2.67 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19414271798614502		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.19414271798614502 | validation: 0.19892318818237673]
	TIME [epoch: 2.68 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19150025450947866		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.19150025450947866 | validation: 0.20855516792647247]
	TIME [epoch: 2.67 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1973003437582996		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.1973003437582996 | validation: 0.21006728053576978]
	TIME [epoch: 2.67 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2080914145012375		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.2080914145012375 | validation: 0.20411882422029026]
	TIME [epoch: 2.67 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21693449637255544		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.21693449637255544 | validation: 0.22605530668632126]
	TIME [epoch: 2.67 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23656617299248925		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.23656617299248925 | validation: 0.19403905304215469]
	TIME [epoch: 2.68 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21033352474334752		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.21033352474334752 | validation: 0.20523257529577524]
	TIME [epoch: 2.68 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19563316208717613		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.19563316208717613 | validation: 0.21138181988681182]
	TIME [epoch: 2.68 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19781781890004188		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.19781781890004188 | validation: 0.2173988175740986]
	TIME [epoch: 2.68 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20015866814753316		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.20015866814753316 | validation: 0.19078064782798737]
	TIME [epoch: 2.68 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18669128161775875		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.18669128161775875 | validation: 0.1971390097159086]
	TIME [epoch: 2.68 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19307023232810422		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.19307023232810422 | validation: 0.20992557731557682]
	TIME [epoch: 2.68 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20653491633187573		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.20653491633187573 | validation: 0.23050652762655438]
	TIME [epoch: 2.67 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2193620944477606		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.2193620944477606 | validation: 0.1717538196117661]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_809.pth
	Model improved!!!
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2035468568917775		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.2035468568917775 | validation: 0.19658271284320128]
	TIME [epoch: 2.68 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19686776352436655		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.19686776352436655 | validation: 0.2106391227428798]
	TIME [epoch: 2.68 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2009869320207698		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.2009869320207698 | validation: 0.2120908040017774]
	TIME [epoch: 2.68 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1924666703641492		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.1924666703641492 | validation: 0.18273481051860618]
	TIME [epoch: 2.68 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19795120731002855		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.19795120731002855 | validation: 0.20537268803896774]
	TIME [epoch: 2.68 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1954964399643401		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.1954964399643401 | validation: 0.21844329633258774]
	TIME [epoch: 2.68 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19961426110592595		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.19961426110592595 | validation: 0.23831614095477766]
	TIME [epoch: 2.68 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21904785381873393		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.21904785381873393 | validation: 0.19432757169940684]
	TIME [epoch: 2.68 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1798307479451423		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.1798307479451423 | validation: 0.18185649459225967]
	TIME [epoch: 2.68 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16836097766933178		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.16836097766933178 | validation: 0.1936658190707926]
	TIME [epoch: 2.68 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17282960929341604		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.17282960929341604 | validation: 0.18341551887890561]
	TIME [epoch: 2.68 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1812707803777011		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.1812707803777011 | validation: 0.23670755310039368]
	TIME [epoch: 2.68 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23339873881107984		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.23339873881107984 | validation: 0.2110909641830829]
	TIME [epoch: 2.68 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21804669003903696		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.21804669003903696 | validation: 0.1988699667714942]
	TIME [epoch: 2.68 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18175844174145003		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.18175844174145003 | validation: 0.19853044063926373]
	TIME [epoch: 2.68 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18488662620490406		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.18488662620490406 | validation: 0.20878702065106491]
	TIME [epoch: 2.68 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20008516204418506		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.20008516204418506 | validation: 0.19930533491538718]
	TIME [epoch: 2.68 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1848531524638946		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.1848531524638946 | validation: 0.18859182963875076]
	TIME [epoch: 2.68 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19180269158605256		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.19180269158605256 | validation: 0.1749057666899843]
	TIME [epoch: 2.68 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18189374163166386		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.18189374163166386 | validation: 0.19769771792248514]
	TIME [epoch: 2.68 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19404906876934683		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.19404906876934683 | validation: 0.17952249106260157]
	TIME [epoch: 2.68 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19204474573614827		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.19204474573614827 | validation: 0.19643964716591478]
	TIME [epoch: 2.68 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1885011424854699		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.1885011424854699 | validation: 0.18835168465481067]
	TIME [epoch: 2.68 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18016298309726395		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.18016298309726395 | validation: 0.2111846387875576]
	TIME [epoch: 2.68 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17965907689074553		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.17965907689074553 | validation: 0.2018274119563488]
	TIME [epoch: 2.68 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18028189325147637		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.18028189325147637 | validation: 0.19151302937438597]
	TIME [epoch: 2.68 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17639599993504615		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.17639599993504615 | validation: 0.21178596329700478]
	TIME [epoch: 2.68 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17670075802108662		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.17670075802108662 | validation: 0.19457419226010753]
	TIME [epoch: 2.68 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17317493674916729		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.17317493674916729 | validation: 0.19479116996831275]
	TIME [epoch: 2.68 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17348234078948616		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.17348234078948616 | validation: 0.17543602668152866]
	TIME [epoch: 2.68 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21234401716071838		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.21234401716071838 | validation: 0.214114548633393]
	TIME [epoch: 2.68 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19842197737599843		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.19842197737599843 | validation: 0.17797060162878584]
	TIME [epoch: 2.68 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16881855419685862		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.16881855419685862 | validation: 0.16905228192828725]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_842.pth
	Model improved!!!
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15618681179982014		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.15618681179982014 | validation: 0.17646072704429683]
	TIME [epoch: 2.67 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1589428656888795		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.1589428656888795 | validation: 0.18040180889196025]
	TIME [epoch: 2.67 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16193528483798353		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.16193528483798353 | validation: 0.19035698372412937]
	TIME [epoch: 2.67 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18492811912382343		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.18492811912382343 | validation: 0.24175251591538383]
	TIME [epoch: 2.68 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22708380786610963		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.22708380786610963 | validation: 0.21683139396819515]
	TIME [epoch: 2.68 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20142064640877239		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.20142064640877239 | validation: 0.1972016551933788]
	TIME [epoch: 2.68 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17625373272387013		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.17625373272387013 | validation: 0.17501192526312673]
	TIME [epoch: 2.68 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16317193452892528		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.16317193452892528 | validation: 0.17112362097957312]
	TIME [epoch: 2.68 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15604420942717517		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.15604420942717517 | validation: 0.18054055609088537]
	TIME [epoch: 2.68 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1628197877231164		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.1628197877231164 | validation: 0.19717892449750285]
	TIME [epoch: 2.68 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1702537120918032		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.1702537120918032 | validation: 0.21642453382382654]
	TIME [epoch: 2.68 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20117233448544752		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.20117233448544752 | validation: 0.20906268636023284]
	TIME [epoch: 2.68 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19071812026014065		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.19071812026014065 | validation: 0.1608142452352215]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_855.pth
	Model improved!!!
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16728979807337005		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.16728979807337005 | validation: 0.22030078187648927]
	TIME [epoch: 2.67 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18247040209198478		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.18247040209198478 | validation: 0.23918191084027557]
	TIME [epoch: 2.67 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1943702241302568		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.1943702241302568 | validation: 0.18817673826119158]
	TIME [epoch: 2.67 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1570289009751403		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.1570289009751403 | validation: 0.1622524302569569]
	TIME [epoch: 2.67 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15609028987875337		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.15609028987875337 | validation: 0.17990799542728486]
	TIME [epoch: 2.68 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16761277795648763		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.16761277795648763 | validation: 0.18421015851731293]
	TIME [epoch: 2.69 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.182957355703669		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.182957355703669 | validation: 0.20390920271960528]
	TIME [epoch: 2.68 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17906930935265322		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.17906930935265322 | validation: 0.15787769328788595]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_863.pth
	Model improved!!!
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1654592121426964		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.1654592121426964 | validation: 0.1820998245708163]
	TIME [epoch: 2.67 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15931863120995884		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.15931863120995884 | validation: 0.1967298593654534]
	TIME [epoch: 2.67 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1695873194415132		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.1695873194415132 | validation: 0.2042820057917985]
	TIME [epoch: 2.67 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16761352241580865		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.16761352241580865 | validation: 0.17025114747132264]
	TIME [epoch: 2.67 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16647932523945655		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.16647932523945655 | validation: 0.17874463341190755]
	TIME [epoch: 2.67 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1631409911787796		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.1631409911787796 | validation: 0.16867282203531153]
	TIME [epoch: 2.68 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.160560007686693		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.160560007686693 | validation: 0.19000272917721353]
	TIME [epoch: 2.67 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17160160987540019		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.17160160987540019 | validation: 0.1695635825783587]
	TIME [epoch: 2.67 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16061148194242847		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.16061148194242847 | validation: 0.1829684531205983]
	TIME [epoch: 2.68 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15892004950705757		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.15892004950705757 | validation: 0.19537860338607022]
	TIME [epoch: 2.67 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17597751749146653		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.17597751749146653 | validation: 0.2244802300582876]
	TIME [epoch: 2.67 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19635491012811465		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.19635491012811465 | validation: 0.1975401274194342]
	TIME [epoch: 2.67 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17282969515994745		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.17282969515994745 | validation: 0.1831991315495768]
	TIME [epoch: 2.67 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15894408897509832		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.15894408897509832 | validation: 0.1664780725069794]
	TIME [epoch: 2.67 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15651329506026357		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.15651329506026357 | validation: 0.17396630087053866]
	TIME [epoch: 2.67 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15973866552668828		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.15973866552668828 | validation: 0.1653416267112668]
	TIME [epoch: 2.68 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16530619059195992		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.16530619059195992 | validation: 0.1823608902292768]
	TIME [epoch: 2.67 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16336433369957154		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.16336433369957154 | validation: 0.21089114765691566]
	TIME [epoch: 2.67 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18127596239606777		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.18127596239606777 | validation: 0.21375325064145922]
	TIME [epoch: 2.67 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1712473264676707		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.1712473264676707 | validation: 0.15655322004233263]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_883.pth
	Model improved!!!
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14638608456651203		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.14638608456651203 | validation: 0.15712957491439894]
	TIME [epoch: 2.67 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1441083518159669		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.1441083518159669 | validation: 0.18100077748784738]
	TIME [epoch: 2.67 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15265037109019655		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.15265037109019655 | validation: 0.16593789798177125]
	TIME [epoch: 2.67 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1638080586898883		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.1638080586898883 | validation: 0.192952292325974]
	TIME [epoch: 2.67 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1783051583187448		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.1783051583187448 | validation: 0.1794667805079947]
	TIME [epoch: 2.68 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16835889186627867		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.16835889186627867 | validation: 0.17096208677640523]
	TIME [epoch: 2.67 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15223427554499208		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.15223427554499208 | validation: 0.1792950763067366]
	TIME [epoch: 2.68 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15763743551975734		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.15763743551975734 | validation: 0.18794422796531554]
	TIME [epoch: 2.67 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15581346029299936		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.15581346029299936 | validation: 0.21083256967376318]
	TIME [epoch: 2.67 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18188395721544381		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.18188395721544381 | validation: 0.18832235021727517]
	TIME [epoch: 2.67 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16099339911239302		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.16099339911239302 | validation: 0.16607268397089026]
	TIME [epoch: 2.68 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15652096966701787		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.15652096966701787 | validation: 0.18108632198787392]
	TIME [epoch: 2.67 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16169106956439663		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.16169106956439663 | validation: 0.17777698970136827]
	TIME [epoch: 2.67 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1553235591869375		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.1553235591869375 | validation: 0.1732739066336448]
	TIME [epoch: 2.67 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15083536964619954		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.15083536964619954 | validation: 0.1561056903572351]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_898.pth
	Model improved!!!
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15131055351048742		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.15131055351048742 | validation: 0.1796495893097836]
	TIME [epoch: 2.67 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15105653409881126		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.15105653409881126 | validation: 0.17181344403896526]
	TIME [epoch: 2.67 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15362781812120843		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.15362781812120843 | validation: 0.2006960163689279]
	TIME [epoch: 2.67 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15766665882670217		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.15766665882670217 | validation: 0.17964476536978924]
	TIME [epoch: 2.67 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1502325298627226		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.1502325298627226 | validation: 0.15635248258855075]
	TIME [epoch: 2.67 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1423791387716037		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.1423791387716037 | validation: 0.17275351984520934]
	TIME [epoch: 2.67 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15023070510365202		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.15023070510365202 | validation: 0.16565056134851272]
	TIME [epoch: 2.68 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1684759347306197		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.1684759347306197 | validation: 0.19630865427184552]
	TIME [epoch: 2.68 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1741820013910707		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.1741820013910707 | validation: 0.1674919248113494]
	TIME [epoch: 2.67 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1496802024889879		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.1496802024889879 | validation: 0.17548536771485468]
	TIME [epoch: 2.68 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14403414730513478		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.14403414730513478 | validation: 0.1733873087798535]
	TIME [epoch: 2.67 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1460037181488849		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.1460037181488849 | validation: 0.17887482310671732]
	TIME [epoch: 2.68 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14659046859037003		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.14659046859037003 | validation: 0.17824975796010814]
	TIME [epoch: 2.67 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16034398251972223		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.16034398251972223 | validation: 0.18477592667791864]
	TIME [epoch: 2.67 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16718818207672512		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.16718818207672512 | validation: 0.1645433847436833]
	TIME [epoch: 2.67 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14923313539801958		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.14923313539801958 | validation: 0.1656772577079082]
	TIME [epoch: 2.68 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14534557518036084		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.14534557518036084 | validation: 0.17419856075439602]
	TIME [epoch: 2.67 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14559221642908685		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.14559221642908685 | validation: 0.18769513226190193]
	TIME [epoch: 2.68 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15055683526691596		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.15055683526691596 | validation: 0.18455640313281402]
	TIME [epoch: 2.67 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15310559598583065		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.15310559598583065 | validation: 0.18337258995766692]
	TIME [epoch: 2.67 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15040312933291286		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.15040312933291286 | validation: 0.17637790666997324]
	TIME [epoch: 2.67 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14449548215253244		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.14449548215253244 | validation: 0.1721228032136944]
	TIME [epoch: 2.67 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14435675009776588		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.14435675009776588 | validation: 0.15736589809703824]
	TIME [epoch: 2.67 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14211564260001702		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.14211564260001702 | validation: 0.17104062230593794]
	TIME [epoch: 2.67 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14435923292945313		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.14435923292945313 | validation: 0.17594165326254163]
	TIME [epoch: 2.67 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15352181547225643		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.15352181547225643 | validation: 0.18374115401532398]
	TIME [epoch: 2.67 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1560102369718185		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.1560102369718185 | validation: 0.1617261273344005]
	TIME [epoch: 2.67 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1513246590232163		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.1513246590232163 | validation: 0.16800993869124553]
	TIME [epoch: 2.67 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14360389047955277		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.14360389047955277 | validation: 0.15770253016449978]
	TIME [epoch: 2.68 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14035469374782916		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.14035469374782916 | validation: 0.17537607579893014]
	TIME [epoch: 2.67 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14480044728485048		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.14480044728485048 | validation: 0.15603871797269678]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_929.pth
	Model improved!!!
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14715947040624083		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.14715947040624083 | validation: 0.18000337303327496]
	TIME [epoch: 2.67 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14831496674233094		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.14831496674233094 | validation: 0.16110287856436567]
	TIME [epoch: 2.68 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1494016049519824		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.1494016049519824 | validation: 0.20135177484779448]
	TIME [epoch: 2.68 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16112458550082479		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.16112458550082479 | validation: 0.17675719042820096]
	TIME [epoch: 2.68 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13648405098045824		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.13648405098045824 | validation: 0.15121977748469506]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_934.pth
	Model improved!!!
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14040093202487205		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.14040093202487205 | validation: 0.20816086861479463]
	TIME [epoch: 2.67 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1661978574504937		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.1661978574504937 | validation: 0.19024165088014008]
	TIME [epoch: 2.67 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1432172980524109		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.1432172980524109 | validation: 0.16963620196457183]
	TIME [epoch: 2.68 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12894947707870494		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.12894947707870494 | validation: 0.1671953455596198]
	TIME [epoch: 2.67 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1359470974876812		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.1359470974876812 | validation: 0.17999384102056193]
	TIME [epoch: 2.68 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1433799395586734		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.1433799395586734 | validation: 0.18012805081092498]
	TIME [epoch: 2.67 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1519690728130756		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.1519690728130756 | validation: 0.1698304373398349]
	TIME [epoch: 2.68 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15501166744515912		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.15501166744515912 | validation: 0.1510951945207705]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_942.pth
	Model improved!!!
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13832934855723408		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.13832934855723408 | validation: 0.17367759815965153]
	TIME [epoch: 2.67 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13986012947837828		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.13986012947837828 | validation: 0.18106257523449729]
	TIME [epoch: 2.67 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14203340823667937		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.14203340823667937 | validation: 0.17388068470452486]
	TIME [epoch: 2.67 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1331794489102888		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.1331794489102888 | validation: 0.15476500160730652]
	TIME [epoch: 2.67 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1341843133465968		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.1341843133465968 | validation: 0.15895757147126138]
	TIME [epoch: 2.67 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12737091698996		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.12737091698996 | validation: 0.17044098847688008]
	TIME [epoch: 2.68 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1482979408017435		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.1482979408017435 | validation: 0.2164116119716647]
	TIME [epoch: 2.67 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17155166549834774		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.17155166549834774 | validation: 0.16157189971030705]
	TIME [epoch: 2.68 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14729566825976556		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.14729566825976556 | validation: 0.15949188754563626]
	TIME [epoch: 2.67 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13616578474300525		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.13616578474300525 | validation: 0.16508872987178402]
	TIME [epoch: 2.67 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13229327638054497		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.13229327638054497 | validation: 0.17703785057329696]
	TIME [epoch: 2.67 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13966648737028522		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.13966648737028522 | validation: 0.16684253803382074]
	TIME [epoch: 2.67 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1378741096398389		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.1378741096398389 | validation: 0.1600632862601441]
	TIME [epoch: 2.67 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14131815953860055		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.14131815953860055 | validation: 0.1545709116166498]
	TIME [epoch: 2.68 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13520713547714888		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.13520713547714888 | validation: 0.17112761744994748]
	TIME [epoch: 2.67 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1379385890094539		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.1379385890094539 | validation: 0.17171072569916893]
	TIME [epoch: 2.67 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1552760820245402		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.1552760820245402 | validation: 0.18296688337831069]
	TIME [epoch: 2.68 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14368102994924875		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.14368102994924875 | validation: 0.18234549470598843]
	TIME [epoch: 2.67 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17131958113164658		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.17131958113164658 | validation: 0.21486751585154967]
	TIME [epoch: 2.67 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17774151210256137		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.17774151210256137 | validation: 0.14593944340490894]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_962.pth
	Model improved!!!
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1375318229379496		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.1375318229379496 | validation: 0.16917495219173556]
	TIME [epoch: 2.67 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1506220785376202		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.1506220785376202 | validation: 0.17794370421049716]
	TIME [epoch: 2.67 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14462058508613534		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.14462058508613534 | validation: 0.15814059307715428]
	TIME [epoch: 2.67 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1378851561854708		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.1378851561854708 | validation: 0.17222972243133794]
	TIME [epoch: 2.67 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13965266317923422		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.13965266317923422 | validation: 0.16500326771725082]
	TIME [epoch: 2.67 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13974739157454077		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.13974739157454077 | validation: 0.17470104135823486]
	TIME [epoch: 2.67 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13470663738667316		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.13470663738667316 | validation: 0.15414456588537756]
	TIME [epoch: 2.67 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1292342359151149		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.1292342359151149 | validation: 0.16547308774304964]
	TIME [epoch: 2.68 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.127184498504787		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.127184498504787 | validation: 0.1557225113568458]
	TIME [epoch: 2.67 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13629807406769653		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.13629807406769653 | validation: 0.18375848532964612]
	TIME [epoch: 2.67 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1373167874899401		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.1373167874899401 | validation: 0.17266313300521946]
	TIME [epoch: 2.67 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13653723899186063		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.13653723899186063 | validation: 0.1738989447004855]
	TIME [epoch: 2.67 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14183013456985763		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.14183013456985763 | validation: 0.15772191062684277]
	TIME [epoch: 2.67 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12836960992943855		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.12836960992943855 | validation: 0.16018654727071263]
	TIME [epoch: 2.67 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12653706347505822		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.12653706347505822 | validation: 0.16802908189464205]
	TIME [epoch: 2.67 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1326392241077381		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.1326392241077381 | validation: 0.192902599229815]
	TIME [epoch: 2.67 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13997183471712085		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.13997183471712085 | validation: 0.1562518509996536]
	TIME [epoch: 2.67 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13497375906826245		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.13497375906826245 | validation: 0.1837040448668874]
	TIME [epoch: 2.67 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13604010784559573		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.13604010784559573 | validation: 0.18268478522550183]
	TIME [epoch: 2.68 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14373588651948693		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.14373588651948693 | validation: 0.17228806092418483]
	TIME [epoch: 2.67 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13013863923831917		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.13013863923831917 | validation: 0.15039609263705864]
	TIME [epoch: 2.67 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12571522389289363		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.12571522389289363 | validation: 0.1570687374640436]
	TIME [epoch: 2.67 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12286013635418976		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.12286013635418976 | validation: 0.14648223965867024]
	TIME [epoch: 2.67 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11876629428535111		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.11876629428535111 | validation: 0.15336349126217466]
	TIME [epoch: 2.67 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12035549973330635		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.12035549973330635 | validation: 0.15555679635761083]
	TIME [epoch: 2.67 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1268148372276219		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.1268148372276219 | validation: 0.17350693540915857]
	TIME [epoch: 2.67 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1394753222274732		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.1394753222274732 | validation: 0.167146916283066]
	TIME [epoch: 2.67 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14426710731064593		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.14426710731064593 | validation: 0.1759543547880329]
	TIME [epoch: 2.67 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14120344767810727		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.14120344767810727 | validation: 0.14795039376741564]
	TIME [epoch: 2.67 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13041561158168585		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.13041561158168585 | validation: 0.1609329127714748]
	TIME [epoch: 2.68 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1275650769187912		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.1275650769187912 | validation: 0.15537511998551348]
	TIME [epoch: 2.67 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13277966431442842		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.13277966431442842 | validation: 0.15745701301060874]
	TIME [epoch: 2.67 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1269599746958306		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.1269599746958306 | validation: 0.16189252027010234]
	TIME [epoch: 2.67 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13166212383692755		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.13166212383692755 | validation: 0.15683014584349764]
	TIME [epoch: 2.67 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13172763001119214		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.13172763001119214 | validation: 0.17241400920176714]
	TIME [epoch: 2.67 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13592790802358953		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.13592790802358953 | validation: 0.17299759120231606]
	TIME [epoch: 2.67 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1429994639706032		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.1429994639706032 | validation: 0.166171768467714]
	TIME [epoch: 2.67 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13738568430813122		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.13738568430813122 | validation: 0.15304076127863137]
	TIME [epoch: 2.67 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12114194877327844		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.12114194877327844 | validation: 0.150787698847825]
	TIME [epoch: 180 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11983439148789782		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.11983439148789782 | validation: 0.1410922976344333]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_1002.pth
	Model improved!!!
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12694291255844825		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.12694291255844825 | validation: 0.17474443465976103]
	TIME [epoch: 5.71 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1304021327542563		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.1304021327542563 | validation: 0.16129535594828984]
	TIME [epoch: 5.71 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13699484387651847		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.13699484387651847 | validation: 0.16620491695130635]
	TIME [epoch: 5.71 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13331407509859058		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.13331407509859058 | validation: 0.15062419754910872]
	TIME [epoch: 5.71 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12473399581873318		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.12473399581873318 | validation: 0.16095225248954478]
	TIME [epoch: 5.71 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.119968058897929		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.119968058897929 | validation: 0.1557139200773816]
	TIME [epoch: 5.72 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12221912589998665		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.12221912589998665 | validation: 0.16103485593302128]
	TIME [epoch: 5.71 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13042809339920414		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.13042809339920414 | validation: 0.17251498231734053]
	TIME [epoch: 5.72 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13131317319433336		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.13131317319433336 | validation: 0.15907861687150773]
	TIME [epoch: 5.71 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13196021039854966		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.13196021039854966 | validation: 0.14931905590859723]
	TIME [epoch: 5.71 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12755778098383014		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.12755778098383014 | validation: 0.15432902539956758]
	TIME [epoch: 5.72 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12557215368943161		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.12557215368943161 | validation: 0.13911274811176916]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_1014.pth
	Model improved!!!
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12326692278747471		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.12326692278747471 | validation: 0.17237450022611692]
	TIME [epoch: 5.71 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12536065489480017		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.12536065489480017 | validation: 0.1571668899250147]
	TIME [epoch: 5.72 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12528372335231405		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.12528372335231405 | validation: 0.1581678539960513]
	TIME [epoch: 5.71 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1237452291198791		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.1237452291198791 | validation: 0.16229590200265767]
	TIME [epoch: 5.72 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1278603772463144		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.1278603772463144 | validation: 0.1552501999077523]
	TIME [epoch: 5.72 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13472289213886762		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.13472289213886762 | validation: 0.16817823767085097]
	TIME [epoch: 5.73 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1328030072823125		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.1328030072823125 | validation: 0.15224343358856704]
	TIME [epoch: 5.72 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1227223923602677		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.1227223923602677 | validation: 0.15329518788925645]
	TIME [epoch: 5.72 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11856250696908344		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.11856250696908344 | validation: 0.1565233590418084]
	TIME [epoch: 5.74 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12245357654042703		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.12245357654042703 | validation: 0.16473594937954178]
	TIME [epoch: 5.74 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12496572658595982		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.12496572658595982 | validation: 0.15732430245264142]
	TIME [epoch: 5.74 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12278224344327207		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.12278224344327207 | validation: 0.1489107621115458]
	TIME [epoch: 5.75 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11921465014701255		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.11921465014701255 | validation: 0.14629397207021047]
	TIME [epoch: 5.75 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12121911641815994		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.12121911641815994 | validation: 0.1519949777241364]
	TIME [epoch: 5.75 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12734381291955424		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.12734381291955424 | validation: 0.14096041016953978]
	TIME [epoch: 5.76 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12625845432044316		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.12625845432044316 | validation: 0.15781226757706315]
	TIME [epoch: 5.75 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1296090711732711		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.1296090711732711 | validation: 0.1532189696325702]
	TIME [epoch: 5.75 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12521975909184263		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.12521975909184263 | validation: 0.1567113603994958]
	TIME [epoch: 5.75 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12540114394102883		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.12540114394102883 | validation: 0.15344790148875587]
	TIME [epoch: 5.75 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12092775564490957		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.12092775564490957 | validation: 0.1666621823007437]
	TIME [epoch: 5.75 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12231492987488043		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.12231492987488043 | validation: 0.1684952237611128]
	TIME [epoch: 5.75 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12598971950889606		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.12598971950889606 | validation: 0.16710714460609832]
	TIME [epoch: 5.75 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12099693174221383		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.12099693174221383 | validation: 0.15479839726148]
	TIME [epoch: 5.74 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11798313882353682		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.11798313882353682 | validation: 0.1444357481174396]
	TIME [epoch: 5.75 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11517226113048837		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.11517226113048837 | validation: 0.1472385832154801]
	TIME [epoch: 5.75 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1129927297548141		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.1129927297548141 | validation: 0.14998599475842062]
	TIME [epoch: 5.75 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11373455363516344		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.11373455363516344 | validation: 0.20491688030577732]
	TIME [epoch: 5.74 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17381191707439075		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.17381191707439075 | validation: 0.20052648118595232]
	TIME [epoch: 5.76 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14427871519419705		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.14427871519419705 | validation: 0.1657267823490567]
	TIME [epoch: 5.75 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13698883668678105		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.13698883668678105 | validation: 0.16674521157894875]
	TIME [epoch: 5.76 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12394945873944681		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.12394945873944681 | validation: 0.15770932174367036]
	TIME [epoch: 5.75 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12557016813345917		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.12557016813345917 | validation: 0.1511717984649515]
	TIME [epoch: 5.75 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11666120149537981		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.11666120149537981 | validation: 0.1449503308088764]
	TIME [epoch: 5.75 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11751731924686062		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.11751731924686062 | validation: 0.14670170364676727]
	TIME [epoch: 5.72 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12271660861500597		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.12271660861500597 | validation: 0.1645469059453563]
	TIME [epoch: 5.72 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12557699641180137		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.12557699641180137 | validation: 0.20399969750313673]
	TIME [epoch: 5.72 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18007746001330308		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.18007746001330308 | validation: 0.190971648127651]
	TIME [epoch: 5.72 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13435858181352703		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.13435858181352703 | validation: 0.15261527486303672]
	TIME [epoch: 5.71 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11797640924819051		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.11797640924819051 | validation: 0.1564758752514958]
	TIME [epoch: 5.71 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12235162344863548		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.12235162344863548 | validation: 0.1428699023031602]
	TIME [epoch: 5.71 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11758924259949001		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.11758924259949001 | validation: 0.14591464472891058]
	TIME [epoch: 5.72 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11468625261956275		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.11468625261956275 | validation: 0.14029992561353738]
	TIME [epoch: 5.71 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11513747829652887		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.11513747829652887 | validation: 0.14009814915666374]
	TIME [epoch: 5.72 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11702468580874242		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.11702468580874242 | validation: 0.15233572574977733]
	TIME [epoch: 5.71 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11969345876045204		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.11969345876045204 | validation: 0.1504369776654718]
	TIME [epoch: 5.74 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11954468592035503		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.11954468592035503 | validation: 0.166408980244812]
	TIME [epoch: 5.72 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12346149506756852		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.12346149506756852 | validation: 0.1631900328693003]
	TIME [epoch: 5.71 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11818681088345927		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.11818681088345927 | validation: 0.15602509145863894]
	TIME [epoch: 5.72 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11625949616114062		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.11625949616114062 | validation: 0.14013647480323474]
	TIME [epoch: 5.7 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11513480351370844		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.11513480351370844 | validation: 0.15317008876005955]
	TIME [epoch: 5.71 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11620285208903659		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.11620285208903659 | validation: 0.1472520532605809]
	TIME [epoch: 5.72 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11837340908071531		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.11837340908071531 | validation: 0.16769408130926783]
	TIME [epoch: 5.71 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1276432937062069		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.1276432937062069 | validation: 0.15255088150640578]
	TIME [epoch: 5.71 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12009840983310657		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.12009840983310657 | validation: 0.1527609674110515]
	TIME [epoch: 5.71 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11321782343836943		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.11321782343836943 | validation: 0.1474478118210559]
	TIME [epoch: 5.72 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11390220582217055		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.11390220582217055 | validation: 0.14952135460417515]
	TIME [epoch: 5.72 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11356988102315586		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.11356988102315586 | validation: 0.14020299609077821]
	TIME [epoch: 5.71 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11031856081875134		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.11031856081875134 | validation: 0.1497188179926199]
	TIME [epoch: 5.71 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11510694725127821		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.11510694725127821 | validation: 0.14471526895952772]
	TIME [epoch: 5.72 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11591888892031844		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.11591888892031844 | validation: 0.15610742781802958]
	TIME [epoch: 5.72 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11839037424815375		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.11839037424815375 | validation: 0.14674563479784392]
	TIME [epoch: 5.72 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12014127811845934		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.12014127811845934 | validation: 0.16685918879801662]
	TIME [epoch: 5.71 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12289011531507146		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.12289011531507146 | validation: 0.14220265405416196]
	TIME [epoch: 5.71 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11964382576547707		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.11964382576547707 | validation: 0.15410530425296456]
	TIME [epoch: 5.71 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11236943947194952		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.11236943947194952 | validation: 0.1627682223635606]
	TIME [epoch: 5.71 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11645254068255594		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.11645254068255594 | validation: 0.1620755248434187]
	TIME [epoch: 5.72 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1220887474564077		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.1220887474564077 | validation: 0.1581288376705697]
	TIME [epoch: 5.72 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1174867193545828		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.1174867193545828 | validation: 0.15718909831353817]
	TIME [epoch: 5.72 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11944853024631283		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.11944853024631283 | validation: 0.14449324910535147]
	TIME [epoch: 5.71 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11844774593476079		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.11844774593476079 | validation: 0.14860886446781912]
	TIME [epoch: 5.72 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11624562981202534		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.11624562981202534 | validation: 0.14911049861714337]
	TIME [epoch: 5.72 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11030515808112501		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.11030515808112501 | validation: 0.14008985689932704]
	TIME [epoch: 5.72 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11557700088502436		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.11557700088502436 | validation: 0.14567909199301093]
	TIME [epoch: 5.72 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11209328867356946		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.11209328867356946 | validation: 0.1415455424183991]
	TIME [epoch: 5.72 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11570642596999937		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.11570642596999937 | validation: 0.15725788092728846]
	TIME [epoch: 5.72 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11940944851433871		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.11940944851433871 | validation: 0.15505256541289225]
	TIME [epoch: 5.71 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12086017282023108		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.12086017282023108 | validation: 0.15974094564635635]
	TIME [epoch: 5.72 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11481159181903523		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.11481159181903523 | validation: 0.14486581707078255]
	TIME [epoch: 5.72 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10964154869257133		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.10964154869257133 | validation: 0.1529186896478682]
	TIME [epoch: 5.72 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11121715060141289		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.11121715060141289 | validation: 0.14025859226021012]
	TIME [epoch: 5.72 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11141729821240975		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.11141729821240975 | validation: 0.13564497531767267]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_1095.pth
	Model improved!!!
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11004939085950376		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.11004939085950376 | validation: 0.15732973933639538]
	TIME [epoch: 5.72 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11341987577928545		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.11341987577928545 | validation: 0.1503685489573825]
	TIME [epoch: 5.71 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11383467732386801		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.11383467732386801 | validation: 0.15290988454667992]
	TIME [epoch: 5.71 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11690687116739203		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.11690687116739203 | validation: 0.14644020720767334]
	TIME [epoch: 5.71 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11345974983175452		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.11345974983175452 | validation: 0.1381248319734971]
	TIME [epoch: 5.71 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11062119255739697		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.11062119255739697 | validation: 0.1688877191037018]
	TIME [epoch: 5.71 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11864487376251134		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.11864487376251134 | validation: 0.1648227463206339]
	TIME [epoch: 5.71 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12269210266274874		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.12269210266274874 | validation: 0.15581547861829925]
	TIME [epoch: 5.72 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10675955657082935		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.10675955657082935 | validation: 0.13905042029110312]
	TIME [epoch: 5.71 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10760299806046543		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.10760299806046543 | validation: 0.15287432277639884]
	TIME [epoch: 5.72 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12320001117239254		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.12320001117239254 | validation: 0.16434404813944986]
	TIME [epoch: 5.71 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13370847231903393		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.13370847231903393 | validation: 0.15580546285214109]
	TIME [epoch: 5.71 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11196612949871629		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.11196612949871629 | validation: 0.14571523442269507]
	TIME [epoch: 5.7 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10564369551483495		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.10564369551483495 | validation: 0.14353877434785087]
	TIME [epoch: 5.72 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10868306006761438		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.10868306006761438 | validation: 0.14965104241771926]
	TIME [epoch: 5.71 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1111259591403098		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.1111259591403098 | validation: 0.17300510932993662]
	TIME [epoch: 5.71 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13415731111432488		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.13415731111432488 | validation: 0.15796143465483148]
	TIME [epoch: 5.73 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11546501670909161		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.11546501670909161 | validation: 0.1439696676069477]
	TIME [epoch: 5.71 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10839811139715863		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.10839811139715863 | validation: 0.14699338145552507]
	TIME [epoch: 5.71 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11142581075577593		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.11142581075577593 | validation: 0.14417700558955768]
	TIME [epoch: 5.71 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10994108979052963		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.10994108979052963 | validation: 0.1528197800331098]
	TIME [epoch: 5.71 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10879419779270762		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.10879419779270762 | validation: 0.14874769321307732]
	TIME [epoch: 5.71 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1067113682977575		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.1067113682977575 | validation: 0.1488177018264186]
	TIME [epoch: 5.71 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10902188805048707		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.10902188805048707 | validation: 0.1566886240901134]
	TIME [epoch: 5.71 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1156651051338346		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.1156651051338346 | validation: 0.14773277560814332]
	TIME [epoch: 5.72 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11769870679421068		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.11769870679421068 | validation: 0.14633969961995183]
	TIME [epoch: 5.72 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11718688180750853		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.11718688180750853 | validation: 0.14385519287321544]
	TIME [epoch: 5.72 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11275975873442555		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.11275975873442555 | validation: 0.1481862121544647]
	TIME [epoch: 5.71 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10913371695805531		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.10913371695805531 | validation: 0.15928946156041654]
	TIME [epoch: 5.71 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1100512082847244		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.1100512082847244 | validation: 0.14741770396629933]
	TIME [epoch: 5.71 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10535366581456504		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.10535366581456504 | validation: 0.1480211086521993]
	TIME [epoch: 5.71 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10689002877788095		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.10689002877788095 | validation: 0.13949690955312213]
	TIME [epoch: 5.72 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10788375885273237		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.10788375885273237 | validation: 0.1495359700230324]
	TIME [epoch: 5.72 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11205152867673485		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.11205152867673485 | validation: 0.1554158673189133]
	TIME [epoch: 5.72 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11798829849946493		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.11798829849946493 | validation: 0.16267413443741494]
	TIME [epoch: 5.71 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12014061221271835		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.12014061221271835 | validation: 0.14994859397224067]
	TIME [epoch: 5.72 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11124833626408898		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.11124833626408898 | validation: 0.14524704955817144]
	TIME [epoch: 5.72 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10715278524447293		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.10715278524447293 | validation: 0.13443548007903017]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_1133.pth
	Model improved!!!
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.104584097156293		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.104584097156293 | validation: 0.1496788267050703]
	TIME [epoch: 5.7 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1058155489922896		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.1058155489922896 | validation: 0.15162833913103627]
	TIME [epoch: 5.71 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11116873953622991		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.11116873953622991 | validation: 0.15369433552398395]
	TIME [epoch: 5.72 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11500606798706002		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.11500606798706002 | validation: 0.1567655772460796]
	TIME [epoch: 5.72 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11264073192543377		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.11264073192543377 | validation: 0.14095073996444343]
	TIME [epoch: 5.72 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1101944560875797		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.1101944560875797 | validation: 0.13774851706919666]
	TIME [epoch: 5.71 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10965870435735953		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.10965870435735953 | validation: 0.16239651826202345]
	TIME [epoch: 5.71 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11728535544864421		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.11728535544864421 | validation: 0.14933242101388897]
	TIME [epoch: 5.71 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11143346619025986		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.11143346619025986 | validation: 0.14217555872237383]
	TIME [epoch: 5.7 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10894761329561732		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.10894761329561732 | validation: 0.1417274005981716]
	TIME [epoch: 5.72 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10996307971295811		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.10996307971295811 | validation: 0.15031536009887367]
	TIME [epoch: 5.71 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1098521463762249		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.1098521463762249 | validation: 0.15790233384266597]
	TIME [epoch: 5.71 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11210494777082954		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.11210494777082954 | validation: 0.15001796498427805]
	TIME [epoch: 5.71 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11082726726352359		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.11082726726352359 | validation: 0.14004692918597342]
	TIME [epoch: 5.71 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1085122248702116		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.1085122248702116 | validation: 0.14547189433712387]
	TIME [epoch: 5.72 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10908918867254695		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.10908918867254695 | validation: 0.14501474250891264]
	TIME [epoch: 5.71 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1082488516045682		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.1082488516045682 | validation: 0.15112486231120326]
	TIME [epoch: 5.71 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10743880360829376		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.10743880360829376 | validation: 0.14268729706457992]
	TIME [epoch: 5.71 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10640835038558276		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.10640835038558276 | validation: 0.14474279175399973]
	TIME [epoch: 5.71 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10748262895096204		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.10748262895096204 | validation: 0.15194029640179585]
	TIME [epoch: 5.71 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10903455022719653		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.10903455022719653 | validation: 0.1462921299780657]
	TIME [epoch: 5.71 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10625004409047438		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.10625004409047438 | validation: 0.16132637246265838]
	TIME [epoch: 5.72 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11279722235747337		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.11279722235747337 | validation: 0.15533350888264436]
	TIME [epoch: 5.71 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10861982530699432		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.10861982530699432 | validation: 0.1448537284320829]
	TIME [epoch: 5.7 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10571164065152208		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.10571164065152208 | validation: 0.1442134358239378]
	TIME [epoch: 5.71 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10683134804495797		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.10683134804495797 | validation: 0.14790446377085129]
	TIME [epoch: 5.71 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11153335833104665		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.11153335833104665 | validation: 0.14752814719246865]
	TIME [epoch: 5.72 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10895680098345559		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.10895680098345559 | validation: 0.14496689114525363]
	TIME [epoch: 5.71 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10519633230776813		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.10519633230776813 | validation: 0.1492213297993133]
	TIME [epoch: 5.72 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10722135414382301		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.10722135414382301 | validation: 0.15095166168071802]
	TIME [epoch: 5.71 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10897830627410271		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.10897830627410271 | validation: 0.1408777352636918]
	TIME [epoch: 5.72 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10958919409031999		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.10958919409031999 | validation: 0.14490110162016853]
	TIME [epoch: 5.72 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10470676080740024		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.10470676080740024 | validation: 0.14574792523125302]
	TIME [epoch: 5.71 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10194154846179064		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.10194154846179064 | validation: 0.1490658756958345]
	TIME [epoch: 5.71 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1042715408690834		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.1042715408690834 | validation: 0.13733913412928014]
	TIME [epoch: 5.71 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10597429538788306		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.10597429538788306 | validation: 0.15367396822002774]
	TIME [epoch: 5.71 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10293211703975645		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.10293211703975645 | validation: 0.14658271068985804]
	TIME [epoch: 5.7 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10446510377223323		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.10446510377223323 | validation: 0.13517870246517433]
	TIME [epoch: 5.71 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10363592092982099		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.10363592092982099 | validation: 0.14737435173055183]
	TIME [epoch: 5.71 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10809223328225454		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.10809223328225454 | validation: 0.13793882897923362]
	TIME [epoch: 5.72 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.111024279319269		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.111024279319269 | validation: 0.1503931843299675]
	TIME [epoch: 5.72 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10777694517786904		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.10777694517786904 | validation: 0.1516125333002395]
	TIME [epoch: 5.71 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10761910134895546		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.10761910134895546 | validation: 0.15102205073030073]
	TIME [epoch: 5.72 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11118924357525307		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.11118924357525307 | validation: 0.1625145109309895]
	TIME [epoch: 5.72 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1107918131442057		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.1107918131442057 | validation: 0.14764807392887205]
	TIME [epoch: 5.71 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10821044184817223		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.10821044184817223 | validation: 0.14299713020049692]
	TIME [epoch: 5.71 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10565995511170061		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.10565995511170061 | validation: 0.19277625156531786]
	TIME [epoch: 5.71 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14217724826284997		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.14217724826284997 | validation: 0.16896972979834643]
	TIME [epoch: 5.72 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11081974582894816		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.11081974582894816 | validation: 0.15839131862571268]
	TIME [epoch: 5.72 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11567227246471586		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.11567227246471586 | validation: 0.14730861019502528]
	TIME [epoch: 5.72 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10474764098691104		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.10474764098691104 | validation: 0.15225416275185324]
	TIME [epoch: 5.72 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10635026672347322		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.10635026672347322 | validation: 0.14632551601712665]
	TIME [epoch: 5.72 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.104918445805817		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.104918445805817 | validation: 0.13271690466102748]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_1186.pth
	Model improved!!!
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1031234729994688		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.1031234729994688 | validation: 0.15688083005755676]
	TIME [epoch: 5.71 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10261226811823747		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.10261226811823747 | validation: 0.14336664602351262]
	TIME [epoch: 5.71 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10424374269151926		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.10424374269151926 | validation: 0.14819286543854568]
	TIME [epoch: 5.71 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1035370503853927		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.1035370503853927 | validation: 0.15506303434846253]
	TIME [epoch: 5.72 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10805890782151505		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.10805890782151505 | validation: 0.14275769198255903]
	TIME [epoch: 5.71 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10780872694920397		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.10780872694920397 | validation: 0.14875865979903982]
	TIME [epoch: 5.72 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10686530953269528		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.10686530953269528 | validation: 0.1473335099435378]
	TIME [epoch: 5.71 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10453316997845882		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.10453316997845882 | validation: 0.15348348278090188]
	TIME [epoch: 5.72 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10684288714338097		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.10684288714338097 | validation: 0.1569058526930045]
	TIME [epoch: 5.72 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10878976522371517		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.10878976522371517 | validation: 0.1527282356230106]
	TIME [epoch: 5.71 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10465889700315223		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.10465889700315223 | validation: 0.14059414831044773]
	TIME [epoch: 5.71 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10513015851687733		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.10513015851687733 | validation: 0.1412844259948822]
	TIME [epoch: 5.71 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10382757219846939		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.10382757219846939 | validation: 0.14808800675297626]
	TIME [epoch: 5.71 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10323362809987487		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.10323362809987487 | validation: 0.13546502099181038]
	TIME [epoch: 5.72 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10232131270560398		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.10232131270560398 | validation: 0.13990347538008546]
	TIME [epoch: 5.71 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10696243741792003		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.10696243741792003 | validation: 0.14685371124806815]
	TIME [epoch: 5.71 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10879469919824258		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.10879469919824258 | validation: 0.13831516155572307]
	TIME [epoch: 5.7 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10732926217835777		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.10732926217835777 | validation: 0.14137180528162335]
	TIME [epoch: 5.71 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10688609438174997		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.10688609438174997 | validation: 0.13276693751935506]
	TIME [epoch: 5.71 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10123083449823955		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.10123083449823955 | validation: 0.14362706366024225]
	TIME [epoch: 5.71 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1067869065573532		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.1067869065573532 | validation: 0.13214474955720074]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_1207.pth
	Model improved!!!
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10182613436224607		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.10182613436224607 | validation: 0.13536089833904993]
	TIME [epoch: 5.72 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10373170110863597		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.10373170110863597 | validation: 0.13889666550373234]
	TIME [epoch: 5.71 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10517279049438848		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.10517279049438848 | validation: 0.1474724978055629]
	TIME [epoch: 5.72 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10430321610423889		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.10430321610423889 | validation: 0.15363918536999266]
	TIME [epoch: 5.7 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10609845896487144		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.10609845896487144 | validation: 0.1449453709459554]
	TIME [epoch: 5.71 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1032055665334107		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.1032055665334107 | validation: 0.1435258226736121]
	TIME [epoch: 5.71 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10189910677251714		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.10189910677251714 | validation: 0.13945606398392374]
	TIME [epoch: 5.71 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10321150352872574		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.10321150352872574 | validation: 0.14246764116160238]
	TIME [epoch: 5.71 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10715428616086213		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.10715428616086213 | validation: 0.1535853779848982]
	TIME [epoch: 5.71 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10409532684823561		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.10409532684823561 | validation: 0.13944724063164965]
	TIME [epoch: 5.72 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1093601108368836		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.1093601108368836 | validation: 0.14465185297897606]
	TIME [epoch: 5.72 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10548825802114714		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.10548825802114714 | validation: 0.14480360570483827]
	TIME [epoch: 5.71 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10476080621613086		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.10476080621613086 | validation: 0.1445112953780403]
	TIME [epoch: 5.71 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11102733448217662		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.11102733448217662 | validation: 0.1423150689700068]
	TIME [epoch: 5.73 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10291571493251418		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.10291571493251418 | validation: 0.13470105351470113]
	TIME [epoch: 5.72 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10227588397879966		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.10227588397879966 | validation: 0.1487763565781668]
	TIME [epoch: 5.72 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10408901710688918		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.10408901710688918 | validation: 0.1455101262999448]
	TIME [epoch: 5.71 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10323365545452362		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.10323365545452362 | validation: 0.14505949467146992]
	TIME [epoch: 5.71 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10309621484041649		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.10309621484041649 | validation: 0.13434301915762528]
	TIME [epoch: 5.72 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10324699327681319		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.10324699327681319 | validation: 0.14095859766978316]
	TIME [epoch: 5.71 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10166431921578938		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.10166431921578938 | validation: 0.15028762064309872]
	TIME [epoch: 5.71 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10414252239200048		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.10414252239200048 | validation: 0.14221999558782333]
	TIME [epoch: 5.7 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09888629679407572		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.09888629679407572 | validation: 0.14207337518110882]
	TIME [epoch: 5.71 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09659166269226913		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.09659166269226913 | validation: 0.14448000287091142]
	TIME [epoch: 5.71 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1006111024996531		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.1006111024996531 | validation: 0.13620921824496027]
	TIME [epoch: 5.71 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10551304097448887		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.10551304097448887 | validation: 0.153570145520438]
	TIME [epoch: 5.71 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10788076967247229		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.10788076967247229 | validation: 0.13987917434458402]
	TIME [epoch: 5.71 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1005119639350604		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.1005119639350604 | validation: 0.14088035322676265]
	TIME [epoch: 5.71 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09823835279867972		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.09823835279867972 | validation: 0.18383156736503536]
	TIME [epoch: 5.72 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13441440684098663		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.13441440684098663 | validation: 0.15267671774646982]
	TIME [epoch: 5.71 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1072715755105688		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.1072715755105688 | validation: 0.14798687051578788]
	TIME [epoch: 5.71 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10162558633019149		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.10162558633019149 | validation: 0.14393924394008614]
	TIME [epoch: 5.71 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1007672815824171		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.1007672815824171 | validation: 0.1369910146673553]
	TIME [epoch: 5.72 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10239616030331342		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.10239616030331342 | validation: 0.1398185479712482]
	TIME [epoch: 5.72 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10392326096738529		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.10392326096738529 | validation: 0.1426439999799298]
	TIME [epoch: 5.72 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10050982254381535		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.10050982254381535 | validation: 0.14482773314953984]
	TIME [epoch: 5.72 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10251588440848682		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.10251588440848682 | validation: 0.14440873632696866]
	TIME [epoch: 5.72 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09938075087618822		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.09938075087618822 | validation: 0.13761129065845282]
	TIME [epoch: 5.71 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10163810755748016		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.10163810755748016 | validation: 0.1441612823783927]
	TIME [epoch: 5.71 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10106939572950106		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.10106939572950106 | validation: 0.12995790619787873]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_1247.pth
	Model improved!!!
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10170848373142637		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.10170848373142637 | validation: 0.1414960659292207]
	TIME [epoch: 5.7 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10515914611127165		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.10515914611127165 | validation: 0.14705202983584234]
	TIME [epoch: 5.71 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10273254158536169		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.10273254158536169 | validation: 0.14098398889863084]
	TIME [epoch: 5.7 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10203026663225155		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.10203026663225155 | validation: 0.165781741243881]
	TIME [epoch: 5.71 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10938353572993283		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.10938353572993283 | validation: 0.1558303047995588]
	TIME [epoch: 5.71 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10592752055161576		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.10592752055161576 | validation: 0.1433199957575584]
	TIME [epoch: 5.71 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10259675470253492		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.10259675470253492 | validation: 0.143445944929775]
	TIME [epoch: 5.71 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10396360338385387		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.10396360338385387 | validation: 0.14218741402512103]
	TIME [epoch: 5.71 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10177754624922197		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.10177754624922197 | validation: 0.14982443235882206]
	TIME [epoch: 5.71 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10130987546740126		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.10130987546740126 | validation: 0.13436619210079231]
	TIME [epoch: 5.7 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10166429728957203		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.10166429728957203 | validation: 0.14084891326318166]
	TIME [epoch: 5.7 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10099438387187767		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.10099438387187767 | validation: 0.1467348618522243]
	TIME [epoch: 5.71 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10346454202977774		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.10346454202977774 | validation: 0.14790295375552504]
	TIME [epoch: 5.7 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10511311092204735		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.10511311092204735 | validation: 0.13814484434629862]
	TIME [epoch: 5.71 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10387432181002215		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.10387432181002215 | validation: 0.14443028242244732]
	TIME [epoch: 5.71 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10118458847845763		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.10118458847845763 | validation: 0.1406980794355833]
	TIME [epoch: 5.71 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.104247579690226		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.104247579690226 | validation: 0.13799226222607838]
	TIME [epoch: 5.71 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1014606393122781		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.1014606393122781 | validation: 0.13466002682586403]
	TIME [epoch: 5.71 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10374406767387796		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.10374406767387796 | validation: 0.1478488029098942]
	TIME [epoch: 5.71 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10319316441517978		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.10319316441517978 | validation: 0.1475245375850395]
	TIME [epoch: 5.71 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1044330468858608		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.1044330468858608 | validation: 0.14448227819969714]
	TIME [epoch: 5.72 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10115961896199543		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.10115961896199543 | validation: 0.13666924943678602]
	TIME [epoch: 5.72 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10004634194781377		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.10004634194781377 | validation: 0.1416843116389429]
	TIME [epoch: 5.71 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10091771427771441		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.10091771427771441 | validation: 0.14629902263540126]
	TIME [epoch: 5.71 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10305371645696636		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.10305371645696636 | validation: 0.139483864453099]
	TIME [epoch: 5.71 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09554561462113358		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.09554561462113358 | validation: 0.1344286325055788]
	TIME [epoch: 5.72 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10896787375717187		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.10896787375717187 | validation: 0.14139397868858133]
	TIME [epoch: 5.71 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10333033564043728		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.10333033564043728 | validation: 0.13789957857755358]
	TIME [epoch: 5.71 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09925572997154465		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.09925572997154465 | validation: 0.1400121182751298]
	TIME [epoch: 5.71 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10196196929818602		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.10196196929818602 | validation: 0.13978892390800926]
	TIME [epoch: 5.72 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10103949557045859		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.10103949557045859 | validation: 0.1378206998680636]
	TIME [epoch: 5.71 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1009282497185579		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.1009282497185579 | validation: 0.1484660691604135]
	TIME [epoch: 5.71 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10644503609215523		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.10644503609215523 | validation: 0.15005164777827013]
	TIME [epoch: 5.71 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10263824868493457		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.10263824868493457 | validation: 0.1433623293898742]
	TIME [epoch: 5.7 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09826767661759143		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.09826767661759143 | validation: 0.1451531313732275]
	TIME [epoch: 5.71 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10032826962063776		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.10032826962063776 | validation: 0.1468219610593997]
	TIME [epoch: 5.71 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09982550108659165		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.09982550108659165 | validation: 0.14735740148977886]
	TIME [epoch: 5.71 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1024264055923279		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.1024264055923279 | validation: 0.1435120222383873]
	TIME [epoch: 5.71 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09858933452904473		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.09858933452904473 | validation: 0.1479551386603719]
	TIME [epoch: 5.71 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10515472058421584		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.10515472058421584 | validation: 0.1378912621229398]
	TIME [epoch: 5.7 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09874197592038658		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.09874197592038658 | validation: 0.137064493374733]
	TIME [epoch: 5.71 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09717835006274651		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.09717835006274651 | validation: 0.1442000008851735]
	TIME [epoch: 5.7 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09786725403011624		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.09786725403011624 | validation: 0.1402945985951325]
	TIME [epoch: 5.71 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09799581847827632		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.09799581847827632 | validation: 0.15281858703736603]
	TIME [epoch: 5.71 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10502761955272377		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.10502761955272377 | validation: 0.15085581295108422]
	TIME [epoch: 5.71 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09884996194932687		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.09884996194932687 | validation: 0.14750303809304416]
	TIME [epoch: 5.71 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09734630582693175		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.09734630582693175 | validation: 0.15744427046052284]
	TIME [epoch: 5.72 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1080529846144744		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.1080529846144744 | validation: 0.15144250236471116]
	TIME [epoch: 5.71 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10229888584624132		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.10229888584624132 | validation: 0.14251954366393396]
	TIME [epoch: 5.71 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10248620821758614		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.10248620821758614 | validation: 0.14977284492117038]
	TIME [epoch: 5.7 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1000532787708431		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.1000532787708431 | validation: 0.141067826953654]
	TIME [epoch: 5.71 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10029936269209978		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.10029936269209978 | validation: 0.14164719230697276]
	TIME [epoch: 5.72 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10089893975006256		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.10089893975006256 | validation: 0.13772569995217968]
	TIME [epoch: 5.71 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09998333405089352		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.09998333405089352 | validation: 0.13805833581862528]
	TIME [epoch: 5.71 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09907517342893594		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.09907517342893594 | validation: 0.13142868040189543]
	TIME [epoch: 5.71 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.104103425777597		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.104103425777597 | validation: 0.143374681690069]
	TIME [epoch: 5.71 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10005561844050222		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.10005561844050222 | validation: 0.14553624120704464]
	TIME [epoch: 5.71 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0992746315480974		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.0992746315480974 | validation: 0.15053537888467095]
	TIME [epoch: 5.71 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10142854699619439		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.10142854699619439 | validation: 0.1480090998260931]
	TIME [epoch: 5.7 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10469330132557296		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.10469330132557296 | validation: 0.14600519138450416]
	TIME [epoch: 5.7 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10028605410598501		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.10028605410598501 | validation: 0.14796463176459493]
	TIME [epoch: 5.71 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0990228335578896		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.0990228335578896 | validation: 0.1397205304293594]
	TIME [epoch: 5.71 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09743221349180813		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.09743221349180813 | validation: 0.1428919930014237]
	TIME [epoch: 5.71 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10070520480429761		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.10070520480429761 | validation: 0.1491559917693782]
	TIME [epoch: 5.71 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09984112810531472		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.09984112810531472 | validation: 0.12669506051810053]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_1312.pth
	Model improved!!!
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10080864507984422		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.10080864507984422 | validation: 0.1461106955831066]
	TIME [epoch: 5.7 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.098380666492308		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.098380666492308 | validation: 0.14375002531167494]
	TIME [epoch: 5.71 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09850400453257145		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.09850400453257145 | validation: 0.14285244892037996]
	TIME [epoch: 5.7 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09773834859144602		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.09773834859144602 | validation: 0.14915292412221925]
	TIME [epoch: 5.7 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09926183086858412		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.09926183086858412 | validation: 0.13700031736247487]
	TIME [epoch: 5.7 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09946171938644507		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.09946171938644507 | validation: 0.14444007050855392]
	TIME [epoch: 5.71 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09744637786810863		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.09744637786810863 | validation: 0.14449810078619754]
	TIME [epoch: 5.71 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10260628929421746		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.10260628929421746 | validation: 0.14149362040896304]
	TIME [epoch: 5.71 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10006284829455885		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.10006284829455885 | validation: 0.1353340369498983]
	TIME [epoch: 5.71 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10115199975818354		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.10115199975818354 | validation: 0.13277901039127107]
	TIME [epoch: 5.7 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09683812194418154		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.09683812194418154 | validation: 0.1358693894392706]
	TIME [epoch: 5.7 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09710502843372504		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.09710502843372504 | validation: 0.13703983637513775]
	TIME [epoch: 5.7 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09719284047199374		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.09719284047199374 | validation: 0.13470749552594108]
	TIME [epoch: 5.71 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09675216185203805		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.09675216185203805 | validation: 0.13987396646962694]
	TIME [epoch: 5.7 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09708011552107404		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.09708011552107404 | validation: 0.14365651617082406]
	TIME [epoch: 5.71 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09795550816871827		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.09795550816871827 | validation: 0.1319872752343243]
	TIME [epoch: 5.71 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10196003622403438		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.10196003622403438 | validation: 0.13660875599007846]
	TIME [epoch: 5.71 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09858331210701163		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.09858331210701163 | validation: 0.13958028401911413]
	TIME [epoch: 5.71 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10002324025752027		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.10002324025752027 | validation: 0.1436159278876935]
	TIME [epoch: 5.71 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10209108987595811		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.10209108987595811 | validation: 0.1447162624627996]
	TIME [epoch: 5.71 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09693029400696532		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.09693029400696532 | validation: 0.13985284717686675]
	TIME [epoch: 5.7 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09888328102863775		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.09888328102863775 | validation: 0.1402288163113656]
	TIME [epoch: 5.71 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09755903920179972		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.09755903920179972 | validation: 0.13900647288024245]
	TIME [epoch: 5.72 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09718829445005418		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.09718829445005418 | validation: 0.146929703958229]
	TIME [epoch: 5.72 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09887059769455844		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.09887059769455844 | validation: 0.13859684443618914]
	TIME [epoch: 5.72 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09779239544731522		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.09779239544731522 | validation: 0.16024031912605452]
	TIME [epoch: 5.72 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10410325832044581		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.10410325832044581 | validation: 0.14758296252302475]
	TIME [epoch: 5.72 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10073590938741496		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.10073590938741496 | validation: 0.14273664562857755]
	TIME [epoch: 5.72 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10114097130560387		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.10114097130560387 | validation: 0.14038126254889824]
	TIME [epoch: 5.72 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10259471986251402		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.10259471986251402 | validation: 0.13284127710777094]
	TIME [epoch: 5.71 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09980630567805512		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.09980630567805512 | validation: 0.13844013272284642]
	TIME [epoch: 5.71 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09710843969171631		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.09710843969171631 | validation: 0.14724256106802164]
	TIME [epoch: 5.71 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09785600499453592		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.09785600499453592 | validation: 0.1454707842989311]
	TIME [epoch: 5.71 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09643775821165332		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.09643775821165332 | validation: 0.1458641551404057]
	TIME [epoch: 5.72 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09728895992508399		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.09728895992508399 | validation: 0.14061547634277335]
	TIME [epoch: 5.71 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09841415602855047		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.09841415602855047 | validation: 0.13694848053550532]
	TIME [epoch: 5.72 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09824805754818701		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.09824805754818701 | validation: 0.13288401967704375]
	TIME [epoch: 5.71 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10023855078352348		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.10023855078352348 | validation: 0.1337859330557053]
	TIME [epoch: 5.71 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09590917373898293		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.09590917373898293 | validation: 0.1361142205437201]
	TIME [epoch: 5.71 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10219655734050126		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.10219655734050126 | validation: 0.14492027436083282]
	TIME [epoch: 5.71 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09732254072223233		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.09732254072223233 | validation: 0.13431549700590684]
	TIME [epoch: 5.71 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09646376625563016		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.09646376625563016 | validation: 0.14113200055463068]
	TIME [epoch: 5.71 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09642024695381256		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.09642024695381256 | validation: 0.133915848890973]
	TIME [epoch: 5.71 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09804268531388033		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.09804268531388033 | validation: 0.14663198179689904]
	TIME [epoch: 5.72 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09640480155187571		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.09640480155187571 | validation: 0.1456557054695766]
	TIME [epoch: 5.71 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09595466887398572		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.09595466887398572 | validation: 0.14472285283250605]
	TIME [epoch: 5.71 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09574942447702224		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.09574942447702224 | validation: 0.14359335612298763]
	TIME [epoch: 5.72 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09869854050834148		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.09869854050834148 | validation: 0.1465054355909343]
	TIME [epoch: 5.71 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09767978812401942		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.09767978812401942 | validation: 0.13414671827857605]
	TIME [epoch: 5.73 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09642661220672487		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.09642661220672487 | validation: 0.13580300161236128]
	TIME [epoch: 5.71 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1002371095238355		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.1002371095238355 | validation: 0.14263742451428332]
	TIME [epoch: 5.72 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10241704573628005		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.10241704573628005 | validation: 0.13531364920180877]
	TIME [epoch: 5.71 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09889412897135351		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.09889412897135351 | validation: 0.14197977344296422]
	TIME [epoch: 5.71 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09677721293320372		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.09677721293320372 | validation: 0.14006422073353395]
	TIME [epoch: 5.72 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09605368987780358		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.09605368987780358 | validation: 0.14683854276797698]
	TIME [epoch: 5.71 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09814579311721236		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.09814579311721236 | validation: 0.13908379388217199]
	TIME [epoch: 5.71 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09730862533620331		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.09730862533620331 | validation: 0.13834464065288912]
	TIME [epoch: 5.72 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09980801384272572		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.09980801384272572 | validation: 0.1404433025076198]
	TIME [epoch: 5.71 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10068478810861707		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.10068478810861707 | validation: 0.13326010304362698]
	TIME [epoch: 5.71 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09803046277421086		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.09803046277421086 | validation: 0.13429890912279913]
	TIME [epoch: 5.73 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09646208168447902		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.09646208168447902 | validation: 0.143344325386127]
	TIME [epoch: 5.72 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09781915367899334		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.09781915367899334 | validation: 0.13439398570931121]
	TIME [epoch: 5.72 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09524289676996965		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.09524289676996965 | validation: 0.13371283051033447]
	TIME [epoch: 5.73 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09866497831927032		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.09866497831927032 | validation: 0.13418630706109136]
	TIME [epoch: 5.72 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09736920752250715		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.09736920752250715 | validation: 0.128163461608208]
	TIME [epoch: 5.73 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09994215107479235		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.09994215107479235 | validation: 0.13424228716088837]
	TIME [epoch: 5.73 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09668193572960763		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.09668193572960763 | validation: 0.1441949045349434]
	TIME [epoch: 5.71 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09528636698805043		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.09528636698805043 | validation: 0.1414440267035095]
	TIME [epoch: 5.72 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09438034668032039		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.09438034668032039 | validation: 0.13915210032273737]
	TIME [epoch: 5.71 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09572657249172588		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.09572657249172588 | validation: 0.13462137371911861]
	TIME [epoch: 5.72 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09627638653022427		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.09627638653022427 | validation: 0.13680453116627056]
	TIME [epoch: 5.71 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09722175778522928		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.09722175778522928 | validation: 0.14395960297969565]
	TIME [epoch: 5.71 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09769409415239654		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.09769409415239654 | validation: 0.13944108597754723]
	TIME [epoch: 5.71 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10059879162393191		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.10059879162393191 | validation: 0.1435432042326506]
	TIME [epoch: 5.71 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0996281511387316		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.0996281511387316 | validation: 0.14323235530612685]
	TIME [epoch: 5.72 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09804976640289992		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.09804976640289992 | validation: 0.13725826099718938]
	TIME [epoch: 5.71 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09280441101927876		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.09280441101927876 | validation: 0.13616275746800474]
	TIME [epoch: 5.71 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09405169822625059		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.09405169822625059 | validation: 0.1405772100939772]
	TIME [epoch: 5.71 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09562536916163014		[learning rate: 8.6788e-05]
	Learning Rate: 8.67884e-05
	LOSS [training: 0.09562536916163014 | validation: 0.13964312028324186]
	TIME [epoch: 5.71 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09996161277267508		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.09996161277267508 | validation: 0.13580219514405245]
	TIME [epoch: 5.72 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10128239750506193		[learning rate: 8.6176e-05]
	Learning Rate: 8.61757e-05
	LOSS [training: 0.10128239750506193 | validation: 0.13395221945958005]
	TIME [epoch: 5.72 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1002321013603406		[learning rate: 8.5871e-05]
	Learning Rate: 8.5871e-05
	LOSS [training: 0.1002321013603406 | validation: 0.1271154355702453]
	TIME [epoch: 5.72 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09847337968507099		[learning rate: 8.5567e-05]
	Learning Rate: 8.55673e-05
	LOSS [training: 0.09847337968507099 | validation: 0.13735651155500422]
	TIME [epoch: 5.72 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09712408108508683		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.09712408108508683 | validation: 0.1375617984030608]
	TIME [epoch: 5.72 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09817881019913163		[learning rate: 8.4963e-05]
	Learning Rate: 8.49632e-05
	LOSS [training: 0.09817881019913163 | validation: 0.13812091152449077]
	TIME [epoch: 5.72 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09485073141252215		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.09485073141252215 | validation: 0.15408884497369693]
	TIME [epoch: 5.72 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09949076707237658		[learning rate: 8.4363e-05]
	Learning Rate: 8.43634e-05
	LOSS [training: 0.09949076707237658 | validation: 0.14854930213475692]
	TIME [epoch: 5.71 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09785422811585895		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.09785422811585895 | validation: 0.13610554049786602]
	TIME [epoch: 5.71 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0958670294031725		[learning rate: 8.3768e-05]
	Learning Rate: 8.37678e-05
	LOSS [training: 0.0958670294031725 | validation: 0.14206012402969861]
	TIME [epoch: 5.73 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0956648643996492		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.0956648643996492 | validation: 0.13466210620893612]
	TIME [epoch: 5.73 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09235565275501163		[learning rate: 8.3176e-05]
	Learning Rate: 8.31764e-05
	LOSS [training: 0.09235565275501163 | validation: 0.15258767783779148]
	TIME [epoch: 5.73 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10014304510818711		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.10014304510818711 | validation: 0.14574840316067342]
	TIME [epoch: 5.73 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0962030639299424		[learning rate: 8.2589e-05]
	Learning Rate: 8.25892e-05
	LOSS [training: 0.0962030639299424 | validation: 0.14937250781291075]
	TIME [epoch: 5.74 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0973693664151826		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.0973693664151826 | validation: 0.14073325245926493]
	TIME [epoch: 5.72 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09253052232117476		[learning rate: 8.2006e-05]
	Learning Rate: 8.20061e-05
	LOSS [training: 0.09253052232117476 | validation: 0.14686136089764176]
	TIME [epoch: 5.72 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09231731326121341		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.09231731326121341 | validation: 0.13511133549079918]
	TIME [epoch: 5.72 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09818019151916164		[learning rate: 8.1427e-05]
	Learning Rate: 8.14272e-05
	LOSS [training: 0.09818019151916164 | validation: 0.14419519813019335]
	TIME [epoch: 5.72 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.096821429233515		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.096821429233515 | validation: 0.14087469627685795]
	TIME [epoch: 5.73 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09474789510454537		[learning rate: 8.0852e-05]
	Learning Rate: 8.08523e-05
	LOSS [training: 0.09474789510454537 | validation: 0.13901086187832934]
	TIME [epoch: 5.73 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09468611566029772		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.09468611566029772 | validation: 0.138032177848165]
	TIME [epoch: 5.73 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09481594345025418		[learning rate: 8.0281e-05]
	Learning Rate: 8.02815e-05
	LOSS [training: 0.09481594345025418 | validation: 0.1352660993025976]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_3_v_mmd4_1413.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 5080.040 seconds.
