Args:
Namespace(name='model_phi1_4a_distortion_v1_10_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_10/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_10/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.050962366, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 216095570

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.756766685778691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.756766685778691 | validation: 5.8463712975004185]
	TIME [epoch: 160 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.4035642524008045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.4035642524008045 | validation: 6.017674515752842]
	TIME [epoch: 0.771 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.46761550455482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.46761550455482 | validation: 5.635766294300907]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.14522241778001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.14522241778001 | validation: 6.0672700834905395]
	TIME [epoch: 0.706 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.374073686538123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.374073686538123 | validation: 5.526805945036976]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.896676267143344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.896676267143344 | validation: 5.591758945063479]
	TIME [epoch: 0.711 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.821045500128029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.821045500128029 | validation: 5.759518532238008]
	TIME [epoch: 0.706 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.845056445609167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.845056445609167 | validation: 5.500892825627861]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.676371487511562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.676371487511562 | validation: 5.5580671011600336]
	TIME [epoch: 0.706 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.612914345261252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.612914345261252 | validation: 5.558174236472926]
	TIME [epoch: 0.705 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.648032493458059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.648032493458059 | validation: 5.588361879115296]
	TIME [epoch: 0.703 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.555766508515954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.555766508515954 | validation: 5.319047551602289]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.56375324509848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.56375324509848 | validation: 5.381055948444126]
	TIME [epoch: 0.711 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.384481463184332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.384481463184332 | validation: 5.709055097287267]
	TIME [epoch: 0.705 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.40246066023241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.40246066023241 | validation: 5.290731860360537]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.63267762603449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.63267762603449 | validation: 5.289683923826231]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.323497465436411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.323497465436411 | validation: 5.425088495226752]
	TIME [epoch: 0.707 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.307731409145988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.307731409145988 | validation: 5.328787347442073]
	TIME [epoch: 0.706 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.210994441850031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.210994441850031 | validation: 5.421716207641183]
	TIME [epoch: 0.707 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.099326741820366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.099326741820366 | validation: 5.36389179246164]
	TIME [epoch: 0.705 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.042229756406686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.042229756406686 | validation: 5.24909342830921]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0179667443954274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0179667443954274 | validation: 5.518345222566878]
	TIME [epoch: 0.711 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.095592980147315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.095592980147315 | validation: 5.077584158645277]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.258435171046614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.258435171046614 | validation: 5.176466462444465]
	TIME [epoch: 0.706 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9628684288399083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9628684288399083 | validation: 5.262902562796768]
	TIME [epoch: 0.706 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.875553391998726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.875553391998726 | validation: 5.309072128519356]
	TIME [epoch: 0.704 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9461805740572045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9461805740572045 | validation: 5.326689942724309]
	TIME [epoch: 0.703 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9134898187413105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9134898187413105 | validation: 5.031223348394517]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7723853158209533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7723853158209533 | validation: 5.043423217957008]
	TIME [epoch: 0.71 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.727849658341012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.727849658341012 | validation: 5.38748084592012]
	TIME [epoch: 0.708 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.768858596588531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.768858596588531 | validation: 4.961018776229982]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7039332329111487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7039332329111487 | validation: 5.072339605635883]
	TIME [epoch: 0.711 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.639657181711872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.639657181711872 | validation: 5.16863816677417]
	TIME [epoch: 0.705 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5968222093692224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5968222093692224 | validation: 5.077461131597758]
	TIME [epoch: 0.704 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5687210002242478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5687210002242478 | validation: 5.120260377106864]
	TIME [epoch: 0.703 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.566774728098477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.566774728098477 | validation: 4.990943078992553]
	TIME [epoch: 0.704 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.57588872674221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.57588872674221 | validation: 5.23209570868186]
	TIME [epoch: 0.703 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.589121309930751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.589121309930751 | validation: 4.854946503272664]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5825750762708104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5825750762708104 | validation: 4.986297248723142]
	TIME [epoch: 0.71 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.482997708630247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.482997708630247 | validation: 5.1616180593073615]
	TIME [epoch: 0.707 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.476319079748447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.476319079748447 | validation: 4.864712723015962]
	TIME [epoch: 0.706 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4848020478552684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4848020478552684 | validation: 5.001264820113757]
	TIME [epoch: 0.709 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4826796264421636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4826796264421636 | validation: 4.86493169881437]
	TIME [epoch: 0.71 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.451878770708991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.451878770708991 | validation: 5.049203015702286]
	TIME [epoch: 0.706 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4386286300206894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4386286300206894 | validation: 4.794747175869292]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.447982367750116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.447982367750116 | validation: 4.916784319204801]
	TIME [epoch: 0.71 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4021259549208676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4021259549208676 | validation: 4.822729486128994]
	TIME [epoch: 0.707 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3796049152591134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3796049152591134 | validation: 4.8954634817564795]
	TIME [epoch: 0.706 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.363217391026631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.363217391026631 | validation: 4.723139203976542]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.363426398219583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.363426398219583 | validation: 4.831118280142032]
	TIME [epoch: 0.709 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3569155535667576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3569155535667576 | validation: 4.676612237549612]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.374482380383917		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.374482380383917 | validation: 4.759922333250332]
	TIME [epoch: 0.709 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3279502389907902		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 3.3279502389907902 | validation: 4.691135139773235]
	TIME [epoch: 0.709 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.308859479868997		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 3.308859479868997 | validation: 4.720017890615124]
	TIME [epoch: 0.707 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.192312536026382		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 3.192312536026382 | validation: 4.7366410095107785]
	TIME [epoch: 0.706 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.116017035591875		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 3.116017035591875 | validation: 4.706967787367211]
	TIME [epoch: 0.706 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.143994883435882		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 3.143994883435882 | validation: 5.292700856727615]
	TIME [epoch: 0.707 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.310292562880169		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 4.310292562880169 | validation: 4.691152848171888]
	TIME [epoch: 0.704 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.235880938209257		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 3.235880938209257 | validation: 4.854372669464422]
	TIME [epoch: 0.705 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4831426631525044		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 3.4831426631525044 | validation: 4.66955496994323]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1401747481667184		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 3.1401747481667184 | validation: 4.594727487158465]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0470387091119524		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 3.0470387091119524 | validation: 4.5832792217471034]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0432352616854734		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 3.0432352616854734 | validation: 4.559612383326591]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9891191527196788		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 2.9891191527196788 | validation: 4.540265772796659]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9695727687361804		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 2.9695727687361804 | validation: 4.532394700680112]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9335867868960936		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 2.9335867868960936 | validation: 4.502590763240671]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9223018158716143		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 2.9223018158716143 | validation: 4.502784529138906]
	TIME [epoch: 0.707 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.909011782318655		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 2.909011782318655 | validation: 4.462712709537195]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.885539619221429		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 2.885539619221429 | validation: 4.470937643288781]
	TIME [epoch: 0.708 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8748929775490173		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 2.8748929775490173 | validation: 4.461393194979032]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.864859304542027		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 2.864859304542027 | validation: 4.446120823561993]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8802844955211135		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 2.8802844955211135 | validation: 4.443289245329598]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9314831683147644		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 2.9314831683147644 | validation: 4.5759014946369305]
	TIME [epoch: 0.709 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.029039470821654		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 3.029039470821654 | validation: 4.468887978349735]
	TIME [epoch: 0.708 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.075345275947011		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 3.075345275947011 | validation: 4.395501138741046]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9025231122535575		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 2.9025231122535575 | validation: 4.506930323609235]
	TIME [epoch: 0.708 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9737666888236043		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 2.9737666888236043 | validation: 4.354588720859231]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8740315438895503		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 2.8740315438895503 | validation: 4.345045006710516]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8639710685784086		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 2.8639710685784086 | validation: 4.369192756760941]
	TIME [epoch: 0.707 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.855990600209574		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 2.855990600209574 | validation: 4.320327234352575]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8382993103745457		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 2.8382993103745457 | validation: 4.308168954578702]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.824119927023577		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 2.824119927023577 | validation: 4.293288528948792]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.826327041520006		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 2.826327041520006 | validation: 4.289576774675657]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8412338756541953		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 2.8412338756541953 | validation: 4.267800773301057]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.828007729396853		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 2.828007729396853 | validation: 4.245197520116032]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8480968481273625		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 2.8480968481273625 | validation: 4.238288142830032]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.815212228994659		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 2.815212228994659 | validation: 4.2075408875339715]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.847213738699909		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 2.847213738699909 | validation: 4.249366402569312]
	TIME [epoch: 0.712 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8356633390019694		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 2.8356633390019694 | validation: 4.171535798904087]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.865723232621542		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 2.865723232621542 | validation: 4.146561952598783]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7775245212460753		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 2.7775245212460753 | validation: 4.106998865421143]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7677521798989004		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 2.7677521798989004 | validation: 4.0794403575599105]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7718701982611025		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 2.7718701982611025 | validation: 4.100254386046007]
	TIME [epoch: 0.708 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7594551986964553		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 2.7594551986964553 | validation: 4.069667472020492]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8396283197634036		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 2.8396283197634036 | validation: 4.175133392419446]
	TIME [epoch: 0.706 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9010317628121505		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 2.9010317628121505 | validation: 4.044541197777403]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8819607504690254		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 2.8819607504690254 | validation: 3.98525834553653]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.773493531303959		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 2.773493531303959 | validation: 4.07181765436794]
	TIME [epoch: 0.711 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8001035602969213		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 2.8001035602969213 | validation: 3.9381145818120284]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7416072431148995		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 2.7416072431148995 | validation: 3.9153371269768984]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.727902429313965		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 2.727902429313965 | validation: 3.917918049811111]
	TIME [epoch: 0.709 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.723302454359696		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 2.723302454359696 | validation: 3.875916500367501]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7278460031911154		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 2.7278460031911154 | validation: 3.888190482190612]
	TIME [epoch: 0.715 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.720694549595437		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 2.720694549595437 | validation: 3.8201961537620193]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.738444094878204		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 2.738444094878204 | validation: 3.8493432528700398]
	TIME [epoch: 0.709 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.694394497927703		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 2.694394497927703 | validation: 3.80439340253011]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7118536259059174		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 2.7118536259059174 | validation: 3.8345876026550343]
	TIME [epoch: 0.709 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.739175586179584		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 2.739175586179584 | validation: 3.8133053725164565]
	TIME [epoch: 0.708 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7870495832742472		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 2.7870495832742472 | validation: 3.756869842175465]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6922024493263668		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 2.6922024493263668 | validation: 3.665346214407768]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6814277713009527		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 2.6814277713009527 | validation: 3.67613847682484]
	TIME [epoch: 0.71 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.665449688121222		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 2.665449688121222 | validation: 3.637078764772563]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.655182745801487		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 2.655182745801487 | validation: 3.688445365653314]
	TIME [epoch: 0.71 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6687034749103726		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 2.6687034749103726 | validation: 3.659630589827768]
	TIME [epoch: 0.708 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.74212723542481		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 2.74212723542481 | validation: 3.7408405753237446]
	TIME [epoch: 0.707 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7230485564850317		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 2.7230485564850317 | validation: 3.6516556967548173]
	TIME [epoch: 0.706 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7311729916782874		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 2.7311729916782874 | validation: 3.6015383402202406]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.637518107371102		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 2.637518107371102 | validation: 3.610642846889418]
	TIME [epoch: 0.711 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.635752369113617		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 2.635752369113617 | validation: 3.55702928495837]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6408050030060286		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 2.6408050030060286 | validation: 3.6364899948800566]
	TIME [epoch: 0.708 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.650709774885139		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 2.650709774885139 | validation: 3.5547668372855714]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.659817763811208		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 2.659817763811208 | validation: 3.6074068409659734]
	TIME [epoch: 0.71 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.637981869538952		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 2.637981869538952 | validation: 3.5119147131229895]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.641959221968366		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 2.641959221968366 | validation: 3.5924085365077874]
	TIME [epoch: 0.711 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6348679856631163		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 2.6348679856631163 | validation: 3.530213355619992]
	TIME [epoch: 0.708 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6524289600088595		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 2.6524289600088595 | validation: 3.549605705320417]
	TIME [epoch: 0.709 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.638121912165106		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 2.638121912165106 | validation: 3.56333662337656]
	TIME [epoch: 0.716 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6390117832494755		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 2.6390117832494755 | validation: 3.4828801312177817]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.618432038960957		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 2.618432038960957 | validation: 3.5081020008175807]
	TIME [epoch: 0.71 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.596745585017378		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 2.596745585017378 | validation: 3.4206060165097827]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5745416383965414		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 2.5745416383965414 | validation: 3.462806409315789]
	TIME [epoch: 0.712 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.584252770822658		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 2.584252770822658 | validation: 3.430278812457961]
	TIME [epoch: 0.71 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.645543512924844		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 2.645543512924844 | validation: 3.526391684259311]
	TIME [epoch: 0.709 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6389765982037217		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 2.6389765982037217 | validation: 3.4062529857831607]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6455854179186136		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 2.6455854179186136 | validation: 3.398749698269623]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.543616052708098		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 2.543616052708098 | validation: 3.383007330679389]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.554646391129034		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 2.554646391129034 | validation: 3.3162147817341103]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.556499178171594		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 2.556499178171594 | validation: 3.378135027950647]
	TIME [epoch: 0.711 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.534769549281662		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 2.534769549281662 | validation: 3.2752353273353005]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_139.pth
	Model improved!!!
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5542283930046996		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 2.5542283930046996 | validation: 3.360081865530974]
	TIME [epoch: 0.707 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.561505225041131		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 2.561505225041131 | validation: 3.2402747708070616]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5441059712422054		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 2.5441059712422054 | validation: 3.3061350891622427]
	TIME [epoch: 0.707 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.494614428041561		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 2.494614428041561 | validation: 3.1465658690522442]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4714407196172092		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 2.4714407196172092 | validation: 3.1544794152534203]
	TIME [epoch: 0.71 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4540732171440856		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 2.4540732171440856 | validation: 3.0466322571358635]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.538879434452007		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 2.538879434452007 | validation: 3.3459555728723993]
	TIME [epoch: 0.711 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.59158548334297		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 2.59158548334297 | validation: 2.9680379756004474]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.454031237179701		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 2.454031237179701 | validation: 2.9089168637729266]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3276587017405252		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 2.3276587017405252 | validation: 2.8159706177227317]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3193248284581993		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 2.3193248284581993 | validation: 2.746224458367161]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_150.pth
	Model improved!!!
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.337208943383585		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 2.337208943383585 | validation: 2.8173916848080958]
	TIME [epoch: 0.711 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.238779989021846		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 2.238779989021846 | validation: 2.354402832197785]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_152.pth
	Model improved!!!
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0815510978920977		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 2.0815510978920977 | validation: 2.7401814920189302]
	TIME [epoch: 0.709 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3595051158453315		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 2.3595051158453315 | validation: 2.2933108933573423]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.221309135942163		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 2.221309135942163 | validation: 2.2640815620982173]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9169666436956017		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.9169666436956017 | validation: 2.1489394486838758]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9444521844582157		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.9444521844582157 | validation: 2.251748900914961]
	TIME [epoch: 0.709 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9603888529571012		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.9603888529571012 | validation: 1.7752039849346546]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6686886503036857		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.6686886503036857 | validation: 1.6797348947053354]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_159.pth
	Model improved!!!
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5890170830023147		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.5890170830023147 | validation: 1.6261107751060297]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5673703560247583		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.5673703560247583 | validation: 1.6796580386373419]
	TIME [epoch: 0.71 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5845081218369441		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.5845081218369441 | validation: 1.867205924941779]
	TIME [epoch: 0.712 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6420581226864617		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.6420581226864617 | validation: 1.7004106516076782]
	TIME [epoch: 0.708 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.601210420670609		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.601210420670609 | validation: 1.621411350885807]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5462431741536677		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.5462431741536677 | validation: 1.616023777685866]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5368034815871123		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.5368034815871123 | validation: 1.4513941727620483]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.542756487159121		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.542756487159121 | validation: 1.4881530269419967]
	TIME [epoch: 0.711 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.403063810169757		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.403063810169757 | validation: 1.4440073281900432]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4050730218879954		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.4050730218879954 | validation: 1.5477155779501257]
	TIME [epoch: 0.711 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.446998078009367		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.446998078009367 | validation: 1.643183471372964]
	TIME [epoch: 0.71 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4873071355006464		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.4873071355006464 | validation: 1.490428085724315]
	TIME [epoch: 0.709 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4314902912088263		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.4314902912088263 | validation: 1.4248442970908657]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3756131252400219		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.3756131252400219 | validation: 1.3937622377367724]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3476779124569935		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.3476779124569935 | validation: 1.3360165023536874]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3621285217793593		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.3621285217793593 | validation: 1.4071427948290258]
	TIME [epoch: 0.711 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4600286984678439		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.4600286984678439 | validation: 1.3240569435030023]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3195090921035342		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.3195090921035342 | validation: 1.238692571078643]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.254515286105053		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.254515286105053 | validation: 1.2191371274624356]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.240219059069026		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.240219059069026 | validation: 1.180883351025993]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2215590553777138		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.2215590553777138 | validation: 1.3652131462704213]
	TIME [epoch: 0.711 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3111877005405286		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.3111877005405286 | validation: 1.890250842718725]
	TIME [epoch: 0.711 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.707789181842478		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.707789181842478 | validation: 1.316993731367627]
	TIME [epoch: 0.708 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.335725066423131		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.335725066423131 | validation: 1.1167294718372351]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_183.pth
	Model improved!!!
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.20799473239327		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.20799473239327 | validation: 1.2603400329636607]
	TIME [epoch: 0.711 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2244939109642115		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.2244939109642115 | validation: 1.28143250353532]
	TIME [epoch: 0.708 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2110224631130064		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.2110224631130064 | validation: 1.296619841410709]
	TIME [epoch: 0.708 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2853851332498958		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.2853851332498958 | validation: 1.4012774100711352]
	TIME [epoch: 0.708 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2786304684612		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.2786304684612 | validation: 1.2371136773498197]
	TIME [epoch: 0.713 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2498092877986349		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.2498092877986349 | validation: 1.2086588935463383]
	TIME [epoch: 0.708 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3975075940098685		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.3975075940098685 | validation: 1.2204863237771706]
	TIME [epoch: 0.708 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.211937064259252		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.211937064259252 | validation: 1.2330360323197747]
	TIME [epoch: 0.709 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1881571089958036		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.1881571089958036 | validation: 1.2042493552678541]
	TIME [epoch: 0.708 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2104462906844398		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.2104462906844398 | validation: 1.2969321496155388]
	TIME [epoch: 0.707 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2540404542090147		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.2540404542090147 | validation: 1.2608287292641371]
	TIME [epoch: 0.707 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2393545961434895		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.2393545961434895 | validation: 1.126286136443058]
	TIME [epoch: 0.708 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.141251738738803		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.141251738738803 | validation: 1.0444463415313228]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1167402796565387		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.1167402796565387 | validation: 1.0764104579938294]
	TIME [epoch: 0.709 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0879620988738479		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.0879620988738479 | validation: 1.0407610683400577]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0988316566110055		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.0988316566110055 | validation: 1.2875497475817779]
	TIME [epoch: 0.707 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.212979328457789		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.212979328457789 | validation: 1.3161733732636511]
	TIME [epoch: 0.705 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3039562845047743		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.3039562845047743 | validation: 1.1301597045900686]
	TIME [epoch: 168 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1670526157032273		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.1670526157032273 | validation: 1.0234315384734376]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1101851049253655		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.1101851049253655 | validation: 1.0168487500196708]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.079158908203149		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.079158908203149 | validation: 1.0686674944637329]
	TIME [epoch: 1.39 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0881650724397938		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.0881650724397938 | validation: 1.021508625875524]
	TIME [epoch: 1.38 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0863102308801105		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.0863102308801105 | validation: 1.1256645426025684]
	TIME [epoch: 1.39 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1446280084077254		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.1446280084077254 | validation: 1.3754506556996546]
	TIME [epoch: 1.38 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2665641160254157		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.2665641160254157 | validation: 1.2235399022362476]
	TIME [epoch: 1.39 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1618573467664013		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.1618573467664013 | validation: 0.9873937243614093]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_209.pth
	Model improved!!!
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0198362435346775		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.0198362435346775 | validation: 0.946525832889827]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_210.pth
	Model improved!!!
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9962041154380658		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.9962041154380658 | validation: 0.9782924304543679]
	TIME [epoch: 1.39 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9899294122279823		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.9899294122279823 | validation: 0.8890033941743369]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.987086834513678		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.987086834513678 | validation: 0.9937144140204458]
	TIME [epoch: 1.39 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0144379438366342		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.0144379438366342 | validation: 1.1007877119646554]
	TIME [epoch: 1.39 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1388747375651185		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 1.1388747375651185 | validation: 1.2313975287924663]
	TIME [epoch: 1.39 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3487783812722045		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.3487783812722045 | validation: 1.190620296590073]
	TIME [epoch: 1.39 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1261734193042854		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.1261734193042854 | validation: 0.971501220157915]
	TIME [epoch: 1.38 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9828661911552581		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.9828661911552581 | validation: 0.9386820081635229]
	TIME [epoch: 1.39 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0091859892207253		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.0091859892207253 | validation: 1.0531931256073737]
	TIME [epoch: 1.39 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.071499150419967		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.071499150419967 | validation: 1.1054702082253935]
	TIME [epoch: 1.38 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.114915990091236		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 1.114915990091236 | validation: 1.058775967497216]
	TIME [epoch: 1.39 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0393585453398135		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.0393585453398135 | validation: 0.9476851874702593]
	TIME [epoch: 1.39 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9748478328285205		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.9748478328285205 | validation: 0.9382188968249793]
	TIME [epoch: 1.39 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9488669674599137		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.9488669674599137 | validation: 0.905019331507069]
	TIME [epoch: 1.38 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.946992331296688		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.946992331296688 | validation: 0.9271236010170555]
	TIME [epoch: 1.39 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9642262282439273		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.9642262282439273 | validation: 0.999976841498516]
	TIME [epoch: 1.38 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0088637967268068		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.0088637967268068 | validation: 1.0095398557061577]
	TIME [epoch: 1.38 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0674925086040383		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.0674925086040383 | validation: 0.9589643218063086]
	TIME [epoch: 1.38 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0579313261530776		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 1.0579313261530776 | validation: 0.928956873054429]
	TIME [epoch: 1.38 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9763111618712447		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.9763111618712447 | validation: 0.8570850800092705]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_230.pth
	Model improved!!!
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9331092659730962		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.9331092659730962 | validation: 0.9059368090601743]
	TIME [epoch: 1.38 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9432838152323068		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.9432838152323068 | validation: 1.0324673319202031]
	TIME [epoch: 1.38 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0198249775538493		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 1.0198249775538493 | validation: 1.2067940869091534]
	TIME [epoch: 1.38 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.096824717937238		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.096824717937238 | validation: 0.9324749061317598]
	TIME [epoch: 1.38 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9580677418675996		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.9580677418675996 | validation: 0.779196764714778]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_235.pth
	Model improved!!!
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8847949232185239		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.8847949232185239 | validation: 0.9490538267240108]
	TIME [epoch: 1.39 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9143446979798817		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.9143446979798817 | validation: 0.746874865176559]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_237.pth
	Model improved!!!
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8933475982585994		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.8933475982585994 | validation: 0.8402784331451714]
	TIME [epoch: 1.39 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8999402808370597		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.8999402808370597 | validation: 0.885521248738712]
	TIME [epoch: 1.39 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9604138516939884		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.9604138516939884 | validation: 0.9387153457422941]
	TIME [epoch: 1.39 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9944969618315577		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.9944969618315577 | validation: 0.925213514720941]
	TIME [epoch: 1.38 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9487232422959414		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.9487232422959414 | validation: 1.0676431659527112]
	TIME [epoch: 1.39 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9380712617104942		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.9380712617104942 | validation: 0.9756248177485342]
	TIME [epoch: 1.38 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9520705018969642		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.9520705018969642 | validation: 0.9514647341951425]
	TIME [epoch: 1.39 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.915908841079111		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.915908841079111 | validation: 0.8472693750766738]
	TIME [epoch: 1.38 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8714287531789962		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.8714287531789962 | validation: 0.8091886446064316]
	TIME [epoch: 1.38 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8687476167016182		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.8687476167016182 | validation: 0.8926656763550236]
	TIME [epoch: 1.38 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.89812848267127		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.89812848267127 | validation: 0.8324020912621285]
	TIME [epoch: 1.38 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9303725310054309		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.9303725310054309 | validation: 0.9448052167641623]
	TIME [epoch: 1.38 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9539883462555401		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.9539883462555401 | validation: 0.8374055259068811]
	TIME [epoch: 1.38 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8831294200050189		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.8831294200050189 | validation: 0.702047640016799]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_251.pth
	Model improved!!!
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8527285249001545		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.8527285249001545 | validation: 0.8768338243283358]
	TIME [epoch: 1.38 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8313156780486114		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.8313156780486114 | validation: 0.7287229975436175]
	TIME [epoch: 1.38 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8288928558760473		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.8288928558760473 | validation: 0.7891513368071426]
	TIME [epoch: 1.38 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8096468700655374		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.8096468700655374 | validation: 0.7272064196669462]
	TIME [epoch: 1.38 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8178615073146978		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.8178615073146978 | validation: 0.7967758931222072]
	TIME [epoch: 1.38 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.840716097140926		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.840716097140926 | validation: 0.8076779771928756]
	TIME [epoch: 1.38 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9011058481520979		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.9011058481520979 | validation: 0.7407550926354098]
	TIME [epoch: 1.38 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.918757789452149		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.918757789452149 | validation: 0.9093905891186893]
	TIME [epoch: 1.38 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8871869412838124		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.8871869412838124 | validation: 1.1765121536240917]
	TIME [epoch: 1.38 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0784440152001171		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 1.0784440152001171 | validation: 0.8894658232938664]
	TIME [epoch: 1.38 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.942933793317705		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.942933793317705 | validation: 0.7531273542298242]
	TIME [epoch: 1.38 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7855649913799951		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.7855649913799951 | validation: 0.7299702699105741]
	TIME [epoch: 1.38 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7820783995245804		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.7820783995245804 | validation: 0.7161762269535893]
	TIME [epoch: 1.38 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7862776197294082		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.7862776197294082 | validation: 0.740368862010112]
	TIME [epoch: 1.38 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7784226411957402		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.7784226411957402 | validation: 0.7406060758650416]
	TIME [epoch: 1.38 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8030378298340108		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.8030378298340108 | validation: 0.8574425795571566]
	TIME [epoch: 1.38 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8410853445514204		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.8410853445514204 | validation: 0.7234919836493373]
	TIME [epoch: 1.38 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8411732126815465		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.8411732126815465 | validation: 0.8173136888640441]
	TIME [epoch: 1.38 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8186065881040259		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.8186065881040259 | validation: 0.8480129773276577]
	TIME [epoch: 1.38 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8322612907061381		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.8322612907061381 | validation: 0.8968640495008874]
	TIME [epoch: 1.38 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9155133839506345		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.9155133839506345 | validation: 0.9485276291214128]
	TIME [epoch: 1.38 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9022546106369077		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.9022546106369077 | validation: 0.7484108363640617]
	TIME [epoch: 1.38 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7925770735104011		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.7925770735104011 | validation: 0.6542646829118332]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_274.pth
	Model improved!!!
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7924848572146727		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.7924848572146727 | validation: 0.8048224041858323]
	TIME [epoch: 1.38 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8152743431203519		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.8152743431203519 | validation: 0.6899479435386089]
	TIME [epoch: 1.38 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7979533232101185		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.7979533232101185 | validation: 0.7541779779187966]
	TIME [epoch: 1.38 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7817887106834428		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.7817887106834428 | validation: 0.8108904485225926]
	TIME [epoch: 1.38 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7962156924657023		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.7962156924657023 | validation: 0.8427619902424773]
	TIME [epoch: 1.38 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7953100777929882		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.7953100777929882 | validation: 0.7139606749900693]
	TIME [epoch: 1.38 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7661612433763537		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.7661612433763537 | validation: 0.7910959332777372]
	TIME [epoch: 1.39 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7656189278664417		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.7656189278664417 | validation: 0.6532923747796909]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_282.pth
	Model improved!!!
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8095414261945045		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.8095414261945045 | validation: 0.8386496214492619]
	TIME [epoch: 1.38 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8000587036164248		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.8000587036164248 | validation: 0.6807495808917469]
	TIME [epoch: 1.38 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7682490588672793		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.7682490588672793 | validation: 0.7327619393569136]
	TIME [epoch: 1.38 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7449631299619037		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.7449631299619037 | validation: 0.6996410462070013]
	TIME [epoch: 1.38 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7447819911023105		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.7447819911023105 | validation: 0.7547034368894576]
	TIME [epoch: 1.38 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7715452818835263		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.7715452818835263 | validation: 0.7858562301840814]
	TIME [epoch: 1.38 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7753381614915863		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.7753381614915863 | validation: 0.7010983033007354]
	TIME [epoch: 1.39 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7488809248685887		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.7488809248685887 | validation: 0.7047227660450676]
	TIME [epoch: 1.39 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7458746812568946		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.7458746812568946 | validation: 0.7283223204008349]
	TIME [epoch: 1.39 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7730120202612483		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.7730120202612483 | validation: 0.7518798456784617]
	TIME [epoch: 1.39 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8025065755795093		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.8025065755795093 | validation: 0.6843226295821037]
	TIME [epoch: 1.39 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7416886014447787		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.7416886014447787 | validation: 0.7424374823862703]
	TIME [epoch: 1.39 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7272861735790324		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.7272861735790324 | validation: 0.6828376509155126]
	TIME [epoch: 1.39 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7342955013676019		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.7342955013676019 | validation: 0.8152885553321033]
	TIME [epoch: 1.39 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.739788100610246		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.739788100610246 | validation: 0.6420625673853056]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7114164672034269		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.7114164672034269 | validation: 0.7475596990780587]
	TIME [epoch: 1.38 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7275974075023706		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.7275974075023706 | validation: 0.6900749774615322]
	TIME [epoch: 1.38 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8006870310462454		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.8006870310462454 | validation: 0.8199429533909184]
	TIME [epoch: 1.38 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7946411005158065		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.7946411005158065 | validation: 0.6875301673534697]
	TIME [epoch: 1.39 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7574996897133925		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.7574996897133925 | validation: 0.6999765070938084]
	TIME [epoch: 1.4 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7042341538752318		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.7042341538752318 | validation: 0.6752542251933967]
	TIME [epoch: 1.39 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6986260793149939		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.6986260793149939 | validation: 0.6520250153177876]
	TIME [epoch: 1.39 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7017860581250566		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.7017860581250566 | validation: 0.7103128269772706]
	TIME [epoch: 1.39 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7033600305629348		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.7033600305629348 | validation: 0.6441759492516383]
	TIME [epoch: 1.39 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6739650013915558		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.6739650013915558 | validation: 0.6504045299286716]
	TIME [epoch: 1.38 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6740760349814431		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.6740760349814431 | validation: 0.6714290005651552]
	TIME [epoch: 1.39 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6854943018068153		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.6854943018068153 | validation: 0.7309871280732243]
	TIME [epoch: 1.39 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7092543531722671		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.7092543531722671 | validation: 0.7787928404434026]
	TIME [epoch: 1.39 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7486232529331716		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.7486232529331716 | validation: 0.6680161031569667]
	TIME [epoch: 1.39 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6983141287203151		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.6983141287203151 | validation: 0.6627166447447846]
	TIME [epoch: 1.39 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.729299048003074		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.729299048003074 | validation: 0.6970094807309627]
	TIME [epoch: 1.39 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.781294689707448		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.781294689707448 | validation: 0.7768699931745875]
	TIME [epoch: 1.39 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7227160224595189		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.7227160224595189 | validation: 0.6613432527577614]
	TIME [epoch: 1.39 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6652176476675679		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.6652176476675679 | validation: 0.5988120992125663]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_316.pth
	Model improved!!!
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6534441740119666		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.6534441740119666 | validation: 0.6889537259086099]
	TIME [epoch: 1.39 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6915464445065018		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.6915464445065018 | validation: 0.6184046097611514]
	TIME [epoch: 1.39 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7696494232103414		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.7696494232103414 | validation: 0.8556922457524442]
	TIME [epoch: 1.38 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7284508235995967		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.7284508235995967 | validation: 0.5930485070253558]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_320.pth
	Model improved!!!
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6357202025762715		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.6357202025762715 | validation: 0.6274906885173599]
	TIME [epoch: 1.39 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6220119836277485		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.6220119836277485 | validation: 0.5796586868174478]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_322.pth
	Model improved!!!
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6208994399461437		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.6208994399461437 | validation: 0.6262354675397506]
	TIME [epoch: 1.39 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6324473270156836		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.6324473270156836 | validation: 0.6399051812481654]
	TIME [epoch: 1.39 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6720556253761781		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.6720556253761781 | validation: 0.7454790950931138]
	TIME [epoch: 1.39 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7640818572515695		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.7640818572515695 | validation: 0.679390731542775]
	TIME [epoch: 1.39 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6923634044784629		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.6923634044784629 | validation: 0.6352344944469885]
	TIME [epoch: 1.39 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6108449209400353		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.6108449209400353 | validation: 0.5744794745559056]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_328.pth
	Model improved!!!
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6063219149422338		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.6063219149422338 | validation: 0.6209307913884148]
	TIME [epoch: 1.38 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6217959463298801		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.6217959463298801 | validation: 0.5785434075627913]
	TIME [epoch: 1.38 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6421211052524228		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.6421211052524228 | validation: 0.6329743750290165]
	TIME [epoch: 1.38 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6330657479319127		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.6330657479319127 | validation: 0.6134433466755391]
	TIME [epoch: 1.38 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6247355180781251		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.6247355180781251 | validation: 0.6154756037610744]
	TIME [epoch: 1.38 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6284121602176249		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.6284121602176249 | validation: 0.8035100816868763]
	TIME [epoch: 1.38 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6699515576190714		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.6699515576190714 | validation: 0.5455956148564266]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_335.pth
	Model improved!!!
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6268284517467633		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.6268284517467633 | validation: 0.6920181979614091]
	TIME [epoch: 1.38 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6245335217428858		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.6245335217428858 | validation: 0.538278340939384]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_337.pth
	Model improved!!!
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5717312509996868		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.5717312509996868 | validation: 0.5876197453253524]
	TIME [epoch: 1.39 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5834735975935298		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.5834735975935298 | validation: 0.6038393634150933]
	TIME [epoch: 1.39 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6963886043822991		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.6963886043822991 | validation: 0.8725255996468674]
	TIME [epoch: 1.39 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8252269190061736		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.8252269190061736 | validation: 0.6696839912304832]
	TIME [epoch: 1.38 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6659018000618977		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.6659018000618977 | validation: 0.6136499272001688]
	TIME [epoch: 1.39 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5775576116755887		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.5775576116755887 | validation: 0.5574028367800253]
	TIME [epoch: 1.38 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5627745183527165		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.5627745183527165 | validation: 0.6345909878515616]
	TIME [epoch: 1.38 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5789005901051781		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.5789005901051781 | validation: 0.5290606899488983]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_345.pth
	Model improved!!!
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5781268710916393		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.5781268710916393 | validation: 0.6412018630010796]
	TIME [epoch: 1.39 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5854240678322392		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.5854240678322392 | validation: 0.5441679732501278]
	TIME [epoch: 1.39 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5727532146639539		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.5727532146639539 | validation: 0.5904138081140354]
	TIME [epoch: 1.39 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6586152151329375		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.6586152151329375 | validation: 0.6341649591005214]
	TIME [epoch: 1.39 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6286782760494037		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.6286782760494037 | validation: 0.6199599356553778]
	TIME [epoch: 1.39 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6303817973793869		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.6303817973793869 | validation: 0.6745004476543741]
	TIME [epoch: 1.39 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6487631364042429		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.6487631364042429 | validation: 0.6081108775012498]
	TIME [epoch: 1.39 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5837604054441634		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.5837604054441634 | validation: 0.5631388402100374]
	TIME [epoch: 1.39 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5314958704662687		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.5314958704662687 | validation: 0.519849474336795]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_354.pth
	Model improved!!!
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5172668143095781		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.5172668143095781 | validation: 0.5510272083042843]
	TIME [epoch: 1.38 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5344160536196184		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.5344160536196184 | validation: 0.5231447208895411]
	TIME [epoch: 1.38 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5821369911624882		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.5821369911624882 | validation: 0.755966121647608]
	TIME [epoch: 1.38 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6427411042014564		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.6427411042014564 | validation: 0.5679460034753213]
	TIME [epoch: 1.38 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6025984690637686		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.6025984690637686 | validation: 0.6180280117462349]
	TIME [epoch: 1.39 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5728714565373296		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.5728714565373296 | validation: 0.5672929161483897]
	TIME [epoch: 1.39 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5558958357028579		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.5558958357028579 | validation: 0.5835578577236806]
	TIME [epoch: 1.39 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5569461936115868		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.5569461936115868 | validation: 0.5508579780696303]
	TIME [epoch: 1.39 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5447441353333295		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.5447441353333295 | validation: 0.5420608400394367]
	TIME [epoch: 1.4 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5651918041457621		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.5651918041457621 | validation: 0.6058965058897114]
	TIME [epoch: 1.39 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5458697904478111		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.5458697904478111 | validation: 0.5195475082045151]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_365.pth
	Model improved!!!
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5046601646415299		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.5046601646415299 | validation: 0.5233870811699559]
	TIME [epoch: 1.38 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49735677330628947		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.49735677330628947 | validation: 0.499951984721881]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_367.pth
	Model improved!!!
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5100417961593552		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.5100417961593552 | validation: 0.543682592150775]
	TIME [epoch: 1.38 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5384297720785507		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.5384297720785507 | validation: 0.584904678289654]
	TIME [epoch: 1.38 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.575429283573241		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.575429283573241 | validation: 0.5458017064644743]
	TIME [epoch: 1.38 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.600782728071768		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.600782728071768 | validation: 0.6634464826953907]
	TIME [epoch: 1.38 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5489385732564861		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.5489385732564861 | validation: 0.5205640889298343]
	TIME [epoch: 1.38 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5035999286679257		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.5035999286679257 | validation: 0.540126048285468]
	TIME [epoch: 1.38 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48213096147637347		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.48213096147637347 | validation: 0.4741314884204504]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_374.pth
	Model improved!!!
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4608395815917332		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.4608395815917332 | validation: 0.5421808756166614]
	TIME [epoch: 1.38 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4947558137922991		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.4947558137922991 | validation: 0.5074854453244755]
	TIME [epoch: 1.38 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5510568710050385		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.5510568710050385 | validation: 0.5811031210803143]
	TIME [epoch: 1.38 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5702461383078682		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.5702461383078682 | validation: 0.5387862575861515]
	TIME [epoch: 1.38 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5132102010866525		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.5132102010866525 | validation: 0.4917892355950093]
	TIME [epoch: 1.38 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4540311065771897		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.4540311065771897 | validation: 0.5108428191965461]
	TIME [epoch: 1.38 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.447923120797916		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.447923120797916 | validation: 0.5087325040542586]
	TIME [epoch: 1.38 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4806240648925153		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.4806240648925153 | validation: 0.5873798360966394]
	TIME [epoch: 1.38 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5076228379199487		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.5076228379199487 | validation: 0.4715986853959188]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_383.pth
	Model improved!!!
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47644497264285646		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.47644497264285646 | validation: 0.4586652902031502]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_384.pth
	Model improved!!!
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4636888682162267		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.4636888682162267 | validation: 0.5519709870854043]
	TIME [epoch: 1.39 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5098368684036784		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.5098368684036784 | validation: 0.5125404765649736]
	TIME [epoch: 1.38 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5191949899243669		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.5191949899243669 | validation: 0.5715025036836201]
	TIME [epoch: 1.38 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4984307606951292		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.4984307606951292 | validation: 0.4420321704036707]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_388.pth
	Model improved!!!
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44540908538265445		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.44540908538265445 | validation: 0.5240057734193283]
	TIME [epoch: 1.38 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43885684602149583		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.43885684602149583 | validation: 0.4460848234181456]
	TIME [epoch: 1.38 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4091085987648873		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.4091085987648873 | validation: 0.4761326972167898]
	TIME [epoch: 1.38 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40707023138306425		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.40707023138306425 | validation: 0.43245033493796187]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_392.pth
	Model improved!!!
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4037138809458361		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.4037138809458361 | validation: 0.5134138260840103]
	TIME [epoch: 1.38 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4574010933876436		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.4574010933876436 | validation: 0.5528975667851451]
	TIME [epoch: 1.38 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5810788651853772		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.5810788651853772 | validation: 0.4893331371438981]
	TIME [epoch: 1.39 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4875457736243669		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.4875457736243669 | validation: 0.5311512465796145]
	TIME [epoch: 1.39 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4285473868992608		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.4285473868992608 | validation: 0.4207240936488312]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_397.pth
	Model improved!!!
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4129735611174054		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.4129735611174054 | validation: 0.5101338851631931]
	TIME [epoch: 1.38 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4251424872811951		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.4251424872811951 | validation: 0.40385306060645354]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_399.pth
	Model improved!!!
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41542594852397935		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.41542594852397935 | validation: 0.49156713915484884]
	TIME [epoch: 1.38 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40262750496276795		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.40262750496276795 | validation: 0.42156591850790326]
	TIME [epoch: 1.39 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39748491432515065		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.39748491432515065 | validation: 0.49034795194096176]
	TIME [epoch: 1.38 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4240735341879369		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.4240735341879369 | validation: 0.4395783420063632]
	TIME [epoch: 1.38 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41798596418841544		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.41798596418841544 | validation: 0.48236394107392383]
	TIME [epoch: 1.38 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43406473859544054		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.43406473859544054 | validation: 0.4712778580036874]
	TIME [epoch: 1.38 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40913132889443193		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.40913132889443193 | validation: 0.4516605457301462]
	TIME [epoch: 1.38 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4066822902291882		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.4066822902291882 | validation: 0.5180423484009665]
	TIME [epoch: 1.38 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4017760638502913		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.4017760638502913 | validation: 0.4031029346440261]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_408.pth
	Model improved!!!
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.374563853367923		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.374563853367923 | validation: 0.42207498679702804]
	TIME [epoch: 1.38 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3730426661472749		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.3730426661472749 | validation: 0.42501794658853975]
	TIME [epoch: 1.38 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38695044134029133		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.38695044134029133 | validation: 0.4128022105959135]
	TIME [epoch: 1.38 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3909230250190906		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.3909230250190906 | validation: 0.49914471200023947]
	TIME [epoch: 1.38 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4302196502661338		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.4302196502661338 | validation: 0.412229271932157]
	TIME [epoch: 1.38 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3893283748444739		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.3893283748444739 | validation: 0.42446596036099343]
	TIME [epoch: 1.38 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3585224348353615		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.3585224348353615 | validation: 0.35955518992040114]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_415.pth
	Model improved!!!
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3501653365413591		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.3501653365413591 | validation: 0.42319206330775644]
	TIME [epoch: 1.38 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3652946895967709		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.3652946895967709 | validation: 0.4163616738168913]
	TIME [epoch: 1.38 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3737526242532648		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.3737526242532648 | validation: 0.44908325775870034]
	TIME [epoch: 1.38 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39065599412634266		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.39065599412634266 | validation: 0.43806172999347726]
	TIME [epoch: 1.38 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36307715076056113		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.36307715076056113 | validation: 0.3919637778825527]
	TIME [epoch: 1.38 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3499606853128222		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.3499606853128222 | validation: 0.470865108260097]
	TIME [epoch: 1.38 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36577561376612294		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.36577561376612294 | validation: 0.37166582913481366]
	TIME [epoch: 1.39 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32943663088525094		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.32943663088525094 | validation: 0.4139539067762823]
	TIME [epoch: 1.38 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3100518964576783		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.3100518964576783 | validation: 0.3383898608675473]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_424.pth
	Model improved!!!
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29697529220181035		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.29697529220181035 | validation: 0.37473677650792775]
	TIME [epoch: 1.39 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31091702049959496		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.31091702049959496 | validation: 0.4150242199938289]
	TIME [epoch: 1.38 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4093735352493002		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.4093735352493002 | validation: 0.4261847936778921]
	TIME [epoch: 1.38 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45939778103280854		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.45939778103280854 | validation: 0.5721381943383275]
	TIME [epoch: 1.38 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44099509658088765		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.44099509658088765 | validation: 0.356406123819613]
	TIME [epoch: 1.38 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3200489282504981		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.3200489282504981 | validation: 0.36078603700048667]
	TIME [epoch: 1.38 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3017665517730674		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.3017665517730674 | validation: 0.34458782095778395]
	TIME [epoch: 1.38 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3374754740922893		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.3374754740922893 | validation: 0.4961335689657319]
	TIME [epoch: 1.38 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3706283127760743		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.3706283127760743 | validation: 0.3439920433927148]
	TIME [epoch: 1.38 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3047624836132239		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.3047624836132239 | validation: 0.3937934879990665]
	TIME [epoch: 1.38 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32880062534306875		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.32880062534306875 | validation: 0.38394701432486267]
	TIME [epoch: 1.38 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3490752135668567		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.3490752135668567 | validation: 0.4107875194071821]
	TIME [epoch: 1.38 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3599476609384432		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.3599476609384432 | validation: 0.3620072723100374]
	TIME [epoch: 1.38 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31379572550627666		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.31379572550627666 | validation: 0.3358921800966568]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_438.pth
	Model improved!!!
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2921454607017758		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.2921454607017758 | validation: 0.3734206784737475]
	TIME [epoch: 1.38 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29947238629730744		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.29947238629730744 | validation: 0.36912239724221463]
	TIME [epoch: 1.38 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30464975266151456		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.30464975266151456 | validation: 0.36366430699679314]
	TIME [epoch: 1.38 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31569551386970235		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.31569551386970235 | validation: 0.3840901294196748]
	TIME [epoch: 1.39 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31952492303492963		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.31952492303492963 | validation: 0.3239738403495052]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_443.pth
	Model improved!!!
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29752305454574074		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.29752305454574074 | validation: 0.3726559370487952]
	TIME [epoch: 1.38 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29035459973361777		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.29035459973361777 | validation: 0.3225850754481374]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_445.pth
	Model improved!!!
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28443441937558783		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.28443441937558783 | validation: 0.36108115057304496]
	TIME [epoch: 1.38 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29895671313670985		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.29895671313670985 | validation: 0.3049783663644038]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_447.pth
	Model improved!!!
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29800050751519047		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.29800050751519047 | validation: 0.39828187953892064]
	TIME [epoch: 1.39 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32200126850559607		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.32200126850559607 | validation: 0.36851824275561307]
	TIME [epoch: 1.39 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32241957914212516		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.32241957914212516 | validation: 0.4269425366325358]
	TIME [epoch: 1.39 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31556107804955214		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.31556107804955214 | validation: 0.2742934471570341]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_451.pth
	Model improved!!!
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23507949982556525		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.23507949982556525 | validation: 0.27719797090207043]
	TIME [epoch: 1.39 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2266043693061131		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.2266043693061131 | validation: 0.27169694562396673]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_453.pth
	Model improved!!!
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2353861547987139		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.2353861547987139 | validation: 0.3348016680875074]
	TIME [epoch: 1.39 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2877720363979787		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.2877720363979787 | validation: 0.3977420373084211]
	TIME [epoch: 1.38 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35796629621647896		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.35796629621647896 | validation: 0.4656568570041604]
	TIME [epoch: 1.38 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39061128833673475		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.39061128833673475 | validation: 0.37905683503821336]
	TIME [epoch: 1.38 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2836743474526025		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.2836743474526025 | validation: 0.27864495000104333]
	TIME [epoch: 1.38 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22559359224126938		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.22559359224126938 | validation: 0.32951212168051147]
	TIME [epoch: 1.38 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.230448524177967		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.230448524177967 | validation: 0.2915402496640802]
	TIME [epoch: 1.38 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21827378999961933		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.21827378999961933 | validation: 0.28348820354821824]
	TIME [epoch: 1.39 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2279884351847577		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.2279884351847577 | validation: 0.32489252181677913]
	TIME [epoch: 1.38 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28293305613043485		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.28293305613043485 | validation: 0.36151825208286614]
	TIME [epoch: 1.38 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3515218292861397		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.3515218292861397 | validation: 0.4042812717830135]
	TIME [epoch: 1.38 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3210059202233317		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.3210059202233317 | validation: 0.2846427173502594]
	TIME [epoch: 1.39 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24387104322490394		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.24387104322490394 | validation: 0.2915851312412932]
	TIME [epoch: 1.38 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22880802660607466		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.22880802660607466 | validation: 0.3156666743132681]
	TIME [epoch: 1.38 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24782278106314662		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.24782278106314662 | validation: 0.3290011307265459]
	TIME [epoch: 1.38 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2646246711934997		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.2646246711934997 | validation: 0.3317699421336441]
	TIME [epoch: 1.38 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2576661854990809		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.2576661854990809 | validation: 0.28581797371852496]
	TIME [epoch: 1.38 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23019127709768167		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.23019127709768167 | validation: 0.27783135999321534]
	TIME [epoch: 1.38 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22128917633544884		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.22128917633544884 | validation: 0.29986495927272555]
	TIME [epoch: 1.38 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23021282317529715		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.23021282317529715 | validation: 0.2963718347097461]
	TIME [epoch: 1.38 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2849859628085724		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.2849859628085724 | validation: 0.3557368693600519]
	TIME [epoch: 1.38 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29838965241864507		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.29838965241864507 | validation: 0.2645871256718933]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_475.pth
	Model improved!!!
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24390669004131676		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.24390669004131676 | validation: 0.31995672627130567]
	TIME [epoch: 1.38 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23227459729576438		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.23227459729576438 | validation: 0.28404012762062497]
	TIME [epoch: 1.39 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24994354832515925		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.24994354832515925 | validation: 0.3699339665579463]
	TIME [epoch: 1.38 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27580509008818493		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.27580509008818493 | validation: 0.2442063744734702]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_479.pth
	Model improved!!!
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22750081890633997		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.22750081890633997 | validation: 0.2731034071982847]
	TIME [epoch: 1.38 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2146300307166006		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.2146300307166006 | validation: 0.24317846237917734]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_481.pth
	Model improved!!!
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21472236822940383		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.21472236822940383 | validation: 0.32275782561587313]
	TIME [epoch: 1.39 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23861563134758335		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.23861563134758335 | validation: 0.3050483973288325]
	TIME [epoch: 1.38 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2678027262349205		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.2678027262349205 | validation: 0.31625048110283277]
	TIME [epoch: 1.38 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2742077941070203		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.2742077941070203 | validation: 0.3755475000878351]
	TIME [epoch: 1.38 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27793355024622063		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.27793355024622063 | validation: 0.27033786003081645]
	TIME [epoch: 1.38 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21436129074085053		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.21436129074085053 | validation: 0.24180359243904978]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_487.pth
	Model improved!!!
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18713831864860114		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.18713831864860114 | validation: 0.2525932135549937]
	TIME [epoch: 1.38 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18525152808778375		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.18525152808778375 | validation: 0.24648381228313646]
	TIME [epoch: 1.38 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2057880240608155		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.2057880240608155 | validation: 0.29119319836708835]
	TIME [epoch: 1.38 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22698477669332362		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.22698477669332362 | validation: 0.3067780100117888]
	TIME [epoch: 1.38 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.253093713892666		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.253093713892666 | validation: 0.29827301206528506]
	TIME [epoch: 1.38 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24299826703914953		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.24299826703914953 | validation: 0.2687408319423801]
	TIME [epoch: 1.38 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23040992629337673		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.23040992629337673 | validation: 0.2556442120094837]
	TIME [epoch: 1.38 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2090703705018726		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.2090703705018726 | validation: 0.2569612246905503]
	TIME [epoch: 1.38 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20020235376207185		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.20020235376207185 | validation: 0.25273016817662025]
	TIME [epoch: 1.38 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.196574021928775		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.196574021928775 | validation: 0.24860464121478554]
	TIME [epoch: 1.38 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19970654977265245		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.19970654977265245 | validation: 0.2634767158087775]
	TIME [epoch: 1.38 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20335474367025674		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.20335474367025674 | validation: 0.23607867366393648]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_499.pth
	Model improved!!!
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2135881219306565		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.2135881219306565 | validation: 0.27729053000423576]
	TIME [epoch: 1.38 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21931344590158333		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.21931344590158333 | validation: 0.22864688065259112]
	TIME [epoch: 172 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_501.pth
	Model improved!!!
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21304646632450797		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.21304646632450797 | validation: 0.26367398162661243]
	TIME [epoch: 2.74 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2091707578103251		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.2091707578103251 | validation: 0.24742026020054553]
	TIME [epoch: 2.73 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22593130043524653		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.22593130043524653 | validation: 0.3310634636703855]
	TIME [epoch: 2.73 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2430430462959543		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.2430430462959543 | validation: 0.2279726802104965]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_505.pth
	Model improved!!!
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18791545637797577		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.18791545637797577 | validation: 0.22915590661027252]
	TIME [epoch: 2.74 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16593612194505636		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.16593612194505636 | validation: 0.20451371662535342]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_507.pth
	Model improved!!!
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16087481627180403		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.16087481627180403 | validation: 0.24663979809822079]
	TIME [epoch: 2.74 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17534686479769745		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.17534686479769745 | validation: 0.2647780271060255]
	TIME [epoch: 2.73 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1933111929224645		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.1933111929224645 | validation: 0.30888234076936083]
	TIME [epoch: 2.74 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2557756753602093		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.2557756753602093 | validation: 0.3197635876287123]
	TIME [epoch: 2.73 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2955736439801738		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.2955736439801738 | validation: 0.22683901111162064]
	TIME [epoch: 2.73 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2365008940310502		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.2365008940310502 | validation: 0.2371234561507948]
	TIME [epoch: 2.73 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16855323299466604		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.16855323299466604 | validation: 0.19343093472436038]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_514.pth
	Model improved!!!
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1465607207919597		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.1465607207919597 | validation: 0.21063061667323568]
	TIME [epoch: 2.73 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1462497234483899		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.1462497234483899 | validation: 0.19361084955587263]
	TIME [epoch: 2.73 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14746709644573977		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.14746709644573977 | validation: 0.22259856431134653]
	TIME [epoch: 2.74 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18079547815881797		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.18079547815881797 | validation: 0.2194681509268313]
	TIME [epoch: 2.73 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21577499100537179		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.21577499100537179 | validation: 0.26335449232508257]
	TIME [epoch: 2.73 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2041683169483401		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.2041683169483401 | validation: 0.2037686988838585]
	TIME [epoch: 2.73 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1796556643722877		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.1796556643722877 | validation: 0.23126554786219305]
	TIME [epoch: 2.74 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18140376792104299		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.18140376792104299 | validation: 0.2986736857727211]
	TIME [epoch: 2.73 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23093851714638075		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.23093851714638075 | validation: 0.36890150065898575]
	TIME [epoch: 2.74 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2546363492041216		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.2546363492041216 | validation: 0.25723430721760804]
	TIME [epoch: 2.73 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1828112183992231		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.1828112183992231 | validation: 0.18718383517958614]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_525.pth
	Model improved!!!
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13533791653780453		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.13533791653780453 | validation: 0.17795426071514664]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_526.pth
	Model improved!!!
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13208097006445205		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.13208097006445205 | validation: 0.21072534698116718]
	TIME [epoch: 2.74 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15031825347776617		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.15031825347776617 | validation: 0.24369440113120763]
	TIME [epoch: 2.73 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20369535319051543		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.20369535319051543 | validation: 0.2961199433855808]
	TIME [epoch: 2.73 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2490283588839747		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.2490283588839747 | validation: 0.2625396297811884]
	TIME [epoch: 2.74 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21592677680396768		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.21592677680396768 | validation: 0.21635053631452594]
	TIME [epoch: 2.72 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15123107254963306		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.15123107254963306 | validation: 0.1921579236572719]
	TIME [epoch: 2.73 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1366000759009466		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.1366000759009466 | validation: 0.19713401877800346]
	TIME [epoch: 2.73 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1413408598325169		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.1413408598325169 | validation: 0.23076513964193632]
	TIME [epoch: 2.73 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1639136115971042		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.1639136115971042 | validation: 0.24148585197877415]
	TIME [epoch: 2.73 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17182033205173292		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.17182033205173292 | validation: 0.2372825243101204]
	TIME [epoch: 2.74 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18228476079303202		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.18228476079303202 | validation: 0.21094896700308763]
	TIME [epoch: 2.73 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18230311387026277		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.18230311387026277 | validation: 0.20681584475255577]
	TIME [epoch: 2.73 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21009045860728093		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.21009045860728093 | validation: 0.22910266099461235]
	TIME [epoch: 2.73 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18186262880186824		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.18186262880186824 | validation: 0.19217709334321098]
	TIME [epoch: 2.73 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16364008836655417		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.16364008836655417 | validation: 0.23525669121835646]
	TIME [epoch: 2.73 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1580940095303862		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.1580940095303862 | validation: 0.19700370909695067]
	TIME [epoch: 2.73 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14175478061807922		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.14175478061807922 | validation: 0.1902888553191674]
	TIME [epoch: 2.73 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14069966899850112		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.14069966899850112 | validation: 0.18066472640637954]
	TIME [epoch: 2.73 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12633552218200575		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.12633552218200575 | validation: 0.20607834424148114]
	TIME [epoch: 2.74 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13183487165244953		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.13183487165244953 | validation: 0.18929086897460193]
	TIME [epoch: 2.73 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13248209472094308		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.13248209472094308 | validation: 0.20641502376601034]
	TIME [epoch: 2.73 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13993995870623654		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.13993995870623654 | validation: 0.2091938065066147]
	TIME [epoch: 2.73 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15448979089148035		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.15448979089148035 | validation: 0.27356009138958626]
	TIME [epoch: 2.73 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2461910618175933		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.2461910618175933 | validation: 0.3059931470430506]
	TIME [epoch: 2.73 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28086927107165877		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.28086927107165877 | validation: 0.19235010718511625]
	TIME [epoch: 2.73 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15623035784653472		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.15623035784653472 | validation: 0.1808481718013158]
	TIME [epoch: 2.73 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1172584044462017		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.1172584044462017 | validation: 0.17943264449169435]
	TIME [epoch: 2.73 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1189138413047586		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.1189138413047586 | validation: 0.19213586884913184]
	TIME [epoch: 2.73 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13039519766425714		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.13039519766425714 | validation: 0.22267223899647473]
	TIME [epoch: 2.73 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14797292128231732		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.14797292128231732 | validation: 0.1922216249625184]
	TIME [epoch: 2.74 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13168478359871258		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.13168478359871258 | validation: 0.19140866470854456]
	TIME [epoch: 2.74 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14740195699105577		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.14740195699105577 | validation: 0.22453368044662408]
	TIME [epoch: 2.74 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18341168649276385		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.18341168649276385 | validation: 0.2071259650521661]
	TIME [epoch: 2.73 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20692822580391412		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.20692822580391412 | validation: 0.21502532679189096]
	TIME [epoch: 2.73 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15698649365529518		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.15698649365529518 | validation: 0.1753595300973181]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_561.pth
	Model improved!!!
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1338761175380917		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.1338761175380917 | validation: 0.1877676015440689]
	TIME [epoch: 2.74 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13249539362491292		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.13249539362491292 | validation: 0.2340324287456327]
	TIME [epoch: 2.73 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15341186599788134		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.15341186599788134 | validation: 0.21092784100769157]
	TIME [epoch: 2.73 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14915941342030326		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.14915941342030326 | validation: 0.1850928858554207]
	TIME [epoch: 2.74 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.159554691794167		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.159554691794167 | validation: 0.20669120980619968]
	TIME [epoch: 2.74 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16123383828971613		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.16123383828971613 | validation: 0.1806471239588121]
	TIME [epoch: 2.73 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15547494741045728		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.15547494741045728 | validation: 0.18761627948992188]
	TIME [epoch: 2.74 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13620886143760813		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.13620886143760813 | validation: 0.1671311384317442]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_569.pth
	Model improved!!!
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1282362689180817		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.1282362689180817 | validation: 0.17831607519817255]
	TIME [epoch: 2.73 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12674588177056664		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.12674588177056664 | validation: 0.19436619578351425]
	TIME [epoch: 2.73 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13257659116738246		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.13257659116738246 | validation: 0.19614612903481274]
	TIME [epoch: 2.73 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13771187009744826		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.13771187009744826 | validation: 0.22446571601449047]
	TIME [epoch: 2.73 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14483396363150544		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.14483396363150544 | validation: 0.17404140921548394]
	TIME [epoch: 2.73 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11820499714028306		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.11820499714028306 | validation: 0.16991630126738133]
	TIME [epoch: 2.73 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11312472044704878		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.11312472044704878 | validation: 0.15488887771187831]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_576.pth
	Model improved!!!
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11884187111449684		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.11884187111449684 | validation: 0.17345457494853758]
	TIME [epoch: 2.74 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13245994939373926		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.13245994939373926 | validation: 0.1783281540316721]
	TIME [epoch: 2.74 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17268237963343844		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.17268237963343844 | validation: 0.2216929793413298]
	TIME [epoch: 2.74 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17578099345722295		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.17578099345722295 | validation: 0.17928883472767645]
	TIME [epoch: 2.73 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1439549740307008		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.1439549740307008 | validation: 0.2146182222793609]
	TIME [epoch: 2.73 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15244458365333843		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.15244458365333843 | validation: 0.22513790012970408]
	TIME [epoch: 2.73 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15817338600798542		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.15817338600798542 | validation: 0.18127131250683634]
	TIME [epoch: 2.73 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12440960099543294		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.12440960099543294 | validation: 0.16794674645063837]
	TIME [epoch: 2.73 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12208406335844142		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.12208406335844142 | validation: 0.17854774387002356]
	TIME [epoch: 2.74 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11327460143159818		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.11327460143159818 | validation: 0.16126253323094644]
	TIME [epoch: 2.73 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12376199101491667		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.12376199101491667 | validation: 0.19916587166015787]
	TIME [epoch: 2.73 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15842174639575995		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.15842174639575995 | validation: 0.190652883357635]
	TIME [epoch: 2.73 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.169416812710945		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.169416812710945 | validation: 0.19456602214709218]
	TIME [epoch: 2.73 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13516376875330743		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.13516376875330743 | validation: 0.15965357306593647]
	TIME [epoch: 2.73 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1147290473678574		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.1147290473678574 | validation: 0.16501426562225474]
	TIME [epoch: 2.73 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11989036255694042		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.11989036255694042 | validation: 0.15812668689105108]
	TIME [epoch: 2.73 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11076274496028464		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.11076274496028464 | validation: 0.14783828436732696]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_593.pth
	Model improved!!!
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10435513401709749		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.10435513401709749 | validation: 0.14945535701878213]
	TIME [epoch: 2.74 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10006856688249505		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.10006856688249505 | validation: 0.17657859594864528]
	TIME [epoch: 2.74 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11162734189830613		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.11162734189830613 | validation: 0.19538410437821974]
	TIME [epoch: 2.74 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13955722252436373		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.13955722252436373 | validation: 0.18981831230319762]
	TIME [epoch: 2.74 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13463341660248151		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.13463341660248151 | validation: 0.166478217088494]
	TIME [epoch: 2.74 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1488973900310221		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.1488973900310221 | validation: 0.18236731217053773]
	TIME [epoch: 2.74 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1341049990925608		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.1341049990925608 | validation: 0.14301337094804858]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_600.pth
	Model improved!!!
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12914805524220227		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.12914805524220227 | validation: 0.1630336442401802]
	TIME [epoch: 2.75 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1136016675277896		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.1136016675277896 | validation: 0.13174795040385906]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_602.pth
	Model improved!!!
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09864863127786593		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.09864863127786593 | validation: 0.13162067297104127]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_603.pth
	Model improved!!!
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08875828417839211		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.08875828417839211 | validation: 0.13489703639566028]
	TIME [epoch: 2.73 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09413559491945873		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.09413559491945873 | validation: 0.19180076385422715]
	TIME [epoch: 2.73 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12795028883575862		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.12795028883575862 | validation: 0.20920793259948642]
	TIME [epoch: 2.73 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1428935948017753		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.1428935948017753 | validation: 0.1709208549313253]
	TIME [epoch: 2.73 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1152827717381334		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.1152827717381334 | validation: 0.1327110070865439]
	TIME [epoch: 2.74 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08807999702509704		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.08807999702509704 | validation: 0.12960106572339294]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_609.pth
	Model improved!!!
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0829342109725581		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.0829342109725581 | validation: 0.12579399895366267]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_610.pth
	Model improved!!!
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08235292903009399		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.08235292903009399 | validation: 0.13587595462401536]
	TIME [epoch: 2.73 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09675512722450486		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.09675512722450486 | validation: 0.22114898592660795]
	TIME [epoch: 2.74 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17153119816148002		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.17153119816148002 | validation: 0.28963172861166625]
	TIME [epoch: 2.73 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2937511450858009		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.2937511450858009 | validation: 0.15935217934041235]
	TIME [epoch: 2.73 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11072423437484054		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.11072423437484054 | validation: 0.11294567534434818]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_615.pth
	Model improved!!!
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07727684011897587		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.07727684011897587 | validation: 0.1174884380959676]
	TIME [epoch: 2.73 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07805288241689008		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.07805288241689008 | validation: 0.13025955787807242]
	TIME [epoch: 2.73 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.083900429280941		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.083900429280941 | validation: 0.12904230791540325]
	TIME [epoch: 2.74 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09668296309687982		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.09668296309687982 | validation: 0.16070698326871458]
	TIME [epoch: 2.74 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12751168476959188		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.12751168476959188 | validation: 0.1833730349009789]
	TIME [epoch: 2.73 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15867744909972603		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.15867744909972603 | validation: 0.2036210466425141]
	TIME [epoch: 2.74 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1376900695796589		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.1376900695796589 | validation: 0.14603722714677228]
	TIME [epoch: 2.74 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11388935649432791		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.11388935649432791 | validation: 0.14209872471664728]
	TIME [epoch: 2.74 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09488951463632184		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.09488951463632184 | validation: 0.13335890518011903]
	TIME [epoch: 2.73 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0982092969082119		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.0982092969082119 | validation: 0.14116852521012854]
	TIME [epoch: 2.73 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09753352859003007		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.09753352859003007 | validation: 0.1442534580030475]
	TIME [epoch: 2.73 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09894712634960516		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.09894712634960516 | validation: 0.13588995156255287]
	TIME [epoch: 2.73 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09527785393906751		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.09527785393906751 | validation: 0.1380448115926903]
	TIME [epoch: 2.73 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10795794987993688		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.10795794987993688 | validation: 0.15995572783478404]
	TIME [epoch: 2.73 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1141788155184873		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.1141788155184873 | validation: 0.16165677530916633]
	TIME [epoch: 2.73 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1312250751056966		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.1312250751056966 | validation: 0.1711349501809145]
	TIME [epoch: 2.73 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12176912450303927		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.12176912450303927 | validation: 0.1425506569496536]
	TIME [epoch: 2.74 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11135393249258525		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.11135393249258525 | validation: 0.1357080589250956]
	TIME [epoch: 2.73 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1018046545586368		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.1018046545586368 | validation: 0.12185601704719456]
	TIME [epoch: 2.73 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09510098391595374		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.09510098391595374 | validation: 0.12773279762331932]
	TIME [epoch: 2.73 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09028082091430062		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.09028082091430062 | validation: 0.12207834021554938]
	TIME [epoch: 2.73 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.091051752552647		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.091051752552647 | validation: 0.13770879687007995]
	TIME [epoch: 2.73 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0966920262088788		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.0966920262088788 | validation: 0.16087671108044888]
	TIME [epoch: 2.73 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10897071261653367		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.10897071261653367 | validation: 0.18396746328538982]
	TIME [epoch: 2.73 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12094652468601125		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.12094652468601125 | validation: 0.1645631177088438]
	TIME [epoch: 2.73 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11446016228634705		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.11446016228634705 | validation: 0.13264403090299265]
	TIME [epoch: 2.73 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08035887805742391		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.08035887805742391 | validation: 0.10504257284412498]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_642.pth
	Model improved!!!
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0760523697091353		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.0760523697091353 | validation: 0.12064389770267232]
	TIME [epoch: 2.74 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0916453133713879		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.0916453133713879 | validation: 0.12633137632724797]
	TIME [epoch: 2.73 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12478625732968883		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.12478625732968883 | validation: 0.1492588526265053]
	TIME [epoch: 2.73 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11448839885847693		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.11448839885847693 | validation: 0.11849203422337912]
	TIME [epoch: 2.73 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09915370060714977		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.09915370060714977 | validation: 0.12179575683509936]
	TIME [epoch: 2.73 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0852229893314146		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.0852229893314146 | validation: 0.13072288179325203]
	TIME [epoch: 2.73 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09170081888323452		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.09170081888323452 | validation: 0.1876501794863919]
	TIME [epoch: 2.73 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1259177655424741		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.1259177655424741 | validation: 0.17468807313218254]
	TIME [epoch: 2.73 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11828068215224294		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.11828068215224294 | validation: 0.11838916532392472]
	TIME [epoch: 2.73 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07962887350552345		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.07962887350552345 | validation: 0.11413375171120568]
	TIME [epoch: 2.73 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08361570443005484		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.08361570443005484 | validation: 0.12368143457585999]
	TIME [epoch: 2.73 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09402223028533761		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.09402223028533761 | validation: 0.11611054680335862]
	TIME [epoch: 2.74 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11499499301396689		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.11499499301396689 | validation: 0.13778598390686]
	TIME [epoch: 2.73 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10029565640842751		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.10029565640842751 | validation: 0.11944368407836546]
	TIME [epoch: 2.74 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08563551897926003		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.08563551897926003 | validation: 0.14789028443525898]
	TIME [epoch: 2.72 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09361921023458028		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.09361921023458028 | validation: 0.16998554388745232]
	TIME [epoch: 2.73 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1093691984217503		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.1093691984217503 | validation: 0.12944911232374895]
	TIME [epoch: 2.73 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08160209683151294		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.08160209683151294 | validation: 0.10754024022297788]
	TIME [epoch: 2.73 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06875211022426139		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.06875211022426139 | validation: 0.12814538924944616]
	TIME [epoch: 2.73 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08693144460026486		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.08693144460026486 | validation: 0.1448517093350948]
	TIME [epoch: 2.73 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1308418108847418		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.1308418108847418 | validation: 0.1615344097892273]
	TIME [epoch: 2.73 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11732398783643752		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.11732398783643752 | validation: 0.1238047790201736]
	TIME [epoch: 2.74 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09177392737188214		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.09177392737188214 | validation: 0.12069730574089395]
	TIME [epoch: 2.73 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07887335211461378		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.07887335211461378 | validation: 0.116643677154355]
	TIME [epoch: 2.73 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0803315042783753		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.0803315042783753 | validation: 0.12598557195277543]
	TIME [epoch: 2.73 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08068131309388253		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.08068131309388253 | validation: 0.12147261194535082]
	TIME [epoch: 2.73 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08970040227773694		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.08970040227773694 | validation: 0.14338236882242036]
	TIME [epoch: 2.73 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09671530264906009		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.09671530264906009 | validation: 0.13643956728809278]
	TIME [epoch: 2.73 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10673789506193501		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.10673789506193501 | validation: 0.1259960943736001]
	TIME [epoch: 2.73 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08442904988100713		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.08442904988100713 | validation: 0.10712113819219399]
	TIME [epoch: 2.73 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08204264367361042		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.08204264367361042 | validation: 0.10853197080169692]
	TIME [epoch: 2.74 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07075658541328765		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.07075658541328765 | validation: 0.10099496886744969]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_674.pth
	Model improved!!!
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07186151838494731		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.07186151838494731 | validation: 0.10754060894143783]
	TIME [epoch: 2.74 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06961666222582132		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.06961666222582132 | validation: 0.10296604716462868]
	TIME [epoch: 2.73 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07267487016472066		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.07267487016472066 | validation: 0.12190217994810054]
	TIME [epoch: 2.73 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08480925865218655		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.08480925865218655 | validation: 0.15338186548105398]
	TIME [epoch: 2.73 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11657806251217143		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.11657806251217143 | validation: 0.18488059645475904]
	TIME [epoch: 2.73 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12659011867853606		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.12659011867853606 | validation: 0.1364059952956145]
	TIME [epoch: 2.73 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0979010074747003		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.0979010074747003 | validation: 0.1119445845165191]
	TIME [epoch: 2.73 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06911976586849655		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.06911976586849655 | validation: 0.10061837566899007]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_682.pth
	Model improved!!!
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06666190339771774		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.06666190339771774 | validation: 0.09296298260060548]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_683.pth
	Model improved!!!
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06965862059009839		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.06965862059009839 | validation: 0.10279065892007476]
	TIME [epoch: 2.73 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08129540643572494		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.08129540643572494 | validation: 0.1253049423159099]
	TIME [epoch: 2.73 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09474316717361628		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.09474316717361628 | validation: 0.12161860296669165]
	TIME [epoch: 2.74 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09224136931610151		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.09224136931610151 | validation: 0.10722217583072582]
	TIME [epoch: 2.73 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0749796432538489		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.0749796432538489 | validation: 0.10449583217267829]
	TIME [epoch: 2.73 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06754112630717785		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.06754112630717785 | validation: 0.10873317340211495]
	TIME [epoch: 2.73 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06905167406623317		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.06905167406623317 | validation: 0.12265343075668467]
	TIME [epoch: 2.73 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07706192241888807		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.07706192241888807 | validation: 0.13478314414870068]
	TIME [epoch: 2.73 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09414257469046254		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.09414257469046254 | validation: 0.15144756728453357]
	TIME [epoch: 2.73 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11601378039539018		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.11601378039539018 | validation: 0.13842532398624652]
	TIME [epoch: 2.73 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12300321770553345		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.12300321770553345 | validation: 0.12025630665350158]
	TIME [epoch: 2.73 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08956324324000163		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.08956324324000163 | validation: 0.09285441870374778]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_695.pth
	Model improved!!!
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0697196854464825		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.0697196854464825 | validation: 0.0932705565663392]
	TIME [epoch: 2.74 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06371079537997108		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.06371079537997108 | validation: 0.09584555923666328]
	TIME [epoch: 2.73 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06630621012957773		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.06630621012957773 | validation: 0.11705510756241054]
	TIME [epoch: 2.74 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07556095294706086		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.07556095294706086 | validation: 0.12374883433184856]
	TIME [epoch: 2.73 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09380098086557875		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.09380098086557875 | validation: 0.13310694008746857]
	TIME [epoch: 2.73 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0851753449487804		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.0851753449487804 | validation: 0.10439471785865898]
	TIME [epoch: 2.73 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0826436991286328		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.0826436991286328 | validation: 0.10467047618978512]
	TIME [epoch: 2.73 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07880954842074323		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.07880954842074323 | validation: 0.09731483110488288]
	TIME [epoch: 2.72 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07546198136121113		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.07546198136121113 | validation: 0.09717066529998533]
	TIME [epoch: 2.73 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06656677428556342		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.06656677428556342 | validation: 0.0937504518063984]
	TIME [epoch: 2.73 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06711035397386378		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.06711035397386378 | validation: 0.10482370774586043]
	TIME [epoch: 2.73 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07124598146646244		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.07124598146646244 | validation: 0.12486504068682497]
	TIME [epoch: 2.74 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08849759393964328		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.08849759393964328 | validation: 0.14084952545819046]
	TIME [epoch: 2.73 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09249873368189615		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.09249873368189615 | validation: 0.1031861961524928]
	TIME [epoch: 2.73 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08045017748094872		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.08045017748094872 | validation: 0.10899576329446369]
	TIME [epoch: 2.73 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07359974828054364		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.07359974828054364 | validation: 0.09661038729109381]
	TIME [epoch: 2.73 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07637311328446647		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.07637311328446647 | validation: 0.10046314629011036]
	TIME [epoch: 2.73 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0791508427533314		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.0791508427533314 | validation: 0.09077249132296264]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_713.pth
	Model improved!!!
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07250028795546558		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.07250028795546558 | validation: 0.09755945057905825]
	TIME [epoch: 2.73 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06737971316679385		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.06737971316679385 | validation: 0.08437520294537598]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_715.pth
	Model improved!!!
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060009936656549245		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.060009936656549245 | validation: 0.08820435527687487]
	TIME [epoch: 2.74 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05924445789968107		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.05924445789968107 | validation: 0.10712656194649657]
	TIME [epoch: 2.74 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07343554354621276		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.07343554354621276 | validation: 0.15301474577998486]
	TIME [epoch: 2.74 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0971500655891647		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.0971500655891647 | validation: 0.13517921988599801]
	TIME [epoch: 2.74 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09886684301330428		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.09886684301330428 | validation: 0.09899926429620652]
	TIME [epoch: 2.74 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06527003444687747		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.06527003444687747 | validation: 0.08633493437172587]
	TIME [epoch: 2.74 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0648279913075067		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.0648279913075067 | validation: 0.08797514315070566]
	TIME [epoch: 2.73 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06684272674277085		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.06684272674277085 | validation: 0.08335754495233072]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_723.pth
	Model improved!!!
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07354264291080062		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.07354264291080062 | validation: 0.1025659708614586]
	TIME [epoch: 2.74 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06629156545700318		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.06629156545700318 | validation: 0.10048419160942768]
	TIME [epoch: 2.73 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06411889030138346		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.06411889030138346 | validation: 0.10892339453818703]
	TIME [epoch: 2.74 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06692981156888365		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.06692981156888365 | validation: 0.09626876172244397]
	TIME [epoch: 2.74 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06574231911481343		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.06574231911481343 | validation: 0.09851880710728217]
	TIME [epoch: 2.75 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06036298405263236		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.06036298405263236 | validation: 0.10036593541612351]
	TIME [epoch: 2.74 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07027630081731695		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.07027630081731695 | validation: 0.1146884453785839]
	TIME [epoch: 2.74 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08328856552622031		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.08328856552622031 | validation: 0.10861128807348891]
	TIME [epoch: 2.73 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09730477576026864		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.09730477576026864 | validation: 0.10962346161914337]
	TIME [epoch: 2.74 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0815395271143963		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.0815395271143963 | validation: 0.08250695248679812]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_733.pth
	Model improved!!!
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06573515946919609		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.06573515946919609 | validation: 0.09037073270604543]
	TIME [epoch: 2.73 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060843849096290016		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.060843849096290016 | validation: 0.09413704664543636]
	TIME [epoch: 2.73 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06539415695568217		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.06539415695568217 | validation: 0.11125681380334607]
	TIME [epoch: 2.74 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07380554199791046		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.07380554199791046 | validation: 0.10583023134821797]
	TIME [epoch: 2.74 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07071800721681774		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.07071800721681774 | validation: 0.10106283222101081]
	TIME [epoch: 2.74 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06571541509469124		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.06571541509469124 | validation: 0.09052206782266423]
	TIME [epoch: 2.74 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06350129532133413		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.06350129532133413 | validation: 0.09723676874804103]
	TIME [epoch: 2.74 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05452537884131716		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.05452537884131716 | validation: 0.08985918621417173]
	TIME [epoch: 2.74 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05380546371238452		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.05380546371238452 | validation: 0.0927130892973871]
	TIME [epoch: 2.74 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05946792450183333		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.05946792450183333 | validation: 0.09443424469944978]
	TIME [epoch: 2.74 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0627375250992318		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.0627375250992318 | validation: 0.089124762870488]
	TIME [epoch: 2.74 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05984078477354351		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.05984078477354351 | validation: 0.0900388781614312]
	TIME [epoch: 2.74 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06193507632065365		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.06193507632065365 | validation: 0.10700356295725877]
	TIME [epoch: 2.74 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08994220743237434		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.08994220743237434 | validation: 0.10629770148105318]
	TIME [epoch: 2.74 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10218290438285475		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.10218290438285475 | validation: 0.10697439539654843]
	TIME [epoch: 2.74 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07082535349729088		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.07082535349729088 | validation: 0.07906855343744859]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_749.pth
	Model improved!!!
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054960159232970394		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.054960159232970394 | validation: 0.08491105043331819]
	TIME [epoch: 2.73 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05392626901974653		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.05392626901974653 | validation: 0.09148389667918741]
	TIME [epoch: 2.73 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05624295328828472		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.05624295328828472 | validation: 0.09955275299313085]
	TIME [epoch: 2.72 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0571665472677963		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.0571665472677963 | validation: 0.08971155095018203]
	TIME [epoch: 2.73 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053900242938128506		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.053900242938128506 | validation: 0.08604307341619286]
	TIME [epoch: 2.73 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047470273842893644		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.047470273842893644 | validation: 0.07920868795263808]
	TIME [epoch: 2.74 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04678857467674063		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.04678857467674063 | validation: 0.06933871914329102]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_756.pth
	Model improved!!!
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05229656957043481		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.05229656957043481 | validation: 0.08068335506478787]
	TIME [epoch: 2.73 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06816877387335372		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.06816877387335372 | validation: 0.09377491207766439]
	TIME [epoch: 2.73 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09559429593134003		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.09559429593134003 | validation: 0.12993799761038302]
	TIME [epoch: 2.73 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09515600267141248		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.09515600267141248 | validation: 0.1352711845815664]
	TIME [epoch: 2.73 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08883438778179191		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.08883438778179191 | validation: 0.08903648866715524]
	TIME [epoch: 2.73 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05745566090736881		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.05745566090736881 | validation: 0.07954328811747169]
	TIME [epoch: 2.73 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04795779102011025		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.04795779102011025 | validation: 0.07924972745473498]
	TIME [epoch: 2.74 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04880834767526936		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.04880834767526936 | validation: 0.07400085686553359]
	TIME [epoch: 2.74 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05125797956080469		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.05125797956080469 | validation: 0.07864285285330143]
	TIME [epoch: 2.74 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057970782676637966		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.057970782676637966 | validation: 0.08234541442027533]
	TIME [epoch: 2.73 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05474496967288071		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.05474496967288071 | validation: 0.0865537647051143]
	TIME [epoch: 2.74 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05472960449855355		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.05472960449855355 | validation: 0.07753059208099357]
	TIME [epoch: 2.73 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05316359321678807		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.05316359321678807 | validation: 0.07457879588133873]
	TIME [epoch: 2.73 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05055682305072603		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.05055682305072603 | validation: 0.0717978158506608]
	TIME [epoch: 2.73 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04796031090245146		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.04796031090245146 | validation: 0.0787348168084653]
	TIME [epoch: 2.73 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050379819071493824		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.050379819071493824 | validation: 0.07895798851528048]
	TIME [epoch: 2.72 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06249429675048955		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.06249429675048955 | validation: 0.16419890380052654]
	TIME [epoch: 2.73 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10629267580791313		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.10629267580791313 | validation: 0.13282285532265023]
	TIME [epoch: 2.72 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10396806784760111		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.10396806784760111 | validation: 0.08102625070064713]
	TIME [epoch: 2.72 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054163237352902315		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.054163237352902315 | validation: 0.0641477432120278]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_776.pth
	Model improved!!!
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045990809614839485		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.045990809614839485 | validation: 0.07172220872090154]
	TIME [epoch: 2.74 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04312509306158292		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.04312509306158292 | validation: 0.06780607921600236]
	TIME [epoch: 2.73 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043155337660967005		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.043155337660967005 | validation: 0.07102123252283231]
	TIME [epoch: 2.74 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04329868339016428		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.04329868339016428 | validation: 0.0777505766199248]
	TIME [epoch: 2.73 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04447745380169115		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.04447745380169115 | validation: 0.06778682669136059]
	TIME [epoch: 2.74 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0434964409071024		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.0434964409071024 | validation: 0.07376822417390992]
	TIME [epoch: 2.73 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05034432640230477		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.05034432640230477 | validation: 0.0953466343762055]
	TIME [epoch: 2.74 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07346825613467652		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.07346825613467652 | validation: 0.08346534769731251]
	TIME [epoch: 2.73 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07893615724021173		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.07893615724021173 | validation: 0.09469165445357504]
	TIME [epoch: 2.74 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06180336478284017		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.06180336478284017 | validation: 0.08858883275192522]
	TIME [epoch: 2.73 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0623992438192458		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.0623992438192458 | validation: 0.14419449444234175]
	TIME [epoch: 2.73 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08646476243794453		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.08646476243794453 | validation: 0.09061174266589683]
	TIME [epoch: 2.74 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06275810080437585		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.06275810080437585 | validation: 0.08620136004331917]
	TIME [epoch: 2.73 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05469181566823131		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.05469181566823131 | validation: 0.07203394786945369]
	TIME [epoch: 2.74 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050407926609826945		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.050407926609826945 | validation: 0.06963587884705506]
	TIME [epoch: 2.74 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04483583768678562		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.04483583768678562 | validation: 0.06494586630540398]
	TIME [epoch: 2.74 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041285371092983456		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.041285371092983456 | validation: 0.06955738463221978]
	TIME [epoch: 2.73 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04484768925072403		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.04484768925072403 | validation: 0.06884476330970538]
	TIME [epoch: 2.73 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05037450745446052		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.05037450745446052 | validation: 0.10339369724365542]
	TIME [epoch: 2.73 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06753722223419772		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.06753722223419772 | validation: 0.11242599476592821]
	TIME [epoch: 2.73 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08428772008211974		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.08428772008211974 | validation: 0.09665808227939378]
	TIME [epoch: 2.74 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07275132449000309		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.07275132449000309 | validation: 0.07028872925897454]
	TIME [epoch: 2.73 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052427403456862864		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.052427403456862864 | validation: 0.06730200863881665]
	TIME [epoch: 2.73 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0407897534342472		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.0407897534342472 | validation: 0.06497329145306273]
	TIME [epoch: 2.74 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03879010852266374		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.03879010852266374 | validation: 0.060367335947439406]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_801.pth
	Model improved!!!
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041379336662143905		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.041379336662143905 | validation: 0.06942011495113258]
	TIME [epoch: 2.74 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041152023857616724		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.041152023857616724 | validation: 0.0652882200962567]
	TIME [epoch: 2.73 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039683375030125395		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.039683375030125395 | validation: 0.07344087198855513]
	TIME [epoch: 2.73 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045950804167241524		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.045950804167241524 | validation: 0.09696264987989107]
	TIME [epoch: 2.73 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06334722795365798		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.06334722795365798 | validation: 0.12309212473160312]
	TIME [epoch: 2.73 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08038261177880475		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.08038261177880475 | validation: 0.08833495938595672]
	TIME [epoch: 2.73 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08239695825210663		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.08239695825210663 | validation: 0.08837615281782603]
	TIME [epoch: 2.73 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07135981423654351		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.07135981423654351 | validation: 0.07059238929683329]
	TIME [epoch: 2.73 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04941120745721042		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.04941120745721042 | validation: 0.06456833246471579]
	TIME [epoch: 2.73 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03918633145645665		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.03918633145645665 | validation: 0.06169112058078764]
	TIME [epoch: 2.73 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03735763327045821		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.03735763327045821 | validation: 0.0651054287859334]
	TIME [epoch: 2.73 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04144662362380306		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.04144662362380306 | validation: 0.06520273100482199]
	TIME [epoch: 2.74 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042413843611093044		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.042413843611093044 | validation: 0.0751696877943767]
	TIME [epoch: 2.73 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04354155682295431		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.04354155682295431 | validation: 0.07471009763810714]
	TIME [epoch: 2.73 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04693769802695003		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.04693769802695003 | validation: 0.09075737664512142]
	TIME [epoch: 2.73 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05006447120212443		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.05006447120212443 | validation: 0.07554668490846125]
	TIME [epoch: 2.73 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046972682851092636		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.046972682851092636 | validation: 0.09043522475994613]
	TIME [epoch: 2.73 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06254253733777232		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.06254253733777232 | validation: 0.09315790775386]
	TIME [epoch: 2.73 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08798535283714816		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.08798535283714816 | validation: 0.09115274558072099]
	TIME [epoch: 2.73 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07268927341159613		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.07268927341159613 | validation: 0.06650218489456436]
	TIME [epoch: 2.73 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04541348336910188		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.04541348336910188 | validation: 0.07194909153642878]
	TIME [epoch: 2.73 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040864657547974054		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.040864657547974054 | validation: 0.06806771644489817]
	TIME [epoch: 2.73 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04071787057133776		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.04071787057133776 | validation: 0.06500359755836214]
	TIME [epoch: 2.74 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0417273972058274		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.0417273972058274 | validation: 0.06632296282655507]
	TIME [epoch: 2.73 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04163080324041988		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.04163080324041988 | validation: 0.057377215730434784]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_826.pth
	Model improved!!!
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04239617841966206		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.04239617841966206 | validation: 0.06359899620497461]
	TIME [epoch: 2.73 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04939355691860614		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.04939355691860614 | validation: 0.07438128605459184]
	TIME [epoch: 2.73 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05915733529759043		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.05915733529759043 | validation: 0.09516846912519623]
	TIME [epoch: 2.73 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06409704656701344		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.06409704656701344 | validation: 0.08485764258319144]
	TIME [epoch: 2.73 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05908987532699912		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.05908987532699912 | validation: 0.06996570476468532]
	TIME [epoch: 2.73 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04230318590032106		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.04230318590032106 | validation: 0.06365340407750747]
	TIME [epoch: 2.73 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04096682970531285		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.04096682970531285 | validation: 0.06822554681088794]
	TIME [epoch: 2.73 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04214716072892836		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.04214716072892836 | validation: 0.061181411191424076]
	TIME [epoch: 2.73 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04274377482800711		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.04274377482800711 | validation: 0.06810989513218656]
	TIME [epoch: 2.73 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043388530406968114		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.043388530406968114 | validation: 0.07167838779268239]
	TIME [epoch: 2.73 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04760952862213067		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.04760952862213067 | validation: 0.08549772958015533]
	TIME [epoch: 2.73 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057886551185101554		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.057886551185101554 | validation: 0.08665011930031988]
	TIME [epoch: 2.73 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061763739462031876		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.061763739462031876 | validation: 0.07967719475214476]
	TIME [epoch: 2.73 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049406522965636857		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.049406522965636857 | validation: 0.06256863854238019]
	TIME [epoch: 2.72 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04409802478395074		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.04409802478395074 | validation: 0.06747935281545814]
	TIME [epoch: 2.72 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04030398430446366		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.04030398430446366 | validation: 0.06436845782686682]
	TIME [epoch: 2.73 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040461196749737256		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.040461196749737256 | validation: 0.06461571408440973]
	TIME [epoch: 2.73 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03748236115245161		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.03748236115245161 | validation: 0.0643305000189385]
	TIME [epoch: 2.73 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03988082165124098		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.03988082165124098 | validation: 0.06267343404688161]
	TIME [epoch: 2.73 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037831048607342196		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.037831048607342196 | validation: 0.06059949807187889]
	TIME [epoch: 2.73 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04010392719740263		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.04010392719740263 | validation: 0.07710411775516125]
	TIME [epoch: 2.73 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045534346056032876		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.045534346056032876 | validation: 0.07943744090022622]
	TIME [epoch: 2.73 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06891595979647609		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.06891595979647609 | validation: 0.08614636925144088]
	TIME [epoch: 2.73 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06713028997749841		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.06713028997749841 | validation: 0.07293884444045722]
	TIME [epoch: 2.72 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05344700208006624		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.05344700208006624 | validation: 0.0708353161248682]
	TIME [epoch: 2.73 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04275532847853789		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.04275532847853789 | validation: 0.06444247861918877]
	TIME [epoch: 2.73 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0368024891370158		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.0368024891370158 | validation: 0.05904474800934689]
	TIME [epoch: 2.73 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03725875113418728		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.03725875113418728 | validation: 0.05733123133838435]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_854.pth
	Model improved!!!
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03811500270347931		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.03811500270347931 | validation: 0.05444415447813783]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_855.pth
	Model improved!!!
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039408080145606476		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.039408080145606476 | validation: 0.06848025815768884]
	TIME [epoch: 2.74 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04184138613838127		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.04184138613838127 | validation: 0.07217457038811435]
	TIME [epoch: 2.73 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0422236493597002		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.0422236493597002 | validation: 0.08325896260993168]
	TIME [epoch: 2.73 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05223325018310587		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.05223325018310587 | validation: 0.07203816929931864]
	TIME [epoch: 2.73 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053443576130824566		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.053443576130824566 | validation: 0.07951959525072713]
	TIME [epoch: 2.73 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052222271765992756		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.052222271765992756 | validation: 0.06555857302010958]
	TIME [epoch: 2.73 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04558279578008518		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.04558279578008518 | validation: 0.06210812950944623]
	TIME [epoch: 2.73 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03594689407212414		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.03594689407212414 | validation: 0.05701072404606366]
	TIME [epoch: 2.73 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036353328619022955		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.036353328619022955 | validation: 0.06612742202906217]
	TIME [epoch: 2.73 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0371069952292444		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.0371069952292444 | validation: 0.06205812935748627]
	TIME [epoch: 2.73 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04461571268218613		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.04461571268218613 | validation: 0.0813349243908298]
	TIME [epoch: 2.73 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04709170682034735		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.04709170682034735 | validation: 0.06452806541756555]
	TIME [epoch: 2.74 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043619741832322034		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.043619741832322034 | validation: 0.07349789967996007]
	TIME [epoch: 2.73 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04092781924492542		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.04092781924492542 | validation: 0.06185198478689363]
	TIME [epoch: 2.73 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038728074097555576		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.038728074097555576 | validation: 0.06572530589115137]
	TIME [epoch: 2.73 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040412515253320666		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.040412515253320666 | validation: 0.06181446881244105]
	TIME [epoch: 2.73 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042789116235484544		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.042789116235484544 | validation: 0.07298194941464174]
	TIME [epoch: 2.73 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04609889913215952		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.04609889913215952 | validation: 0.06487616624714311]
	TIME [epoch: 2.73 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04416108748806268		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.04416108748806268 | validation: 0.07916091482333389]
	TIME [epoch: 2.73 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049261417020860496		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.049261417020860496 | validation: 0.06615003435157807]
	TIME [epoch: 2.73 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04217903670127062		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.04217903670127062 | validation: 0.06343596000118902]
	TIME [epoch: 2.73 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03891439040412311		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.03891439040412311 | validation: 0.05740605762299176]
	TIME [epoch: 2.72 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035265446346092334		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.035265446346092334 | validation: 0.05899375143719596]
	TIME [epoch: 2.74 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03819523508059329		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.03819523508059329 | validation: 0.061756491803256146]
	TIME [epoch: 2.73 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040564074240291986		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.040564074240291986 | validation: 0.07451788839322027]
	TIME [epoch: 2.73 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050365568392092185		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.050365568392092185 | validation: 0.07530223767042565]
	TIME [epoch: 2.73 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05238145463875265		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.05238145463875265 | validation: 0.07648766043247805]
	TIME [epoch: 2.73 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04701910142832435		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.04701910142832435 | validation: 0.059387970389724654]
	TIME [epoch: 2.72 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041923919586100665		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.041923919586100665 | validation: 0.06165183131706582]
	TIME [epoch: 2.73 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03544502297003277		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.03544502297003277 | validation: 0.05980831291436899]
	TIME [epoch: 2.73 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03240737825428747		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.03240737825428747 | validation: 0.05053490945598282]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_886.pth
	Model improved!!!
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03564849838167797		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.03564849838167797 | validation: 0.05292882169686217]
	TIME [epoch: 2.73 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03363073692745222		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.03363073692745222 | validation: 0.0667103956598253]
	TIME [epoch: 2.73 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04013605622844641		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.04013605622844641 | validation: 0.05891691537414073]
	TIME [epoch: 2.74 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04910986326468348		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.04910986326468348 | validation: 0.07428321188358271]
	TIME [epoch: 2.73 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04629795441313058		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.04629795441313058 | validation: 0.07614823559491522]
	TIME [epoch: 2.73 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04330004690697251		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.04330004690697251 | validation: 0.07126962295797776]
	TIME [epoch: 2.73 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043286907269595554		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.043286907269595554 | validation: 0.05383792357426394]
	TIME [epoch: 2.73 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04018054826464349		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.04018054826464349 | validation: 0.06168315322653517]
	TIME [epoch: 2.73 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03703056337732355		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.03703056337732355 | validation: 0.05791630754241006]
	TIME [epoch: 2.73 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03665754827401895		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.03665754827401895 | validation: 0.051518126207271614]
	TIME [epoch: 2.73 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034404912229666657		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.034404912229666657 | validation: 0.05965567209132573]
	TIME [epoch: 2.73 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03562897487371694		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.03562897487371694 | validation: 0.05625679157371993]
	TIME [epoch: 2.73 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0430256973448647		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.0430256973448647 | validation: 0.07470080494707075]
	TIME [epoch: 2.74 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055661947324335305		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.055661947324335305 | validation: 0.07103443024236293]
	TIME [epoch: 2.73 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05302948885907538		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.05302948885907538 | validation: 0.0655558682781433]
	TIME [epoch: 2.73 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03670890815805922		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.03670890815805922 | validation: 0.05414986278082007]
	TIME [epoch: 2.74 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030617324596748662		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.030617324596748662 | validation: 0.04709047328659325]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_903.pth
	Model improved!!!
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032009590652835124		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.032009590652835124 | validation: 0.05774624344655739]
	TIME [epoch: 2.73 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031556041560188465		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.031556041560188465 | validation: 0.04998875321881355]
	TIME [epoch: 2.73 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031687338345368715		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.031687338345368715 | validation: 0.05973941597321067]
	TIME [epoch: 2.73 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034678087899086316		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.034678087899086316 | validation: 0.05832513053054494]
	TIME [epoch: 2.73 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038992387349035894		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.038992387349035894 | validation: 0.07436380425387581]
	TIME [epoch: 2.73 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047603987977862444		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.047603987977862444 | validation: 0.06667801675525507]
	TIME [epoch: 2.73 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049465407152227914		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.049465407152227914 | validation: 0.06901694649420172]
	TIME [epoch: 2.74 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042177803122173285		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.042177803122173285 | validation: 0.05089642862990389]
	TIME [epoch: 2.73 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03782803648452236		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.03782803648452236 | validation: 0.056081586072137625]
	TIME [epoch: 2.73 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03398162155508909		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.03398162155508909 | validation: 0.05734222461960525]
	TIME [epoch: 2.73 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031582081386877486		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.031582081386877486 | validation: 0.05430158489498435]
	TIME [epoch: 2.73 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03301898947471975		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.03301898947471975 | validation: 0.058696281703995414]
	TIME [epoch: 2.73 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03474493493363734		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.03474493493363734 | validation: 0.055242193562499015]
	TIME [epoch: 2.73 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03313726459087343		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.03313726459087343 | validation: 0.0508841714605863]
	TIME [epoch: 2.73 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031090390688482874		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.031090390688482874 | validation: 0.057696386466855244]
	TIME [epoch: 2.73 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03369003955609372		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.03369003955609372 | validation: 0.06232465206517145]
	TIME [epoch: 2.73 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03673961537300064		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.03673961537300064 | validation: 0.06744036576302352]
	TIME [epoch: 2.73 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03955026659134478		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.03955026659134478 | validation: 0.06215577258872299]
	TIME [epoch: 2.74 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03847863087317154		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.03847863087317154 | validation: 0.06658827624029862]
	TIME [epoch: 2.73 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03693258735966358		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.03693258735966358 | validation: 0.056039086493271655]
	TIME [epoch: 2.73 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04084451401741541		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.04084451401741541 | validation: 0.0697831776664163]
	TIME [epoch: 2.73 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047512700314841116		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.047512700314841116 | validation: 0.07003270170238067]
	TIME [epoch: 2.73 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06320233913014481		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.06320233913014481 | validation: 0.05936847548612104]
	TIME [epoch: 2.73 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03682150013999745		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.03682150013999745 | validation: 0.051816253351188994]
	TIME [epoch: 2.73 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03280033258676411		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.03280033258676411 | validation: 0.05199912682182013]
	TIME [epoch: 2.73 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034773430021591334		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.034773430021591334 | validation: 0.061958194650521936]
	TIME [epoch: 2.73 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03370367445959804		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.03370367445959804 | validation: 0.055827719618336574]
	TIME [epoch: 2.73 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030240802049470546		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.030240802049470546 | validation: 0.060519195610466814]
	TIME [epoch: 2.73 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035644870521071466		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.035644870521071466 | validation: 0.05872101974950278]
	TIME [epoch: 2.73 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03829918864571617		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.03829918864571617 | validation: 0.06070484678225016]
	TIME [epoch: 2.73 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03374330479720355		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.03374330479720355 | validation: 0.05890553967424492]
	TIME [epoch: 2.73 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03770234122996905		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.03770234122996905 | validation: 0.05931203201352338]
	TIME [epoch: 2.72 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03837723124763209		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.03837723124763209 | validation: 0.04982850332988401]
	TIME [epoch: 2.73 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037763603400541086		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.037763603400541086 | validation: 0.051113221539574455]
	TIME [epoch: 2.73 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03381656746272051		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.03381656746272051 | validation: 0.05431181269265284]
	TIME [epoch: 2.73 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03429860019504656		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.03429860019504656 | validation: 0.05828019592577127]
	TIME [epoch: 2.73 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0342416479776159		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.0342416479776159 | validation: 0.05557242018745265]
	TIME [epoch: 2.73 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036571224058967525		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.036571224058967525 | validation: 0.061480773254162326]
	TIME [epoch: 2.73 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03591104604948315		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.03591104604948315 | validation: 0.05895445849725614]
	TIME [epoch: 2.73 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03687504667757569		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.03687504667757569 | validation: 0.05949667229033624]
	TIME [epoch: 2.73 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032935220818956695		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.032935220818956695 | validation: 0.049760501566866705]
	TIME [epoch: 2.73 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029697099285351726		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.029697099285351726 | validation: 0.05539482385550374]
	TIME [epoch: 2.73 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028912690148399634		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.028912690148399634 | validation: 0.04789864914768686]
	TIME [epoch: 2.73 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02952043161194779		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.02952043161194779 | validation: 0.05450726251983504]
	TIME [epoch: 2.73 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031858773370795415		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.031858773370795415 | validation: 0.046382297503051395]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_948.pth
	Model improved!!!
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03471262438498448		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.03471262438498448 | validation: 0.05893026995653942]
	TIME [epoch: 2.74 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041441526021039544		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.041441526021039544 | validation: 0.06143397041195719]
	TIME [epoch: 2.74 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0491699845166034		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.0491699845166034 | validation: 0.06508550739334436]
	TIME [epoch: 2.74 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04803108600228328		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.04803108600228328 | validation: 0.04809673673293213]
	TIME [epoch: 2.73 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034627271373063526		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.034627271373063526 | validation: 0.053498728879262616]
	TIME [epoch: 2.73 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029618354989141605		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.029618354989141605 | validation: 0.046819514095692176]
	TIME [epoch: 2.73 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030085653637726258		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.030085653637726258 | validation: 0.049657092014316166]
	TIME [epoch: 2.73 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02799567639315519		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.02799567639315519 | validation: 0.05449234664988174]
	TIME [epoch: 2.73 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028802062162422124		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.028802062162422124 | validation: 0.04862900613980038]
	TIME [epoch: 2.73 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028023375141639422		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.028023375141639422 | validation: 0.049443024894026126]
	TIME [epoch: 2.73 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03141470589040532		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.03141470589040532 | validation: 0.050403102556622316]
	TIME [epoch: 2.73 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02994296059054584		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.02994296059054584 | validation: 0.06532684527491084]
	TIME [epoch: 2.73 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037125305323682516		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.037125305323682516 | validation: 0.06380613411591847]
	TIME [epoch: 2.73 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04152359415199852		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.04152359415199852 | validation: 0.06496944328293328]
	TIME [epoch: 2.73 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0378583894382817		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.0378583894382817 | validation: 0.05052368511569543]
	TIME [epoch: 2.73 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04599338239394424		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.04599338239394424 | validation: 0.060338268866161754]
	TIME [epoch: 2.74 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04581745798691045		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.04581745798691045 | validation: 0.04854816401967187]
	TIME [epoch: 2.73 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032360262591891314		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.032360262591891314 | validation: 0.04661706800255993]
	TIME [epoch: 2.73 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028511750127670085		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.028511750127670085 | validation: 0.05144587767587342]
	TIME [epoch: 2.72 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030068473469153437		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.030068473469153437 | validation: 0.04370227341954311]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_968.pth
	Model improved!!!
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02884743227547735		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.02884743227547735 | validation: 0.049939177402289606]
	TIME [epoch: 2.74 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028396569583475008		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.028396569583475008 | validation: 0.045450972346960064]
	TIME [epoch: 2.74 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027957425533593386		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.027957425533593386 | validation: 0.05553323567083587]
	TIME [epoch: 2.74 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030472550127002958		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.030472550127002958 | validation: 0.057724915561628835]
	TIME [epoch: 2.74 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03802492564043045		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.03802492564043045 | validation: 0.0661426944456642]
	TIME [epoch: 2.74 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03655358717537873		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.03655358717537873 | validation: 0.04521426394843062]
	TIME [epoch: 2.74 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03251122497391525		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.03251122497391525 | validation: 0.052140183621989224]
	TIME [epoch: 2.74 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03098367628924999		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.03098367628924999 | validation: 0.04532791075637689]
	TIME [epoch: 2.73 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030310447285314467		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.030310447285314467 | validation: 0.04860822923829107]
	TIME [epoch: 2.73 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026725120937620552		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.026725120937620552 | validation: 0.04198053622565299]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_978.pth
	Model improved!!!
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03227838442784374		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.03227838442784374 | validation: 0.053977687106147025]
	TIME [epoch: 2.74 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03655536231125932		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.03655536231125932 | validation: 0.06141818797243651]
	TIME [epoch: 2.74 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04693293181316605		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.04693293181316605 | validation: 0.07034627275375359]
	TIME [epoch: 2.74 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041708159691461265		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.041708159691461265 | validation: 0.04842560054543184]
	TIME [epoch: 2.74 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03427238598899308		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.03427238598899308 | validation: 0.052781743377757664]
	TIME [epoch: 2.73 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03131831132803042		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.03131831132803042 | validation: 0.055763763314541887]
	TIME [epoch: 2.73 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030295851472786732		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.030295851472786732 | validation: 0.04582868932768879]
	TIME [epoch: 2.74 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029406352224155156		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.029406352224155156 | validation: 0.04789208633003466]
	TIME [epoch: 2.73 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02848236555584255		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.02848236555584255 | validation: 0.0509983566333231]
	TIME [epoch: 2.73 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029710573787515085		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.029710573787515085 | validation: 0.04884263894938724]
	TIME [epoch: 2.73 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031089798545337156		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.031089798545337156 | validation: 0.04810294154110069]
	TIME [epoch: 2.73 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029467937795442357		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.029467937795442357 | validation: 0.056294851800898785]
	TIME [epoch: 2.73 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035771061227750096		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.035771061227750096 | validation: 0.05579358434354468]
	TIME [epoch: 2.73 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03715638717757996		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.03715638717757996 | validation: 0.05489547359518468]
	TIME [epoch: 2.73 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03223233613521984		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.03223233613521984 | validation: 0.044925373849791596]
	TIME [epoch: 2.73 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027630552083064558		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.027630552083064558 | validation: 0.0499078601491356]
	TIME [epoch: 2.73 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03049270161731733		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.03049270161731733 | validation: 0.04377831763913919]
	TIME [epoch: 2.73 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028424573224177775		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.028424573224177775 | validation: 0.05344415485538269]
	TIME [epoch: 2.73 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030296764362308437		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.030296764362308437 | validation: 0.04667953291962895]
	TIME [epoch: 2.73 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028015629401965895		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.028015629401965895 | validation: 0.04398178511445904]
	TIME [epoch: 2.73 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02649208044279964		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.02649208044279964 | validation: 0.048776662159037254]
	TIME [epoch: 2.73 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026633899566281344		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.026633899566281344 | validation: 0.05305074020407631]
	TIME [epoch: 2.73 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029349669704834578		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.029349669704834578 | validation: 0.04619450050702924]
	TIME [epoch: 176 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026994170881274482		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.026994170881274482 | validation: 0.05564366115220505]
	TIME [epoch: 5.85 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029700888182434198		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.029700888182434198 | validation: 0.05419366480587239]
	TIME [epoch: 5.86 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03683554196460361		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.03683554196460361 | validation: 0.06679976729729652]
	TIME [epoch: 5.85 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04533590894825844		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.04533590894825844 | validation: 0.05728368334904097]
	TIME [epoch: 5.86 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04196063492919127		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.04196063492919127 | validation: 0.048523525655975674]
	TIME [epoch: 5.85 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028360533101532165		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.028360533101532165 | validation: 0.04475385416353655]
	TIME [epoch: 5.85 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02642401724040379		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.02642401724040379 | validation: 0.041841268580905436]
	TIME [epoch: 5.85 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_1008.pth
	Model improved!!!
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0314054447987452		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.0314054447987452 | validation: 0.050029697631636054]
	TIME [epoch: 5.87 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031067058991727618		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.031067058991727618 | validation: 0.04398261534064843]
	TIME [epoch: 5.86 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028263112224601573		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.028263112224601573 | validation: 0.05252754221968081]
	TIME [epoch: 5.86 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03005822366260845		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.03005822366260845 | validation: 0.0481907326793165]
	TIME [epoch: 5.85 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02769331602004121		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.02769331602004121 | validation: 0.05388411068429149]
	TIME [epoch: 5.85 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028504720878277857		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.028504720878277857 | validation: 0.04441494039760945]
	TIME [epoch: 5.85 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030252286354347183		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.030252286354347183 | validation: 0.05909339035184488]
	TIME [epoch: 5.87 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03124046896104191		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.03124046896104191 | validation: 0.05478247259294943]
	TIME [epoch: 5.85 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03216964789008912		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.03216964789008912 | validation: 0.05273471013015396]
	TIME [epoch: 5.85 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02811356332916124		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.02811356332916124 | validation: 0.03853680402905174]
	TIME [epoch: 5.85 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_1018.pth
	Model improved!!!
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02786444199791507		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.02786444199791507 | validation: 0.054191911663460816]
	TIME [epoch: 5.85 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03194967322513392		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.03194967322513392 | validation: 0.050372749241485854]
	TIME [epoch: 5.86 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03676152844796873		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.03676152844796873 | validation: 0.05527932965053292]
	TIME [epoch: 5.85 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032193213744491855		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.032193213744491855 | validation: 0.04825702566526294]
	TIME [epoch: 5.85 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03012334863300774		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.03012334863300774 | validation: 0.04409776729476768]
	TIME [epoch: 5.86 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02889101689975651		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.02889101689975651 | validation: 0.046010605917042646]
	TIME [epoch: 5.85 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026735243750819686		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.026735243750819686 | validation: 0.04893489880184734]
	TIME [epoch: 5.86 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02833957904199182		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.02833957904199182 | validation: 0.04933893415641051]
	TIME [epoch: 5.85 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02624052930305796		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.02624052930305796 | validation: 0.04748598045814539]
	TIME [epoch: 5.85 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026072710138894162		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.026072710138894162 | validation: 0.04743960425773023]
	TIME [epoch: 5.84 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029250382430124608		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.029250382430124608 | validation: 0.04724976809960259]
	TIME [epoch: 5.85 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030079653482398893		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.030079653482398893 | validation: 0.04853926197205029]
	TIME [epoch: 5.85 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03194303016366396		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.03194303016366396 | validation: 0.05051373526919355]
	TIME [epoch: 5.84 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03340106878105836		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.03340106878105836 | validation: 0.04713229504535885]
	TIME [epoch: 5.84 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03317354162479117		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.03317354162479117 | validation: 0.055597206152972894]
	TIME [epoch: 5.84 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03410444838131721		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.03410444838131721 | validation: 0.04306160997259444]
	TIME [epoch: 5.84 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02740465285764582		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.02740465285764582 | validation: 0.046660217065453785]
	TIME [epoch: 5.85 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027454036165088166		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.027454036165088166 | validation: 0.046918672804759115]
	TIME [epoch: 5.85 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025504545356000687		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.025504545356000687 | validation: 0.0475440140520848]
	TIME [epoch: 5.84 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0282090832437002		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.0282090832437002 | validation: 0.05218781013288444]
	TIME [epoch: 5.85 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027129758569570468		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.027129758569570468 | validation: 0.0451307541069143]
	TIME [epoch: 5.84 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02803801836804844		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.02803801836804844 | validation: 0.0500182141345909]
	TIME [epoch: 5.84 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0285560538477636		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.0285560538477636 | validation: 0.04352486386056381]
	TIME [epoch: 5.86 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033062177796208174		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.033062177796208174 | validation: 0.04987712875348543]
	TIME [epoch: 5.85 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03448121635935317		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.03448121635935317 | validation: 0.05117186366783891]
	TIME [epoch: 5.84 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02946586125090435		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.02946586125090435 | validation: 0.045214980378763495]
	TIME [epoch: 5.85 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02737671234456078		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.02737671234456078 | validation: 0.043137480679183764]
	TIME [epoch: 5.84 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02635420914405449		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.02635420914405449 | validation: 0.04748298724874436]
	TIME [epoch: 5.85 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028435024797641477		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.028435024797641477 | validation: 0.04529854756448495]
	TIME [epoch: 5.85 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028142166830590124		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.028142166830590124 | validation: 0.055702321390068754]
	TIME [epoch: 5.84 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030161683007785177		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.030161683007785177 | validation: 0.047709311227495756]
	TIME [epoch: 5.84 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03010475369351057		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.03010475369351057 | validation: 0.052065211148173654]
	TIME [epoch: 5.84 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028262508274738		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.028262508274738 | validation: 0.04861680299424934]
	TIME [epoch: 5.86 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02946447156847081		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.02946447156847081 | validation: 0.05318137517406524]
	TIME [epoch: 5.85 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029286468331821035		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.029286468331821035 | validation: 0.04228243783579947]
	TIME [epoch: 5.84 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030163163534303247		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.030163163534303247 | validation: 0.04879035644669244]
	TIME [epoch: 5.84 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029187411940338987		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.029187411940338987 | validation: 0.04354941103925766]
	TIME [epoch: 5.84 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027776016061980723		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.027776016061980723 | validation: 0.052283018463779043]
	TIME [epoch: 5.85 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032843197679754764		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.032843197679754764 | validation: 0.04289276257188131]
	TIME [epoch: 5.84 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030953104046606495		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.030953104046606495 | validation: 0.05204020457516056]
	TIME [epoch: 5.85 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029928591309874488		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.029928591309874488 | validation: 0.043005806710734264]
	TIME [epoch: 5.84 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02723234187398969		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.02723234187398969 | validation: 0.043909736509982866]
	TIME [epoch: 5.85 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026173621849833704		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.026173621849833704 | validation: 0.039398606763989534]
	TIME [epoch: 5.85 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02585848864069303		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.02585848864069303 | validation: 0.044690626314438944]
	TIME [epoch: 5.85 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02633418774798483		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.02633418774798483 | validation: 0.042597577741125325]
	TIME [epoch: 5.84 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0245023671318115		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.0245023671318115 | validation: 0.04580145886329781]
	TIME [epoch: 5.84 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027812045414600767		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.027812045414600767 | validation: 0.046903862923518905]
	TIME [epoch: 5.84 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029060969456573025		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.029060969456573025 | validation: 0.051978400953666265]
	TIME [epoch: 5.85 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030993401455430235		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.030993401455430235 | validation: 0.04811323134815813]
	TIME [epoch: 5.85 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030546268409283445		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.030546268409283445 | validation: 0.05093222018924126]
	TIME [epoch: 5.84 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03158676373199711		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.03158676373199711 | validation: 0.04306694203771222]
	TIME [epoch: 5.85 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025842581308266836		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.025842581308266836 | validation: 0.04876977328212698]
	TIME [epoch: 5.85 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02658593971400421		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.02658593971400421 | validation: 0.04275728916854524]
	TIME [epoch: 5.85 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02775762544375843		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.02775762544375843 | validation: 0.04799918590185748]
	TIME [epoch: 5.85 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024326406804085217		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.024326406804085217 | validation: 0.04457086720473863]
	TIME [epoch: 5.85 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026530167875350463		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.026530167875350463 | validation: 0.045747173837689926]
	TIME [epoch: 5.84 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025353848110785997		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.025353848110785997 | validation: 0.04233914259170986]
	TIME [epoch: 5.85 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02716063309358699		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.02716063309358699 | validation: 0.04846769146231227]
	TIME [epoch: 5.85 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027361218828718198		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.027361218828718198 | validation: 0.04108386755599411]
	TIME [epoch: 5.84 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03158762176794867		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.03158762176794867 | validation: 0.05190137468347255]
	TIME [epoch: 5.84 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031802761268221166		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.031802761268221166 | validation: 0.04419058648433562]
	TIME [epoch: 5.85 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028099187843638875		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.028099187843638875 | validation: 0.04529919414271551]
	TIME [epoch: 5.84 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026694470490367116		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.026694470490367116 | validation: 0.04341370445973]
	TIME [epoch: 5.85 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025933952249410412		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.025933952249410412 | validation: 0.04129601774700325]
	TIME [epoch: 5.85 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026345659115529566		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.026345659115529566 | validation: 0.04620708191742953]
	TIME [epoch: 5.84 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025991420456776604		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.025991420456776604 | validation: 0.04444166316179329]
	TIME [epoch: 5.84 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024537434356472865		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.024537434356472865 | validation: 0.04340584628043329]
	TIME [epoch: 5.85 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023301721401323486		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.023301721401323486 | validation: 0.04634209480593924]
	TIME [epoch: 5.85 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023582162235897933		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.023582162235897933 | validation: 0.04370416717135131]
	TIME [epoch: 5.84 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02607817951966316		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.02607817951966316 | validation: 0.048399595868323965]
	TIME [epoch: 5.84 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02905715561948991		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.02905715561948991 | validation: 0.0515175146339065]
	TIME [epoch: 5.85 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03192803756063976		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.03192803756063976 | validation: 0.05435147597907517]
	TIME [epoch: 5.84 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028354564145917777		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.028354564145917777 | validation: 0.04197458299168412]
	TIME [epoch: 5.85 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028190364597028818		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.028190364597028818 | validation: 0.045014418387745614]
	TIME [epoch: 5.85 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02334837342241009		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.02334837342241009 | validation: 0.04138552159757674]
	TIME [epoch: 5.85 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024945309745396382		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.024945309745396382 | validation: 0.04065650392320023]
	TIME [epoch: 5.84 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025672457251111554		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.025672457251111554 | validation: 0.04046854512134579]
	TIME [epoch: 5.85 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02199783885715137		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.02199783885715137 | validation: 0.04334784194430701]
	TIME [epoch: 5.85 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024587370580458866		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.024587370580458866 | validation: 0.0439765841317069]
	TIME [epoch: 5.85 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025277529811957484		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.025277529811957484 | validation: 0.04873464346316693]
	TIME [epoch: 5.84 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02551267969990123		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.02551267969990123 | validation: 0.044327315418883045]
	TIME [epoch: 5.83 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02425226655286953		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.02425226655286953 | validation: 0.04247988293926518]
	TIME [epoch: 5.83 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02755076616195766		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.02755076616195766 | validation: 0.05656987228369122]
	TIME [epoch: 5.84 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03582479874944081		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.03582479874944081 | validation: 0.04975203116632021]
	TIME [epoch: 5.85 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03347036702025042		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.03347036702025042 | validation: 0.04836566381416966]
	TIME [epoch: 5.85 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02730809359253101		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.02730809359253101 | validation: 0.04308123601781823]
	TIME [epoch: 5.84 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022326446289109233		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.022326446289109233 | validation: 0.041290224584249295]
	TIME [epoch: 5.84 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02426771793931329		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.02426771793931329 | validation: 0.04531256679533744]
	TIME [epoch: 5.83 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024354703145494056		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.024354703145494056 | validation: 0.03990213222692668]
	TIME [epoch: 5.85 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025482067513720397		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.025482067513720397 | validation: 0.04244123220692404]
	TIME [epoch: 5.84 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024626018427637755		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.024626018427637755 | validation: 0.046985573199524046]
	TIME [epoch: 5.85 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02477025118442207		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.02477025118442207 | validation: 0.03906920755888708]
	TIME [epoch: 5.85 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02411453817497837		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.02411453817497837 | validation: 0.043858814851703135]
	TIME [epoch: 5.85 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025533485670993387		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.025533485670993387 | validation: 0.04890832805178078]
	TIME [epoch: 5.85 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0311157281709444		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.0311157281709444 | validation: 0.035154572276697396]
	TIME [epoch: 5.85 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_1113.pth
	Model improved!!!
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028793601233108212		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.028793601233108212 | validation: 0.0491678075753563]
	TIME [epoch: 5.85 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02640928399060254		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.02640928399060254 | validation: 0.05050847821971473]
	TIME [epoch: 5.85 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027884557425346734		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.027884557425346734 | validation: 0.03972410175960364]
	TIME [epoch: 5.84 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025309046012671754		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.025309046012671754 | validation: 0.04160762606289097]
	TIME [epoch: 5.84 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02336737805668062		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.02336737805668062 | validation: 0.040994833496389896]
	TIME [epoch: 5.84 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02715031921621209		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.02715031921621209 | validation: 0.04936410333952547]
	TIME [epoch: 5.84 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023762451633941917		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.023762451633941917 | validation: 0.036904775660612676]
	TIME [epoch: 5.85 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03018828877914517		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.03018828877914517 | validation: 0.043781836233457654]
	TIME [epoch: 5.84 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028866695341638597		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.028866695341638597 | validation: 0.046974440864198774]
	TIME [epoch: 5.86 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026337451643254024		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.026337451643254024 | validation: 0.046582557802437766]
	TIME [epoch: 5.85 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023328446881760306		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.023328446881760306 | validation: 0.041360191522168216]
	TIME [epoch: 5.84 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023197681701015157		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.023197681701015157 | validation: 0.04735272489587947]
	TIME [epoch: 5.84 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025585108000315833		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.025585108000315833 | validation: 0.039800691097126306]
	TIME [epoch: 5.84 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02504613905098445		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.02504613905098445 | validation: 0.04376774649553688]
	TIME [epoch: 5.85 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023656260703797366		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.023656260703797366 | validation: 0.04224053334057946]
	TIME [epoch: 5.85 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0233541255482916		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.0233541255482916 | validation: 0.04138746243316184]
	TIME [epoch: 5.85 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023284719437327323		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.023284719437327323 | validation: 0.04667219374833103]
	TIME [epoch: 5.86 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023533028073230735		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.023533028073230735 | validation: 0.03950382943716721]
	TIME [epoch: 5.85 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023414305978810567		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.023414305978810567 | validation: 0.04568428411989738]
	TIME [epoch: 5.86 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023554572378048977		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.023554572378048977 | validation: 0.04329786862222366]
	TIME [epoch: 5.84 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026420923915741995		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.026420923915741995 | validation: 0.05330739109396329]
	TIME [epoch: 5.85 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033259718721331405		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.033259718721331405 | validation: 0.03929749990738079]
	TIME [epoch: 5.84 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028712374070967283		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.028712374070967283 | validation: 0.04500881871106602]
	TIME [epoch: 5.84 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025209876970510782		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.025209876970510782 | validation: 0.04093810243664661]
	TIME [epoch: 5.85 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022118526349565185		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.022118526349565185 | validation: 0.04427697994113216]
	TIME [epoch: 5.84 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02593662844456636		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.02593662844456636 | validation: 0.038511351304327215]
	TIME [epoch: 5.85 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024862107245157174		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.024862107245157174 | validation: 0.04683347678385483]
	TIME [epoch: 5.85 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026345738801397733		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.026345738801397733 | validation: 0.04073392691141279]
	TIME [epoch: 5.86 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02413668813090414		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.02413668813090414 | validation: 0.04604741634295705]
	TIME [epoch: 5.85 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024805441362739575		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.024805441362739575 | validation: 0.037292626782384]
	TIME [epoch: 5.84 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02461394847836348		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.02461394847836348 | validation: 0.04019888609854025]
	TIME [epoch: 5.84 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02399359672717792		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.02399359672717792 | validation: 0.04130789515879976]
	TIME [epoch: 5.84 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023805880472198077		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.023805880472198077 | validation: 0.04089123920524443]
	TIME [epoch: 5.84 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023367282410757983		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.023367282410757983 | validation: 0.04475228031048703]
	TIME [epoch: 5.85 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02368729172542632		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.02368729172542632 | validation: 0.03790175295364746]
	TIME [epoch: 5.84 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020925163879396882		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.020925163879396882 | validation: 0.036164659633455234]
	TIME [epoch: 5.84 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02326122277403622		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.02326122277403622 | validation: 0.041175445593074934]
	TIME [epoch: 5.84 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02285593529816942		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.02285593529816942 | validation: 0.04168097356789228]
	TIME [epoch: 5.84 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02456043855843481		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.02456043855843481 | validation: 0.04018652251649413]
	TIME [epoch: 5.85 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025371307623994647		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.025371307623994647 | validation: 0.04446515922889741]
	TIME [epoch: 5.84 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02983227152837701		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.02983227152837701 | validation: 0.04493729299723659]
	TIME [epoch: 5.84 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027386636478396502		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.027386636478396502 | validation: 0.051550744291416276]
	TIME [epoch: 5.84 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02721005379478261		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.02721005379478261 | validation: 0.04103484451313656]
	TIME [epoch: 5.84 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024857184821598457		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.024857184821598457 | validation: 0.041275401716313]
	TIME [epoch: 5.85 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024148238726357924		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.024148238726357924 | validation: 0.0387203262147606]
	TIME [epoch: 5.84 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024081001029415853		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.024081001029415853 | validation: 0.03795778448765004]
	TIME [epoch: 5.83 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02246069984606437		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.02246069984606437 | validation: 0.03953003420218561]
	TIME [epoch: 5.83 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02426196058605877		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.02426196058605877 | validation: 0.04264673573214903]
	TIME [epoch: 5.84 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022937266596229744		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.022937266596229744 | validation: 0.045423830989033465]
	TIME [epoch: 5.84 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026524042589629274		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.026524042589629274 | validation: 0.04439063631135957]
	TIME [epoch: 5.85 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02806105370776517		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.02806105370776517 | validation: 0.04314857060678906]
	TIME [epoch: 5.84 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023468004550855185		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.023468004550855185 | validation: 0.04405148714260321]
	TIME [epoch: 5.84 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025494359787329498		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.025494359787329498 | validation: 0.03809717130342064]
	TIME [epoch: 5.84 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025470887800391022		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.025470887800391022 | validation: 0.03954460256021615]
	TIME [epoch: 5.84 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023103032443813058		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.023103032443813058 | validation: 0.03761260596889431]
	TIME [epoch: 5.84 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022132355431870705		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.022132355431870705 | validation: 0.040648057965131656]
	TIME [epoch: 5.85 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02254292295956442		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.02254292295956442 | validation: 0.047390249526546246]
	TIME [epoch: 5.84 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022858290842386223		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.022858290842386223 | validation: 0.043346877314205494]
	TIME [epoch: 5.84 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02757287707186631		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.02757287707186631 | validation: 0.04429767575317163]
	TIME [epoch: 5.85 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02554645298770876		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.02554645298770876 | validation: 0.03658741243812398]
	TIME [epoch: 5.85 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0226795090549953		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.0226795090549953 | validation: 0.04343988175881878]
	TIME [epoch: 5.85 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021782361057086738		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.021782361057086738 | validation: 0.03843518784460678]
	TIME [epoch: 5.84 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022120940120958398		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.022120940120958398 | validation: 0.04161031330621254]
	TIME [epoch: 5.84 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023116109255852498		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.023116109255852498 | validation: 0.04292261367236962]
	TIME [epoch: 5.84 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0224664200537285		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.0224664200537285 | validation: 0.038949647101696405]
	TIME [epoch: 5.84 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023388384082873612		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.023388384082873612 | validation: 0.04415102595643309]
	TIME [epoch: 5.84 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027172094308385412		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.027172094308385412 | validation: 0.04507756640245644]
	TIME [epoch: 5.85 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02856144561314963		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.02856144561314963 | validation: 0.04276529874268306]
	TIME [epoch: 5.84 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024159335847733995		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.024159335847733995 | validation: 0.03971865176336186]
	TIME [epoch: 5.84 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02184805450058837		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.02184805450058837 | validation: 0.0387286324763395]
	TIME [epoch: 5.84 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022696203631084728		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.022696203631084728 | validation: 0.03829619380458352]
	TIME [epoch: 5.84 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019646475835452443		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.019646475835452443 | validation: 0.04298818263829229]
	TIME [epoch: 5.84 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02356897326358549		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.02356897326358549 | validation: 0.03884384757492332]
	TIME [epoch: 5.84 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021706082550934466		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.021706082550934466 | validation: 0.04418732028625608]
	TIME [epoch: 5.83 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022233565664779152		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.022233565664779152 | validation: 0.03582268541583549]
	TIME [epoch: 5.84 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023620285864866277		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.023620285864866277 | validation: 0.03642173041376886]
	TIME [epoch: 5.84 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024431994375136032		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.024431994375136032 | validation: 0.0414008414466897]
	TIME [epoch: 5.84 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02761406725083914		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.02761406725083914 | validation: 0.041730069316527445]
	TIME [epoch: 5.84 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026336898154543313		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.026336898154543313 | validation: 0.045029818350015786]
	TIME [epoch: 5.84 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0223092389942122		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.0223092389942122 | validation: 0.03440724218073056]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_1193.pth
	Model improved!!!
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023484398886030024		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.023484398886030024 | validation: 0.03960031670200337]
	TIME [epoch: 5.84 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023062117855764223		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.023062117855764223 | validation: 0.041728202903982305]
	TIME [epoch: 5.83 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024965485266786224		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.024965485266786224 | validation: 0.037317399475455905]
	TIME [epoch: 5.84 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022131226836444622		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.022131226836444622 | validation: 0.03902710918178903]
	TIME [epoch: 5.84 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02116313614697315		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.02116313614697315 | validation: 0.04144155925474018]
	TIME [epoch: 5.85 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022781921136981877		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.022781921136981877 | validation: 0.03660655595667617]
	TIME [epoch: 5.84 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0223268007104686		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.0223268007104686 | validation: 0.04336939247100118]
	TIME [epoch: 5.85 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022640156474287913		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.022640156474287913 | validation: 0.039344435367458824]
	TIME [epoch: 5.84 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024011726677590134		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.024011726677590134 | validation: 0.0441134110154932]
	TIME [epoch: 5.84 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023155728271117643		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.023155728271117643 | validation: 0.04087050044045973]
	TIME [epoch: 5.85 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023793482137860252		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.023793482137860252 | validation: 0.038747560546399024]
	TIME [epoch: 5.84 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021610331822039867		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.021610331822039867 | validation: 0.04202354351069413]
	TIME [epoch: 5.85 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02134861716098817		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.02134861716098817 | validation: 0.03745391273053061]
	TIME [epoch: 5.84 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021423497827863233		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.021423497827863233 | validation: 0.04089727255387844]
	TIME [epoch: 5.85 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021824316710759108		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.021824316710759108 | validation: 0.036032941611976586]
	TIME [epoch: 5.85 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02353629163538389		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.02353629163538389 | validation: 0.04670439619673551]
	TIME [epoch: 5.85 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026506514399949693		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.026506514399949693 | validation: 0.042341843000297044]
	TIME [epoch: 5.84 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02612960608272936		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.02612960608272936 | validation: 0.043215516601887306]
	TIME [epoch: 5.84 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026344577718738656		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.026344577718738656 | validation: 0.036965036483173896]
	TIME [epoch: 5.84 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021471299457595565		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.021471299457595565 | validation: 0.04190305735630537]
	TIME [epoch: 5.85 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021657587365704285		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.021657587365704285 | validation: 0.04443007100630141]
	TIME [epoch: 5.84 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020000403253608977		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.020000403253608977 | validation: 0.04176231136683089]
	TIME [epoch: 5.84 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021442359079026172		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.021442359079026172 | validation: 0.036368452826434854]
	TIME [epoch: 5.85 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023084499298276055		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.023084499298276055 | validation: 0.04008631050612208]
	TIME [epoch: 5.84 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023503100211205687		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.023503100211205687 | validation: 0.03642752817420785]
	TIME [epoch: 5.86 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021769908179407684		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.021769908179407684 | validation: 0.04156653972517839]
	TIME [epoch: 5.84 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026448534611195274		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.026448534611195274 | validation: 0.04081049348472912]
	TIME [epoch: 5.85 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022236182599659468		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.022236182599659468 | validation: 0.03615618863594502]
	TIME [epoch: 5.84 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02335838946957645		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.02335838946957645 | validation: 0.03556399865395955]
	TIME [epoch: 5.85 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022815507718308897		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.022815507718308897 | validation: 0.03673444340027552]
	TIME [epoch: 5.84 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022001320267826596		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.022001320267826596 | validation: 0.03735746723041792]
	TIME [epoch: 5.84 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02121774725269085		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.02121774725269085 | validation: 0.04245953338611404]
	TIME [epoch: 5.84 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022356174693190577		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.022356174693190577 | validation: 0.04064424796283456]
	TIME [epoch: 5.85 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02371964271259999		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.02371964271259999 | validation: 0.03849832621100239]
	TIME [epoch: 5.84 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0238047669257457		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.0238047669257457 | validation: 0.04222385057494336]
	TIME [epoch: 5.85 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022992513156467536		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.022992513156467536 | validation: 0.04107874851872861]
	TIME [epoch: 5.85 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02341664008743513		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.02341664008743513 | validation: 0.04409210918154706]
	TIME [epoch: 5.84 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02392831780482697		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.02392831780482697 | validation: 0.03679408450193616]
	TIME [epoch: 5.84 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022428764884299377		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.022428764884299377 | validation: 0.03795640385967565]
	TIME [epoch: 5.84 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021446919108415143		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.021446919108415143 | validation: 0.038229725970225496]
	TIME [epoch: 5.85 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021562894662603655		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.021562894662603655 | validation: 0.03667584941691539]
	TIME [epoch: 5.85 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022653066909854164		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.022653066909854164 | validation: 0.040911566518581736]
	TIME [epoch: 5.85 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02178172347038414		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.02178172347038414 | validation: 0.03660601248751762]
	TIME [epoch: 5.84 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020731592973061996		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.020731592973061996 | validation: 0.04037658868597857]
	TIME [epoch: 5.84 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024634862231641703		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.024634862231641703 | validation: 0.033730698355116155]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_1238.pth
	Model improved!!!
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02603916693793842		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.02603916693793842 | validation: 0.04862279794884067]
	TIME [epoch: 5.86 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024695959311536866		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.024695959311536866 | validation: 0.03752382801059219]
	TIME [epoch: 5.85 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02140307251156846		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.02140307251156846 | validation: 0.040244817182817666]
	TIME [epoch: 5.86 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021310313792301853		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.021310313792301853 | validation: 0.038425796296821635]
	TIME [epoch: 5.86 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02342191074325718		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.02342191074325718 | validation: 0.03543476339728396]
	TIME [epoch: 5.86 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02193005726831183		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.02193005726831183 | validation: 0.042321552436382026]
	TIME [epoch: 5.86 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02199563021746344		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.02199563021746344 | validation: 0.037886007938519886]
	TIME [epoch: 5.86 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020889627866745707		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.020889627866745707 | validation: 0.04125754316306989]
	TIME [epoch: 5.86 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02197926469072729		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.02197926469072729 | validation: 0.04089534767253822]
	TIME [epoch: 5.86 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023709094925446123		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.023709094925446123 | validation: 0.03974510113390842]
	TIME [epoch: 5.86 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019831447887303018		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.019831447887303018 | validation: 0.036025954216910995]
	TIME [epoch: 5.85 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021224813754530683		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.021224813754530683 | validation: 0.04077515103842311]
	TIME [epoch: 5.85 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02298354969018958		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.02298354969018958 | validation: 0.04089192486389879]
	TIME [epoch: 5.86 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02516047068245494		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.02516047068245494 | validation: 0.04190885814700345]
	TIME [epoch: 5.85 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02129519900091142		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.02129519900091142 | validation: 0.03756791297398764]
	TIME [epoch: 5.84 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020723656252942837		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.020723656252942837 | validation: 0.04037353734521795]
	TIME [epoch: 5.85 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01980115314331504		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.01980115314331504 | validation: 0.04357054397175637]
	TIME [epoch: 5.87 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022657815520516593		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.022657815520516593 | validation: 0.041738968398458746]
	TIME [epoch: 5.85 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025015091497353766		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.025015091497353766 | validation: 0.0405300552544943]
	TIME [epoch: 5.84 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022853788221330218		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.022853788221330218 | validation: 0.04385225972105936]
	TIME [epoch: 5.84 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02119703241729802		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.02119703241729802 | validation: 0.03526890674590538]
	TIME [epoch: 5.85 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02221200121832562		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.02221200121832562 | validation: 0.03461929910180437]
	TIME [epoch: 5.84 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02273060051299958		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.02273060051299958 | validation: 0.03475235985146472]
	TIME [epoch: 5.84 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023397082339489766		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.023397082339489766 | validation: 0.036938569230581364]
	TIME [epoch: 5.83 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022913233869281475		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.022913233869281475 | validation: 0.04314828126850817]
	TIME [epoch: 5.84 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023652306334745653		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.023652306334745653 | validation: 0.03978010530773414]
	TIME [epoch: 5.85 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022987258274463972		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.022987258274463972 | validation: 0.035455159493130675]
	TIME [epoch: 5.84 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01935875493505125		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.01935875493505125 | validation: 0.032948145798242325]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_1266.pth
	Model improved!!!
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02143449165488176		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.02143449165488176 | validation: 0.0413886551382927]
	TIME [epoch: 5.86 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022420353724826255		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.022420353724826255 | validation: 0.036314348716021815]
	TIME [epoch: 5.88 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023233734991366165		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.023233734991366165 | validation: 0.038040401794235935]
	TIME [epoch: 5.88 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02300188082723061		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.02300188082723061 | validation: 0.035715619416897985]
	TIME [epoch: 5.87 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022293687249877986		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.022293687249877986 | validation: 0.03749725808492168]
	TIME [epoch: 5.87 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02279968935310152		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.02279968935310152 | validation: 0.037942447721793574]
	TIME [epoch: 5.87 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021931032110672977		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.021931032110672977 | validation: 0.040602298785098814]
	TIME [epoch: 5.87 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02369584207828285		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.02369584207828285 | validation: 0.030737698437111285]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_1274.pth
	Model improved!!!
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021266390287125018		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.021266390287125018 | validation: 0.03766570958989851]
	TIME [epoch: 5.88 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021216389633028916		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.021216389633028916 | validation: 0.037029824418213265]
	TIME [epoch: 5.87 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020953717768699066		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.020953717768699066 | validation: 0.03608181663541306]
	TIME [epoch: 5.87 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02215582463927537		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.02215582463927537 | validation: 0.03979253245227629]
	TIME [epoch: 5.87 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021265330945680675		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.021265330945680675 | validation: 0.036069889158575764]
	TIME [epoch: 5.88 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02075133840744769		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.02075133840744769 | validation: 0.0369802862938075]
	TIME [epoch: 5.86 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0223188889902866		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.0223188889902866 | validation: 0.04178961551768991]
	TIME [epoch: 5.86 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020546888553773717		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.020546888553773717 | validation: 0.04454045172380805]
	TIME [epoch: 5.87 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020950090638898248		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.020950090638898248 | validation: 0.03754613546387009]
	TIME [epoch: 5.87 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0234211400422798		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.0234211400422798 | validation: 0.042174429009001806]
	TIME [epoch: 5.85 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021890998646556886		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.021890998646556886 | validation: 0.03405416915322571]
	TIME [epoch: 5.84 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020880681609947296		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.020880681609947296 | validation: 0.04429386791801128]
	TIME [epoch: 5.83 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021170983719789677		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.021170983719789677 | validation: 0.03342641423328168]
	TIME [epoch: 5.84 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023015129083649104		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.023015129083649104 | validation: 0.03371248958673091]
	TIME [epoch: 5.84 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02240156060010842		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.02240156060010842 | validation: 0.03486856342038308]
	TIME [epoch: 5.84 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020625261505651027		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.020625261505651027 | validation: 0.03465952635708787]
	TIME [epoch: 5.84 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02062592128831918		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.02062592128831918 | validation: 0.03838071672332244]
	TIME [epoch: 5.84 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020001829950759095		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.020001829950759095 | validation: 0.0344775981277311]
	TIME [epoch: 5.83 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02265693184896878		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.02265693184896878 | validation: 0.03860374818604332]
	TIME [epoch: 5.84 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023785240691180664		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.023785240691180664 | validation: 0.03352152663613252]
	TIME [epoch: 5.83 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02212271928914688		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.02212271928914688 | validation: 0.0351852801703317]
	TIME [epoch: 5.84 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022776005319205685		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.022776005319205685 | validation: 0.0374305662184986]
	TIME [epoch: 5.84 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0206829609420613		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.0206829609420613 | validation: 0.0398007036178359]
	TIME [epoch: 5.83 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022150333635424478		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.022150333635424478 | validation: 0.03573355446716694]
	TIME [epoch: 5.83 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02036896795417812		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.02036896795417812 | validation: 0.04017783251116498]
	TIME [epoch: 5.85 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020882292953151846		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.020882292953151846 | validation: 0.039725034910734563]
	TIME [epoch: 5.83 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02295067682762724		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.02295067682762724 | validation: 0.03662157439083571]
	TIME [epoch: 5.87 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020595647230440297		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.020595647230440297 | validation: 0.040861079185237695]
	TIME [epoch: 5.87 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0210464058038789		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.0210464058038789 | validation: 0.03864596507771933]
	TIME [epoch: 5.86 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021705067713199792		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.021705067713199792 | validation: 0.0401776802735335]
	TIME [epoch: 5.88 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021136190588995026		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.021136190588995026 | validation: 0.0346281391146512]
	TIME [epoch: 5.89 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022401804872166333		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.022401804872166333 | validation: 0.040235531011763595]
	TIME [epoch: 5.87 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0241240453604271		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.0241240453604271 | validation: 0.03420011195407564]
	TIME [epoch: 5.86 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02173304026628496		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.02173304026628496 | validation: 0.03867496477719787]
	TIME [epoch: 5.88 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020567243860508046		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.020567243860508046 | validation: 0.03296209534009327]
	TIME [epoch: 5.85 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02090104828651412		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.02090104828651412 | validation: 0.0411094381572521]
	TIME [epoch: 5.84 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022936678976321484		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.022936678976321484 | validation: 0.029451696942990747]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_1311.pth
	Model improved!!!
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020130885294888743		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.020130885294888743 | validation: 0.037505479384632945]
	TIME [epoch: 5.86 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02211134891238858		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.02211134891238858 | validation: 0.041190407622992246]
	TIME [epoch: 5.86 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01997333959113376		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.01997333959113376 | validation: 0.03941138918631444]
	TIME [epoch: 5.87 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019995253222065076		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.019995253222065076 | validation: 0.03494850076923391]
	TIME [epoch: 5.86 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021432209812819476		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.021432209812819476 | validation: 0.03684727915278999]
	TIME [epoch: 5.86 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021879317949090576		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.021879317949090576 | validation: 0.03466361921580176]
	TIME [epoch: 5.86 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022035932404612223		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.022035932404612223 | validation: 0.04230626065149469]
	TIME [epoch: 5.85 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019868548285117762		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.019868548285117762 | validation: 0.035073476559516106]
	TIME [epoch: 5.86 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02087273102080711		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.02087273102080711 | validation: 0.035981562117378456]
	TIME [epoch: 5.85 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02126084346581349		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.02126084346581349 | validation: 0.042228572043914364]
	TIME [epoch: 5.86 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021233371437667414		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.021233371437667414 | validation: 0.034034885030194154]
	TIME [epoch: 5.85 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02236605402890896		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.02236605402890896 | validation: 0.04020174995077779]
	TIME [epoch: 5.86 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023344005206616964		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.023344005206616964 | validation: 0.03673069753588403]
	TIME [epoch: 5.86 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023743720967689907		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.023743720967689907 | validation: 0.0367322402055988]
	TIME [epoch: 5.87 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021261266824912504		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.021261266824912504 | validation: 0.03484413243035992]
	TIME [epoch: 5.86 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023298150572807007		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.023298150572807007 | validation: 0.033254222043831154]
	TIME [epoch: 5.86 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0222945085272023		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.0222945085272023 | validation: 0.03028878956152893]
	TIME [epoch: 5.85 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019055717665863763		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.019055717665863763 | validation: 0.034661154885500756]
	TIME [epoch: 5.85 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02168444021130137		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.02168444021130137 | validation: 0.03490094347282298]
	TIME [epoch: 5.86 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020801629589862376		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.020801629589862376 | validation: 0.03735355696064876]
	TIME [epoch: 5.86 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021498361231885303		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.021498361231885303 | validation: 0.032724236608039296]
	TIME [epoch: 5.86 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0194283525761119		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.0194283525761119 | validation: 0.0369465976794524]
	TIME [epoch: 5.86 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020319907168836075		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.020319907168836075 | validation: 0.03512212444166011]
	TIME [epoch: 5.85 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022098705720049683		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.022098705720049683 | validation: 0.03812621001066571]
	TIME [epoch: 5.87 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019866432024931668		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.019866432024931668 | validation: 0.03659535099676764]
	TIME [epoch: 5.85 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021368338931296908		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.021368338931296908 | validation: 0.03353785705367276]
	TIME [epoch: 5.86 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019379849181635045		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.019379849181635045 | validation: 0.03769461278771156]
	TIME [epoch: 5.86 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019917854128208283		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.019917854128208283 | validation: 0.03753174489214775]
	TIME [epoch: 5.86 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02006039142693927		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.02006039142693927 | validation: 0.0371737120623913]
	TIME [epoch: 5.86 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020069251708405504		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.020069251708405504 | validation: 0.04087583139139366]
	TIME [epoch: 5.86 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023346100198976424		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.023346100198976424 | validation: 0.03866362261666359]
	TIME [epoch: 5.85 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021518858644299258		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.021518858644299258 | validation: 0.03619134832395805]
	TIME [epoch: 5.86 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021768292210170993		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.021768292210170993 | validation: 0.037271395940430475]
	TIME [epoch: 5.86 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021050846159233538		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.021050846159233538 | validation: 0.03545316921733704]
	TIME [epoch: 5.86 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020879837235585938		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.020879837235585938 | validation: 0.036180696296383145]
	TIME [epoch: 5.86 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019315413209543265		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.019315413209543265 | validation: 0.036963829188677876]
	TIME [epoch: 5.86 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02140408928219328		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.02140408928219328 | validation: 0.036818656067490595]
	TIME [epoch: 5.86 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02108877117096067		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.02108877117096067 | validation: 0.038983337288013765]
	TIME [epoch: 5.86 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019438467306242917		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.019438467306242917 | validation: 0.03394379017284708]
	TIME [epoch: 5.87 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02074099905371183		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.02074099905371183 | validation: 0.035608159942539554]
	TIME [epoch: 5.86 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021646804290722715		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.021646804290722715 | validation: 0.03272730182755739]
	TIME [epoch: 5.86 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01959108136843243		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.01959108136843243 | validation: 0.03512114489558659]
	TIME [epoch: 5.86 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020520787875141427		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.020520787875141427 | validation: 0.03484826792080586]
	TIME [epoch: 5.85 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02100019784573832		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.02100019784573832 | validation: 0.03589825172225721]
	TIME [epoch: 5.87 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021156478462250425		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.021156478462250425 | validation: 0.04123799449663479]
	TIME [epoch: 5.85 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020389463009441496		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.020389463009441496 | validation: 0.040083689421323565]
	TIME [epoch: 5.86 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018526846530451873		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.018526846530451873 | validation: 0.03910717353256862]
	TIME [epoch: 5.85 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021222590996653922		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.021222590996653922 | validation: 0.038661517814213946]
	TIME [epoch: 5.87 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023051777273335457		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.023051777273335457 | validation: 0.03738404934753172]
	TIME [epoch: 5.87 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022777322463756553		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.022777322463756553 | validation: 0.03724776376934343]
	TIME [epoch: 5.85 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019878133534518252		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.019878133534518252 | validation: 0.03364059889532928]
	TIME [epoch: 5.86 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020745499353531188		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.020745499353531188 | validation: 0.039512338172355846]
	TIME [epoch: 5.86 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019817085269139494		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.019817085269139494 | validation: 0.034577794183346534]
	TIME [epoch: 5.86 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020090995570628066		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.020090995570628066 | validation: 0.03797012946461212]
	TIME [epoch: 5.87 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021928533202696734		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.021928533202696734 | validation: 0.0333757228892832]
	TIME [epoch: 5.87 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02034522736732126		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.02034522736732126 | validation: 0.035438371738378695]
	TIME [epoch: 5.85 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020707075270362578		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.020707075270362578 | validation: 0.03723665176926222]
	TIME [epoch: 5.87 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021240107941753194		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.021240107941753194 | validation: 0.03806538535731836]
	TIME [epoch: 5.85 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020242740325185596		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.020242740325185596 | validation: 0.03793112950661881]
	TIME [epoch: 5.87 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019994129501636256		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.019994129501636256 | validation: 0.04113634464729165]
	TIME [epoch: 5.85 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01956178381788795		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.01956178381788795 | validation: 0.034912172513592965]
	TIME [epoch: 5.86 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019562758989768857		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.019562758989768857 | validation: 0.03905428923821752]
	TIME [epoch: 5.85 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02009317189288508		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.02009317189288508 | validation: 0.034126328951109244]
	TIME [epoch: 5.85 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018941884085610115		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.018941884085610115 | validation: 0.03988066454379802]
	TIME [epoch: 5.85 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019314119676187996		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.019314119676187996 | validation: 0.03681159367065325]
	TIME [epoch: 5.85 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01840416849720854		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.01840416849720854 | validation: 0.034467740702563664]
	TIME [epoch: 5.85 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018444934004444635		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.018444934004444635 | validation: 0.03418991139061511]
	TIME [epoch: 5.85 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02183051706280498		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.02183051706280498 | validation: 0.034402139241724854]
	TIME [epoch: 5.85 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020673066663346864		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.020673066663346864 | validation: 0.03675847499039564]
	TIME [epoch: 5.85 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02161560285310197		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.02161560285310197 | validation: 0.039166970280354776]
	TIME [epoch: 5.85 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020354644265836806		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.020354644265836806 | validation: 0.03844310109273206]
	TIME [epoch: 5.84 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022817925946094705		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.022817925946094705 | validation: 0.036267033659748184]
	TIME [epoch: 5.84 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02015921202840384		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.02015921202840384 | validation: 0.04360877090612114]
	TIME [epoch: 5.85 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019253025148030557		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.019253025148030557 | validation: 0.03658066268677431]
	TIME [epoch: 5.84 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019179612574447086		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.019179612574447086 | validation: 0.037792613826663636]
	TIME [epoch: 5.85 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019929109712471933		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.019929109712471933 | validation: 0.037765218633993014]
	TIME [epoch: 5.85 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01896676849663636		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.01896676849663636 | validation: 0.03663016274709167]
	TIME [epoch: 5.84 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021196574256739903		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.021196574256739903 | validation: 0.03591305773242468]
	TIME [epoch: 5.85 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02049414374373832		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.02049414374373832 | validation: 0.03556817329404394]
	TIME [epoch: 5.86 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020169294904993355		[learning rate: 8.6788e-05]
	Learning Rate: 8.67884e-05
	LOSS [training: 0.020169294904993355 | validation: 0.04065160068563026]
	TIME [epoch: 5.86 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020550960509923142		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.020550960509923142 | validation: 0.031201718103929077]
	TIME [epoch: 5.85 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019543046883482355		[learning rate: 8.6176e-05]
	Learning Rate: 8.61757e-05
	LOSS [training: 0.019543046883482355 | validation: 0.03888724203928356]
	TIME [epoch: 5.85 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02077002727261172		[learning rate: 8.5871e-05]
	Learning Rate: 8.5871e-05
	LOSS [training: 0.02077002727261172 | validation: 0.03757544158783336]
	TIME [epoch: 5.84 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01822314078996199		[learning rate: 8.5567e-05]
	Learning Rate: 8.55673e-05
	LOSS [training: 0.01822314078996199 | validation: 0.044198540435009565]
	TIME [epoch: 5.84 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020124795900123546		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.020124795900123546 | validation: 0.0373832567265732]
	TIME [epoch: 5.84 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02001222526000955		[learning rate: 8.4963e-05]
	Learning Rate: 8.49632e-05
	LOSS [training: 0.02001222526000955 | validation: 0.038442377084222025]
	TIME [epoch: 5.84 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021240460868919533		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.021240460868919533 | validation: 0.03388636670497027]
	TIME [epoch: 5.84 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02082660402089621		[learning rate: 8.4363e-05]
	Learning Rate: 8.43634e-05
	LOSS [training: 0.02082660402089621 | validation: 0.033736451227468524]
	TIME [epoch: 5.83 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01922972942143999		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.01922972942143999 | validation: 0.03434371668922536]
	TIME [epoch: 5.84 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020827147409183466		[learning rate: 8.3768e-05]
	Learning Rate: 8.37678e-05
	LOSS [training: 0.020827147409183466 | validation: 0.03541869431687662]
	TIME [epoch: 5.88 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02083618285088828		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.02083618285088828 | validation: 0.03892303524511366]
	TIME [epoch: 5.88 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018708571456837658		[learning rate: 8.3176e-05]
	Learning Rate: 8.31764e-05
	LOSS [training: 0.018708571456837658 | validation: 0.03444462230772656]
	TIME [epoch: 5.87 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020486169674325955		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.020486169674325955 | validation: 0.036590018424662155]
	TIME [epoch: 5.87 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020408279886189717		[learning rate: 8.2589e-05]
	Learning Rate: 8.25892e-05
	LOSS [training: 0.020408279886189717 | validation: 0.03366701867449327]
	TIME [epoch: 5.87 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02042154840368739		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.02042154840368739 | validation: 0.033020466584466536]
	TIME [epoch: 5.88 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022595535887803626		[learning rate: 8.2006e-05]
	Learning Rate: 8.20061e-05
	LOSS [training: 0.022595535887803626 | validation: 0.03431613015102123]
	TIME [epoch: 5.88 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02048995954800321		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.02048995954800321 | validation: 0.039938218627001136]
	TIME [epoch: 5.87 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02019223138037294		[learning rate: 8.1427e-05]
	Learning Rate: 8.14272e-05
	LOSS [training: 0.02019223138037294 | validation: 0.03407811796506854]
	TIME [epoch: 5.87 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022333483016423298		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.022333483016423298 | validation: 0.036433057233837905]
	TIME [epoch: 5.87 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019208740786195215		[learning rate: 8.0852e-05]
	Learning Rate: 8.08523e-05
	LOSS [training: 0.019208740786195215 | validation: 0.027171104662817205]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_1411.pth
	Model improved!!!
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020352169542719442		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.020352169542719442 | validation: 0.034103801629828935]
	TIME [epoch: 5.84 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018445984333121383		[learning rate: 8.0281e-05]
	Learning Rate: 8.02815e-05
	LOSS [training: 0.018445984333121383 | validation: 0.03352550754045549]
	TIME [epoch: 5.83 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01939074126335092		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.01939074126335092 | validation: 0.03376388358712845]
	TIME [epoch: 5.84 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019559841589091572		[learning rate: 7.9715e-05]
	Learning Rate: 7.97147e-05
	LOSS [training: 0.019559841589091572 | validation: 0.03241658698544075]
	TIME [epoch: 5.84 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018543169756696735		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.018543169756696735 | validation: 0.03546258421782692]
	TIME [epoch: 5.84 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01852915496716813		[learning rate: 7.9152e-05]
	Learning Rate: 7.9152e-05
	LOSS [training: 0.01852915496716813 | validation: 0.04087370644906309]
	TIME [epoch: 5.88 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02005070203500022		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.02005070203500022 | validation: 0.03526238928633059]
	TIME [epoch: 5.87 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020942960492264132		[learning rate: 7.8593e-05]
	Learning Rate: 7.85931e-05
	LOSS [training: 0.020942960492264132 | validation: 0.03668957298166638]
	TIME [epoch: 5.88 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020131030329931215		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.020131030329931215 | validation: 0.036573046123544055]
	TIME [epoch: 5.86 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019688181728226967		[learning rate: 7.8038e-05]
	Learning Rate: 7.80383e-05
	LOSS [training: 0.019688181728226967 | validation: 0.035949881447597434]
	TIME [epoch: 5.88 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02064936880234745		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.02064936880234745 | validation: 0.03522456752905202]
	TIME [epoch: 5.86 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02134749321437487		[learning rate: 7.7487e-05]
	Learning Rate: 7.74873e-05
	LOSS [training: 0.02134749321437487 | validation: 0.03618309743033273]
	TIME [epoch: 5.87 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02166782213959026		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.02166782213959026 | validation: 0.03820357671580121]
	TIME [epoch: 5.86 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020799957714981384		[learning rate: 7.694e-05]
	Learning Rate: 7.69403e-05
	LOSS [training: 0.020799957714981384 | validation: 0.03511996181528455]
	TIME [epoch: 5.88 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02049354585572551		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.02049354585572551 | validation: 0.03560044975819728]
	TIME [epoch: 5.87 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019716434482983464		[learning rate: 7.6397e-05]
	Learning Rate: 7.63971e-05
	LOSS [training: 0.019716434482983464 | validation: 0.03402946732109894]
	TIME [epoch: 5.87 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02089103183486215		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.02089103183486215 | validation: 0.036416651058810735]
	TIME [epoch: 5.86 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02123969118311719		[learning rate: 7.5858e-05]
	Learning Rate: 7.58578e-05
	LOSS [training: 0.02123969118311719 | validation: 0.03394888169907813]
	TIME [epoch: 5.87 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019820842817113132		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.019820842817113132 | validation: 0.03583547181788362]
	TIME [epoch: 5.87 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02108000932715675		[learning rate: 7.5322e-05]
	Learning Rate: 7.53222e-05
	LOSS [training: 0.02108000932715675 | validation: 0.030929367995907422]
	TIME [epoch: 5.87 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019949714007989977		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.019949714007989977 | validation: 0.03302981521499539]
	TIME [epoch: 5.86 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019555563962516512		[learning rate: 7.479e-05]
	Learning Rate: 7.47905e-05
	LOSS [training: 0.019555563962516512 | validation: 0.03520316006014895]
	TIME [epoch: 5.86 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020756572557659895		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.020756572557659895 | validation: 0.03831912972664811]
	TIME [epoch: 5.86 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019577364220826038		[learning rate: 7.4262e-05]
	Learning Rate: 7.42625e-05
	LOSS [training: 0.019577364220826038 | validation: 0.037334337589543534]
	TIME [epoch: 5.86 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019118366953769225		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.019118366953769225 | validation: 0.032446613181159094]
	TIME [epoch: 5.87 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019093730349927403		[learning rate: 7.3738e-05]
	Learning Rate: 7.37382e-05
	LOSS [training: 0.019093730349927403 | validation: 0.03460531224568514]
	TIME [epoch: 5.87 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02035588319007255		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.02035588319007255 | validation: 0.03212842379167059]
	TIME [epoch: 5.86 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019414509859575085		[learning rate: 7.3218e-05]
	Learning Rate: 7.32176e-05
	LOSS [training: 0.019414509859575085 | validation: 0.03733996475361745]
	TIME [epoch: 5.87 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018769927703233723		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.018769927703233723 | validation: 0.029464107464652403]
	TIME [epoch: 5.86 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019356527156892255		[learning rate: 7.2701e-05]
	Learning Rate: 7.27007e-05
	LOSS [training: 0.019356527156892255 | validation: 0.03816406663358893]
	TIME [epoch: 5.88 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02107447074239477		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.02107447074239477 | validation: 0.032384895889932964]
	TIME [epoch: 5.87 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019533282447900393		[learning rate: 7.2187e-05]
	Learning Rate: 7.21874e-05
	LOSS [training: 0.019533282447900393 | validation: 0.035164053335108934]
	TIME [epoch: 5.88 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01890746771713995		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.01890746771713995 | validation: 0.038083942163481245]
	TIME [epoch: 5.87 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019988919727613953		[learning rate: 7.1678e-05]
	Learning Rate: 7.16778e-05
	LOSS [training: 0.019988919727613953 | validation: 0.033407413493396376]
	TIME [epoch: 5.87 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02049909459280773		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.02049909459280773 | validation: 0.034888438786663804]
	TIME [epoch: 5.87 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019743545860749313		[learning rate: 7.1172e-05]
	Learning Rate: 7.11718e-05
	LOSS [training: 0.019743545860749313 | validation: 0.03450844875832667]
	TIME [epoch: 5.87 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0190521503064567		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.0190521503064567 | validation: 0.03803705915233374]
	TIME [epoch: 5.84 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020409455868038866		[learning rate: 7.0669e-05]
	Learning Rate: 7.06693e-05
	LOSS [training: 0.020409455868038866 | validation: 0.038144409478615485]
	TIME [epoch: 5.84 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02019071303541515		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.02019071303541515 | validation: 0.030965538117342952]
	TIME [epoch: 5.83 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020845488815521843		[learning rate: 7.017e-05]
	Learning Rate: 7.01704e-05
	LOSS [training: 0.020845488815521843 | validation: 0.03715995661431857]
	TIME [epoch: 5.84 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019134034649037866		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.019134034649037866 | validation: 0.03102938219165551]
	TIME [epoch: 5.84 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01967018900610769		[learning rate: 6.9675e-05]
	Learning Rate: 6.9675e-05
	LOSS [training: 0.01967018900610769 | validation: 0.036463054569605524]
	TIME [epoch: 5.84 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019864373012178683		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.019864373012178683 | validation: 0.03539630551928441]
	TIME [epoch: 5.86 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018122309029014384		[learning rate: 6.9183e-05]
	Learning Rate: 6.91831e-05
	LOSS [training: 0.018122309029014384 | validation: 0.03980533413512635]
	TIME [epoch: 5.86 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020113161572761183		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.020113161572761183 | validation: 0.03293841719509523]
	TIME [epoch: 5.86 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02089164323329592		[learning rate: 6.8695e-05]
	Learning Rate: 6.86947e-05
	LOSS [training: 0.02089164323329592 | validation: 0.03303469077645425]
	TIME [epoch: 5.86 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021885599909770126		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.021885599909770126 | validation: 0.03388114166248133]
	TIME [epoch: 5.85 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020043370529864737		[learning rate: 6.821e-05]
	Learning Rate: 6.82097e-05
	LOSS [training: 0.020043370529864737 | validation: 0.039233627618911616]
	TIME [epoch: 5.85 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018950663116457038		[learning rate: 6.7969e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.018950663116457038 | validation: 0.0331462049857895]
	TIME [epoch: 5.86 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01904188997913475		[learning rate: 6.7728e-05]
	Learning Rate: 6.77282e-05
	LOSS [training: 0.01904188997913475 | validation: 0.03531609105588953]
	TIME [epoch: 5.86 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018152785209160277		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.018152785209160277 | validation: 0.036526787826980045]
	TIME [epoch: 5.87 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018212291072057053		[learning rate: 6.725e-05]
	Learning Rate: 6.725e-05
	LOSS [training: 0.018212291072057053 | validation: 0.03791675189656919]
	TIME [epoch: 5.85 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019718712884676554		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.019718712884676554 | validation: 0.036968671969916246]
	TIME [epoch: 5.86 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01938169332015197		[learning rate: 6.6775e-05]
	Learning Rate: 6.67752e-05
	LOSS [training: 0.01938169332015197 | validation: 0.030683077586992603]
	TIME [epoch: 5.86 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019824879542469698		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.019824879542469698 | validation: 0.03661721699260323]
	TIME [epoch: 5.86 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01952249025622795		[learning rate: 6.6304e-05]
	Learning Rate: 6.63038e-05
	LOSS [training: 0.01952249025622795 | validation: 0.032213534457371185]
	TIME [epoch: 5.86 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020063733943501475		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.020063733943501475 | validation: 0.03419876645536048]
	TIME [epoch: 5.86 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019320540362193356		[learning rate: 6.5836e-05]
	Learning Rate: 6.58357e-05
	LOSS [training: 0.019320540362193356 | validation: 0.03208918315819958]
	TIME [epoch: 5.86 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018878671561745792		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.018878671561745792 | validation: 0.03352188294665755]
	TIME [epoch: 5.86 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020599473707163182		[learning rate: 6.5371e-05]
	Learning Rate: 6.53709e-05
	LOSS [training: 0.020599473707163182 | validation: 0.0344919120528846]
	TIME [epoch: 5.87 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019231577127210586		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.019231577127210586 | validation: 0.037960983272339016]
	TIME [epoch: 5.86 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01778492688138657		[learning rate: 6.4909e-05]
	Learning Rate: 6.49094e-05
	LOSS [training: 0.01778492688138657 | validation: 0.03128687673306775]
	TIME [epoch: 5.87 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019408784251537387		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.019408784251537387 | validation: 0.03646207473509129]
	TIME [epoch: 5.87 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01996848889480236		[learning rate: 6.4451e-05]
	Learning Rate: 6.44512e-05
	LOSS [training: 0.01996848889480236 | validation: 0.03000717023645573]
	TIME [epoch: 5.86 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0182124448914919		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.0182124448914919 | validation: 0.036606793342405484]
	TIME [epoch: 5.87 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02094024861863354		[learning rate: 6.3996e-05]
	Learning Rate: 6.39962e-05
	LOSS [training: 0.02094024861863354 | validation: 0.034614019252814036]
	TIME [epoch: 5.87 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020991393054416704		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.020991393054416704 | validation: 0.03433683737281971]
	TIME [epoch: 5.87 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019992508799890018		[learning rate: 6.3544e-05]
	Learning Rate: 6.35444e-05
	LOSS [training: 0.019992508799890018 | validation: 0.03657404015014078]
	TIME [epoch: 5.86 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01937212413237707		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.01937212413237707 | validation: 0.03502518387079251]
	TIME [epoch: 5.86 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018878337379961026		[learning rate: 6.3096e-05]
	Learning Rate: 6.30958e-05
	LOSS [training: 0.018878337379961026 | validation: 0.036286791215764426]
	TIME [epoch: 5.86 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01840422028872138		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.01840422028872138 | validation: 0.033057929476575314]
	TIME [epoch: 5.88 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019836103735949996		[learning rate: 6.265e-05]
	Learning Rate: 6.26503e-05
	LOSS [training: 0.019836103735949996 | validation: 0.029957010204319026]
	TIME [epoch: 5.86 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019038297165342853		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.019038297165342853 | validation: 0.03559211824671439]
	TIME [epoch: 5.86 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019864920006861464		[learning rate: 6.2208e-05]
	Learning Rate: 6.2208e-05
	LOSS [training: 0.019864920006861464 | validation: 0.03623096690620177]
	TIME [epoch: 5.86 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019971092198353236		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.019971092198353236 | validation: 0.030276678046125296]
	TIME [epoch: 5.86 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01923479329853609		[learning rate: 6.1769e-05]
	Learning Rate: 6.17688e-05
	LOSS [training: 0.01923479329853609 | validation: 0.03380761860598942]
	TIME [epoch: 5.87 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018785530030388534		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.018785530030388534 | validation: 0.029792574492642346]
	TIME [epoch: 5.88 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02019479717995342		[learning rate: 6.1333e-05]
	Learning Rate: 6.13327e-05
	LOSS [training: 0.02019479717995342 | validation: 0.03584519407177846]
	TIME [epoch: 5.87 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019052963996243108		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.019052963996243108 | validation: 0.03452281164261112]
	TIME [epoch: 5.87 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018652625926064165		[learning rate: 6.09e-05]
	Learning Rate: 6.08998e-05
	LOSS [training: 0.018652625926064165 | validation: 0.03505853454972809]
	TIME [epoch: 5.87 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019511215346929976		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.019511215346929976 | validation: 0.03424987750982977]
	TIME [epoch: 5.88 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02207692085838814		[learning rate: 6.047e-05]
	Learning Rate: 6.04698e-05
	LOSS [training: 0.02207692085838814 | validation: 0.03960425831618354]
	TIME [epoch: 5.87 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02050103586492028		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.02050103586492028 | validation: 0.03326182625179254]
	TIME [epoch: 5.86 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018438733966618565		[learning rate: 6.0043e-05]
	Learning Rate: 6.00429e-05
	LOSS [training: 0.018438733966618565 | validation: 0.0340591949329791]
	TIME [epoch: 5.87 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019086863295965255		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.019086863295965255 | validation: 0.03374876376664587]
	TIME [epoch: 5.86 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01884646726446309		[learning rate: 5.9619e-05]
	Learning Rate: 5.9619e-05
	LOSS [training: 0.01884646726446309 | validation: 0.033766296119713435]
	TIME [epoch: 5.85 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017143423114351436		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.017143423114351436 | validation: 0.032336380469065895]
	TIME [epoch: 5.85 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020957714487827798		[learning rate: 5.9198e-05]
	Learning Rate: 5.91981e-05
	LOSS [training: 0.020957714487827798 | validation: 0.037620095553406764]
	TIME [epoch: 5.85 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01937974970893752		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.01937974970893752 | validation: 0.035899881370788936]
	TIME [epoch: 5.85 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01905380314906822		[learning rate: 5.878e-05]
	Learning Rate: 5.87802e-05
	LOSS [training: 0.01905380314906822 | validation: 0.03249945088840378]
	TIME [epoch: 5.87 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018704347899089473		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.018704347899089473 | validation: 0.03895986261878703]
	TIME [epoch: 5.88 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019856342079724882		[learning rate: 5.8365e-05]
	Learning Rate: 5.83652e-05
	LOSS [training: 0.019856342079724882 | validation: 0.03511008485017101]
	TIME [epoch: 5.87 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019384499247298563		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.019384499247298563 | validation: 0.03361638217089137]
	TIME [epoch: 5.87 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019598273139926327		[learning rate: 5.7953e-05]
	Learning Rate: 5.79531e-05
	LOSS [training: 0.019598273139926327 | validation: 0.032738054667477035]
	TIME [epoch: 5.87 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01971017725838501		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.01971017725838501 | validation: 0.036250532368898684]
	TIME [epoch: 5.87 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019627193262203368		[learning rate: 5.7544e-05]
	Learning Rate: 5.7544e-05
	LOSS [training: 0.019627193262203368 | validation: 0.040339177917704784]
	TIME [epoch: 5.88 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0191638383846625		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.0191638383846625 | validation: 0.03493794208176567]
	TIME [epoch: 5.86 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017665226291664168		[learning rate: 5.7138e-05]
	Learning Rate: 5.71378e-05
	LOSS [training: 0.017665226291664168 | validation: 0.039315974593205995]
	TIME [epoch: 5.87 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020224905540177484		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.020224905540177484 | validation: 0.03661800849088548]
	TIME [epoch: 5.87 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019412675736348067		[learning rate: 5.6734e-05]
	Learning Rate: 5.67344e-05
	LOSS [training: 0.019412675736348067 | validation: 0.03590291660368128]
	TIME [epoch: 5.86 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019690460523708802		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.019690460523708802 | validation: 0.03255324802858341]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_10_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_10_v_mmd4_1512.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 5748.315 seconds.
