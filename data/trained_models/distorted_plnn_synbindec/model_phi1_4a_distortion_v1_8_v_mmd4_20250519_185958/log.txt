Args:
Namespace(name='model_phi1_4a_distortion_v1_8_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_8/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_8/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.027978465, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 573833712

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.5878740842594175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.5878740842594175 | validation: 5.727049378820863]
	TIME [epoch: 168 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.804376223010029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.804376223010029 | validation: 5.020355069378121]
	TIME [epoch: 0.766 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.31307591330564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.31307591330564 | validation: 4.733055972776758]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.1004397252664795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.1004397252664795 | validation: 4.0992098977606375]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.046463893164892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.046463893164892 | validation: 4.279312238616538]
	TIME [epoch: 0.699 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.832920482916716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.832920482916716 | validation: 3.488113226703719]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.428983415500081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.428983415500081 | validation: 3.432720136485706]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.624550520270489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.624550520270489 | validation: 3.146859667214965]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.2652941330058045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2652941330058045 | validation: 2.901241493038012]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.162315972165162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.162315972165162 | validation: 2.578924135267962]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.015572438203917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.015572438203917 | validation: 2.49081746547823]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.085161015352407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.085161015352407 | validation: 2.5842991711329333]
	TIME [epoch: 0.697 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.025198822244997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.025198822244997 | validation: 2.407908215002006]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9316273961785924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9316273961785924 | validation: 2.327749514506665]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9166273925442567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9166273925442567 | validation: 2.2910651751402162]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8460864348251502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8460864348251502 | validation: 2.256390681574071]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7910629661218174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7910629661218174 | validation: 2.186248049657282]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7580069133301666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7580069133301666 | validation: 2.16948745584931]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.713556670635654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.713556670635654 | validation: 2.1120588521168027]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6653115585226823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6653115585226823 | validation: 2.061902100250118]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6323743018014953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6323743018014953 | validation: 2.0290473540649754]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5816555555852627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5816555555852627 | validation: 2.007044516469355]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.52362301548535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.52362301548535 | validation: 2.0007186177173146]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4527013243937446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4527013243937446 | validation: 2.070361315309345]
	TIME [epoch: 0.701 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.356539237321364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.356539237321364 | validation: 2.013166671412316]
	TIME [epoch: 0.697 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.275193051461656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.275193051461656 | validation: 1.8914208830539714]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1764330135680043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1764330135680043 | validation: 2.0677225330686007]
	TIME [epoch: 0.696 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.062110916303168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.062110916303168 | validation: 1.7130433275985166]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9296490136117304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9296490136117304 | validation: 2.491726772023106]
	TIME [epoch: 0.696 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.901988962841936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.901988962841936 | validation: 1.3660434174672653]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.864562249270032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.864562249270032 | validation: 2.1275543069337215]
	TIME [epoch: 0.698 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.555052977929287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.555052977929287 | validation: 1.8179288822350625]
	TIME [epoch: 0.697 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.356943043352929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.356943043352929 | validation: 1.3693138735440296]
	TIME [epoch: 0.699 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3190097170544584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3190097170544584 | validation: 1.977651100686343]
	TIME [epoch: 0.696 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2635350178029188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2635350178029188 | validation: 1.6122478106991054]
	TIME [epoch: 0.697 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.118233432741608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.118233432741608 | validation: 1.7060585105162027]
	TIME [epoch: 0.699 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.096132398327817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.096132398327817 | validation: 1.8800025707007018]
	TIME [epoch: 0.7 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.090493012563613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.090493012563613 | validation: 1.6392411264828235]
	TIME [epoch: 0.697 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.070043301433494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.070043301433494 | validation: 2.2157436164942226]
	TIME [epoch: 0.697 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1358756685488096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1358756685488096 | validation: 1.5440060695844346]
	TIME [epoch: 0.699 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.204913879812856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.204913879812856 | validation: 2.0993342131922694]
	TIME [epoch: 0.699 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0579785982222267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0579785982222267 | validation: 1.9805035153537043]
	TIME [epoch: 0.698 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9830891295297077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9830891295297077 | validation: 1.6984200989109899]
	TIME [epoch: 0.697 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9914111514256865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9914111514256865 | validation: 1.9704506329735592]
	TIME [epoch: 0.699 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9475384911941973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9475384911941973 | validation: 1.8687942740371621]
	TIME [epoch: 0.699 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.895210714037846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.895210714037846 | validation: 1.8019162432454938]
	TIME [epoch: 0.698 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8692407349992155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8692407349992155 | validation: 1.9521075132413952]
	TIME [epoch: 0.699 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8331242466280526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8331242466280526 | validation: 1.6551389020031753]
	TIME [epoch: 0.698 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9293469350519785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9293469350519785 | validation: 3.0562258540033946]
	TIME [epoch: 0.698 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.610030962978893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.610030962978893 | validation: 2.0773382589460327]
	TIME [epoch: 0.697 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9445873357486319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9445873357486319 | validation: 1.6680112763540564]
	TIME [epoch: 0.697 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0133399649074692		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 2.0133399649074692 | validation: 1.876677931883142]
	TIME [epoch: 0.697 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8560838832051354		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.8560838832051354 | validation: 1.9597052159655413]
	TIME [epoch: 0.697 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8371547673184785		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.8371547673184785 | validation: 1.6973834921114332]
	TIME [epoch: 0.696 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.777948930114127		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.777948930114127 | validation: 1.7086708842014469]
	TIME [epoch: 0.696 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7580667034955964		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.7580667034955964 | validation: 1.8956104825215285]
	TIME [epoch: 0.697 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.752400143420254		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.752400143420254 | validation: 1.701321002901468]
	TIME [epoch: 0.696 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7431384697351309		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.7431384697351309 | validation: 2.2384870504088576]
	TIME [epoch: 0.697 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.85347140775199		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.85347140775199 | validation: 1.6718839718412397]
	TIME [epoch: 0.703 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.864090212251959		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.864090212251959 | validation: 2.008306975835569]
	TIME [epoch: 0.701 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7655662322008334		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.7655662322008334 | validation: 1.8561011482330931]
	TIME [epoch: 0.698 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7007854377354352		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.7007854377354352 | validation: 1.720050828228595]
	TIME [epoch: 0.697 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8221820213869995		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.8221820213869995 | validation: 2.2813376235536675]
	TIME [epoch: 0.697 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8971676663833454		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.8971676663833454 | validation: 1.8646560666403607]
	TIME [epoch: 0.698 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7698386039382183		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.7698386039382183 | validation: 1.6599055288401594]
	TIME [epoch: 0.698 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.788788585551697		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.788788585551697 | validation: 1.930618369983414]
	TIME [epoch: 0.698 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7411332930310979		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.7411332930310979 | validation: 1.7031423657943137]
	TIME [epoch: 0.697 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6699191456899416		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.6699191456899416 | validation: 1.7162432836270893]
	TIME [epoch: 0.698 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6568286581112848		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.6568286581112848 | validation: 1.6445420097691057]
	TIME [epoch: 0.699 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6106458193861233		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.6106458193861233 | validation: 1.7377695484383264]
	TIME [epoch: 0.698 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.607645521089156		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.607645521089156 | validation: 1.5548393204475537]
	TIME [epoch: 0.698 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.695501094664852		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.695501094664852 | validation: 2.4626810752237462]
	TIME [epoch: 0.698 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.989810522939929		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.989810522939929 | validation: 1.9594235057201002]
	TIME [epoch: 0.698 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9415559301709335		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.9415559301709335 | validation: 1.5405402191457962]
	TIME [epoch: 0.698 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.859739057898139		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.859739057898139 | validation: 2.040902490059152]
	TIME [epoch: 0.698 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7643082237092025		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.7643082237092025 | validation: 1.6880455029573598]
	TIME [epoch: 0.698 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.625797065489138		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.625797065489138 | validation: 1.5556582581242322]
	TIME [epoch: 0.698 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6460993252858673		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.6460993252858673 | validation: 1.7602968385577444]
	TIME [epoch: 0.698 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.607935133660232		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.607935133660232 | validation: 1.555297121064829]
	TIME [epoch: 0.698 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5777582340979646		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.5777582340979646 | validation: 1.5790873129665466]
	TIME [epoch: 0.698 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5301347182923446		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.5301347182923446 | validation: 1.5289943411486289]
	TIME [epoch: 0.698 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5047534185295979		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.5047534185295979 | validation: 1.5308235829232597]
	TIME [epoch: 0.697 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.489739753341821		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.489739753341821 | validation: 1.4726914820276673]
	TIME [epoch: 0.697 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4642334885249302		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.4642334885249302 | validation: 1.5664423162852328]
	TIME [epoch: 0.699 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4809775821980458		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.4809775821980458 | validation: 1.5871600291856325]
	TIME [epoch: 0.698 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6703589866546116		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.6703589866546116 | validation: 2.54311538238227]
	TIME [epoch: 0.698 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2003319872694327		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 2.2003319872694327 | validation: 1.741706088318858]
	TIME [epoch: 0.698 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7984581622322162		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.7984581622322162 | validation: 1.4022045064333584]
	TIME [epoch: 0.698 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7732650823207092		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.7732650823207092 | validation: 1.6150594157755895]
	TIME [epoch: 0.697 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5334949385560859		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.5334949385560859 | validation: 1.4624166427754237]
	TIME [epoch: 0.697 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4540214354042906		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.4540214354042906 | validation: 1.3703057404446743]
	TIME [epoch: 0.698 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4899452424484312		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.4899452424484312 | validation: 1.480833447827716]
	TIME [epoch: 0.699 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4180853402741054		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.4180853402741054 | validation: 1.390497837249356]
	TIME [epoch: 0.698 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4037839074720506		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.4037839074720506 | validation: 1.3553281075109291]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3971091083428717		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.3971091083428717 | validation: 1.3565560687438796]
	TIME [epoch: 0.699 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.384089849653638		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.384089849653638 | validation: 1.3002549006782531]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.36587462638937		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.36587462638937 | validation: 1.3203341345390163]
	TIME [epoch: 0.698 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3594099819389311		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.3594099819389311 | validation: 1.3366957087712863]
	TIME [epoch: 0.699 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3604538129085233		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.3604538129085233 | validation: 1.404132250007482]
	TIME [epoch: 0.698 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4256787626741823		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.4256787626741823 | validation: 1.5270494880770404]
	TIME [epoch: 0.706 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.505780750182399		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.505780750182399 | validation: 1.4178383686412062]
	TIME [epoch: 0.7 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4549664940747709		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.4549664940747709 | validation: 1.3944973760878305]
	TIME [epoch: 0.699 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4751770356857643		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.4751770356857643 | validation: 1.2378582081021623]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5867314810125717		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.5867314810125717 | validation: 1.4347407564724475]
	TIME [epoch: 0.696 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3720626625670758		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.3720626625670758 | validation: 1.4006167699666183]
	TIME [epoch: 0.697 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4060805919094228		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.4060805919094228 | validation: 1.242951525709274]
	TIME [epoch: 0.695 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.352247924901645		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.352247924901645 | validation: 1.3908755060593472]
	TIME [epoch: 0.695 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.337532816669625		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.337532816669625 | validation: 1.2608688251191922]
	TIME [epoch: 0.694 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.33895208937191		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.33895208937191 | validation: 1.2881104742296503]
	TIME [epoch: 0.695 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3085070404903012		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.3085070404903012 | validation: 1.2682476176196427]
	TIME [epoch: 0.694 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.32797731683782		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.32797731683782 | validation: 1.308040884542148]
	TIME [epoch: 0.694 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3373030781602324		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.3373030781602324 | validation: 1.165680866202527]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3388312470761314		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.3388312470761314 | validation: 1.3144004110497978]
	TIME [epoch: 0.695 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2963680599565082		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.2963680599565082 | validation: 1.1224429579487203]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.275625931818518		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.275625931818518 | validation: 1.2403674067866384]
	TIME [epoch: 0.696 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2609370076344677		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.2609370076344677 | validation: 1.0617454586983015]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2447293776885069		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.2447293776885069 | validation: 1.2432771113531187]
	TIME [epoch: 0.695 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2400389520905295		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.2400389520905295 | validation: 1.1032857346929206]
	TIME [epoch: 0.694 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2978921764856302		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.2978921764856302 | validation: 1.483485586477073]
	TIME [epoch: 0.694 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3956563319583268		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.3956563319583268 | validation: 1.113843479599406]
	TIME [epoch: 0.693 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2108557036017105		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.2108557036017105 | validation: 0.9688403951112261]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2890607198780992		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.2890607198780992 | validation: 1.3883290080311999]
	TIME [epoch: 0.697 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3089807871364911		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.3089807871364911 | validation: 1.2460800498562798]
	TIME [epoch: 0.698 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.266441673204091		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.266441673204091 | validation: 1.133894715848108]
	TIME [epoch: 0.694 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2844361199044423		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.2844361199044423 | validation: 1.1514445435463665]
	TIME [epoch: 0.695 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1893159245387355		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.1893159245387355 | validation: 1.1146051396637968]
	TIME [epoch: 0.694 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1730164486822958		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.1730164486822958 | validation: 0.9808774614660392]
	TIME [epoch: 0.693 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1749708934949972		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.1749708934949972 | validation: 1.1667350362218754]
	TIME [epoch: 0.695 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1743636523221692		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.1743636523221692 | validation: 0.9551100887862796]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2084956014546548		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.2084956014546548 | validation: 1.4043534968336686]
	TIME [epoch: 0.697 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2940202063564041		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.2940202063564041 | validation: 1.2353036808367168]
	TIME [epoch: 0.696 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3212361016408787		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.3212361016408787 | validation: 0.9225667119156835]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1582266689387337		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.1582266689387337 | validation: 1.229121706620718]
	TIME [epoch: 0.694 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1926212873001232		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.1926212873001232 | validation: 1.0766466064014453]
	TIME [epoch: 0.695 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2309772868295865		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.2309772868295865 | validation: 1.0127057650882492]
	TIME [epoch: 0.694 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.115189209586015		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.115189209586015 | validation: 0.9470908678163792]
	TIME [epoch: 0.702 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0985506014381345		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.0985506014381345 | validation: 1.049419092109873]
	TIME [epoch: 0.693 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0970868999296899		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.0970868999296899 | validation: 0.8902249745904287]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1016555143264528		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.1016555143264528 | validation: 1.1295748115316808]
	TIME [epoch: 0.694 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.126784934897275		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.126784934897275 | validation: 1.010866854812397]
	TIME [epoch: 0.693 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3019508433933333		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.3019508433933333 | validation: 1.4393612576622619]
	TIME [epoch: 0.696 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.310792889706978		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.310792889706978 | validation: 1.0655565401559535]
	TIME [epoch: 0.694 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0833986525909232		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.0833986525909232 | validation: 0.8615101380116006]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1474045885066562		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.1474045885066562 | validation: 1.18707953727191]
	TIME [epoch: 0.694 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.142635899814959		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.142635899814959 | validation: 1.0203516282385139]
	TIME [epoch: 0.694 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1063909120883426		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.1063909120883426 | validation: 0.9678724280318343]
	TIME [epoch: 0.694 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1277492810699468		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.1277492810699468 | validation: 1.0542766957975573]
	TIME [epoch: 0.693 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.109385159111492		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.109385159111492 | validation: 1.055137365830634]
	TIME [epoch: 0.696 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0730539507591597		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.0730539507591597 | validation: 0.8265905473692968]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1337315418146177		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.1337315418146177 | validation: 1.239679494313387]
	TIME [epoch: 0.694 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1541659859384379		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.1541659859384379 | validation: 1.126665826406654]
	TIME [epoch: 0.694 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1281305184787929		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.1281305184787929 | validation: 0.8828680425068273]
	TIME [epoch: 0.694 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1391130974866277		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.1391130974866277 | validation: 1.0922603301176614]
	TIME [epoch: 0.693 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0893293819620435		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.0893293819620435 | validation: 0.9310449267137373]
	TIME [epoch: 0.693 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0517558720877842		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.0517558720877842 | validation: 0.933372752067241]
	TIME [epoch: 0.695 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.064032723493626		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.064032723493626 | validation: 0.939137409171427]
	TIME [epoch: 0.694 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0593685267982087		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.0593685267982087 | validation: 0.9759146543519045]
	TIME [epoch: 0.694 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0561129213448128		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.0561129213448128 | validation: 0.7985829358301177]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1115289974856166		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.1115289974856166 | validation: 1.298426341785306]
	TIME [epoch: 0.7 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.208324782597128		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.208324782597128 | validation: 1.1646556569396544]
	TIME [epoch: 0.697 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1499825107553228		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.1499825107553228 | validation: 0.8599281951205118]
	TIME [epoch: 0.698 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1598412490723198		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.1598412490723198 | validation: 1.2158700496989931]
	TIME [epoch: 0.7 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1509506495926254		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.1509506495926254 | validation: 1.0088708709877516]
	TIME [epoch: 0.699 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1069092952042605		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.1069092952042605 | validation: 0.8920632860657002]
	TIME [epoch: 0.697 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.034513560424354		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.034513560424354 | validation: 0.9505148851085674]
	TIME [epoch: 0.697 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0359437797482303		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.0359437797482303 | validation: 0.8914031025297554]
	TIME [epoch: 0.698 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0272268292112232		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.0272268292112232 | validation: 0.8902975971217146]
	TIME [epoch: 0.697 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0246725002471897		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.0246725002471897 | validation: 0.8561694225980807]
	TIME [epoch: 0.697 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0347169538884373		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.0347169538884373 | validation: 1.1054130127539528]
	TIME [epoch: 0.697 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0785093081307087		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.0785093081307087 | validation: 0.9819939013191455]
	TIME [epoch: 0.698 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0878243000628578		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.0878243000628578 | validation: 1.017549428748462]
	TIME [epoch: 0.697 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0770754016618855		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.0770754016618855 | validation: 0.989129623241821]
	TIME [epoch: 0.698 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.036867516341796		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.036867516341796 | validation: 0.7900773609178744]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.119528817108215		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.119528817108215 | validation: 1.2002775528521752]
	TIME [epoch: 0.702 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1269379771740822		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.1269379771740822 | validation: 1.0855174261046607]
	TIME [epoch: 0.695 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0998098656804611		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.0998098656804611 | validation: 0.8954174391224479]
	TIME [epoch: 0.694 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0832902983464863		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.0832902983464863 | validation: 1.007520416127906]
	TIME [epoch: 0.695 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0459699937999136		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.0459699937999136 | validation: 0.9162645110784045]
	TIME [epoch: 0.696 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0219404418977025		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.0219404418977025 | validation: 0.8504670030238505]
	TIME [epoch: 0.693 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0239618043113305		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.0239618043113305 | validation: 1.0095975627103353]
	TIME [epoch: 0.694 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.025542616144728		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.025542616144728 | validation: 0.863117004017296]
	TIME [epoch: 0.696 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0478435788028577		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.0478435788028577 | validation: 1.0411825777106443]
	TIME [epoch: 0.695 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0500551389240216		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.0500551389240216 | validation: 0.927285814987866]
	TIME [epoch: 0.697 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0570205493054012		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.0570205493054012 | validation: 0.996589569932295]
	TIME [epoch: 0.696 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0378463417837112		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.0378463417837112 | validation: 0.8758194605522936]
	TIME [epoch: 0.696 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0297634266680664		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.0297634266680664 | validation: 0.9750677185223655]
	TIME [epoch: 0.695 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0362091143924521		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.0362091143924521 | validation: 0.8729897995746398]
	TIME [epoch: 0.694 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0339152864953678		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.0339152864953678 | validation: 0.9327925684715513]
	TIME [epoch: 0.694 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0121914226316095		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.0121914226316095 | validation: 0.8243774060610511]
	TIME [epoch: 0.695 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.017305492692464		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.017305492692464 | validation: 1.0546363706753155]
	TIME [epoch: 0.696 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0508206153586503		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.0508206153586503 | validation: 0.8461753836415856]
	TIME [epoch: 0.694 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0946336314460134		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.0946336314460134 | validation: 1.0617954355583166]
	TIME [epoch: 0.693 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0554934715586872		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.0554934715586872 | validation: 0.9617276369964689]
	TIME [epoch: 0.694 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0350234955144384		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.0350234955144384 | validation: 0.890326101657063]
	TIME [epoch: 0.694 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0519930063934062		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.0519930063934062 | validation: 0.9355443235411752]
	TIME [epoch: 0.693 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0170954254945037		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.0170954254945037 | validation: 0.8679501475055615]
	TIME [epoch: 0.694 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0108484501099886		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.0108484501099886 | validation: 0.9771381124237749]
	TIME [epoch: 0.694 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0230410031880655		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.0230410031880655 | validation: 0.8573652368898049]
	TIME [epoch: 0.694 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0167575401471338		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.0167575401471338 | validation: 0.9031607378793688]
	TIME [epoch: 0.693 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.001420510643375		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.001420510643375 | validation: 0.9092924544886982]
	TIME [epoch: 0.693 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0016623505540696		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.0016623505540696 | validation: 0.7874813854478039]
	TIME [epoch: 178 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0375157685924055		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.0375157685924055 | validation: 1.1541716528623016]
	TIME [epoch: 1.37 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0996617368862394		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.0996617368862394 | validation: 0.8406491706704569]
	TIME [epoch: 1.36 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0214527020806292		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.0214527020806292 | validation: 0.9042563571107174]
	TIME [epoch: 1.36 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0073946675234828		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.0073946675234828 | validation: 0.9486155250652653]
	TIME [epoch: 1.35 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0125191190275493		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.0125191190275493 | validation: 0.7863887861393084]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_206.pth
	Model improved!!!
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0392981702328157		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.0392981702328157 | validation: 1.0321528722744275]
	TIME [epoch: 1.36 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.043819457523567		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.043819457523567 | validation: 0.8792732947725552]
	TIME [epoch: 1.36 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.037770087286368		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.037770087286368 | validation: 0.8717040955980565]
	TIME [epoch: 1.36 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9956305383721273		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.9956305383721273 | validation: 0.9556914127110906]
	TIME [epoch: 1.36 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9980842931534801		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.9980842931534801 | validation: 0.7659475390643489]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.006154554517106		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.006154554517106 | validation: 1.0217344950847789]
	TIME [epoch: 1.36 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0249127084185532		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 1.0249127084185532 | validation: 0.8062331521754281]
	TIME [epoch: 1.36 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0122944635263103		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.0122944635263103 | validation: 0.964449848335624]
	TIME [epoch: 1.36 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9976158522444578		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.9976158522444578 | validation: 0.9048252227947251]
	TIME [epoch: 1.36 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9904152720719209		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.9904152720719209 | validation: 0.8415915249069729]
	TIME [epoch: 1.36 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0199121267793614		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.0199121267793614 | validation: 1.0463513685516876]
	TIME [epoch: 1.36 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.044221869008819		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.044221869008819 | validation: 0.8861322095383477]
	TIME [epoch: 1.36 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0274324368826782		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.0274324368826782 | validation: 0.8846031301843829]
	TIME [epoch: 1.36 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9915894688261889		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.9915894688261889 | validation: 0.9444202143589577]
	TIME [epoch: 1.36 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9912499119662178		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.9912499119662178 | validation: 0.7463385895471095]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0096971387717517		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.0096971387717517 | validation: 0.9443186969303334]
	TIME [epoch: 1.36 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9925358751909797		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.9925358751909797 | validation: 0.9053493096509744]
	TIME [epoch: 1.36 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9763331937383491		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.9763331937383491 | validation: 0.8467918483633053]
	TIME [epoch: 1.36 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9784106435604373		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.9784106435604373 | validation: 0.9789699634920027]
	TIME [epoch: 1.36 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0042907969847317		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 1.0042907969847317 | validation: 0.7717990361584507]
	TIME [epoch: 1.36 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0008318727708139		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.0008318727708139 | validation: 0.9821744884432144]
	TIME [epoch: 1.36 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9983126590850263		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.9983126590850263 | validation: 0.8483039565988186]
	TIME [epoch: 1.35 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9993584777164733		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.9993584777164733 | validation: 0.9888603606300675]
	TIME [epoch: 1.35 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0240751232539365		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.0240751232539365 | validation: 0.8643826206634707]
	TIME [epoch: 1.36 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9722757236822495		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.9722757236822495 | validation: 0.8150240825191025]
	TIME [epoch: 1.36 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.970323632160592		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.970323632160592 | validation: 0.9098869207848698]
	TIME [epoch: 1.36 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9775353689022067		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.9775353689022067 | validation: 0.8353918758494544]
	TIME [epoch: 1.36 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9693367202316939		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.9693367202316939 | validation: 0.9444808389923257]
	TIME [epoch: 1.36 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9723897404202211		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.9723897404202211 | validation: 0.8802039276150083]
	TIME [epoch: 1.36 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.987841362017701		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.987841362017701 | validation: 0.8562083937654403]
	TIME [epoch: 1.36 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9863222947638502		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.9863222947638502 | validation: 0.987030703887556]
	TIME [epoch: 1.36 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0062829567062874		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 1.0062829567062874 | validation: 0.7872296411965509]
	TIME [epoch: 1.36 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9939513426376243		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.9939513426376243 | validation: 0.9746855841435945]
	TIME [epoch: 1.36 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9840282122804022		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.9840282122804022 | validation: 0.7888765353707597]
	TIME [epoch: 1.36 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9626706490828124		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.9626706490828124 | validation: 0.8760575761200903]
	TIME [epoch: 1.36 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9606560370120377		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.9606560370120377 | validation: 0.8341693094558998]
	TIME [epoch: 1.36 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9839597734979979		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.9839597734979979 | validation: 0.9094788397903417]
	TIME [epoch: 1.36 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9934235999772655		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.9934235999772655 | validation: 0.8773906892499828]
	TIME [epoch: 1.36 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9923303088649672		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.9923303088649672 | validation: 0.8785222186735675]
	TIME [epoch: 1.36 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9594291743092392		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.9594291743092392 | validation: 0.8719021835695276]
	TIME [epoch: 1.36 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9577056511814013		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.9577056511814013 | validation: 0.8631969803565582]
	TIME [epoch: 1.36 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9828243776679204		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.9828243776679204 | validation: 0.8961883606904562]
	TIME [epoch: 1.36 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9768429488719675		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.9768429488719675 | validation: 0.8980371233481977]
	TIME [epoch: 1.36 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9593021878348128		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.9593021878348128 | validation: 0.7854730713564545]
	TIME [epoch: 1.36 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9576473878077275		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.9576473878077275 | validation: 0.9700893094365668]
	TIME [epoch: 1.35 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9711729562678035		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.9711729562678035 | validation: 0.6995030598511539]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_252.pth
	Model improved!!!
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9879308711129529		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.9879308711129529 | validation: 0.9290999028447559]
	TIME [epoch: 1.37 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9467323928730081		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.9467323928730081 | validation: 0.8670144745090138]
	TIME [epoch: 1.36 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9457027295634132		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.9457027295634132 | validation: 0.7745639883867772]
	TIME [epoch: 1.36 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9569938794653501		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.9569938794653501 | validation: 0.9125698045865553]
	TIME [epoch: 1.36 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9666236657865889		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.9666236657865889 | validation: 0.9554784839285997]
	TIME [epoch: 1.36 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0002798202883445		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 1.0002798202883445 | validation: 0.8303427319984297]
	TIME [epoch: 1.36 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9604497079258951		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.9604497079258951 | validation: 0.9162838457367242]
	TIME [epoch: 1.36 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9544977193923483		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.9544977193923483 | validation: 0.7366264590419083]
	TIME [epoch: 1.36 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9649135090394582		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.9649135090394582 | validation: 0.9386012844652201]
	TIME [epoch: 1.36 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9492155133620243		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.9492155133620243 | validation: 0.7900142154620492]
	TIME [epoch: 1.36 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9227168046577385		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.9227168046577385 | validation: 0.8201144840764147]
	TIME [epoch: 1.36 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9137874085564883		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.9137874085564883 | validation: 0.861125898495811]
	TIME [epoch: 1.36 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9413864457312363		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.9413864457312363 | validation: 0.8438216874958462]
	TIME [epoch: 1.35 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9440503511707361		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.9440503511707361 | validation: 0.8194363499566587]
	TIME [epoch: 1.36 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9505420686578572		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.9505420686578572 | validation: 0.9808521283832214]
	TIME [epoch: 1.35 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9906228001509584		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.9906228001509584 | validation: 0.7125583615798959]
	TIME [epoch: 1.36 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9678888630709943		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.9678888630709943 | validation: 0.9614073335781148]
	TIME [epoch: 1.35 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9634477018274588		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.9634477018274588 | validation: 0.7879951840132657]
	TIME [epoch: 1.36 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9157115190670418		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.9157115190670418 | validation: 0.7844880384887454]
	TIME [epoch: 1.35 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9059878852791452		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.9059878852791452 | validation: 0.8874053766946708]
	TIME [epoch: 1.36 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9195037789415244		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.9195037789415244 | validation: 0.739301472134589]
	TIME [epoch: 1.36 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9412803203910132		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.9412803203910132 | validation: 0.9762353719256279]
	TIME [epoch: 1.36 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9572010048977982		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.9572010048977982 | validation: 0.8772888894816027]
	TIME [epoch: 1.36 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9535820901483671		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.9535820901483671 | validation: 0.8040958477461477]
	TIME [epoch: 1.36 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9658072587755225		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.9658072587755225 | validation: 0.8858237751653792]
	TIME [epoch: 1.36 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9266529728405019		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.9266529728405019 | validation: 0.7884653421812149]
	TIME [epoch: 1.36 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8846887931537621		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.8846887931537621 | validation: 0.7757550039747171]
	TIME [epoch: 1.36 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8973079318480989		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.8973079318480989 | validation: 0.8221701840146483]
	TIME [epoch: 1.36 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8934135134168539		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.8934135134168539 | validation: 0.7355414994834377]
	TIME [epoch: 1.35 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8854898918814521		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.8854898918814521 | validation: 0.8497772282208373]
	TIME [epoch: 1.36 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8833222699461406		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.8833222699461406 | validation: 0.6829939022365258]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_283.pth
	Model improved!!!
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9328270011035972		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.9328270011035972 | validation: 1.1992793807299686]
	TIME [epoch: 1.36 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.097054507701998		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 1.097054507701998 | validation: 0.7560771413341688]
	TIME [epoch: 1.36 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9391282199920218		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.9391282199920218 | validation: 0.7548612360452238]
	TIME [epoch: 1.36 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9032950293429747		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.9032950293429747 | validation: 0.8681210007627176]
	TIME [epoch: 1.36 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8961280044524003		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.8961280044524003 | validation: 0.7697106683741831]
	TIME [epoch: 1.36 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8978564641282597		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.8978564641282597 | validation: 0.7930868931747829]
	TIME [epoch: 1.36 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8900966410839005		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.8900966410839005 | validation: 0.8297759695949465]
	TIME [epoch: 1.36 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8933940824894547		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.8933940824894547 | validation: 0.756037020610522]
	TIME [epoch: 1.36 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9184673717293977		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.9184673717293977 | validation: 0.9900192418209102]
	TIME [epoch: 1.36 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9407054604765867		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.9407054604765867 | validation: 0.737908126833847]
	TIME [epoch: 1.36 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8984489721296357		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.8984489721296357 | validation: 0.7634616676244792]
	TIME [epoch: 1.36 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8750377189304702		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.8750377189304702 | validation: 0.8303158151784139]
	TIME [epoch: 1.35 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.887929148114859		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.887929148114859 | validation: 0.6894119384670425]
	TIME [epoch: 1.36 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9124644132120696		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.9124644132120696 | validation: 0.9369451871226797]
	TIME [epoch: 1.36 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.899867935905358		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.899867935905358 | validation: 0.7393330328122583]
	TIME [epoch: 1.35 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8836363541562493		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.8836363541562493 | validation: 0.8059443991587023]
	TIME [epoch: 1.35 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8689606229449579		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.8689606229449579 | validation: 0.7506363171478002]
	TIME [epoch: 1.35 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8575651004385367		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.8575651004385367 | validation: 0.8411475070759441]
	TIME [epoch: 1.36 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8580189059610673		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.8580189059610673 | validation: 0.6810716062656038]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_302.pth
	Model improved!!!
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8925947086588218		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.8925947086588218 | validation: 1.2735027777052657]
	TIME [epoch: 1.35 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.06820814536967		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 1.06820814536967 | validation: 0.7345266280562689]
	TIME [epoch: 1.35 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9159402667863684		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.9159402667863684 | validation: 0.6884558435762168]
	TIME [epoch: 1.35 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8637686928243811		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.8637686928243811 | validation: 0.8667429148416255]
	TIME [epoch: 1.35 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8726772625074344		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.8726772625074344 | validation: 0.6775728892645043]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8570719865775847		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.8570719865775847 | validation: 0.840258497120082]
	TIME [epoch: 1.36 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8432609222360256		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.8432609222360256 | validation: 0.6818464211233846]
	TIME [epoch: 1.36 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8305066797172405		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.8305066797172405 | validation: 0.8041263920892053]
	TIME [epoch: 1.36 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8309788741757559		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.8309788741757559 | validation: 0.6539890791807009]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8527454876857086		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.8527454876857086 | validation: 0.9634714273255803]
	TIME [epoch: 1.36 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.879010060826728		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.879010060826728 | validation: 0.6647165201383768]
	TIME [epoch: 1.36 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8614617396376184		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.8614617396376184 | validation: 0.9077885800021651]
	TIME [epoch: 1.36 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8651949776277712		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.8651949776277712 | validation: 0.7275325539422133]
	TIME [epoch: 1.36 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8771302917176652		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.8771302917176652 | validation: 0.8594753722351545]
	TIME [epoch: 1.37 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8748039655279493		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.8748039655279493 | validation: 0.799467166908582]
	TIME [epoch: 1.36 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8403945578164206		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.8403945578164206 | validation: 0.6707702961629656]
	TIME [epoch: 1.36 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8316392858434563		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.8316392858434563 | validation: 0.8724211391470207]
	TIME [epoch: 1.36 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8331523620330569		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.8331523620330569 | validation: 0.6521547903089103]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_320.pth
	Model improved!!!
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8034888480172541		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.8034888480172541 | validation: 0.7946180395768578]
	TIME [epoch: 1.35 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8032396579994782		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.8032396579994782 | validation: 0.618347781093169]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_322.pth
	Model improved!!!
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8178990196419323		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.8178990196419323 | validation: 1.0343980163736877]
	TIME [epoch: 1.36 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8999644960476698		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.8999644960476698 | validation: 0.6285677304710641]
	TIME [epoch: 1.36 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8447397567115402		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.8447397567115402 | validation: 0.8181782947540974]
	TIME [epoch: 1.36 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8113809085410532		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.8113809085410532 | validation: 0.6803187730408138]
	TIME [epoch: 1.36 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7928072333325129		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.7928072333325129 | validation: 0.7630002412113547]
	TIME [epoch: 1.36 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7932588244113237		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.7932588244113237 | validation: 0.755290766497183]
	TIME [epoch: 1.36 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7750607955703578		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.7750607955703578 | validation: 0.6170016245616661]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8096985754527181		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.8096985754527181 | validation: 0.9870955625825423]
	TIME [epoch: 1.36 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8831924648888384		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.8831924648888384 | validation: 0.6447776115764462]
	TIME [epoch: 1.36 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8100069653564705		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.8100069653564705 | validation: 0.7009529297824982]
	TIME [epoch: 1.36 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7437022786866321		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.7437022786866321 | validation: 0.7342588823455204]
	TIME [epoch: 1.36 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7433892310587744		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.7433892310587744 | validation: 0.5787282823305951]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_334.pth
	Model improved!!!
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7637885402898301		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.7637885402898301 | validation: 1.065494174255609]
	TIME [epoch: 1.36 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8722580781955784		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.8722580781955784 | validation: 0.7789106543206701]
	TIME [epoch: 1.37 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9553242361821566		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.9553242361821566 | validation: 0.9548033803066893]
	TIME [epoch: 1.36 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8419613309491575		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.8419613309491575 | validation: 0.6610417330640144]
	TIME [epoch: 1.36 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.743430193944979		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.743430193944979 | validation: 0.6301978956299397]
	TIME [epoch: 1.36 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7282528838942943		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.7282528838942943 | validation: 0.7561411854882636]
	TIME [epoch: 1.36 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7477139276670306		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.7477139276670306 | validation: 0.6072769342198177]
	TIME [epoch: 1.36 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7283300517597351		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.7283300517597351 | validation: 0.7830068521581771]
	TIME [epoch: 1.36 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7401415779172521		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.7401415779172521 | validation: 0.5588219348835245]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7849857561894745		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.7849857561894745 | validation: 0.9900811290638765]
	TIME [epoch: 1.36 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8437993286751632		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.8437993286751632 | validation: 0.6192629287798543]
	TIME [epoch: 1.36 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7200824327436866		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.7200824327436866 | validation: 0.6385687827919582]
	TIME [epoch: 1.36 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7043403038634339		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.7043403038634339 | validation: 0.8402756806190624]
	TIME [epoch: 1.36 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7360334015338935		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.7360334015338935 | validation: 0.587408482119525]
	TIME [epoch: 1.36 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7878427360455146		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.7878427360455146 | validation: 1.0378730871935624]
	TIME [epoch: 1.36 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8448320599893084		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.8448320599893084 | validation: 0.6087531107947403]
	TIME [epoch: 1.36 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7019929550256905		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.7019929550256905 | validation: 0.6116110167695471]
	TIME [epoch: 1.36 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7107308478644222		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.7107308478644222 | validation: 0.827751861939452]
	TIME [epoch: 1.36 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7158438176359556		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.7158438176359556 | validation: 0.5492422840717697]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_353.pth
	Model improved!!!
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6919000207004453		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.6919000207004453 | validation: 0.7545742281867878]
	TIME [epoch: 1.36 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6761911435298581		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.6761911435298581 | validation: 0.5425289847830178]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6935443620589763		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.6935443620589763 | validation: 0.887734732729221]
	TIME [epoch: 1.36 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7327149015809202		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.7327149015809202 | validation: 0.5955401585309885]
	TIME [epoch: 1.37 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7497817557076212		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.7497817557076212 | validation: 0.9946469784979188]
	TIME [epoch: 1.36 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8043856573075816		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.8043856573075816 | validation: 0.5947749056834633]
	TIME [epoch: 1.36 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6925619841294473		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.6925619841294473 | validation: 0.6022825127171636]
	TIME [epoch: 1.36 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6306279942853051		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.6306279942853051 | validation: 0.727781208133301]
	TIME [epoch: 1.36 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6457701280598096		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.6457701280598096 | validation: 0.5104769224431077]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_362.pth
	Model improved!!!
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6535289413948903		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.6535289413948903 | validation: 0.8486844104124519]
	TIME [epoch: 1.36 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7010953189369112		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.7010953189369112 | validation: 0.5024690574724192]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_364.pth
	Model improved!!!
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7125990469134401		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.7125990469134401 | validation: 0.852363328429431]
	TIME [epoch: 1.36 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6896857222484515		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.6896857222484515 | validation: 0.5338541496189217]
	TIME [epoch: 1.36 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6198081848110037		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.6198081848110037 | validation: 0.6910581815114472]
	TIME [epoch: 1.36 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6046342425125986		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.6046342425125986 | validation: 0.5150876838204288]
	TIME [epoch: 1.36 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6019923252982138		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.6019923252982138 | validation: 0.7868185647832816]
	TIME [epoch: 1.36 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.63798479399395		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.63798479399395 | validation: 0.4846800244292415]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_370.pth
	Model improved!!!
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6434539378960175		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.6434539378960175 | validation: 0.8839912007140175]
	TIME [epoch: 1.36 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6967826066145045		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.6967826066145045 | validation: 0.4699287999265591]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_372.pth
	Model improved!!!
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6204285619571828		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.6204285619571828 | validation: 0.7480897421825307]
	TIME [epoch: 1.36 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6120655978055377		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.6120655978055377 | validation: 0.475203533789896]
	TIME [epoch: 1.36 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.616025262541963		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.616025262541963 | validation: 0.7276529470040022]
	TIME [epoch: 1.36 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6095867696834774		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.6095867696834774 | validation: 0.5069097927140312]
	TIME [epoch: 1.36 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5663607651285351		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.5663607651285351 | validation: 0.6509946731800321]
	TIME [epoch: 1.37 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5693235332421142		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.5693235332421142 | validation: 0.5299790858694469]
	TIME [epoch: 1.36 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5932528673934103		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.5932528673934103 | validation: 0.6035721831183714]
	TIME [epoch: 1.36 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.592131709810878		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.592131709810878 | validation: 0.9422922812860199]
	TIME [epoch: 1.36 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7535470554321461		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.7535470554321461 | validation: 0.5336184955384833]
	TIME [epoch: 1.36 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6387574886484078		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.6387574886484078 | validation: 0.74701070418694]
	TIME [epoch: 1.36 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.578649706018055		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.578649706018055 | validation: 0.43429890601370325]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_383.pth
	Model improved!!!
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5842094245980021		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.5842094245980021 | validation: 0.799667729777463]
	TIME [epoch: 1.35 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6349952162515227		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.6349952162515227 | validation: 0.46796727774699143]
	TIME [epoch: 1.35 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5449605427378288		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.5449605427378288 | validation: 0.6406291436345244]
	TIME [epoch: 1.36 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5250049870824308		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.5250049870824308 | validation: 0.48816778150517603]
	TIME [epoch: 1.36 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5226478211933089		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.5226478211933089 | validation: 0.7182175367716334]
	TIME [epoch: 1.35 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5371374864167304		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.5371374864167304 | validation: 0.4933332237903092]
	TIME [epoch: 1.36 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5467965442127632		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.5467965442127632 | validation: 0.9724238153704668]
	TIME [epoch: 1.35 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7418191103779996		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.7418191103779996 | validation: 0.5821715280586267]
	TIME [epoch: 1.36 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5344714111532217		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.5344714111532217 | validation: 0.4066604839310524]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_392.pth
	Model improved!!!
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6592720748610382		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.6592720748610382 | validation: 0.8512662181529947]
	TIME [epoch: 1.36 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6580461248677433		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.6580461248677433 | validation: 0.4986290693025943]
	TIME [epoch: 1.35 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5045086707515045		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.5045086707515045 | validation: 0.5097836998811576]
	TIME [epoch: 1.35 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4772191479097315		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.4772191479097315 | validation: 0.598310002464384]
	TIME [epoch: 1.36 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4746417413767729		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.4746417413767729 | validation: 0.5011190872369129]
	TIME [epoch: 1.35 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.466860350692386		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.466860350692386 | validation: 0.5780487631129148]
	TIME [epoch: 1.36 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4776710884581779		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.4776710884581779 | validation: 0.47724951816714506]
	TIME [epoch: 1.35 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5289695359562631		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.5289695359562631 | validation: 0.8336633447797017]
	TIME [epoch: 1.35 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5877509318703967		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.5877509318703967 | validation: 0.4076589160406622]
	TIME [epoch: 1.36 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5108075926050776		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.5108075926050776 | validation: 0.6602168980410075]
	TIME [epoch: 1.36 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.532420885852275		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.532420885852275 | validation: 0.415262317844992]
	TIME [epoch: 1.36 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5248107599722243		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.5248107599722243 | validation: 0.6090560498154892]
	TIME [epoch: 1.36 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.453054351284668		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.453054351284668 | validation: 0.4676523937256029]
	TIME [epoch: 1.36 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4327808495817472		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.4327808495817472 | validation: 0.49973485023034736]
	TIME [epoch: 1.36 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43722185163666466		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.43722185163666466 | validation: 0.6115886479149438]
	TIME [epoch: 1.36 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5149384693794805		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.5149384693794805 | validation: 0.6523419383523721]
	TIME [epoch: 1.36 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6862639729523875		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.6862639729523875 | validation: 0.6853668985804063]
	TIME [epoch: 1.36 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4972757779620852		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.4972757779620852 | validation: 0.5160377646773743]
	TIME [epoch: 1.35 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41123111926781064		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.41123111926781064 | validation: 0.4171733402248583]
	TIME [epoch: 1.35 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49033888291322697		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.49033888291322697 | validation: 0.854803207049768]
	TIME [epoch: 1.35 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6212902549610585		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.6212902549610585 | validation: 0.4141713300305804]
	TIME [epoch: 1.35 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4181938046350058		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.4181938046350058 | validation: 0.476579983847893]
	TIME [epoch: 1.35 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3991112397612367		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.3991112397612367 | validation: 0.5430906782703273]
	TIME [epoch: 1.35 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43009392096072874		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.43009392096072874 | validation: 0.5078843953239794]
	TIME [epoch: 1.35 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4456726087511376		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.4456726087511376 | validation: 0.586611318691101]
	TIME [epoch: 1.35 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4474031268203514		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.4474031268203514 | validation: 0.5199870311658625]
	TIME [epoch: 1.35 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42843467057824697		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.42843467057824697 | validation: 0.43136956513582003]
	TIME [epoch: 1.35 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41113274267955435		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.41113274267955435 | validation: 0.6163940363615401]
	TIME [epoch: 1.36 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4560026139690912		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.4560026139690912 | validation: 0.36028874577739656]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_421.pth
	Model improved!!!
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5555921889356796		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.5555921889356796 | validation: 0.7868007650798492]
	TIME [epoch: 1.36 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5356983246846245		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.5356983246846245 | validation: 0.49447963317613763]
	TIME [epoch: 1.36 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46363988672375533		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.46363988672375533 | validation: 0.6063570701240528]
	TIME [epoch: 1.36 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43197539750432795		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.43197539750432795 | validation: 0.4659172791754458]
	TIME [epoch: 1.36 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3751342243593898		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.3751342243593898 | validation: 0.45799493438926053]
	TIME [epoch: 1.36 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35657393928558917		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.35657393928558917 | validation: 0.48602202874180644]
	TIME [epoch: 1.36 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3528983147115163		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.3528983147115163 | validation: 0.38984523274228233]
	TIME [epoch: 1.36 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3694983812071355		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.3694983812071355 | validation: 0.6106976343808801]
	TIME [epoch: 1.36 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4226939988277711		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.4226939988277711 | validation: 0.4180261078667217]
	TIME [epoch: 1.36 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5608107556161939		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.5608107556161939 | validation: 0.8658499774232479]
	TIME [epoch: 1.36 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5976533264190793		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.5976533264190793 | validation: 0.38912619267884874]
	TIME [epoch: 1.36 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3463271438145338		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.3463271438145338 | validation: 0.3960531177961514]
	TIME [epoch: 1.36 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3723961169414211		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.3723961169414211 | validation: 0.7124579616475928]
	TIME [epoch: 1.36 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46853056673415167		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.46853056673415167 | validation: 0.39847969245706905]
	TIME [epoch: 1.36 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45109015183094076		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.45109015183094076 | validation: 0.6939483097140658]
	TIME [epoch: 1.36 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4902808219382992		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.4902808219382992 | validation: 0.3696329530519674]
	TIME [epoch: 1.36 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3573788821654624		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.3573788821654624 | validation: 0.43547430893635775]
	TIME [epoch: 1.36 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37076995570786525		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.37076995570786525 | validation: 0.5854059514252863]
	TIME [epoch: 1.36 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4294957996278157		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.4294957996278157 | validation: 0.49822366140168467]
	TIME [epoch: 1.36 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.438276368869324		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.438276368869324 | validation: 0.5019054579121471]
	TIME [epoch: 1.37 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4707319697857365		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.4707319697857365 | validation: 0.49291330330323135]
	TIME [epoch: 1.36 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36377694233604163		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.36377694233604163 | validation: 0.3641861048330415]
	TIME [epoch: 1.36 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3780552738799372		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.3780552738799372 | validation: 0.616631532952804]
	TIME [epoch: 1.36 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4084283892179057		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.4084283892179057 | validation: 0.3961673730857513]
	TIME [epoch: 1.36 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3852832665125688		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.3852832665125688 | validation: 0.5864556857991386]
	TIME [epoch: 1.36 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37350164185966284		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.37350164185966284 | validation: 0.367284494794171]
	TIME [epoch: 1.36 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34779344727581213		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.34779344727581213 | validation: 0.5450037990533997]
	TIME [epoch: 1.36 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37017961096486546		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.37017961096486546 | validation: 0.34520326228763276]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_449.pth
	Model improved!!!
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37113109400515376		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.37113109400515376 | validation: 0.6108838740660775]
	TIME [epoch: 1.36 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40223818911851583		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.40223818911851583 | validation: 0.3709166377358172]
	TIME [epoch: 1.36 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3716554272044037		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.3716554272044037 | validation: 0.5786962580870093]
	TIME [epoch: 1.36 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36382841341405864		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.36382841341405864 | validation: 0.3850424515343933]
	TIME [epoch: 2.02 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33781419092571036		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.33781419092571036 | validation: 0.5433915543872707]
	TIME [epoch: 1.35 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3535781827999576		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.3535781827999576 | validation: 0.3999707118984695]
	TIME [epoch: 1.36 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35211965353129926		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.35211965353129926 | validation: 0.5690638021643839]
	TIME [epoch: 1.36 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3945699708796462		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.3945699708796462 | validation: 0.49878991368730025]
	TIME [epoch: 1.36 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42708705722547335		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.42708705722547335 | validation: 0.4076074399018418]
	TIME [epoch: 1.35 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41098353213996275		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.41098353213996275 | validation: 0.5825179200492386]
	TIME [epoch: 1.36 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3882016150493523		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.3882016150493523 | validation: 0.32968094178122476]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_460.pth
	Model improved!!!
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34252587267447254		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.34252587267447254 | validation: 0.5609227030620918]
	TIME [epoch: 1.36 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.360099870189443		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.360099870189443 | validation: 0.34983796397949485]
	TIME [epoch: 1.37 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3530488225931467		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.3530488225931467 | validation: 0.5602365086279149]
	TIME [epoch: 1.36 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36195700751583204		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.36195700751583204 | validation: 0.3633924640687983]
	TIME [epoch: 1.36 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3137974565950733		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.3137974565950733 | validation: 0.5228766805934199]
	TIME [epoch: 1.36 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3247024381402893		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.3247024381402893 | validation: 0.3710119488813815]
	TIME [epoch: 1.36 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3427277515898291		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.3427277515898291 | validation: 0.5435338096546819]
	TIME [epoch: 1.35 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33526834404743133		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.33526834404743133 | validation: 0.3748460949423441]
	TIME [epoch: 1.36 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32009049408504653		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.32009049408504653 | validation: 0.5565392336617256]
	TIME [epoch: 1.35 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.358750039280827		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.358750039280827 | validation: 0.3384597893329447]
	TIME [epoch: 1.36 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33547103897820285		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.33547103897820285 | validation: 0.6510044860571425]
	TIME [epoch: 1.36 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41980775010550775		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.41980775010550775 | validation: 0.332514794002804]
	TIME [epoch: 1.35 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33454843837215675		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.33454843837215675 | validation: 0.4460151934146394]
	TIME [epoch: 1.36 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32051344257487674		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.32051344257487674 | validation: 0.3512920445077663]
	TIME [epoch: 1.35 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3657649731217633		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.3657649731217633 | validation: 0.4574458705616866]
	TIME [epoch: 1.35 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35240842542697265		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.35240842542697265 | validation: 0.4761637128381934]
	TIME [epoch: 1.35 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37289994234853757		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.37289994234853757 | validation: 0.4219071866441437]
	TIME [epoch: 1.35 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33864615326006253		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.33864615326006253 | validation: 0.6381228219180497]
	TIME [epoch: 1.36 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.415365451327053		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.415365451327053 | validation: 0.31574236085706703]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_479.pth
	Model improved!!!
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2918287534636522		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.2918287534636522 | validation: 0.4392365101770425]
	TIME [epoch: 1.36 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2904912910188337		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.2904912910188337 | validation: 0.32786702007263363]
	TIME [epoch: 1.36 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3275061529782262		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.3275061529782262 | validation: 0.40938406462632726]
	TIME [epoch: 1.35 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2792293779801429		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.2792293779801429 | validation: 0.32693078034787715]
	TIME [epoch: 1.36 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31749186067234264		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.31749186067234264 | validation: 0.39877521146453926]
	TIME [epoch: 1.35 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2752018151963924		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.2752018151963924 | validation: 0.3859178415037332]
	TIME [epoch: 1.35 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2841247010981049		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.2841247010981049 | validation: 0.5046960012235806]
	TIME [epoch: 1.35 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3343792526538147		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.3343792526538147 | validation: 0.4801811371181923]
	TIME [epoch: 1.35 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37291249396705517		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.37291249396705517 | validation: 0.39249908213294815]
	TIME [epoch: 1.36 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3381679395653493		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.3381679395653493 | validation: 0.6997902511143211]
	TIME [epoch: 1.35 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.463077704874315		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.463077704874315 | validation: 0.3411439806756804]
	TIME [epoch: 1.35 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28739316545047305		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.28739316545047305 | validation: 0.3761492369436815]
	TIME [epoch: 1.35 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2681465061716098		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.2681465061716098 | validation: 0.33492331761813465]
	TIME [epoch: 1.35 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24777301246413688		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.24777301246413688 | validation: 0.40824537789577253]
	TIME [epoch: 1.36 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27306120614002743		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.27306120614002743 | validation: 0.28918314379355464]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_494.pth
	Model improved!!!
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3486576961334778		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.3486576961334778 | validation: 0.4989118709426393]
	TIME [epoch: 1.35 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3659241509407541		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.3659241509407541 | validation: 0.27749353754864325]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_496.pth
	Model improved!!!
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2912040781394401		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.2912040781394401 | validation: 0.3800672036069136]
	TIME [epoch: 1.36 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2468259713639464		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.2468259713639464 | validation: 0.31500368788341815]
	TIME [epoch: 1.35 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2423616300551668		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.2423616300551668 | validation: 0.41401611075267336]
	TIME [epoch: 1.36 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2658957161559248		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.2658957161559248 | validation: 0.5760474848747834]
	TIME [epoch: 1.35 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4346155599015464		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.4346155599015464 | validation: 0.5837451751678814]
	TIME [epoch: 181 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4155423949309941		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.4155423949309941 | validation: 0.33755104763576516]
	TIME [epoch: 2.69 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3364212163730512		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.3364212163730512 | validation: 0.6894089199531579]
	TIME [epoch: 2.69 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48231511893833173		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.48231511893833173 | validation: 0.4737235376615502]
	TIME [epoch: 2.68 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3849632254263095		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.3849632254263095 | validation: 0.32908773132495006]
	TIME [epoch: 2.68 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34140944433949444		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.34140944433949444 | validation: 0.5997734620542989]
	TIME [epoch: 2.68 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40017624655805906		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.40017624655805906 | validation: 0.3209911017657284]
	TIME [epoch: 2.68 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2711472885497851		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.2711472885497851 | validation: 0.4234223314853573]
	TIME [epoch: 2.68 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2667400061509574		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.2667400061509574 | validation: 0.3763658874158109]
	TIME [epoch: 2.68 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2518888337488341		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.2518888337488341 | validation: 0.31380077586039956]
	TIME [epoch: 2.68 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24634725077876773		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.24634725077876773 | validation: 0.39604450053228973]
	TIME [epoch: 2.68 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25861332103427515		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.25861332103427515 | validation: 0.36489947870480227]
	TIME [epoch: 2.68 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26900779215917103		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.26900779215917103 | validation: 0.4107296012606976]
	TIME [epoch: 2.68 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29954788620085937		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.29954788620085937 | validation: 0.38867907765289306]
	TIME [epoch: 2.68 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2968569362615378		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.2968569362615378 | validation: 0.3704766749864084]
	TIME [epoch: 2.68 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29462152055709717		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.29462152055709717 | validation: 0.39598808373266875]
	TIME [epoch: 2.68 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2506185702508141		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.2506185702508141 | validation: 0.2905847208483198]
	TIME [epoch: 2.68 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25456910846177655		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.25456910846177655 | validation: 0.40772161119383477]
	TIME [epoch: 2.68 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27210827495903156		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.27210827495903156 | validation: 0.2995386957017578]
	TIME [epoch: 2.68 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3830583184880325		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.3830583184880325 | validation: 0.6520001660936985]
	TIME [epoch: 2.68 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38081935294324243		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.38081935294324243 | validation: 0.3462132389021145]
	TIME [epoch: 2.68 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27013021563258455		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.27013021563258455 | validation: 0.3162124266603421]
	TIME [epoch: 2.68 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25812407999965464		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.25812407999965464 | validation: 0.3755933269340596]
	TIME [epoch: 2.68 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2702408323848033		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.2702408323848033 | validation: 0.3154658459719424]
	TIME [epoch: 2.68 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2565326224100465		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.2565326224100465 | validation: 0.328429112998328]
	TIME [epoch: 2.68 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22919865865130362		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.22919865865130362 | validation: 0.4038110004414973]
	TIME [epoch: 2.68 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24468638485191718		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.24468638485191718 | validation: 0.3546685503448266]
	TIME [epoch: 2.68 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3029904186183591		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.3029904186183591 | validation: 0.5875181049072701]
	TIME [epoch: 2.68 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35022522368412085		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.35022522368412085 | validation: 0.28317785833262343]
	TIME [epoch: 2.68 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2538364083210837		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.2538364083210837 | validation: 0.39095291339943766]
	TIME [epoch: 2.68 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2358962471570462		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.2358962471570462 | validation: 0.2793942416434222]
	TIME [epoch: 2.68 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23963969623513806		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.23963969623513806 | validation: 0.34943678902434083]
	TIME [epoch: 2.68 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23751879244269142		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.23751879244269142 | validation: 0.27628767410581623]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_533.pth
	Model improved!!!
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.266228338716602		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.266228338716602 | validation: 0.37706720688965983]
	TIME [epoch: 2.68 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25671335720414484		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.25671335720414484 | validation: 0.3263689775770888]
	TIME [epoch: 2.68 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27030210336180455		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.27030210336180455 | validation: 0.45635281533777355]
	TIME [epoch: 2.69 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33476198781204225		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.33476198781204225 | validation: 0.5903241325184886]
	TIME [epoch: 2.68 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38167459720572433		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.38167459720572433 | validation: 0.2973473177279324]
	TIME [epoch: 2.68 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23537098755203126		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.23537098755203126 | validation: 0.3805088626527566]
	TIME [epoch: 2.68 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23337072724337227		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.23337072724337227 | validation: 0.258618791690052]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_540.pth
	Model improved!!!
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24686750816912756		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.24686750816912756 | validation: 0.4006554221201801]
	TIME [epoch: 2.68 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2560415446162427		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.2560415446162427 | validation: 0.27627903846563134]
	TIME [epoch: 2.68 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2602855293865852		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.2602855293865852 | validation: 0.4439866305118969]
	TIME [epoch: 2.68 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27394183130571514		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.27394183130571514 | validation: 0.30598459331996253]
	TIME [epoch: 2.68 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2613953236294064		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.2613953236294064 | validation: 0.4368812429843675]
	TIME [epoch: 2.68 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26120872054417477		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.26120872054417477 | validation: 0.30265420402298676]
	TIME [epoch: 2.68 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23865741305935437		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.23865741305935437 | validation: 0.4006722719458886]
	TIME [epoch: 2.69 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2390052678306526		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.2390052678306526 | validation: 0.26659027658241913]
	TIME [epoch: 2.68 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2629251649786017		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.2629251649786017 | validation: 0.4736032954207911]
	TIME [epoch: 2.68 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29412798972927223		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.29412798972927223 | validation: 0.26595252666118635]
	TIME [epoch: 2.68 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2494515209894521		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.2494515209894521 | validation: 0.310230799217812]
	TIME [epoch: 2.68 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21420425410103938		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.21420425410103938 | validation: 0.3318806178101239]
	TIME [epoch: 2.68 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22805354498274127		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.22805354498274127 | validation: 0.3522136793373168]
	TIME [epoch: 2.68 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26943409994206746		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.26943409994206746 | validation: 0.41705990662491166]
	TIME [epoch: 2.67 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3117456587782584		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.3117456587782584 | validation: 0.3564989119320108]
	TIME [epoch: 2.68 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24357480396215267		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.24357480396215267 | validation: 0.2762290974115465]
	TIME [epoch: 2.67 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22923387279071938		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.22923387279071938 | validation: 0.32942943563202487]
	TIME [epoch: 2.68 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21842252175404384		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.21842252175404384 | validation: 0.31604979934672184]
	TIME [epoch: 2.68 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24373552558417622		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.24373552558417622 | validation: 0.3513555369953767]
	TIME [epoch: 2.68 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26053821042537334		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.26053821042537334 | validation: 0.4364364604204276]
	TIME [epoch: 2.68 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29638495069833076		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.29638495069833076 | validation: 0.30694345076078816]
	TIME [epoch: 2.68 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22573083767059207		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.22573083767059207 | validation: 0.3225064463432326]
	TIME [epoch: 2.68 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2076932832764454		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.2076932832764454 | validation: 0.2718559333440382]
	TIME [epoch: 2.68 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21235672056761595		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.21235672056761595 | validation: 0.4178529358003588]
	TIME [epoch: 2.68 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25428709641988884		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.25428709641988884 | validation: 0.25967293779692086]
	TIME [epoch: 2.68 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2714053664123909		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.2714053664123909 | validation: 0.5263415928010815]
	TIME [epoch: 2.68 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.324290432815699		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.324290432815699 | validation: 0.25725934568699993]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_567.pth
	Model improved!!!
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20962067942635507		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.20962067942635507 | validation: 0.2616631971646487]
	TIME [epoch: 2.67 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19355645021028203		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.19355645021028203 | validation: 0.31873437951918815]
	TIME [epoch: 2.68 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1972083103712877		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.1972083103712877 | validation: 0.308259540142296]
	TIME [epoch: 2.68 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28783227801030736		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.28783227801030736 | validation: 0.5099933116261613]
	TIME [epoch: 2.68 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30667967575874083		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.30667967575874083 | validation: 0.2644887407140858]
	TIME [epoch: 2.68 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2078606022827357		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.2078606022827357 | validation: 0.2777490247030252]
	TIME [epoch: 2.68 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20752484102803664		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.20752484102803664 | validation: 0.3142681501998916]
	TIME [epoch: 2.68 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25355320103710566		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.25355320103710566 | validation: 0.398053935130985]
	TIME [epoch: 2.68 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26850372480357715		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.26850372480357715 | validation: 0.28778696386889563]
	TIME [epoch: 2.68 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22900088573139166		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.22900088573139166 | validation: 0.27742937126779127]
	TIME [epoch: 2.68 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19865756276390203		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.19865756276390203 | validation: 0.2823064636881812]
	TIME [epoch: 2.68 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19608569497471529		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.19608569497471529 | validation: 0.2829502675712872]
	TIME [epoch: 2.67 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19938837533846593		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.19938837533846593 | validation: 0.29501628157717963]
	TIME [epoch: 2.68 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22691776861523916		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.22691776861523916 | validation: 0.34753688052626985]
	TIME [epoch: 2.68 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26114424091163946		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.26114424091163946 | validation: 0.3511773816467134]
	TIME [epoch: 2.68 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2553116201304794		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.2553116201304794 | validation: 0.30254327551422994]
	TIME [epoch: 2.67 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20127522415353605		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.20127522415353605 | validation: 0.23591158337406903]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_584.pth
	Model improved!!!
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20395411768676133		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.20395411768676133 | validation: 0.32563424998238066]
	TIME [epoch: 2.68 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20436939051338374		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.20436939051338374 | validation: 0.22011544180705167]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_586.pth
	Model improved!!!
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20804841398902923		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.20804841398902923 | validation: 0.42163777942446123]
	TIME [epoch: 2.68 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24376216849168877		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.24376216849168877 | validation: 0.2935847939506184]
	TIME [epoch: 2.67 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2768283540206712		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.2768283540206712 | validation: 0.5301749388431485]
	TIME [epoch: 2.68 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2938591470309907		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.2938591470309907 | validation: 0.2540211788249511]
	TIME [epoch: 2.69 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25611142245859136		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.25611142245859136 | validation: 0.2804910774437516]
	TIME [epoch: 2.68 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17466136934440865		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.17466136934440865 | validation: 0.2906155223147097]
	TIME [epoch: 2.67 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17875213726378533		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.17875213726378533 | validation: 0.23897992930083004]
	TIME [epoch: 2.67 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16855166980546904		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.16855166980546904 | validation: 0.2589189750805076]
	TIME [epoch: 2.68 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17729880442896978		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.17729880442896978 | validation: 0.22801175654628986]
	TIME [epoch: 2.67 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20849893958751348		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.20849893958751348 | validation: 0.34740733827589176]
	TIME [epoch: 2.67 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2418451869493033		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.2418451869493033 | validation: 0.3087971928081514]
	TIME [epoch: 2.67 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25762722945892497		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.25762722945892497 | validation: 0.33171157262137396]
	TIME [epoch: 2.67 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24325220793873037		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.24325220793873037 | validation: 0.392889652192969]
	TIME [epoch: 2.67 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23292924690643954		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.23292924690643954 | validation: 0.2575299477047516]
	TIME [epoch: 2.67 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19068419867648267		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.19068419867648267 | validation: 0.3042269096586885]
	TIME [epoch: 2.68 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18181837249072408		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.18181837249072408 | validation: 0.26528195694885304]
	TIME [epoch: 2.68 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18743100675705368		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.18743100675705368 | validation: 0.3492151962228119]
	TIME [epoch: 2.68 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20345164951447622		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.20345164951447622 | validation: 0.24501196981361437]
	TIME [epoch: 2.67 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19700743182707434		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.19700743182707434 | validation: 0.342983289622836]
	TIME [epoch: 2.67 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2015714020312796		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.2015714020312796 | validation: 0.22531205606451749]
	TIME [epoch: 2.67 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18963406312857511		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.18963406312857511 | validation: 0.461832701169164]
	TIME [epoch: 2.67 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2834615923724941		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.2834615923724941 | validation: 0.23509099205087144]
	TIME [epoch: 2.68 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2163304838200557		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.2163304838200557 | validation: 0.24910250208977136]
	TIME [epoch: 2.68 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20361851156136013		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.20361851156136013 | validation: 0.28280703222294046]
	TIME [epoch: 2.67 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2013011770062424		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.2013011770062424 | validation: 0.3296192509948016]
	TIME [epoch: 2.67 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23127050061390025		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.23127050061390025 | validation: 0.3096995137908282]
	TIME [epoch: 2.68 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2259689713632336		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.2259689713632336 | validation: 0.27653629733942464]
	TIME [epoch: 2.67 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1995217479435488		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.1995217479435488 | validation: 0.2294691230161804]
	TIME [epoch: 2.68 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1783505201665519		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.1783505201665519 | validation: 0.24478067911699625]
	TIME [epoch: 2.67 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17461672110978235		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.17461672110978235 | validation: 0.2561760982762915]
	TIME [epoch: 2.68 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17450858652241294		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.17450858652241294 | validation: 0.2904200228722229]
	TIME [epoch: 2.67 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20864401921885936		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.20864401921885936 | validation: 0.3873237786242172]
	TIME [epoch: 2.67 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2857372577704653		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.2857372577704653 | validation: 0.24441629788832595]
	TIME [epoch: 2.67 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1849656730479515		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.1849656730479515 | validation: 0.2881734714006268]
	TIME [epoch: 2.67 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18135101682378704		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.18135101682378704 | validation: 0.2301470825636896]
	TIME [epoch: 2.68 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18663046504721656		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.18663046504721656 | validation: 0.3475255772203736]
	TIME [epoch: 2.68 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21499621228509985		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.21499621228509985 | validation: 0.20913293439284583]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_623.pth
	Model improved!!!
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2206795633115602		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.2206795633115602 | validation: 0.3657374479313781]
	TIME [epoch: 2.67 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20742301353511386		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.20742301353511386 | validation: 0.2222125762676973]
	TIME [epoch: 2.68 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15610396179865083		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.15610396179865083 | validation: 0.2344026651219934]
	TIME [epoch: 2.68 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15493954912775562		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.15493954912775562 | validation: 0.27429630120735404]
	TIME [epoch: 2.68 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15715123720899787		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.15715123720899787 | validation: 0.2135285923840148]
	TIME [epoch: 2.68 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16301050332000522		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.16301050332000522 | validation: 0.28078483000995413]
	TIME [epoch: 2.68 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18992019739653976		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.18992019739653976 | validation: 0.22767156463061122]
	TIME [epoch: 2.68 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22649382477860244		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.22649382477860244 | validation: 0.30718882675460146]
	TIME [epoch: 2.68 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19024799746379903		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.19024799746379903 | validation: 0.2525820283721578]
	TIME [epoch: 2.68 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1992797985230038		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.1992797985230038 | validation: 0.26171079872148323]
	TIME [epoch: 2.68 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22141865048884363		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.22141865048884363 | validation: 0.44107935651201036]
	TIME [epoch: 2.69 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2507424837753316		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.2507424837753316 | validation: 0.26655769784129296]
	TIME [epoch: 2.68 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24534910952064182		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.24534910952064182 | validation: 0.2170176586631059]
	TIME [epoch: 2.68 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19820880522880488		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.19820880522880488 | validation: 0.42417658681754516]
	TIME [epoch: 2.68 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24589551425984746		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.24589551425984746 | validation: 0.2562442973008042]
	TIME [epoch: 2.68 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.170262170757412		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.170262170757412 | validation: 0.23229514540424756]
	TIME [epoch: 2.68 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.158846871776853		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.158846871776853 | validation: 0.2082233515611412]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_640.pth
	Model improved!!!
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15631818537027442		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.15631818537027442 | validation: 0.27417904346290056]
	TIME [epoch: 2.69 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16649868041373048		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.16649868041373048 | validation: 0.20522632814266284]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_642.pth
	Model improved!!!
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17447275019884115		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.17447275019884115 | validation: 0.2480002888503604]
	TIME [epoch: 2.69 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18462888507157352		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.18462888507157352 | validation: 0.31687809807881184]
	TIME [epoch: 2.69 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22377217211964542		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.22377217211964542 | validation: 0.27837204884071926]
	TIME [epoch: 2.7 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20828581949107786		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.20828581949107786 | validation: 0.3152012086952714]
	TIME [epoch: 2.69 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1840877639962268		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.1840877639962268 | validation: 0.21068641610056027]
	TIME [epoch: 2.69 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15692098411694208		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.15692098411694208 | validation: 0.2281265443991467]
	TIME [epoch: 2.69 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15735668491978236		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.15735668491978236 | validation: 0.1921858130873867]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_649.pth
	Model improved!!!
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1674340435767747		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.1674340435767747 | validation: 0.33577378232673]
	TIME [epoch: 2.67 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19003941252873763		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.19003941252873763 | validation: 0.19388199254870248]
	TIME [epoch: 2.67 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16532146177858204		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.16532146177858204 | validation: 0.28561263429976896]
	TIME [epoch: 2.67 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1732193012574494		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.1732193012574494 | validation: 0.23080058984381713]
	TIME [epoch: 2.67 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17744688406630002		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.17744688406630002 | validation: 0.33327581135615597]
	TIME [epoch: 2.67 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18714346265041834		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.18714346265041834 | validation: 0.2411070188912471]
	TIME [epoch: 2.67 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19492202091653407		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.19492202091653407 | validation: 0.28363534409159713]
	TIME [epoch: 2.68 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19375491291448282		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.19375491291448282 | validation: 0.27794189747644593]
	TIME [epoch: 2.67 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2013710625790808		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.2013710625790808 | validation: 0.19606578144931863]
	TIME [epoch: 2.67 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18714009952534036		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.18714009952534036 | validation: 0.2438137080709274]
	TIME [epoch: 2.67 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15258301529308768		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.15258301529308768 | validation: 0.18934137789809716]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_660.pth
	Model improved!!!
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13968409560335474		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.13968409560335474 | validation: 0.20630278854080475]
	TIME [epoch: 2.69 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14037383179897542		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.14037383179897542 | validation: 0.24579992051088648]
	TIME [epoch: 2.69 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15413207918430644		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.15413207918430644 | validation: 0.2412677804790977]
	TIME [epoch: 2.69 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17953151617016574		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.17953151617016574 | validation: 0.37053035463805495]
	TIME [epoch: 2.69 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22671948474615966		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.22671948474615966 | validation: 0.2412739728175681]
	TIME [epoch: 2.69 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17340943990234167		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.17340943990234167 | validation: 0.19023375963823186]
	TIME [epoch: 2.69 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17589058498757432		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.17589058498757432 | validation: 0.2533949571727043]
	TIME [epoch: 2.7 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1641945544171822		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.1641945544171822 | validation: 0.17950363430003857]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_668.pth
	Model improved!!!
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15661009443056867		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.15661009443056867 | validation: 0.25596834315847783]
	TIME [epoch: 2.68 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1523368538882485		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.1523368538882485 | validation: 0.18543558395967216]
	TIME [epoch: 2.68 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15299349057795755		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.15299349057795755 | validation: 0.3353187475547394]
	TIME [epoch: 2.68 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19543590527907787		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.19543590527907787 | validation: 0.2397185312587621]
	TIME [epoch: 2.67 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1810255486099916		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.1810255486099916 | validation: 0.25062591001773993]
	TIME [epoch: 2.68 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15522924236053523		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.15522924236053523 | validation: 0.18899362714458834]
	TIME [epoch: 2.68 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15554867605259945		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.15554867605259945 | validation: 0.34676631970794514]
	TIME [epoch: 2.67 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19278737689735775		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.19278737689735775 | validation: 0.2003594690409798]
	TIME [epoch: 2.68 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1544492146386065		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.1544492146386065 | validation: 0.24587088500607052]
	TIME [epoch: 2.68 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15745264679757623		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.15745264679757623 | validation: 0.19138457436889067]
	TIME [epoch: 2.69 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1463633042213347		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.1463633042213347 | validation: 0.22017424879146127]
	TIME [epoch: 2.67 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13759552813911466		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.13759552813911466 | validation: 0.18063759815900973]
	TIME [epoch: 2.68 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1463265203270338		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.1463265203270338 | validation: 0.3062185073513375]
	TIME [epoch: 2.67 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16888915180633557		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.16888915180633557 | validation: 0.19777607002207134]
	TIME [epoch: 2.68 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15325661225326223		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.15325661225326223 | validation: 0.21974913303408813]
	TIME [epoch: 2.67 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15992387001454691		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.15992387001454691 | validation: 0.22297530795766418]
	TIME [epoch: 2.68 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16178380460923447		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.16178380460923447 | validation: 0.2317035699231278]
	TIME [epoch: 2.67 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2008140790236794		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.2008140790236794 | validation: 0.2934683800370945]
	TIME [epoch: 2.68 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22939739875058002		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.22939739875058002 | validation: 0.26018345377321933]
	TIME [epoch: 2.68 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16499332169276681		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.16499332169276681 | validation: 0.19480242807130188]
	TIME [epoch: 2.68 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13104867665677056		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.13104867665677056 | validation: 0.16726697647729097]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_689.pth
	Model improved!!!
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1378604639997111		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.1378604639997111 | validation: 0.20515341037031548]
	TIME [epoch: 2.69 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1365814025820437		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.1365814025820437 | validation: 0.18149942571232644]
	TIME [epoch: 2.69 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16004938586703318		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.16004938586703318 | validation: 0.21986484704418682]
	TIME [epoch: 2.68 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1459315611223897		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.1459315611223897 | validation: 0.20889308729705994]
	TIME [epoch: 2.69 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15052253176043318		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.15052253176043318 | validation: 0.2530320233157067]
	TIME [epoch: 2.68 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19551267718359608		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.19551267718359608 | validation: 0.3458598925980139]
	TIME [epoch: 2.69 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.200125211327763		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.200125211327763 | validation: 0.18870200706312265]
	TIME [epoch: 2.69 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12819298379206306		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.12819298379206306 | validation: 0.17440265079412387]
	TIME [epoch: 2.68 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13402259992550064		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.13402259992550064 | validation: 0.2105313639458859]
	TIME [epoch: 2.68 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1320822805094849		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.1320822805094849 | validation: 0.1826576545980837]
	TIME [epoch: 2.67 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13323997805521337		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.13323997805521337 | validation: 0.2686860376822751]
	TIME [epoch: 2.69 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16180604498687656		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.16180604498687656 | validation: 0.19714051235960953]
	TIME [epoch: 2.69 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1726673799680298		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.1726673799680298 | validation: 0.30564385331356914]
	TIME [epoch: 2.68 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.193645724978553		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.193645724978553 | validation: 0.18404020196031984]
	TIME [epoch: 2.69 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16545774069207433		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.16545774069207433 | validation: 0.19534526063630653]
	TIME [epoch: 2.68 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1499354070034687		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.1499354070034687 | validation: 0.24968634516901045]
	TIME [epoch: 2.69 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1681872372482372		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.1681872372482372 | validation: 0.23206115473193717]
	TIME [epoch: 2.68 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1595142175557882		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.1595142175557882 | validation: 0.16768916397013145]
	TIME [epoch: 2.69 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1487956759980975		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.1487956759980975 | validation: 0.203106145499065]
	TIME [epoch: 2.69 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13699169662835117		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.13699169662835117 | validation: 0.18997740256389006]
	TIME [epoch: 2.69 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13216605830430933		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.13216605830430933 | validation: 0.1919317893098523]
	TIME [epoch: 2.69 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13081886974849918		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.13081886974849918 | validation: 0.21908744379163353]
	TIME [epoch: 2.69 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1499528222124786		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.1499528222124786 | validation: 0.2274530464020967]
	TIME [epoch: 2.69 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1707150440886965		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.1707150440886965 | validation: 0.20543401665166822]
	TIME [epoch: 2.69 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1646203543236733		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.1646203543236733 | validation: 0.18691077734402103]
	TIME [epoch: 2.69 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1363909307943218		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.1363909307943218 | validation: 0.19268627053981427]
	TIME [epoch: 2.69 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12855058606674466		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.12855058606674466 | validation: 0.1675280309549459]
	TIME [epoch: 2.69 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12627719724810604		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.12627719724810604 | validation: 0.22497422598007688]
	TIME [epoch: 2.69 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14108001966416184		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.14108001966416184 | validation: 0.21131464836149308]
	TIME [epoch: 2.68 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21098836636001467		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.21098836636001467 | validation: 0.3179066995763158]
	TIME [epoch: 2.69 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18426789111044944		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.18426789111044944 | validation: 0.22566340676128985]
	TIME [epoch: 2.68 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13797052839851884		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.13797052839851884 | validation: 0.17659866842082042]
	TIME [epoch: 2.69 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1284431659473505		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.1284431659473505 | validation: 0.18120692248219664]
	TIME [epoch: 2.69 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12655825617091584		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.12655825617091584 | validation: 0.21360111403524884]
	TIME [epoch: 2.69 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1344934305522073		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.1344934305522073 | validation: 0.17104951450751435]
	TIME [epoch: 2.69 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12975389237183457		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.12975389237183457 | validation: 0.19360511076904066]
	TIME [epoch: 2.69 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1445728377033183		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.1445728377033183 | validation: 0.2051582524624796]
	TIME [epoch: 2.69 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1575697215757161		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.1575697215757161 | validation: 0.2173320264176419]
	TIME [epoch: 2.69 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13943722331625347		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.13943722331625347 | validation: 0.2000026206137347]
	TIME [epoch: 2.69 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13836922008202476		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.13836922008202476 | validation: 0.18596335390601493]
	TIME [epoch: 2.69 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13334943865236995		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.13334943865236995 | validation: 0.20176754569153427]
	TIME [epoch: 2.69 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12458404404727265		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.12458404404727265 | validation: 0.1903178315500681]
	TIME [epoch: 2.68 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12185786073464117		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.12185786073464117 | validation: 0.14854811402680276]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_732.pth
	Model improved!!!
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1299641956987282		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.1299641956987282 | validation: 0.23142903157837236]
	TIME [epoch: 2.68 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1493044285502786		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.1493044285502786 | validation: 0.1501941597991724]
	TIME [epoch: 2.69 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1455524591088508		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.1455524591088508 | validation: 0.1829264097602178]
	TIME [epoch: 2.69 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12474931679281742		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.12474931679281742 | validation: 0.1714034357968027]
	TIME [epoch: 2.69 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11294746777265291		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.11294746777265291 | validation: 0.18148515844343638]
	TIME [epoch: 2.69 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1150452795801428		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.1150452795801428 | validation: 0.21012597998610283]
	TIME [epoch: 2.7 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16784758489646365		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.16784758489646365 | validation: 0.38211913403831016]
	TIME [epoch: 2.7 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21484266295540763		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.21484266295540763 | validation: 0.179866781720805]
	TIME [epoch: 2.7 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11517403809864923		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.11517403809864923 | validation: 0.1438188371143023]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_741.pth
	Model improved!!!
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12689349755943444		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.12689349755943444 | validation: 0.22979141851625984]
	TIME [epoch: 2.7 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13500821448872302		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.13500821448872302 | validation: 0.20753020306369097]
	TIME [epoch: 2.71 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1625361662962764		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.1625361662962764 | validation: 0.2551649978069384]
	TIME [epoch: 2.7 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14974584988748738		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.14974584988748738 | validation: 0.1448688638970681]
	TIME [epoch: 2.7 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11437646130569991		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.11437646130569991 | validation: 0.15522452512805185]
	TIME [epoch: 2.7 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11083077457491594		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.11083077457491594 | validation: 0.17426395519469756]
	TIME [epoch: 2.71 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12090472523717828		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.12090472523717828 | validation: 0.21801915220904236]
	TIME [epoch: 2.7 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15440871669955777		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.15440871669955777 | validation: 0.22458704114381495]
	TIME [epoch: 2.7 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1709943207190906		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.1709943207190906 | validation: 0.1771782030380032]
	TIME [epoch: 2.7 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12415866381763098		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.12415866381763098 | validation: 0.16688711278264376]
	TIME [epoch: 2.71 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11220788934478901		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.11220788934478901 | validation: 0.15447229681445937]
	TIME [epoch: 2.7 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10913305357206733		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.10913305357206733 | validation: 0.17559413817881114]
	TIME [epoch: 2.7 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11068920535774054		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.11068920535774054 | validation: 0.15952686894757362]
	TIME [epoch: 2.72 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11849572591937481		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.11849572591937481 | validation: 0.20794260180586732]
	TIME [epoch: 2.7 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13106237113737898		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.13106237113737898 | validation: 0.17818782985043324]
	TIME [epoch: 2.7 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12987361305450154		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.12987361305450154 | validation: 0.2147855063076848]
	TIME [epoch: 2.7 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13430006941042957		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.13430006941042957 | validation: 0.18254591722264946]
	TIME [epoch: 2.71 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14803064681171715		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.14803064681171715 | validation: 0.1975316843852041]
	TIME [epoch: 2.7 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15158653540216363		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.15158653540216363 | validation: 0.187698138889823]
	TIME [epoch: 2.7 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12390825785772176		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.12390825785772176 | validation: 0.14766774178015366]
	TIME [epoch: 2.7 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13797540106990042		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.13797540106990042 | validation: 0.2086573943058646]
	TIME [epoch: 2.7 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13049069106114133		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.13049069106114133 | validation: 0.1549278655645329]
	TIME [epoch: 2.7 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11700868003811156		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.11700868003811156 | validation: 0.1668788741327314]
	TIME [epoch: 2.7 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11490938133160312		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.11490938133160312 | validation: 0.15065823215489194]
	TIME [epoch: 2.71 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11070422471614379		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.11070422471614379 | validation: 0.19209766507876105]
	TIME [epoch: 2.7 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11952908752769954		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.11952908752769954 | validation: 0.17143019076397936]
	TIME [epoch: 2.7 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12508399559159383		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.12508399559159383 | validation: 0.24138680763283027]
	TIME [epoch: 2.7 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14662684599893255		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.14662684599893255 | validation: 0.17792308038035173]
	TIME [epoch: 2.7 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12490834429781429		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.12490834429781429 | validation: 0.1485116739908503]
	TIME [epoch: 2.7 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12789126370733492		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.12789126370733492 | validation: 0.17975690268736885]
	TIME [epoch: 2.7 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13238278811423745		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.13238278811423745 | validation: 0.1449354221342717]
	TIME [epoch: 2.7 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1183058049737278		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.1183058049737278 | validation: 0.16581730578357262]
	TIME [epoch: 2.7 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11177276135643467		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.11177276135643467 | validation: 0.1704645915903806]
	TIME [epoch: 2.7 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11905898969939407		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.11905898969939407 | validation: 0.18762233065680375]
	TIME [epoch: 2.7 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13284762165850858		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.13284762165850858 | validation: 0.2028062291175422]
	TIME [epoch: 2.71 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1355278360630474		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.1355278360630474 | validation: 0.187737331596474]
	TIME [epoch: 2.7 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14756655667294474		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.14756655667294474 | validation: 0.17038013218159456]
	TIME [epoch: 2.7 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10786100081562333		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.10786100081562333 | validation: 0.15549369063981494]
	TIME [epoch: 2.7 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10836215963664167		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.10836215963664167 | validation: 0.15842096286400964]
	TIME [epoch: 2.7 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10633454281034073		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.10633454281034073 | validation: 0.1364494689664276]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_781.pth
	Model improved!!!
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11060002834503926		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.11060002834503926 | validation: 0.18659648335771092]
	TIME [epoch: 2.7 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12189099452253363		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.12189099452253363 | validation: 0.12796279765751373]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_783.pth
	Model improved!!!
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12285696928758942		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.12285696928758942 | validation: 0.1776875866735396]
	TIME [epoch: 2.7 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11655487339204589		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.11655487339204589 | validation: 0.16563119542182433]
	TIME [epoch: 2.7 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11802039276371261		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.11802039276371261 | validation: 0.1713430872560574]
	TIME [epoch: 2.71 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.117064741416091		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.117064741416091 | validation: 0.16693864701099226]
	TIME [epoch: 2.7 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1201199982753065		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.1201199982753065 | validation: 0.22045363676632712]
	TIME [epoch: 2.7 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15902751229697917		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.15902751229697917 | validation: 0.17628693757728203]
	TIME [epoch: 2.7 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12261735984229528		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.12261735984229528 | validation: 0.15770019095103827]
	TIME [epoch: 2.7 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10895541488772935		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.10895541488772935 | validation: 0.1625166130771667]
	TIME [epoch: 2.7 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10579883091251688		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.10579883091251688 | validation: 0.14714499230032665]
	TIME [epoch: 2.7 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09732347304405614		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.09732347304405614 | validation: 0.14188006637246048]
	TIME [epoch: 2.7 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0959438808217114		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.0959438808217114 | validation: 0.13871896643152293]
	TIME [epoch: 2.7 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09966284033210938		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.09966284033210938 | validation: 0.1602046438875872]
	TIME [epoch: 2.7 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11149175560939847		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.11149175560939847 | validation: 0.15690233006168]
	TIME [epoch: 2.7 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13020892288334435		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.13020892288334435 | validation: 0.1817562356927674]
	TIME [epoch: 2.7 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1279731749298677		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.1279731749298677 | validation: 0.19888709004103]
	TIME [epoch: 2.7 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12298742833587864		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.12298742833587864 | validation: 0.15165282011017484]
	TIME [epoch: 2.7 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11023066699713091		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.11023066699713091 | validation: 0.18018539614586773]
	TIME [epoch: 2.7 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10997291178222224		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.10997291178222224 | validation: 0.15456977604577532]
	TIME [epoch: 2.7 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10604620947869739		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.10604620947869739 | validation: 0.17936071069148518]
	TIME [epoch: 2.7 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11440310243100126		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.11440310243100126 | validation: 0.16433515939736765]
	TIME [epoch: 2.7 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12014253297924372		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.12014253297924372 | validation: 0.1592259502035546]
	TIME [epoch: 2.69 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1123714131925002		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.1123714131925002 | validation: 0.1641100107553876]
	TIME [epoch: 2.69 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11639856973561269		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.11639856973561269 | validation: 0.13623280440585842]
	TIME [epoch: 2.69 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11340534925563973		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.11340534925563973 | validation: 0.1559831478921235]
	TIME [epoch: 2.68 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10476920581317943		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.10476920581317943 | validation: 0.12515974617680473]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_808.pth
	Model improved!!!
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09916466749878562		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.09916466749878562 | validation: 0.15799834480290215]
	TIME [epoch: 2.69 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09928314777857189		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.09928314777857189 | validation: 0.19511308420469353]
	TIME [epoch: 2.69 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20806096266746896		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.20806096266746896 | validation: 0.22559275692894032]
	TIME [epoch: 2.69 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14014316786017536		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.14014316786017536 | validation: 0.16027268277790885]
	TIME [epoch: 2.69 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10365160255563481		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.10365160255563481 | validation: 0.13407081877385013]
	TIME [epoch: 2.69 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09980312495568675		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.09980312495568675 | validation: 0.15355156364669245]
	TIME [epoch: 2.69 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1064902073805394		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.1064902073805394 | validation: 0.13320246806816835]
	TIME [epoch: 2.69 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09875939546540366		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.09875939546540366 | validation: 0.1399288841457957]
	TIME [epoch: 2.69 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0966857011449367		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.0966857011449367 | validation: 0.12374614024766438]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_817.pth
	Model improved!!!
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09867902657278085		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.09867902657278085 | validation: 0.15398046811877647]
	TIME [epoch: 2.69 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1029625317416912		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.1029625317416912 | validation: 0.16579911717148546]
	TIME [epoch: 2.7 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11786912491930827		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.11786912491930827 | validation: 0.18226813518283044]
	TIME [epoch: 2.69 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13042006879969806		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.13042006879969806 | validation: 0.15343956637352238]
	TIME [epoch: 2.69 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11123457408548305		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.11123457408548305 | validation: 0.14038098679095382]
	TIME [epoch: 2.69 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09991165755619466		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.09991165755619466 | validation: 0.15781231891549277]
	TIME [epoch: 2.68 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10658782133046314		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.10658782133046314 | validation: 0.13506649787168643]
	TIME [epoch: 2.69 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09583277975470499		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.09583277975470499 | validation: 0.14473086605642976]
	TIME [epoch: 2.68 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09968665279244404		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.09968665279244404 | validation: 0.14721662425258014]
	TIME [epoch: 2.68 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10835338118540215		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.10835338118540215 | validation: 0.1674812647511313]
	TIME [epoch: 2.68 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11055925406832375		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.11055925406832375 | validation: 0.15971732627704954]
	TIME [epoch: 2.69 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1166611774914114		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.1166611774914114 | validation: 0.16143153613387162]
	TIME [epoch: 2.69 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11925868387795735		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.11925868387795735 | validation: 0.14961685967204955]
	TIME [epoch: 2.69 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10169904683539845		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.10169904683539845 | validation: 0.14589695724492438]
	TIME [epoch: 2.68 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1309689502945938		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.1309689502945938 | validation: 0.19452705744725926]
	TIME [epoch: 2.69 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1231421558433802		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.1231421558433802 | validation: 0.17576069864736862]
	TIME [epoch: 2.68 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11234600306289255		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.11234600306289255 | validation: 0.12677218729439035]
	TIME [epoch: 2.69 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09234866288355031		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.09234866288355031 | validation: 0.1241822124485839]
	TIME [epoch: 2.68 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08825222452271785		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.08825222452271785 | validation: 0.12576517600188342]
	TIME [epoch: 2.68 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08887707010622506		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.08887707010622506 | validation: 0.12928773742898897]
	TIME [epoch: 2.68 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08824931218308234		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.08824931218308234 | validation: 0.14133246837570376]
	TIME [epoch: 2.69 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09446338836206049		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.09446338836206049 | validation: 0.16517479081611353]
	TIME [epoch: 2.68 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11614250555089134		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.11614250555089134 | validation: 0.18968007215393554]
	TIME [epoch: 2.68 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13635737690365157		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.13635737690365157 | validation: 0.1476138493053448]
	TIME [epoch: 2.69 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.111824847403386		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.111824847403386 | validation: 0.1119628746100529]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_842.pth
	Model improved!!!
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10298307009079903		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.10298307009079903 | validation: 0.13638258130436715]
	TIME [epoch: 2.67 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09185236701985615		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.09185236701985615 | validation: 0.12995364313897612]
	TIME [epoch: 2.67 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09133738655849367		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.09133738655849367 | validation: 0.16565019109612314]
	TIME [epoch: 2.68 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10557604509044868		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.10557604509044868 | validation: 0.1515978531930583]
	TIME [epoch: 2.69 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11760791556155813		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.11760791556155813 | validation: 0.17034691238253302]
	TIME [epoch: 2.69 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1127086879846817		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.1127086879846817 | validation: 0.1286591514173245]
	TIME [epoch: 2.69 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09135080146535178		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.09135080146535178 | validation: 0.12130102718946954]
	TIME [epoch: 2.69 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0866762279688523		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.0866762279688523 | validation: 0.12538197601353876]
	TIME [epoch: 2.69 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09345611206860195		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.09345611206860195 | validation: 0.1443458720006292]
	TIME [epoch: 2.69 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10361209655533894		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.10361209655533894 | validation: 0.14131516902478378]
	TIME [epoch: 2.7 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12387032340353091		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.12387032340353091 | validation: 0.14810913976681217]
	TIME [epoch: 2.69 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10407406902248857		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.10407406902248857 | validation: 0.16977314809023564]
	TIME [epoch: 2.68 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10777022082139351		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.10777022082139351 | validation: 0.1346940725716621]
	TIME [epoch: 2.68 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09125456127229128		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.09125456127229128 | validation: 0.14230619357289903]
	TIME [epoch: 2.68 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09763590760456638		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.09763590760456638 | validation: 0.12748464395859468]
	TIME [epoch: 2.69 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09681175761121692		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.09681175761121692 | validation: 0.20325951476542975]
	TIME [epoch: 2.69 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1230827834301982		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.1230827834301982 | validation: 0.13164767100055752]
	TIME [epoch: 2.69 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09174846458630324		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.09174846458630324 | validation: 0.12189410552622737]
	TIME [epoch: 2.68 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0866801541461568		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.0866801541461568 | validation: 0.13332207076290592]
	TIME [epoch: 2.69 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09127395842387191		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.09127395842387191 | validation: 0.1258005847141886]
	TIME [epoch: 2.68 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11312310522790718		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.11312310522790718 | validation: 0.136433357055257]
	TIME [epoch: 2.7 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09616869038823624		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.09616869038823624 | validation: 0.12315269737229989]
	TIME [epoch: 2.69 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08880320249418981		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.08880320249418981 | validation: 0.14347628425542996]
	TIME [epoch: 2.69 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10434462587016018		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.10434462587016018 | validation: 0.17649357192898385]
	TIME [epoch: 2.68 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11165184657607852		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.11165184657607852 | validation: 0.1264187006160614]
	TIME [epoch: 2.68 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09428035626707437		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.09428035626707437 | validation: 0.12464988435557212]
	TIME [epoch: 2.69 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09035057571865603		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.09035057571865603 | validation: 0.12617194190154044]
	TIME [epoch: 2.69 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09352689820096564		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.09352689820096564 | validation: 0.17183598958351998]
	TIME [epoch: 2.68 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10585687339298455		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.10585687339298455 | validation: 0.13046717927936893]
	TIME [epoch: 2.68 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09085127774913922		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.09085127774913922 | validation: 0.12457328316300216]
	TIME [epoch: 2.68 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08773425469768777		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.08773425469768777 | validation: 0.11266452959680234]
	TIME [epoch: 2.68 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08934060949717196		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.08934060949717196 | validation: 0.13752799657368653]
	TIME [epoch: 2.7 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09379996041278225		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.09379996041278225 | validation: 0.1135978631330116]
	TIME [epoch: 2.68 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09839091686156187		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.09839091686156187 | validation: 0.12923786769694676]
	TIME [epoch: 2.69 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09279577142714017		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.09279577142714017 | validation: 0.14206601063225538]
	TIME [epoch: 2.69 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09768816874183589		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.09768816874183589 | validation: 0.1628352846852097]
	TIME [epoch: 2.69 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11058835546255966		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.11058835546255966 | validation: 0.14632892717908916]
	TIME [epoch: 2.69 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08898791294433131		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.08898791294433131 | validation: 0.11282415865215532]
	TIME [epoch: 2.69 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08044852476678392		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.08044852476678392 | validation: 0.11965224307402228]
	TIME [epoch: 2.69 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08063038649918806		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.08063038649918806 | validation: 0.10851066034666541]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_882.pth
	Model improved!!!
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07784058879177753		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.07784058879177753 | validation: 0.11881157501949231]
	TIME [epoch: 2.68 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08055895056819934		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.08055895056819934 | validation: 0.12259932980396365]
	TIME [epoch: 2.68 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08660454120114426		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.08660454120114426 | validation: 0.1408074146852242]
	TIME [epoch: 2.67 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11030951734240471		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.11030951734240471 | validation: 0.18807548886339998]
	TIME [epoch: 2.67 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1367876493676016		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.1367876493676016 | validation: 0.12660482466120604]
	TIME [epoch: 2.68 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09809659234550207		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.09809659234550207 | validation: 0.11755764088340155]
	TIME [epoch: 2.67 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0791119610684487		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.0791119610684487 | validation: 0.1143533993268847]
	TIME [epoch: 2.68 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07705601527578428		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.07705601527578428 | validation: 0.10015016075883519]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_890.pth
	Model improved!!!
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0790974193698993		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.0790974193698993 | validation: 0.14459581025447923]
	TIME [epoch: 2.67 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09408937477449054		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.09408937477449054 | validation: 0.1096017040250747]
	TIME [epoch: 2.67 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08946561321572831		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.08946561321572831 | validation: 0.12891063790020385]
	TIME [epoch: 2.67 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08840544877906656		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.08840544877906656 | validation: 0.13210972083619685]
	TIME [epoch: 2.67 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08874789696918171		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.08874789696918171 | validation: 0.1502948240288749]
	TIME [epoch: 2.68 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10101340391266356		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.10101340391266356 | validation: 0.14125964610395245]
	TIME [epoch: 2.67 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09854225586497214		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.09854225586497214 | validation: 0.14066117224496213]
	TIME [epoch: 2.67 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09828445318364074		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.09828445318364074 | validation: 0.13068682549694466]
	TIME [epoch: 2.67 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09211453907895979		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.09211453907895979 | validation: 0.10622331306984659]
	TIME [epoch: 2.67 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09309543697384128		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.09309543697384128 | validation: 0.12149004077666122]
	TIME [epoch: 2.67 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08392240485081574		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.08392240485081574 | validation: 0.11524393273702116]
	TIME [epoch: 2.68 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08049461481837644		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.08049461481837644 | validation: 0.10923365657878903]
	TIME [epoch: 2.68 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07885896591181873		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.07885896591181873 | validation: 0.12723702028927983]
	TIME [epoch: 2.68 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08659583618583415		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.08659583618583415 | validation: 0.11571782141473985]
	TIME [epoch: 2.68 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08808323435070065		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.08808323435070065 | validation: 0.1534431248981083]
	TIME [epoch: 2.68 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09786931237554822		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.09786931237554822 | validation: 0.11657481877413352]
	TIME [epoch: 2.69 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08971976525268542		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.08971976525268542 | validation: 0.12074630641448048]
	TIME [epoch: 2.68 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08005762319065522		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.08005762319065522 | validation: 0.10040036502948238]
	TIME [epoch: 2.68 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08312886039992755		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.08312886039992755 | validation: 0.1162584153489398]
	TIME [epoch: 2.68 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08507837058310579		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.08507837058310579 | validation: 0.10607700955651654]
	TIME [epoch: 2.68 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08322911228050629		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.08322911228050629 | validation: 0.11988597806012109]
	TIME [epoch: 2.68 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08335022094073721		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.08335022094073721 | validation: 0.11839385220462875]
	TIME [epoch: 2.68 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08681014662707728		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.08681014662707728 | validation: 0.1543015247615588]
	TIME [epoch: 2.68 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09608225370494217		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.09608225370494217 | validation: 0.15076663170239293]
	TIME [epoch: 2.68 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10218831979241905		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.10218831979241905 | validation: 0.12075524016738681]
	TIME [epoch: 2.68 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08629511110125111		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.08629511110125111 | validation: 0.12160929605657987]
	TIME [epoch: 2.68 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08243523013912558		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.08243523013912558 | validation: 0.11169060724544787]
	TIME [epoch: 2.69 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08011112646422675		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.08011112646422675 | validation: 0.10538500201824302]
	TIME [epoch: 2.68 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08355433946091614		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.08355433946091614 | validation: 0.12472513622135378]
	TIME [epoch: 2.68 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08322632510416578		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.08322632510416578 | validation: 0.10978948906292403]
	TIME [epoch: 2.68 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0821470698293301		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.0821470698293301 | validation: 0.11943792513218128]
	TIME [epoch: 2.68 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07872093649507482		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.07872093649507482 | validation: 0.11681156298365006]
	TIME [epoch: 2.68 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08634288554366411		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.08634288554366411 | validation: 0.13273486079290095]
	TIME [epoch: 2.68 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09511456170984739		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.09511456170984739 | validation: 0.15887470600491035]
	TIME [epoch: 2.68 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10540347058120604		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.10540347058120604 | validation: 0.12521900903314095]
	TIME [epoch: 2.68 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09641778117389667		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.09641778117389667 | validation: 0.123718608352625]
	TIME [epoch: 2.68 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08083140699416262		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.08083140699416262 | validation: 0.10957202115869887]
	TIME [epoch: 2.68 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08221163542729834		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.08221163542729834 | validation: 0.11587791047683327]
	TIME [epoch: 2.69 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08340961317967859		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.08340961317967859 | validation: 0.12686139497687682]
	TIME [epoch: 2.68 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08814410149469208		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.08814410149469208 | validation: 0.12398793578230008]
	TIME [epoch: 2.68 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08833623920538312		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.08833623920538312 | validation: 0.11855344722456918]
	TIME [epoch: 2.68 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07916918668830966		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.07916918668830966 | validation: 0.11553221437096961]
	TIME [epoch: 2.68 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07335310201200776		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.07335310201200776 | validation: 0.10001018603015255]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_933.pth
	Model improved!!!
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07340780777624596		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.07340780777624596 | validation: 0.11325839042117791]
	TIME [epoch: 2.68 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07904667083432973		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.07904667083432973 | validation: 0.10527011126158065]
	TIME [epoch: 2.68 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08387897057146176		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.08387897057146176 | validation: 0.11450609828923364]
	TIME [epoch: 2.68 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08320025429378543		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.08320025429378543 | validation: 0.11962754324726049]
	TIME [epoch: 2.68 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0829640966237271		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.0829640966237271 | validation: 0.13146415152348065]
	TIME [epoch: 2.68 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08775595655490749		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.08775595655490749 | validation: 0.12573633669819337]
	TIME [epoch: 2.68 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08572121974991123		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.08572121974991123 | validation: 0.10948968456236839]
	TIME [epoch: 2.67 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0769865445733088		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.0769865445733088 | validation: 0.10099215399204185]
	TIME [epoch: 2.68 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0763176179312308		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.0763176179312308 | validation: 0.10612338529947776]
	TIME [epoch: 2.69 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07554341451875293		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.07554341451875293 | validation: 0.09942499930379073]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_943.pth
	Model improved!!!
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07999298151607977		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.07999298151607977 | validation: 0.12158051121819136]
	TIME [epoch: 2.68 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07901063337258803		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.07901063337258803 | validation: 0.09460341982877316]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_945.pth
	Model improved!!!
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07943129348588791		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.07943129348588791 | validation: 0.10885533512097674]
	TIME [epoch: 2.68 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07715564865797941		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.07715564865797941 | validation: 0.1381215130017527]
	TIME [epoch: 2.68 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09097143626699584		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.09097143626699584 | validation: 0.13013560620902542]
	TIME [epoch: 2.67 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0934870599877424		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.0934870599877424 | validation: 0.1300783830787296]
	TIME [epoch: 2.67 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.083846433925666		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.083846433925666 | validation: 0.10343319161853622]
	TIME [epoch: 2.68 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07467424678679467		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.07467424678679467 | validation: 0.10260709596870249]
	TIME [epoch: 2.67 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07308359398034141		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.07308359398034141 | validation: 0.09571661856214646]
	TIME [epoch: 2.68 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.076000990360808		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.076000990360808 | validation: 0.10917143830824046]
	TIME [epoch: 2.68 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07417542059996253		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.07417542059996253 | validation: 0.10060256356192457]
	TIME [epoch: 2.67 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07653707977644493		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.07653707977644493 | validation: 0.11388754608559076]
	TIME [epoch: 2.67 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0783845214296355		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.0783845214296355 | validation: 0.11176700614738416]
	TIME [epoch: 2.68 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08240135962746785		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.08240135962746785 | validation: 0.13006610192071685]
	TIME [epoch: 2.67 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0901178565563257		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.0901178565563257 | validation: 0.14148822734743702]
	TIME [epoch: 2.67 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09414276553308276		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.09414276553308276 | validation: 0.11136051205735194]
	TIME [epoch: 2.67 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07249893502812121		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.07249893502812121 | validation: 0.0973212998365185]
	TIME [epoch: 2.67 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07585301765081544		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.07585301765081544 | validation: 0.12721922895069776]
	TIME [epoch: 2.68 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08091234078450171		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.08091234078450171 | validation: 0.11268323130887047]
	TIME [epoch: 2.67 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08054742732820222		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.08054742732820222 | validation: 0.10918377532447301]
	TIME [epoch: 2.67 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07894466200571809		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.07894466200571809 | validation: 0.09660038864600183]
	TIME [epoch: 2.68 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07118002203304699		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.07118002203304699 | validation: 0.10700306704272888]
	TIME [epoch: 2.67 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07480040337147122		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.07480040337147122 | validation: 0.09414818151651899]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_966.pth
	Model improved!!!
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07249295300858176		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.07249295300858176 | validation: 0.10334924846336047]
	TIME [epoch: 2.67 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0741125412665723		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.0741125412665723 | validation: 0.10142565287225291]
	TIME [epoch: 2.67 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07367687306565468		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.07367687306565468 | validation: 0.11522246169719384]
	TIME [epoch: 2.67 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07534648711286136		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.07534648711286136 | validation: 0.141512228854731]
	TIME [epoch: 2.67 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09682146045363445		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.09682146045363445 | validation: 0.12020269301509748]
	TIME [epoch: 2.67 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08774163755916695		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.08774163755916695 | validation: 0.10489424908991407]
	TIME [epoch: 2.68 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07646196356168479		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.07646196356168479 | validation: 0.10041754190847421]
	TIME [epoch: 2.67 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06887698026770836		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.06887698026770836 | validation: 0.09282224873607715]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_974.pth
	Model improved!!!
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07144483676323554		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.07144483676323554 | validation: 0.10812207390522009]
	TIME [epoch: 2.68 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07171258535368281		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.07171258535368281 | validation: 0.09002735452729033]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_976.pth
	Model improved!!!
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.072119584284727		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.072119584284727 | validation: 0.10716082747665463]
	TIME [epoch: 2.68 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07273099146267341		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.07273099146267341 | validation: 0.1110919165780857]
	TIME [epoch: 2.67 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07806741226294045		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.07806741226294045 | validation: 0.11039969821431349]
	TIME [epoch: 2.67 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07858425958462578		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.07858425958462578 | validation: 0.09914315137481539]
	TIME [epoch: 2.68 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08095765154292557		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.08095765154292557 | validation: 0.11500654416522443]
	TIME [epoch: 2.67 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07768912067369392		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.07768912067369392 | validation: 0.09493311727500478]
	TIME [epoch: 2.69 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06843882462782908		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.06843882462782908 | validation: 0.1007947511499066]
	TIME [epoch: 2.68 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0715023707825188		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.0715023707825188 | validation: 0.1183220385752398]
	TIME [epoch: 2.68 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08172931439660924		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.08172931439660924 | validation: 0.14169806920117964]
	TIME [epoch: 2.68 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0985681004157552		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.0985681004157552 | validation: 0.11813550776981718]
	TIME [epoch: 2.68 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08466400279211041		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.08466400279211041 | validation: 0.09573576994540813]
	TIME [epoch: 2.68 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06814636725111241		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.06814636725111241 | validation: 0.09880358910965957]
	TIME [epoch: 2.68 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06428832165236649		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.06428832165236649 | validation: 0.10333404041619194]
	TIME [epoch: 2.68 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06828227727486587		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.06828227727486587 | validation: 0.09229106623273417]
	TIME [epoch: 2.68 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06900334192383797		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.06900334192383797 | validation: 0.09563511916207004]
	TIME [epoch: 2.68 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07217171128945862		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.07217171128945862 | validation: 0.08967590747715362]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_992.pth
	Model improved!!!
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07105796907703191		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.07105796907703191 | validation: 0.10958484672997147]
	TIME [epoch: 2.68 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06825231137122753		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.06825231137122753 | validation: 0.09721783423308603]
	TIME [epoch: 2.67 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06999758649973377		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.06999758649973377 | validation: 0.11318103889436162]
	TIME [epoch: 2.67 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07225995151198623		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.07225995151198623 | validation: 0.10795445031886049]
	TIME [epoch: 2.67 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07530689882708169		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.07530689882708169 | validation: 0.11655444498114909]
	TIME [epoch: 2.67 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08075672699159686		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.08075672699159686 | validation: 0.12161048084854298]
	TIME [epoch: 2.68 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09016527302725855		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.09016527302725855 | validation: 0.11006744679838676]
	TIME [epoch: 2.67 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07959329828305882		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.07959329828305882 | validation: 0.11003331113508562]
	TIME [epoch: 2.67 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07129388932295505		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.07129388932295505 | validation: 0.09365622892434441]
	TIME [epoch: 181 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06589503724266901		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.06589503724266901 | validation: 0.1027552071318445]
	TIME [epoch: 5.76 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06758043690293639		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.06758043690293639 | validation: 0.09106069540924733]
	TIME [epoch: 5.76 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07172891240879051		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.07172891240879051 | validation: 0.11763579300629462]
	TIME [epoch: 5.75 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07464775235684581		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.07464775235684581 | validation: 0.09506101805917333]
	TIME [epoch: 5.75 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06990195066723834		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.06990195066723834 | validation: 0.09993070596990612]
	TIME [epoch: 5.74 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0691793758729539		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.0691793758729539 | validation: 0.09872591887385468]
	TIME [epoch: 5.75 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06937621636813901		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.06937621636813901 | validation: 0.10252432175777448]
	TIME [epoch: 5.75 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07117134523421706		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.07117134523421706 | validation: 0.08927545155937233]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_1009.pth
	Model improved!!!
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06881489321907427		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.06881489321907427 | validation: 0.10048784281667117]
	TIME [epoch: 5.75 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06891113039519683		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.06891113039519683 | validation: 0.08542737549527069]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_1011.pth
	Model improved!!!
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0692474373157022		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.0692474373157022 | validation: 0.09428900592499859]
	TIME [epoch: 5.75 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06647755211931697		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.06647755211931697 | validation: 0.09043050008001288]
	TIME [epoch: 5.76 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06855439987965709		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.06855439987965709 | validation: 0.10538660697928273]
	TIME [epoch: 5.74 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0700343474784885		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.0700343474784885 | validation: 0.09027543706727106]
	TIME [epoch: 5.76 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06705048574344545		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.06705048574344545 | validation: 0.09948826806351774]
	TIME [epoch: 5.76 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06623622501081405		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.06623622501081405 | validation: 0.11624216673556503]
	TIME [epoch: 5.74 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07970638803470788		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.07970638803470788 | validation: 0.1427416302027976]
	TIME [epoch: 5.76 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09916078361252866		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.09916078361252866 | validation: 0.10744795123283285]
	TIME [epoch: 5.75 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07528993664436094		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.07528993664436094 | validation: 0.08671955751924146]
	TIME [epoch: 5.75 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06975525713320739		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.06975525713320739 | validation: 0.0941447103127756]
	TIME [epoch: 5.75 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0690558774776644		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.0690558774776644 | validation: 0.10091418898642952]
	TIME [epoch: 5.75 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06761377656304211		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.06761377656304211 | validation: 0.101313830013481]
	TIME [epoch: 5.76 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06967764135201314		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.06967764135201314 | validation: 0.09707361989302414]
	TIME [epoch: 5.75 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06895838001095965		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.06895838001095965 | validation: 0.09731617632337548]
	TIME [epoch: 5.75 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06835347715171437		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.06835347715171437 | validation: 0.08962896119629421]
	TIME [epoch: 5.75 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06537777729344298		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.06537777729344298 | validation: 0.09635207396687306]
	TIME [epoch: 5.74 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06720328010566924		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.06720328010566924 | validation: 0.09061267460757227]
	TIME [epoch: 5.76 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06723400887237686		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.06723400887237686 | validation: 0.10417898695169406]
	TIME [epoch: 5.75 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07221664698434901		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.07221664698434901 | validation: 0.08961678222085567]
	TIME [epoch: 5.75 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06988423465337615		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.06988423465337615 | validation: 0.11143674713775303]
	TIME [epoch: 5.75 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07060617192935645		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.07060617192935645 | validation: 0.1063593664795952]
	TIME [epoch: 5.75 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07266144996972364		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.07266144996972364 | validation: 0.11543046213973547]
	TIME [epoch: 5.74 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0768100374701053		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.0768100374701053 | validation: 0.10430641324088802]
	TIME [epoch: 5.75 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07732712358409673		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.07732712358409673 | validation: 0.08793691319949172]
	TIME [epoch: 5.75 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07187381242554883		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.07187381242554883 | validation: 0.09845163634481535]
	TIME [epoch: 5.74 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07011108467694196		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.07011108467694196 | validation: 0.0913503388939647]
	TIME [epoch: 5.75 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06337513410478823		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.06337513410478823 | validation: 0.0866874949923908]
	TIME [epoch: 5.74 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06122068048314777		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.06122068048314777 | validation: 0.08728067305198142]
	TIME [epoch: 5.76 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06392461488814725		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.06392461488814725 | validation: 0.09065761566730152]
	TIME [epoch: 5.75 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06379306484898162		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.06379306484898162 | validation: 0.08991860100041557]
	TIME [epoch: 5.75 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06628583694716818		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.06628583694716818 | validation: 0.11142857528300823]
	TIME [epoch: 5.75 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07345742432664895		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.07345742432664895 | validation: 0.10500649665767786]
	TIME [epoch: 5.75 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07540279515548974		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.07540279515548974 | validation: 0.09531581901056348]
	TIME [epoch: 5.76 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06962726163775225		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.06962726163775225 | validation: 0.09480416501019734]
	TIME [epoch: 5.75 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06826773238403666		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.06826773238403666 | validation: 0.0891456615411306]
	TIME [epoch: 5.75 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06712735103641641		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.06712735103641641 | validation: 0.09172166865880524]
	TIME [epoch: 5.75 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06403904295150513		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.06403904295150513 | validation: 0.08334178678832566]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_1048.pth
	Model improved!!!
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0658109103491416		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.0658109103491416 | validation: 0.09217061785840264]
	TIME [epoch: 5.76 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06347554220766932		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.06347554220766932 | validation: 0.08581453088822351]
	TIME [epoch: 5.75 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06555122009559035		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.06555122009559035 | validation: 0.09868195431410043]
	TIME [epoch: 5.75 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06464263548611666		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.06464263548611666 | validation: 0.09361621330702419]
	TIME [epoch: 5.75 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06797573653954875		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.06797573653954875 | validation: 0.10621275625051037]
	TIME [epoch: 5.75 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07641388329550776		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.07641388329550776 | validation: 0.09435757138717567]
	TIME [epoch: 5.76 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07187070726326857		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.07187070726326857 | validation: 0.09271054244718828]
	TIME [epoch: 5.75 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06791928528409287		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.06791928528409287 | validation: 0.09067714616882311]
	TIME [epoch: 5.75 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06427450075944348		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.06427450075944348 | validation: 0.08627263944601363]
	TIME [epoch: 5.75 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06470040728428575		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.06470040728428575 | validation: 0.08799871110921981]
	TIME [epoch: 5.75 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06256852912684059		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.06256852912684059 | validation: 0.08339894696518779]
	TIME [epoch: 5.76 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0616322643267932		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.0616322643267932 | validation: 0.09661650089970776]
	TIME [epoch: 5.75 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0691884817233639		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.0691884817233639 | validation: 0.11260026313387468]
	TIME [epoch: 5.75 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07178407665205586		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.07178407665205586 | validation: 0.09570268860436154]
	TIME [epoch: 5.75 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06937778745312405		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.06937778745312405 | validation: 0.08571263976153282]
	TIME [epoch: 5.75 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06643038696562865		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.06643038696562865 | validation: 0.09837482035203887]
	TIME [epoch: 5.75 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0710531723561598		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.0710531723561598 | validation: 0.08877001577506805]
	TIME [epoch: 5.76 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06909771937475545		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.06909771937475545 | validation: 0.09236884800863676]
	TIME [epoch: 5.75 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06365413130584818		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.06365413130584818 | validation: 0.08718251083551475]
	TIME [epoch: 5.75 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06036722956167439		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.06036722956167439 | validation: 0.08973409972585372]
	TIME [epoch: 5.75 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06070611401130936		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.06070611401130936 | validation: 0.08458027785280864]
	TIME [epoch: 5.75 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06176352350805024		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.06176352350805024 | validation: 0.07917714571226571]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_1070.pth
	Model improved!!!
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06261611439597534		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.06261611439597534 | validation: 0.09311984606241461]
	TIME [epoch: 5.75 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06769392114160168		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.06769392114160168 | validation: 0.09324758747442553]
	TIME [epoch: 5.75 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06796865168019965		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.06796865168019965 | validation: 0.11004608796311133]
	TIME [epoch: 5.75 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07161298897111097		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.07161298897111097 | validation: 0.09695708825338058]
	TIME [epoch: 5.75 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07006336567335296		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.07006336567335296 | validation: 0.08548820531656776]
	TIME [epoch: 5.75 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0631472307463245		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.0631472307463245 | validation: 0.08013515796829884]
	TIME [epoch: 5.75 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06058870613203685		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.06058870613203685 | validation: 0.08672542877578943]
	TIME [epoch: 5.75 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06013475018953114		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.06013475018953114 | validation: 0.08209732988026887]
	TIME [epoch: 5.75 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05830035003823937		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.05830035003823937 | validation: 0.08158909611365711]
	TIME [epoch: 5.75 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05808207733069988		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.05808207733069988 | validation: 0.08258558220486667]
	TIME [epoch: 5.76 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06090828830349497		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.06090828830349497 | validation: 0.08530746215356833]
	TIME [epoch: 5.75 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06074818452610195		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.06074818452610195 | validation: 0.08464138687606003]
	TIME [epoch: 5.75 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05984027385672598		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.05984027385672598 | validation: 0.09614738817392883]
	TIME [epoch: 5.75 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06630838166076891		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.06630838166076891 | validation: 0.08638347872350785]
	TIME [epoch: 5.75 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07239955633315846		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.07239955633315846 | validation: 0.09491247903677798]
	TIME [epoch: 5.76 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0693180358088423		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.0693180358088423 | validation: 0.1076528428335313]
	TIME [epoch: 5.75 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07164724088635525		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.07164724088635525 | validation: 0.08950832418571981]
	TIME [epoch: 5.75 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06432675944756876		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.06432675944756876 | validation: 0.0830869273041166]
	TIME [epoch: 5.75 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05926315833730263		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.05926315833730263 | validation: 0.08887993599781993]
	TIME [epoch: 5.75 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06522810584028002		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.06522810584028002 | validation: 0.09306448831452269]
	TIME [epoch: 5.75 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0596090395330896		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.0596090395330896 | validation: 0.08622663652231463]
	TIME [epoch: 5.75 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06451606471633017		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.06451606471633017 | validation: 0.08700306214597099]
	TIME [epoch: 5.75 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05974986716718973		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.05974986716718973 | validation: 0.07962254707904581]
	TIME [epoch: 5.75 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06044340689252842		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.06044340689252842 | validation: 0.07814421407911894]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_1094.pth
	Model improved!!!
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056460700870327986		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.056460700870327986 | validation: 0.08788392296130944]
	TIME [epoch: 5.74 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062135078546196744		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.062135078546196744 | validation: 0.10274326643420813]
	TIME [epoch: 5.75 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07059115714130489		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.07059115714130489 | validation: 0.09659384918679428]
	TIME [epoch: 5.74 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0733415993884001		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.0733415993884001 | validation: 0.08442111553264094]
	TIME [epoch: 5.74 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06455594758599892		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.06455594758599892 | validation: 0.08962067627037178]
	TIME [epoch: 5.75 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05817313898419974		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.05817313898419974 | validation: 0.0805294820552224]
	TIME [epoch: 5.74 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05975575709414905		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.05975575709414905 | validation: 0.09014247587575745]
	TIME [epoch: 5.76 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062193646890770324		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.062193646890770324 | validation: 0.09346772587112827]
	TIME [epoch: 5.74 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06629703971435094		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.06629703971435094 | validation: 0.08878860592894317]
	TIME [epoch: 5.74 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0612033794110491		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.0612033794110491 | validation: 0.07990446448133182]
	TIME [epoch: 5.75 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05873384335210784		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.05873384335210784 | validation: 0.08475304192483794]
	TIME [epoch: 5.74 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05821981899484383		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.05821981899484383 | validation: 0.08092134025031081]
	TIME [epoch: 5.75 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06028272277367386		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.06028272277367386 | validation: 0.08566275446286181]
	TIME [epoch: 5.74 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06059224084132403		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.06059224084132403 | validation: 0.08511906742685113]
	TIME [epoch: 5.75 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06071317921506298		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.06071317921506298 | validation: 0.08288366794971151]
	TIME [epoch: 5.74 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059280983708028874		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.059280983708028874 | validation: 0.0857163939258629]
	TIME [epoch: 5.74 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05944290233698572		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.05944290233698572 | validation: 0.08412218691689777]
	TIME [epoch: 5.75 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0594574098363218		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.0594574098363218 | validation: 0.08015733341761744]
	TIME [epoch: 5.75 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05692695929211264		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.05692695929211264 | validation: 0.08481681609925953]
	TIME [epoch: 5.74 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05753227658069025		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.05753227658069025 | validation: 0.07859541446633661]
	TIME [epoch: 5.74 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059041554126554534		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.059041554126554534 | validation: 0.08437789348398121]
	TIME [epoch: 5.74 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06276585352497716		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.06276585352497716 | validation: 0.10080818715491911]
	TIME [epoch: 5.76 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07223635649338672		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.07223635649338672 | validation: 0.09867688153553017]
	TIME [epoch: 5.74 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07216149752825249		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.07216149752825249 | validation: 0.08425601041555025]
	TIME [epoch: 5.75 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06359024751495167		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.06359024751495167 | validation: 0.08880108191107745]
	TIME [epoch: 5.74 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05897874233946923		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.05897874233946923 | validation: 0.07641240107442918]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_1120.pth
	Model improved!!!
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05942681942063204		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.05942681942063204 | validation: 0.08024062626658848]
	TIME [epoch: 5.75 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05746959134836499		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.05746959134836499 | validation: 0.08811397691037842]
	TIME [epoch: 5.75 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05930469539349234		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.05930469539349234 | validation: 0.08226117867656796]
	TIME [epoch: 5.75 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06024115053231459		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.06024115053231459 | validation: 0.07922684790035897]
	TIME [epoch: 5.74 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05810300049889817		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.05810300049889817 | validation: 0.08906392473626876]
	TIME [epoch: 5.74 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059245388964977795		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.059245388964977795 | validation: 0.10783070559899054]
	TIME [epoch: 5.75 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07074428753677968		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.07074428753677968 | validation: 0.08978561515964688]
	TIME [epoch: 5.75 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06222157311818429		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.06222157311818429 | validation: 0.07836992916484817]
	TIME [epoch: 5.74 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061032962147753056		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.061032962147753056 | validation: 0.08658794935329588]
	TIME [epoch: 5.75 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05912534505465183		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.05912534505465183 | validation: 0.08392794086562425]
	TIME [epoch: 5.74 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058311572756485845		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.058311572756485845 | validation: 0.08159613915787009]
	TIME [epoch: 5.75 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057575440105847794		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.057575440105847794 | validation: 0.07827884674493096]
	TIME [epoch: 5.75 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05762326428514009		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.05762326428514009 | validation: 0.08055113247241388]
	TIME [epoch: 5.74 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05788525286956679		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.05788525286956679 | validation: 0.09008164684646934]
	TIME [epoch: 5.74 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059524407100507055		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.059524407100507055 | validation: 0.08561637272932908]
	TIME [epoch: 5.74 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06126128974920501		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.06126128974920501 | validation: 0.08097542060159439]
	TIME [epoch: 5.75 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05852862462458055		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.05852862462458055 | validation: 0.0889262782598299]
	TIME [epoch: 5.75 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06002386193488095		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.06002386193488095 | validation: 0.07678417466467807]
	TIME [epoch: 5.74 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061082230102971195		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.061082230102971195 | validation: 0.07917137046771061]
	TIME [epoch: 5.75 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059534984928385826		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.059534984928385826 | validation: 0.07650600466617052]
	TIME [epoch: 5.75 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05696798687667866		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.05696798687667866 | validation: 0.08128337350967281]
	TIME [epoch: 5.74 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05800091173348462		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.05800091173348462 | validation: 0.09001921699238159]
	TIME [epoch: 5.75 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05791841151947356		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.05791841151947356 | validation: 0.07392238149288734]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_1143.pth
	Model improved!!!
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058864790274782204		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.058864790274782204 | validation: 0.08134945770297781]
	TIME [epoch: 5.74 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061442750076756064		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.061442750076756064 | validation: 0.08830099211762937]
	TIME [epoch: 5.75 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062124479251986386		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.062124479251986386 | validation: 0.09041996506721309]
	TIME [epoch: 5.74 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06415147199719004		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.06415147199719004 | validation: 0.08016797433861608]
	TIME [epoch: 5.75 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05911675551028413		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.05911675551028413 | validation: 0.07330189592411122]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_1148.pth
	Model improved!!!
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05803671501146889		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.05803671501146889 | validation: 0.08230723182237397]
	TIME [epoch: 5.74 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05615932267211632		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.05615932267211632 | validation: 0.08376041844396377]
	TIME [epoch: 5.74 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055907761675790375		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.055907761675790375 | validation: 0.076376602043653]
	TIME [epoch: 5.75 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05585055144102098		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.05585055144102098 | validation: 0.07687927022984675]
	TIME [epoch: 5.75 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057790483474995186		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.057790483474995186 | validation: 0.08291737399460421]
	TIME [epoch: 5.75 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05476799414626909		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.05476799414626909 | validation: 0.08260119556846994]
	TIME [epoch: 5.74 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05872750654422191		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.05872750654422191 | validation: 0.08554804230097934]
	TIME [epoch: 5.74 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06051081742601358		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.06051081742601358 | validation: 0.08941902793000846]
	TIME [epoch: 5.75 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06153960915119267		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.06153960915119267 | validation: 0.08605800785071285]
	TIME [epoch: 5.75 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06181111767358565		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.06181111767358565 | validation: 0.08355378251699537]
	TIME [epoch: 5.75 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05883327478387825		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.05883327478387825 | validation: 0.08087993019487254]
	TIME [epoch: 5.75 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057728710555401686		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.057728710555401686 | validation: 0.07369865493799942]
	TIME [epoch: 5.75 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05732549665009067		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.05732549665009067 | validation: 0.08358425257579244]
	TIME [epoch: 5.74 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05596619295477641		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.05596619295477641 | validation: 0.07692948115950696]
	TIME [epoch: 5.75 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05588157557725005		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.05588157557725005 | validation: 0.08627451350263345]
	TIME [epoch: 5.75 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057806412738348134		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.057806412738348134 | validation: 0.0742545589268508]
	TIME [epoch: 5.74 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05898697435557744		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.05898697435557744 | validation: 0.08411401677483872]
	TIME [epoch: 5.75 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05822774229319805		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.05822774229319805 | validation: 0.07812930624992862]
	TIME [epoch: 5.74 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06051519080849504		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.06051519080849504 | validation: 0.08161342391199142]
	TIME [epoch: 5.74 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06339896986764565		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.06339896986764565 | validation: 0.08075001559171223]
	TIME [epoch: 5.75 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06053885691632286		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.06053885691632286 | validation: 0.07694299194469509]
	TIME [epoch: 5.74 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055224986196809156		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.055224986196809156 | validation: 0.07687477616165597]
	TIME [epoch: 5.74 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05547903088604187		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.05547903088604187 | validation: 0.07637843398365936]
	TIME [epoch: 5.74 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05242581943596804		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.05242581943596804 | validation: 0.07367530233225887]
	TIME [epoch: 5.74 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05301511375300383		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.05301511375300383 | validation: 0.07654506420847494]
	TIME [epoch: 5.76 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05228600933895626		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.05228600933895626 | validation: 0.07989085736834119]
	TIME [epoch: 5.74 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05594459561478383		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.05594459561478383 | validation: 0.07962755430741891]
	TIME [epoch: 5.75 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060293367584101726		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.060293367584101726 | validation: 0.08949432373662698]
	TIME [epoch: 5.75 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062378823055915876		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.062378823055915876 | validation: 0.08872084671262867]
	TIME [epoch: 5.74 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062056619837951764		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.062056619837951764 | validation: 0.07606430242517999]
	TIME [epoch: 5.75 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058310192660518866		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.058310192660518866 | validation: 0.07976259704881002]
	TIME [epoch: 5.74 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054455798360126685		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.054455798360126685 | validation: 0.07931200959458079]
	TIME [epoch: 5.74 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05378704050406993		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.05378704050406993 | validation: 0.08650305323520797]
	TIME [epoch: 5.74 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05885897847293405		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.05885897847293405 | validation: 0.07309767162783787]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_1182.pth
	Model improved!!!
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056036548886491726		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.056036548886491726 | validation: 0.07906570962737514]
	TIME [epoch: 5.76 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05771283132753212		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.05771283132753212 | validation: 0.0727422083927933]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_1184.pth
	Model improved!!!
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055016092216158066		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.055016092216158066 | validation: 0.07960569751773777]
	TIME [epoch: 5.75 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05877816376841011		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.05877816376841011 | validation: 0.07687468509055019]
	TIME [epoch: 5.74 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05705640244150659		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.05705640244150659 | validation: 0.07972162191848088]
	TIME [epoch: 5.74 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05930696699845644		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.05930696699845644 | validation: 0.0769770921135331]
	TIME [epoch: 5.75 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05754459340876057		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.05754459340876057 | validation: 0.07740951678320371]
	TIME [epoch: 5.74 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054713933796389606		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.054713933796389606 | validation: 0.0754005807575513]
	TIME [epoch: 5.74 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054996407265205945		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.054996407265205945 | validation: 0.07357119996446639]
	TIME [epoch: 5.74 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0543702906007327		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.0543702906007327 | validation: 0.07829292133927158]
	TIME [epoch: 5.75 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05479334998903463		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.05479334998903463 | validation: 0.07522063285684762]
	TIME [epoch: 5.75 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056178461749524226		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.056178461749524226 | validation: 0.0759264680151821]
	TIME [epoch: 5.74 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054843901723338924		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.054843901723338924 | validation: 0.07611040382909981]
	TIME [epoch: 5.74 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05487245399812136		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.05487245399812136 | validation: 0.08098569691116037]
	TIME [epoch: 5.74 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056826433985225966		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.056826433985225966 | validation: 0.08214969398803279]
	TIME [epoch: 5.75 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057108675989839985		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.057108675989839985 | validation: 0.08187330920326236]
	TIME [epoch: 5.74 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05616538468665645		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.05616538468665645 | validation: 0.0761680694938099]
	TIME [epoch: 5.75 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05614991756130433		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.05614991756130433 | validation: 0.07637751259035928]
	TIME [epoch: 5.74 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05343270925400345		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.05343270925400345 | validation: 0.07915223263607396]
	TIME [epoch: 5.74 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057082338297592995		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.057082338297592995 | validation: 0.07395968461884495]
	TIME [epoch: 5.74 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056991926053274196		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.056991926053274196 | validation: 0.07345031228299943]
	TIME [epoch: 5.76 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05498359156102008		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.05498359156102008 | validation: 0.07576898388231526]
	TIME [epoch: 5.74 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05371665813897163		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.05371665813897163 | validation: 0.09204124239080501]
	TIME [epoch: 5.75 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0658422157745843		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.0658422157745843 | validation: 0.0905406784065896]
	TIME [epoch: 5.74 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05796394193592269		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.05796394193592269 | validation: 0.07399838476092511]
	TIME [epoch: 5.75 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05783957129020921		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.05783957129020921 | validation: 0.08037363331336127]
	TIME [epoch: 5.74 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0564108972276697		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.0564108972276697 | validation: 0.08686910005765908]
	TIME [epoch: 5.75 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05606131728006026		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.05606131728006026 | validation: 0.0752162719480715]
	TIME [epoch: 5.75 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056183173801299835		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.056183173801299835 | validation: 0.07136822515626401]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_1211.pth
	Model improved!!!
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05355655937661841		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.05355655937661841 | validation: 0.07343042068510078]
	TIME [epoch: 5.74 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05488645521944488		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.05488645521944488 | validation: 0.08045524408037802]
	TIME [epoch: 5.76 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05557007890238003		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.05557007890238003 | validation: 0.07605341657188253]
	TIME [epoch: 5.76 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0551367225040728		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.0551367225040728 | validation: 0.0768462779544367]
	TIME [epoch: 5.75 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053630144635789176		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.053630144635789176 | validation: 0.0749667385417764]
	TIME [epoch: 5.74 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053836549121454295		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.053836549121454295 | validation: 0.07579348247592359]
	TIME [epoch: 5.75 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05311240375830295		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.05311240375830295 | validation: 0.07837588068555858]
	TIME [epoch: 5.75 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0548200010026103		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.0548200010026103 | validation: 0.0767670425857549]
	TIME [epoch: 5.76 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05617488788041795		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.05617488788041795 | validation: 0.07684871974556172]
	TIME [epoch: 5.75 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05601842143344698		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.05601842143344698 | validation: 0.07381029007880566]
	TIME [epoch: 5.75 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05463134561472451		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.05463134561472451 | validation: 0.07799697294543423]
	TIME [epoch: 5.75 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054273891041521874		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.054273891041521874 | validation: 0.07612368723105034]
	TIME [epoch: 5.75 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05462001454969614		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.05462001454969614 | validation: 0.08285488492847465]
	TIME [epoch: 5.75 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057411635204662914		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.057411635204662914 | validation: 0.08261867392318767]
	TIME [epoch: 5.75 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059255891391864955		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.059255891391864955 | validation: 0.0770078580733835]
	TIME [epoch: 5.74 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05564523479262878		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.05564523479262878 | validation: 0.07316207171408183]
	TIME [epoch: 5.75 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05452591363932485		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.05452591363932485 | validation: 0.07353108875559063]
	TIME [epoch: 5.75 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05361960149831927		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.05361960149831927 | validation: 0.07210417579494381]
	TIME [epoch: 5.75 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05281228835559481		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.05281228835559481 | validation: 0.0679779886638277]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_1230.pth
	Model improved!!!
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051801483094816324		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.051801483094816324 | validation: 0.06917365942466719]
	TIME [epoch: 5.75 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05405330744318		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.05405330744318 | validation: 0.08018367511105902]
	TIME [epoch: 5.74 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05356724183233231		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.05356724183233231 | validation: 0.07387722290915102]
	TIME [epoch: 5.75 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054673312967360714		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.054673312967360714 | validation: 0.07523265559123699]
	TIME [epoch: 5.75 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05332993658507297		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.05332993658507297 | validation: 0.07947694655274051]
	TIME [epoch: 5.74 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054941339739017464		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.054941339739017464 | validation: 0.07650464485494686]
	TIME [epoch: 5.74 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054451552324058135		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.054451552324058135 | validation: 0.0749466329806459]
	TIME [epoch: 5.75 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05531407620136412		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.05531407620136412 | validation: 0.07527930152130446]
	TIME [epoch: 5.74 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05428932548618122		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.05428932548618122 | validation: 0.07845346584414158]
	TIME [epoch: 5.76 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05264649268072951		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.05264649268072951 | validation: 0.0743149679395311]
	TIME [epoch: 5.74 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053142852819575384		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.053142852819575384 | validation: 0.06983886340539007]
	TIME [epoch: 5.75 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05165213341711917		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.05165213341711917 | validation: 0.06855398573135156]
	TIME [epoch: 5.74 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05197191926996575		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.05197191926996575 | validation: 0.06560216361413052]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_1243.pth
	Model improved!!!
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050679345527152614		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.050679345527152614 | validation: 0.07550350260346994]
	TIME [epoch: 5.74 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05170179457722343		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.05170179457722343 | validation: 0.07635035086422698]
	TIME [epoch: 5.75 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05497321460902782		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.05497321460902782 | validation: 0.07708840337160011]
	TIME [epoch: 5.74 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058852316606089146		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.058852316606089146 | validation: 0.07516361138068937]
	TIME [epoch: 5.74 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05463739752599585		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.05463739752599585 | validation: 0.07397057964666863]
	TIME [epoch: 5.74 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050994629383698156		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.050994629383698156 | validation: 0.0743509685477218]
	TIME [epoch: 5.74 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051748674850301914		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.051748674850301914 | validation: 0.07050364500569127]
	TIME [epoch: 5.75 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051224550999800535		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.051224550999800535 | validation: 0.07061309119209752]
	TIME [epoch: 5.74 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05273009933162765		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.05273009933162765 | validation: 0.07410351268310024]
	TIME [epoch: 5.74 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05311915262710365		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.05311915262710365 | validation: 0.0742987186954999]
	TIME [epoch: 5.75 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05292622037778261		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.05292622037778261 | validation: 0.07413135193930813]
	TIME [epoch: 5.74 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05245439133718282		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.05245439133718282 | validation: 0.07743700200365061]
	TIME [epoch: 5.75 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053272534425062565		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.053272534425062565 | validation: 0.07502691819815205]
	TIME [epoch: 5.74 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05324264786715434		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.05324264786715434 | validation: 0.08328049018272071]
	TIME [epoch: 5.74 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05651680175443374		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.05651680175443374 | validation: 0.07645716796271725]
	TIME [epoch: 5.74 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05280838751168303		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.05280838751168303 | validation: 0.07169513911797992]
	TIME [epoch: 5.74 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05798014632303314		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.05798014632303314 | validation: 0.07377883149810875]
	TIME [epoch: 5.74 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053370375790333595		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.053370375790333595 | validation: 0.07056294115870289]
	TIME [epoch: 5.75 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051451587901170645		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.051451587901170645 | validation: 0.07281209493431728]
	TIME [epoch: 5.74 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05126226677890365		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.05126226677890365 | validation: 0.07138088699955718]
	TIME [epoch: 5.75 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05199031330182021		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.05199031330182021 | validation: 0.07158017709732471]
	TIME [epoch: 5.74 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05250596148033201		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.05250596148033201 | validation: 0.07366987984820353]
	TIME [epoch: 5.75 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052009429774342995		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.052009429774342995 | validation: 0.07956692871255967]
	TIME [epoch: 5.74 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052570627625477755		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.052570627625477755 | validation: 0.07664173168282101]
	TIME [epoch: 5.74 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05225060166876257		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.05225060166876257 | validation: 0.07350558709788792]
	TIME [epoch: 5.75 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052619191062523		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.052619191062523 | validation: 0.07146833171254068]
	TIME [epoch: 5.74 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05205513364196657		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.05205513364196657 | validation: 0.07520031940523349]
	TIME [epoch: 5.75 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052229825306300715		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.052229825306300715 | validation: 0.07301721891682622]
	TIME [epoch: 5.76 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05121988930139554		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.05121988930139554 | validation: 0.06598650432679228]
	TIME [epoch: 5.78 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05041912869052399		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.05041912869052399 | validation: 0.07422632615428958]
	TIME [epoch: 5.78 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051424046553544385		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.051424046553544385 | validation: 0.06707296163091363]
	TIME [epoch: 5.79 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049596346576934416		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.049596346576934416 | validation: 0.0717709948481168]
	TIME [epoch: 5.78 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05282619341188625		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.05282619341188625 | validation: 0.06840590290466318]
	TIME [epoch: 5.8 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05098091742924552		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.05098091742924552 | validation: 0.07002864658902484]
	TIME [epoch: 5.78 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050404736677590226		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.050404736677590226 | validation: 0.07131371925527505]
	TIME [epoch: 5.78 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05270196122242455		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.05270196122242455 | validation: 0.07227961348674815]
	TIME [epoch: 5.78 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0533704951538759		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.0533704951538759 | validation: 0.07562338887992032]
	TIME [epoch: 5.78 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05436374805991485		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.05436374805991485 | validation: 0.07902869113414641]
	TIME [epoch: 5.78 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05301814621292845		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.05301814621292845 | validation: 0.07311783571985446]
	TIME [epoch: 5.78 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051177094461387025		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.051177094461387025 | validation: 0.07232521188491249]
	TIME [epoch: 5.78 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0506763481392087		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.0506763481392087 | validation: 0.07316272720675053]
	TIME [epoch: 5.78 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051266373337025835		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.051266373337025835 | validation: 0.07327997809320873]
	TIME [epoch: 5.77 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05079877936392066		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.05079877936392066 | validation: 0.07522314209198498]
	TIME [epoch: 5.79 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05223134776901988		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.05223134776901988 | validation: 0.06914248816064682]
	TIME [epoch: 5.77 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051633949889303976		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.051633949889303976 | validation: 0.07580958777282877]
	TIME [epoch: 5.78 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05020332453179413		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.05020332453179413 | validation: 0.06903278131230532]
	TIME [epoch: 5.79 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05122128356851503		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.05122128356851503 | validation: 0.06832414070464428]
	TIME [epoch: 5.78 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04994466634246482		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.04994466634246482 | validation: 0.06507381370067487]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_1291.pth
	Model improved!!!
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051384929888395214		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.051384929888395214 | validation: 0.07413541345043304]
	TIME [epoch: 5.77 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0515762931917436		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.0515762931917436 | validation: 0.0702638506552318]
	TIME [epoch: 5.77 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05197930975429322		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.05197930975429322 | validation: 0.07159629399020617]
	TIME [epoch: 5.77 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05100180910514606		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.05100180910514606 | validation: 0.06957230740435542]
	TIME [epoch: 5.77 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051357369855905256		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.051357369855905256 | validation: 0.0697923469883894]
	TIME [epoch: 5.77 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050831233861323696		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.050831233861323696 | validation: 0.0709580406725943]
	TIME [epoch: 5.77 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05083540266737449		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.05083540266737449 | validation: 0.07092048053924449]
	TIME [epoch: 5.77 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05224577924044055		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.05224577924044055 | validation: 0.07436288551088194]
	TIME [epoch: 5.76 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050418807562775614		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.050418807562775614 | validation: 0.06913499592960638]
	TIME [epoch: 5.77 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05216078754067504		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.05216078754067504 | validation: 0.07022491935826278]
	TIME [epoch: 5.74 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0522526088126603		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.0522526088126603 | validation: 0.07271719105168781]
	TIME [epoch: 5.73 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0524107312405641		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.0524107312405641 | validation: 0.0695413002537185]
	TIME [epoch: 5.72 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0505785702460053		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.0505785702460053 | validation: 0.06919687116836928]
	TIME [epoch: 5.71 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053203086658890564		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.053203086658890564 | validation: 0.07546544204748756]
	TIME [epoch: 5.71 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050731644510601734		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.050731644510601734 | validation: 0.070962075967854]
	TIME [epoch: 5.76 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0484906592073431		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.0484906592073431 | validation: 0.06839810515467731]
	TIME [epoch: 5.71 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0506161692526948		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.0506161692526948 | validation: 0.06942797754987261]
	TIME [epoch: 5.71 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04795756022095193		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.04795756022095193 | validation: 0.07155550414098663]
	TIME [epoch: 5.71 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05000076721681351		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.05000076721681351 | validation: 0.07193783571115968]
	TIME [epoch: 5.71 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04755102279906215		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.04755102279906215 | validation: 0.07071098753536555]
	TIME [epoch: 5.71 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05165111169249454		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.05165111169249454 | validation: 0.07381225824672437]
	TIME [epoch: 5.72 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053382430796671036		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.053382430796671036 | validation: 0.06953752621530822]
	TIME [epoch: 5.71 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05122703708405361		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.05122703708405361 | validation: 0.07553240963868786]
	TIME [epoch: 5.71 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05194478033464554		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.05194478033464554 | validation: 0.06960843021551534]
	TIME [epoch: 5.72 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05035021763536223		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.05035021763536223 | validation: 0.06951164204261386]
	TIME [epoch: 5.71 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04909198002584652		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.04909198002584652 | validation: 0.06768429007021612]
	TIME [epoch: 5.72 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04980007785380813		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.04980007785380813 | validation: 0.06870922543919449]
	TIME [epoch: 5.71 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04968891080913707		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.04968891080913707 | validation: 0.0703073069135272]
	TIME [epoch: 5.71 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0511309485797075		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.0511309485797075 | validation: 0.07198388894431706]
	TIME [epoch: 5.71 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05031069547470583		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.05031069547470583 | validation: 0.06929985941596618]
	TIME [epoch: 5.71 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04901020673953799		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.04901020673953799 | validation: 0.07044240506384594]
	TIME [epoch: 5.72 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051179518742621395		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.051179518742621395 | validation: 0.06757540600691496]
	TIME [epoch: 5.71 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04973367800474545		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.04973367800474545 | validation: 0.07161022039405328]
	TIME [epoch: 5.71 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04915133178710537		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.04915133178710537 | validation: 0.06881481142102076]
	TIME [epoch: 5.71 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05022680915087159		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.05022680915087159 | validation: 0.0719848000173831]
	TIME [epoch: 5.71 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04934265123191813		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.04934265123191813 | validation: 0.06922346534441427]
	TIME [epoch: 5.71 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049712115214410525		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.049712115214410525 | validation: 0.07223398064039364]
	TIME [epoch: 5.71 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04972550747822385		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.04972550747822385 | validation: 0.0731335077419498]
	TIME [epoch: 5.71 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0501894251813382		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.0501894251813382 | validation: 0.07027262293698258]
	TIME [epoch: 5.71 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04974556338096688		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.04974556338096688 | validation: 0.06315669697632287]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_1331.pth
	Model improved!!!
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04835456262147039		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.04835456262147039 | validation: 0.06946076243039338]
	TIME [epoch: 5.76 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04837195733237831		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.04837195733237831 | validation: 0.07150443226446082]
	TIME [epoch: 5.75 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049002939749410376		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.049002939749410376 | validation: 0.06676835548359622]
	TIME [epoch: 5.74 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04842850876120667		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.04842850876120667 | validation: 0.07333762641810444]
	TIME [epoch: 5.75 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05234280787301678		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.05234280787301678 | validation: 0.06694519788484043]
	TIME [epoch: 5.75 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05200477198298894		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.05200477198298894 | validation: 0.06728976912508446]
	TIME [epoch: 5.75 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04959038633156312		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.04959038633156312 | validation: 0.07222119966976674]
	TIME [epoch: 5.74 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0491111244554074		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.0491111244554074 | validation: 0.0721645288880245]
	TIME [epoch: 5.74 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05142689274355746		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.05142689274355746 | validation: 0.06503578436912673]
	TIME [epoch: 5.74 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050175360213272645		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.050175360213272645 | validation: 0.0718863853636297]
	TIME [epoch: 5.71 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048021887470871855		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.048021887470871855 | validation: 0.0683342451932643]
	TIME [epoch: 5.71 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04741428330316472		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.04741428330316472 | validation: 0.06514811575555247]
	TIME [epoch: 5.71 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048790514086780194		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.048790514086780194 | validation: 0.06957725617311841]
	TIME [epoch: 5.71 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049084278579180615		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.049084278579180615 | validation: 0.0697443915296317]
	TIME [epoch: 5.71 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05118160470299772		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.05118160470299772 | validation: 0.07030316973041355]
	TIME [epoch: 5.71 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04965409171731534		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.04965409171731534 | validation: 0.06872894879222187]
	TIME [epoch: 5.71 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049735549580258914		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.049735549580258914 | validation: 0.06560471636794159]
	TIME [epoch: 5.72 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05025974002309812		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.05025974002309812 | validation: 0.06717430118803366]
	TIME [epoch: 5.72 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04966755461969184		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.04966755461969184 | validation: 0.06769709139572937]
	TIME [epoch: 5.71 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047768044805654206		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.047768044805654206 | validation: 0.07092915285512101]
	TIME [epoch: 5.71 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048997030928516934		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.048997030928516934 | validation: 0.06677325241586775]
	TIME [epoch: 5.71 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04823514842507845		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.04823514842507845 | validation: 0.07005458998033921]
	TIME [epoch: 5.72 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04919036199349883		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.04919036199349883 | validation: 0.07025321421029528]
	TIME [epoch: 5.7 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04885538788586511		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.04885538788586511 | validation: 0.07015475140643798]
	TIME [epoch: 5.71 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05082836856537145		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.05082836856537145 | validation: 0.07147567591063357]
	TIME [epoch: 5.71 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05194506506406001		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.05194506506406001 | validation: 0.06461915696303076]
	TIME [epoch: 5.71 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05067904289858958		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.05067904289858958 | validation: 0.07387451109769524]
	TIME [epoch: 5.71 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049145891170419516		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.049145891170419516 | validation: 0.06656579005491035]
	TIME [epoch: 5.7 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0479396630791625		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.0479396630791625 | validation: 0.06534914945789085]
	TIME [epoch: 5.71 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04791980249890823		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.04791980249890823 | validation: 0.06425837728572385]
	TIME [epoch: 5.7 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0474665317232841		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.0474665317232841 | validation: 0.09453817954912984]
	TIME [epoch: 5.71 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06678729846357649		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.06678729846357649 | validation: 0.08756628773473532]
	TIME [epoch: 5.71 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.064265924103539		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.064265924103539 | validation: 0.07213035726253647]
	TIME [epoch: 5.71 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0505110649565958		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.0505110649565958 | validation: 0.06515514351425934]
	TIME [epoch: 5.71 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047857543120532614		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.047857543120532614 | validation: 0.0677223486707109]
	TIME [epoch: 5.71 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049468521676530895		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.049468521676530895 | validation: 0.0655543960583803]
	TIME [epoch: 5.71 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04837987255770026		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.04837987255770026 | validation: 0.06357654815890344]
	TIME [epoch: 5.71 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048479849451850916		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.048479849451850916 | validation: 0.0675849508477962]
	TIME [epoch: 5.72 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0475440915881508		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.0475440915881508 | validation: 0.07341752511844571]
	TIME [epoch: 5.71 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04755177444751198		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.04755177444751198 | validation: 0.06680216515032844]
	TIME [epoch: 5.71 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04857230235354499		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.04857230235354499 | validation: 0.0652950560353246]
	TIME [epoch: 5.71 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04844701613787443		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.04844701613787443 | validation: 0.06691168592901296]
	TIME [epoch: 5.71 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04922758224952469		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.04922758224952469 | validation: 0.0693669588516884]
	TIME [epoch: 5.72 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04696924988940977		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.04696924988940977 | validation: 0.0659496366445359]
	TIME [epoch: 5.71 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04747174508612311		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.04747174508612311 | validation: 0.06491664420092845]
	TIME [epoch: 5.72 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047328050135327544		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.047328050135327544 | validation: 0.06653553562852516]
	TIME [epoch: 5.71 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050570810090076836		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.050570810090076836 | validation: 0.06923922342511797]
	TIME [epoch: 5.72 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04844610402008748		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.04844610402008748 | validation: 0.07068267402689656]
	TIME [epoch: 5.71 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04820042528188156		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.04820042528188156 | validation: 0.06207922887294022]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_1380.pth
	Model improved!!!
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04759176584940298		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.04759176584940298 | validation: 0.06704207200484795]
	TIME [epoch: 5.75 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04820167058807627		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.04820167058807627 | validation: 0.06985887535587341]
	TIME [epoch: 5.75 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04918307124567755		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.04918307124567755 | validation: 0.06640206585572149]
	TIME [epoch: 5.76 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045454558925962604		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.045454558925962604 | validation: 0.06763444396833233]
	TIME [epoch: 5.76 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048319115069684886		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.048319115069684886 | validation: 0.07186486394091844]
	TIME [epoch: 5.76 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049763012622934304		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.049763012622934304 | validation: 0.07096318749769831]
	TIME [epoch: 5.75 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049728839543231054		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.049728839543231054 | validation: 0.07093973253441187]
	TIME [epoch: 5.75 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04792113648062721		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.04792113648062721 | validation: 0.06447936288548954]
	TIME [epoch: 5.75 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047682114105109204		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.047682114105109204 | validation: 0.06820558043944028]
	TIME [epoch: 5.75 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04716214208112996		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.04716214208112996 | validation: 0.06579650036685918]
	TIME [epoch: 5.75 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0464581711219863		[learning rate: 8.6788e-05]
	Learning Rate: 8.67884e-05
	LOSS [training: 0.0464581711219863 | validation: 0.06728044920659863]
	TIME [epoch: 5.75 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047728597646699226		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.047728597646699226 | validation: 0.06420301013368111]
	TIME [epoch: 5.75 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04793173110584384		[learning rate: 8.6176e-05]
	Learning Rate: 8.61757e-05
	LOSS [training: 0.04793173110584384 | validation: 0.06788548143789873]
	TIME [epoch: 5.75 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04721932719064369		[learning rate: 8.5871e-05]
	Learning Rate: 8.5871e-05
	LOSS [training: 0.04721932719064369 | validation: 0.06727603709028522]
	TIME [epoch: 5.75 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04788855930066921		[learning rate: 8.5567e-05]
	Learning Rate: 8.55673e-05
	LOSS [training: 0.04788855930066921 | validation: 0.06320058909301673]
	TIME [epoch: 5.75 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048406477950313345		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.048406477950313345 | validation: 0.06876902897933458]
	TIME [epoch: 5.74 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04893330713951228		[learning rate: 8.4963e-05]
	Learning Rate: 8.49632e-05
	LOSS [training: 0.04893330713951228 | validation: 0.06761541029883598]
	TIME [epoch: 5.74 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04776433360076947		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.04776433360076947 | validation: 0.07024981426833887]
	TIME [epoch: 5.75 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04843243379009712		[learning rate: 8.4363e-05]
	Learning Rate: 8.43634e-05
	LOSS [training: 0.04843243379009712 | validation: 0.06867949750308092]
	TIME [epoch: 5.75 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04929157741395367		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.04929157741395367 | validation: 0.06446397143385069]
	TIME [epoch: 5.75 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04808599085559089		[learning rate: 8.3768e-05]
	Learning Rate: 8.37678e-05
	LOSS [training: 0.04808599085559089 | validation: 0.06823696143250504]
	TIME [epoch: 5.72 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04717074508919046		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.04717074508919046 | validation: 0.0675676377652907]
	TIME [epoch: 5.72 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04702835382767308		[learning rate: 8.3176e-05]
	Learning Rate: 8.31764e-05
	LOSS [training: 0.04702835382767308 | validation: 0.06522515042448028]
	TIME [epoch: 5.72 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04624819418676552		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.04624819418676552 | validation: 0.06471089825544299]
	TIME [epoch: 5.72 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046083452261568354		[learning rate: 8.2589e-05]
	Learning Rate: 8.25892e-05
	LOSS [training: 0.046083452261568354 | validation: 0.07045381221922407]
	TIME [epoch: 5.72 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04664145745663038		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.04664145745663038 | validation: 0.06738371922746571]
	TIME [epoch: 5.72 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04853070904992574		[learning rate: 8.2006e-05]
	Learning Rate: 8.20061e-05
	LOSS [training: 0.04853070904992574 | validation: 0.06678713784263607]
	TIME [epoch: 5.72 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04802451079433095		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.04802451079433095 | validation: 0.06824821312763653]
	TIME [epoch: 5.72 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046542786858587315		[learning rate: 8.1427e-05]
	Learning Rate: 8.14272e-05
	LOSS [training: 0.046542786858587315 | validation: 0.06651269356515041]
	TIME [epoch: 5.71 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045591007665818986		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.045591007665818986 | validation: 0.07142995978939547]
	TIME [epoch: 5.72 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04660094054672093		[learning rate: 8.0852e-05]
	Learning Rate: 8.08523e-05
	LOSS [training: 0.04660094054672093 | validation: 0.069277752658691]
	TIME [epoch: 5.74 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048034340776277794		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.048034340776277794 | validation: 0.06860768457958903]
	TIME [epoch: 5.73 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04792008747957681		[learning rate: 8.0281e-05]
	Learning Rate: 8.02815e-05
	LOSS [training: 0.04792008747957681 | validation: 0.057780121389132945]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_1413.pth
	Model improved!!!
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047392603436162986		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.047392603436162986 | validation: 0.06575485741932274]
	TIME [epoch: 5.71 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04708178471408272		[learning rate: 7.9715e-05]
	Learning Rate: 7.97147e-05
	LOSS [training: 0.04708178471408272 | validation: 0.06834378288568625]
	TIME [epoch: 5.72 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047641246842111115		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.047641246842111115 | validation: 0.06828622051291995]
	TIME [epoch: 5.72 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04575409888913173		[learning rate: 7.9152e-05]
	Learning Rate: 7.9152e-05
	LOSS [training: 0.04575409888913173 | validation: 0.0700032653720762]
	TIME [epoch: 5.71 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04607384518291371		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.04607384518291371 | validation: 0.06802456390699661]
	TIME [epoch: 5.71 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04591647487114974		[learning rate: 7.8593e-05]
	Learning Rate: 7.85931e-05
	LOSS [training: 0.04591647487114974 | validation: 0.06090162836775662]
	TIME [epoch: 5.72 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047053717621313676		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.047053717621313676 | validation: 0.06684684030830944]
	TIME [epoch: 5.73 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046965939199981606		[learning rate: 7.8038e-05]
	Learning Rate: 7.80383e-05
	LOSS [training: 0.046965939199981606 | validation: 0.06629337868863398]
	TIME [epoch: 5.71 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0475598117609154		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.0475598117609154 | validation: 0.06483147642230201]
	TIME [epoch: 5.74 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047722024938840014		[learning rate: 7.7487e-05]
	Learning Rate: 7.74873e-05
	LOSS [training: 0.047722024938840014 | validation: 0.06695814691086367]
	TIME [epoch: 5.74 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047860556611937215		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.047860556611937215 | validation: 0.0703281767118931]
	TIME [epoch: 5.74 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04760575219418728		[learning rate: 7.694e-05]
	Learning Rate: 7.69403e-05
	LOSS [training: 0.04760575219418728 | validation: 0.067309620930095]
	TIME [epoch: 5.73 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04594776726176621		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.04594776726176621 | validation: 0.06726288909936692]
	TIME [epoch: 5.74 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048171014440290164		[learning rate: 7.6397e-05]
	Learning Rate: 7.63971e-05
	LOSS [training: 0.048171014440290164 | validation: 0.07026149136939608]
	TIME [epoch: 5.74 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04750207093048535		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.04750207093048535 | validation: 0.06547074356480607]
	TIME [epoch: 5.75 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0478018877822186		[learning rate: 7.5858e-05]
	Learning Rate: 7.58578e-05
	LOSS [training: 0.0478018877822186 | validation: 0.06585257395372576]
	TIME [epoch: 5.74 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04607831794665317		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.04607831794665317 | validation: 0.06689536825954531]
	TIME [epoch: 5.73 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04772960626271699		[learning rate: 7.5322e-05]
	Learning Rate: 7.53222e-05
	LOSS [training: 0.04772960626271699 | validation: 0.0660532668859241]
	TIME [epoch: 5.74 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04691270206250172		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.04691270206250172 | validation: 0.0686379692877542]
	TIME [epoch: 5.73 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046796575180821025		[learning rate: 7.479e-05]
	Learning Rate: 7.47905e-05
	LOSS [training: 0.046796575180821025 | validation: 0.06465578675532514]
	TIME [epoch: 5.73 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046530808325836885		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.046530808325836885 | validation: 0.06488619811539552]
	TIME [epoch: 5.73 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046698500182340834		[learning rate: 7.4262e-05]
	Learning Rate: 7.42625e-05
	LOSS [training: 0.046698500182340834 | validation: 0.06459207127026066]
	TIME [epoch: 5.74 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044641066591767656		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.044641066591767656 | validation: 0.06672629391941158]
	TIME [epoch: 5.73 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04686121954963695		[learning rate: 7.3738e-05]
	Learning Rate: 7.37382e-05
	LOSS [training: 0.04686121954963695 | validation: 0.06001327113796572]
	TIME [epoch: 5.74 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04611514668987416		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.04611514668987416 | validation: 0.06368931792009305]
	TIME [epoch: 5.73 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0464548813572332		[learning rate: 7.3218e-05]
	Learning Rate: 7.32176e-05
	LOSS [training: 0.0464548813572332 | validation: 0.06111842338462123]
	TIME [epoch: 5.74 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04709449012945477		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.04709449012945477 | validation: 0.06795905625966533]
	TIME [epoch: 5.74 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051734721901210146		[learning rate: 7.2701e-05]
	Learning Rate: 7.27007e-05
	LOSS [training: 0.051734721901210146 | validation: 0.06230441291539249]
	TIME [epoch: 5.74 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048096384335417124		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.048096384335417124 | validation: 0.06419747045443029]
	TIME [epoch: 5.74 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04652631237266174		[learning rate: 7.2187e-05]
	Learning Rate: 7.21874e-05
	LOSS [training: 0.04652631237266174 | validation: 0.0658458561119627]
	TIME [epoch: 5.74 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04692589327660086		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.04692589327660086 | validation: 0.0632575048706666]
	TIME [epoch: 5.74 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04658944614867721		[learning rate: 7.1678e-05]
	Learning Rate: 7.16778e-05
	LOSS [training: 0.04658944614867721 | validation: 0.06383500479900786]
	TIME [epoch: 5.74 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04609635222507425		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.04609635222507425 | validation: 0.06247841776268656]
	TIME [epoch: 5.75 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04670366342772002		[learning rate: 7.1172e-05]
	Learning Rate: 7.11718e-05
	LOSS [training: 0.04670366342772002 | validation: 0.06862537672445473]
	TIME [epoch: 5.74 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044545444597176916		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.044545444597176916 | validation: 0.06792148304873949]
	TIME [epoch: 5.74 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04631977467335511		[learning rate: 7.0669e-05]
	Learning Rate: 7.06693e-05
	LOSS [training: 0.04631977467335511 | validation: 0.06260778770567886]
	TIME [epoch: 5.74 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048108237372752785		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.048108237372752785 | validation: 0.06420252564053049]
	TIME [epoch: 5.74 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046166224290266254		[learning rate: 7.017e-05]
	Learning Rate: 7.01704e-05
	LOSS [training: 0.046166224290266254 | validation: 0.06359472996582197]
	TIME [epoch: 5.74 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04687424471284293		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.04687424471284293 | validation: 0.06738163370805465]
	TIME [epoch: 5.73 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04677255564774695		[learning rate: 6.9675e-05]
	Learning Rate: 6.9675e-05
	LOSS [training: 0.04677255564774695 | validation: 0.06194254809108382]
	TIME [epoch: 5.73 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04655341515598243		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.04655341515598243 | validation: 0.06165049754353033]
	TIME [epoch: 5.73 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046581823119206736		[learning rate: 6.9183e-05]
	Learning Rate: 6.91831e-05
	LOSS [training: 0.046581823119206736 | validation: 0.06523657288091837]
	TIME [epoch: 5.74 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0464369054538428		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.0464369054538428 | validation: 0.06636630931672487]
	TIME [epoch: 5.74 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04616687449681047		[learning rate: 6.8695e-05]
	Learning Rate: 6.86947e-05
	LOSS [training: 0.04616687449681047 | validation: 0.06555438602310361]
	TIME [epoch: 5.74 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04904328623709336		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.04904328623709336 | validation: 0.07024890923024041]
	TIME [epoch: 5.73 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04716010622597988		[learning rate: 6.821e-05]
	Learning Rate: 6.82097e-05
	LOSS [training: 0.04716010622597988 | validation: 0.06989421077712217]
	TIME [epoch: 5.74 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04509823304815704		[learning rate: 6.7969e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.04509823304815704 | validation: 0.0609067159966057]
	TIME [epoch: 5.73 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04568772757597234		[learning rate: 6.7728e-05]
	Learning Rate: 6.77282e-05
	LOSS [training: 0.04568772757597234 | validation: 0.06645363991838096]
	TIME [epoch: 5.73 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045115406778690706		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.045115406778690706 | validation: 0.06328117016397365]
	TIME [epoch: 5.71 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04542839804151844		[learning rate: 6.725e-05]
	Learning Rate: 6.725e-05
	LOSS [training: 0.04542839804151844 | validation: 0.06426947772015743]
	TIME [epoch: 5.72 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046441824970112684		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.046441824970112684 | validation: 0.061539455802295265]
	TIME [epoch: 5.72 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046467806183813444		[learning rate: 6.6775e-05]
	Learning Rate: 6.67752e-05
	LOSS [training: 0.046467806183813444 | validation: 0.06770322791758575]
	TIME [epoch: 5.72 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0468354940496317		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.0468354940496317 | validation: 0.0657999300017807]
	TIME [epoch: 5.72 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04582620084867488		[learning rate: 6.6304e-05]
	Learning Rate: 6.63038e-05
	LOSS [training: 0.04582620084867488 | validation: 0.07169945884306916]
	TIME [epoch: 5.72 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04612137328565931		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.04612137328565931 | validation: 0.06383583758987321]
	TIME [epoch: 5.72 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04709389287506845		[learning rate: 6.5836e-05]
	Learning Rate: 6.58357e-05
	LOSS [training: 0.04709389287506845 | validation: 0.06399972839029144]
	TIME [epoch: 5.72 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0458829249013006		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.0458829249013006 | validation: 0.06320132003371988]
	TIME [epoch: 5.72 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0463527895275734		[learning rate: 6.5371e-05]
	Learning Rate: 6.53709e-05
	LOSS [training: 0.0463527895275734 | validation: 0.06568914631402387]
	TIME [epoch: 5.71 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04625189802989102		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.04625189802989102 | validation: 0.0671693881028664]
	TIME [epoch: 5.73 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04605949880274588		[learning rate: 6.4909e-05]
	Learning Rate: 6.49094e-05
	LOSS [training: 0.04605949880274588 | validation: 0.06503258744009512]
	TIME [epoch: 5.72 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0475920792201129		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.0475920792201129 | validation: 0.06418470955803561]
	TIME [epoch: 5.72 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05315068135728576		[learning rate: 6.4451e-05]
	Learning Rate: 6.44512e-05
	LOSS [training: 0.05315068135728576 | validation: 0.0608525181806274]
	TIME [epoch: 5.71 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049730182078148125		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.049730182078148125 | validation: 0.06148401652081059]
	TIME [epoch: 5.73 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04616454636117389		[learning rate: 6.3996e-05]
	Learning Rate: 6.39962e-05
	LOSS [training: 0.04616454636117389 | validation: 0.06333678355206326]
	TIME [epoch: 5.73 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04671029699582446		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.04671029699582446 | validation: 0.068814532855813]
	TIME [epoch: 5.72 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044942334436312285		[learning rate: 6.3544e-05]
	Learning Rate: 6.35444e-05
	LOSS [training: 0.044942334436312285 | validation: 0.06305999169037549]
	TIME [epoch: 5.72 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046441788861243886		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.046441788861243886 | validation: 0.06471765899811104]
	TIME [epoch: 5.72 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045284149503522546		[learning rate: 6.3096e-05]
	Learning Rate: 6.30958e-05
	LOSS [training: 0.045284149503522546 | validation: 0.06303611585351472]
	TIME [epoch: 5.72 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047114523794575494		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.047114523794575494 | validation: 0.06188235180525642]
	TIME [epoch: 5.73 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0450076129998277		[learning rate: 6.265e-05]
	Learning Rate: 6.26503e-05
	LOSS [training: 0.0450076129998277 | validation: 0.06354418525654283]
	TIME [epoch: 5.72 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045731759866522996		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.045731759866522996 | validation: 0.0649665131582877]
	TIME [epoch: 5.72 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0444008875339372		[learning rate: 6.2208e-05]
	Learning Rate: 6.2208e-05
	LOSS [training: 0.0444008875339372 | validation: 0.05975273687464327]
	TIME [epoch: 5.72 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04590658228001131		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.04590658228001131 | validation: 0.0673193092576653]
	TIME [epoch: 5.73 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04768502815399032		[learning rate: 6.1769e-05]
	Learning Rate: 6.17688e-05
	LOSS [training: 0.04768502815399032 | validation: 0.06954463781662149]
	TIME [epoch: 5.72 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048415334951162334		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.048415334951162334 | validation: 0.06411573777249646]
	TIME [epoch: 5.73 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04646032414621169		[learning rate: 6.1333e-05]
	Learning Rate: 6.13327e-05
	LOSS [training: 0.04646032414621169 | validation: 0.06506415135256868]
	TIME [epoch: 5.72 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04550276429282008		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.04550276429282008 | validation: 0.06540401535863609]
	TIME [epoch: 5.75 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04596898777203763		[learning rate: 6.09e-05]
	Learning Rate: 6.08998e-05
	LOSS [training: 0.04596898777203763 | validation: 0.06656141372320268]
	TIME [epoch: 5.74 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044891894578544686		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.044891894578544686 | validation: 0.06552795382708014]
	TIME [epoch: 5.74 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04566462634449591		[learning rate: 6.047e-05]
	Learning Rate: 6.04698e-05
	LOSS [training: 0.04566462634449591 | validation: 0.0656172073078791]
	TIME [epoch: 5.74 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04519168000564901		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.04519168000564901 | validation: 0.06536024831157532]
	TIME [epoch: 5.75 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04470925435974208		[learning rate: 6.0043e-05]
	Learning Rate: 6.00429e-05
	LOSS [training: 0.04470925435974208 | validation: 0.06726314024383083]
	TIME [epoch: 5.75 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04658793938401047		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.04658793938401047 | validation: 0.06582578102001427]
	TIME [epoch: 5.75 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04372526267079417		[learning rate: 5.9619e-05]
	Learning Rate: 5.9619e-05
	LOSS [training: 0.04372526267079417 | validation: 0.06262977529946916]
	TIME [epoch: 5.75 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04620356695744282		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.04620356695744282 | validation: 0.06197714274352987]
	TIME [epoch: 5.75 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04552619951938074		[learning rate: 5.9198e-05]
	Learning Rate: 5.91981e-05
	LOSS [training: 0.04552619951938074 | validation: 0.06258867067050744]
	TIME [epoch: 5.73 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04645951938220445		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.04645951938220445 | validation: 0.06438912579486247]
	TIME [epoch: 5.74 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044706558491941294		[learning rate: 5.878e-05]
	Learning Rate: 5.87802e-05
	LOSS [training: 0.044706558491941294 | validation: 0.06666949346324595]
	TIME [epoch: 5.74 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04557625053709768		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.04557625053709768 | validation: 0.0666284530602022]
	TIME [epoch: 5.74 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04475148687234141		[learning rate: 5.8365e-05]
	Learning Rate: 5.83652e-05
	LOSS [training: 0.04475148687234141 | validation: 0.06310675841418965]
	TIME [epoch: 5.76 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044723462099377616		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.044723462099377616 | validation: 0.06684586989856948]
	TIME [epoch: 5.73 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04442512322594376		[learning rate: 5.7953e-05]
	Learning Rate: 5.79531e-05
	LOSS [training: 0.04442512322594376 | validation: 0.06596128587596857]
	TIME [epoch: 5.73 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04553773158519554		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.04553773158519554 | validation: 0.06258405291379718]
	TIME [epoch: 5.73 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04672650501750015		[learning rate: 5.7544e-05]
	Learning Rate: 5.7544e-05
	LOSS [training: 0.04672650501750015 | validation: 0.06740207047038659]
	TIME [epoch: 5.72 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04447411847804468		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.04447411847804468 | validation: 0.06214680912467219]
	TIME [epoch: 5.73 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04534090005850238		[learning rate: 5.7138e-05]
	Learning Rate: 5.71378e-05
	LOSS [training: 0.04534090005850238 | validation: 0.06347658519189563]
	TIME [epoch: 5.73 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04588036810062534		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.04588036810062534 | validation: 0.06241376239013069]
	TIME [epoch: 5.73 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04432421400538566		[learning rate: 5.6734e-05]
	Learning Rate: 5.67344e-05
	LOSS [training: 0.04432421400538566 | validation: 0.06800397334452583]
	TIME [epoch: 5.73 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044200934461996876		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.044200934461996876 | validation: 0.06157906420320165]
	TIME [epoch: 5.72 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04491545617757337		[learning rate: 5.6334e-05]
	Learning Rate: 5.63338e-05
	LOSS [training: 0.04491545617757337 | validation: 0.06528285000974053]
	TIME [epoch: 5.72 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04477159920072659		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.04477159920072659 | validation: 0.0619085429514675]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_8_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_8_v_mmd4_1514.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 5662.042 seconds.
