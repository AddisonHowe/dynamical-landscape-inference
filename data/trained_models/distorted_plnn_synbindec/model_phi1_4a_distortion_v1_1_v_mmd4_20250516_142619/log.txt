Args:
Namespace(name='model_phi1_4a_distortion_v1_1_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_1/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_1/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.04892115, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 880558225

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.886781792645431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.886781792645431 | validation: 6.793196538332038]
	TIME [epoch: 164 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.6235032692788565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.6235032692788565 | validation: 6.113740054443599]
	TIME [epoch: 0.8 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.392432048213793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.392432048213793 | validation: 5.880073214652465]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.89280882124793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.89280882124793 | validation: 5.4403154582597875]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.215265070670975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.215265070670975 | validation: 5.371475173101683]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.07260163883659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.07260163883659 | validation: 5.038159859637522]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.931889840600925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.931889840600925 | validation: 5.2404438571149665]
	TIME [epoch: 0.699 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.93766442557727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.93766442557727 | validation: 4.996001721993804]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.852559276891691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.852559276891691 | validation: 4.832389544951474]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.755328951360062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.755328951360062 | validation: 4.876628111664867]
	TIME [epoch: 0.698 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.692549379983537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.692549379983537 | validation: 4.637915512967078]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.59909967291713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.59909967291713 | validation: 4.581165114389925]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.515551971832865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.515551971832865 | validation: 4.500599287013018]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.442455276641071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.442455276641071 | validation: 4.380140075872101]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.364044191389792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.364044191389792 | validation: 4.299651235385211]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.2753241030135545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.2753241030135545 | validation: 4.197454746446697]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.186107380986783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.186107380986783 | validation: 4.056434805627816]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.0841274151233975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.0841274151233975 | validation: 4.067683753881093]
	TIME [epoch: 0.696 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.004776577244087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.004776577244087 | validation: 3.989843509751961]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.177138268244997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.177138268244997 | validation: 4.208375172449142]
	TIME [epoch: 0.696 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.909811672913488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.909811672913488 | validation: 3.7886054281775396]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.727564043025058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.727564043025058 | validation: 3.4039079380893216]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.668849545142759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.668849545142759 | validation: 3.549078188160662]
	TIME [epoch: 0.697 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.503546587460252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.503546587460252 | validation: 3.2345150681456065]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.327654163975169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.327654163975169 | validation: 2.8950731776087837]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.175073652253478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.175073652253478 | validation: 3.945158927700974]
	TIME [epoch: 0.697 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.413908683361137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.413908683361137 | validation: 2.82279551640753]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.175614421057051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.175614421057051 | validation: 3.2679909783272367]
	TIME [epoch: 0.696 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9655881593353683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9655881593353683 | validation: 2.719929648011251]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.750573788384563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.750573788384563 | validation: 2.34275329010502]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7755931947404227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7755931947404227 | validation: 2.9460746282424335]
	TIME [epoch: 0.698 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.717972913328559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.717972913328559 | validation: 2.4419614279829864]
	TIME [epoch: 0.699 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5308060807639503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5308060807639503 | validation: 2.122368152310995]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.590290108515672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.590290108515672 | validation: 2.6613412426755634]
	TIME [epoch: 0.696 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5413978467686853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5413978467686853 | validation: 2.1214381344325406]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.33800257338045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.33800257338045 | validation: 1.953608015070379]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.387011973794854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.387011973794854 | validation: 2.368796838050969]
	TIME [epoch: 0.7 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.376238370571906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.376238370571906 | validation: 1.8894390259511553]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2169896247754908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2169896247754908 | validation: 1.8320178159870464]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2177676157646444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2177676157646444 | validation: 2.1753427572057054]
	TIME [epoch: 0.701 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2592417979194046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2592417979194046 | validation: 1.8674007290508747]
	TIME [epoch: 0.699 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.228487995195781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.228487995195781 | validation: 2.0819250614297804]
	TIME [epoch: 0.698 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2177589166227594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2177589166227594 | validation: 1.7869209573499938]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.154001126843989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.154001126843989 | validation: 1.9267342510958194]
	TIME [epoch: 0.696 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1506446048416814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1506446048416814 | validation: 1.9253750130641514]
	TIME [epoch: 0.695 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2047344439899095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2047344439899095 | validation: 2.2208164026946844]
	TIME [epoch: 0.694 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2831718632352516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2831718632352516 | validation: 1.8318543420782427]
	TIME [epoch: 0.698 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1359859202977174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1359859202977174 | validation: 2.097936471812808]
	TIME [epoch: 0.694 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1770658236877067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1770658236877067 | validation: 1.832947109767133]
	TIME [epoch: 0.697 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1177607711315125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1177607711315125 | validation: 1.9698663494703696]
	TIME [epoch: 0.701 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1095984996438255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1095984996438255 | validation: 1.824604838813336]
	TIME [epoch: 0.698 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0895420188777387		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.0895420188777387 | validation: 2.1449193575410193]
	TIME [epoch: 0.698 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.131847503122643		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 3.131847503122643 | validation: 1.8767196384784275]
	TIME [epoch: 0.697 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1658758548556634		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 3.1658758548556634 | validation: 2.253483922021851]
	TIME [epoch: 0.696 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.175351089141458		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 3.175351089141458 | validation: 1.828032669648266]
	TIME [epoch: 0.698 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.023703742974376		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 3.023703742974376 | validation: 1.8284926002375117]
	TIME [epoch: 0.696 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.013117835261015		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 3.013117835261015 | validation: 2.0551157454400832]
	TIME [epoch: 0.695 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0380323363758404		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 3.0380323363758404 | validation: 1.8711728289526737]
	TIME [epoch: 0.695 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0281846021945906		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 3.0281846021945906 | validation: 2.204056801631553]
	TIME [epoch: 0.697 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.071806672824787		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 3.071806672824787 | validation: 1.843897235462809]
	TIME [epoch: 0.696 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.045412520314343		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 3.045412520314343 | validation: 2.028549870359506]
	TIME [epoch: 0.693 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.994958950942246		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 2.994958950942246 | validation: 1.8691469976068786]
	TIME [epoch: 0.693 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9279142947552637		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 2.9279142947552637 | validation: 1.9073051067673568]
	TIME [epoch: 0.693 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.902604177036119		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 2.902604177036119 | validation: 2.0204168180087314]
	TIME [epoch: 0.692 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9236325565050776		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 2.9236325565050776 | validation: 1.871990238154792]
	TIME [epoch: 0.693 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0718426931098386		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 3.0718426931098386 | validation: 2.4212128237987347]
	TIME [epoch: 0.693 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1116357419745317		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 3.1116357419745317 | validation: 1.9378508044433105]
	TIME [epoch: 0.695 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.89040514235144		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 2.89040514235144 | validation: 1.8637729397331293]
	TIME [epoch: 0.692 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9200203811413177		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 2.9200203811413177 | validation: 2.2225622345441276]
	TIME [epoch: 0.693 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.967967183730924		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 2.967967183730924 | validation: 1.8678744808184953]
	TIME [epoch: 0.693 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.881074566190431		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 2.881074566190431 | validation: 1.9407204896771688]
	TIME [epoch: 0.694 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.811307523598753		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 2.811307523598753 | validation: 1.9509941053064033]
	TIME [epoch: 0.692 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.780003936148566		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 2.780003936148566 | validation: 1.8943726838554622]
	TIME [epoch: 0.693 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.796937039909465		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 2.796937039909465 | validation: 2.2948776444672445]
	TIME [epoch: 0.693 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8422247232344393		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 2.8422247232344393 | validation: 1.9345950069007611]
	TIME [epoch: 0.694 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8940568534260755		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 2.8940568534260755 | validation: 2.1028254948690766]
	TIME [epoch: 0.692 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.743518433697224		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 2.743518433697224 | validation: 1.9938679967363098]
	TIME [epoch: 0.694 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6281279345636173		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 2.6281279345636173 | validation: 1.9562396283673529]
	TIME [epoch: 0.694 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6175281055108077		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 2.6175281055108077 | validation: 2.773506701189968]
	TIME [epoch: 0.695 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8126767552157195		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 2.8126767552157195 | validation: 2.067875835490696]
	TIME [epoch: 0.692 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.706429298746857		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 2.706429298746857 | validation: 1.9080185344645146]
	TIME [epoch: 0.693 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4019962944224513		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 2.4019962944224513 | validation: 2.257727809557674]
	TIME [epoch: 0.693 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2596681402337624		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 2.2596681402337624 | validation: 1.8076205753830346]
	TIME [epoch: 0.693 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.303612134507477		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 2.303612134507477 | validation: 1.531797335351791]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0315796377209034		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 2.0315796377209034 | validation: 1.8796024202509956]
	TIME [epoch: 0.697 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8655868055214562		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.8655868055214562 | validation: 1.4742405051967566]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.802168792164181		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.802168792164181 | validation: 1.583235876333102]
	TIME [epoch: 0.694 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8008190913288065		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.8008190913288065 | validation: 1.5835766305510544]
	TIME [epoch: 0.694 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7085143589132432		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.7085143589132432 | validation: 1.46936152202576]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.751078449803228		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.751078449803228 | validation: 1.4348851923536294]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5672362637315262		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.5672362637315262 | validation: 1.425596871849707]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5461446833866699		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.5461446833866699 | validation: 1.3343754751116266]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5964893971914325		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.5964893971914325 | validation: 1.4799488513860966]
	TIME [epoch: 0.698 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.709941459839858		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.709941459839858 | validation: 1.3613145779890166]
	TIME [epoch: 0.695 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5351851303854727		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.5351851303854727 | validation: 1.3201812422485082]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5294407244198578		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.5294407244198578 | validation: 1.5558795330356436]
	TIME [epoch: 0.693 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.637057025701932		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.637057025701932 | validation: 1.4266984796255298]
	TIME [epoch: 0.693 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8011660773955833		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.8011660773955833 | validation: 1.2715442584688121]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.412092960369925		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.412092960369925 | validation: 1.4425278351314752]
	TIME [epoch: 0.693 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5648090305143627		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.5648090305143627 | validation: 1.3882762650812024]
	TIME [epoch: 0.691 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7744178308434948		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.7744178308434948 | validation: 1.3783006836250347]
	TIME [epoch: 0.692 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5219455947195082		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.5219455947195082 | validation: 1.3179767747606563]
	TIME [epoch: 0.691 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4251086751031783		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.4251086751031783 | validation: 1.27061669322213]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5271285445149863		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.5271285445149863 | validation: 1.3736822098362982]
	TIME [epoch: 0.694 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5476202700023007		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.5476202700023007 | validation: 1.391194678392321]
	TIME [epoch: 0.701 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.523342676072082		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.523342676072082 | validation: 1.130817889925997]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4920643436349617		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.4920643436349617 | validation: 1.4744390907138194]
	TIME [epoch: 0.697 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5191910444341603		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.5191910444341603 | validation: 1.1475699586623307]
	TIME [epoch: 0.694 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4055766600801685		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.4055766600801685 | validation: 1.1963691489630548]
	TIME [epoch: 0.697 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3585580150251144		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.3585580150251144 | validation: 1.2434322349199385]
	TIME [epoch: 0.694 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4067382081709139		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.4067382081709139 | validation: 1.2325682090937318]
	TIME [epoch: 0.695 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5377802709078947		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.5377802709078947 | validation: 1.3171770744195879]
	TIME [epoch: 0.695 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4034797195680444		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.4034797195680444 | validation: 1.1518669707999991]
	TIME [epoch: 0.697 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3359488604547716		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.3359488604547716 | validation: 1.1346623496752766]
	TIME [epoch: 0.694 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3516278861579167		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.3516278861579167 | validation: 1.2311493550628638]
	TIME [epoch: 0.696 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4085892619192228		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.4085892619192228 | validation: 1.1378024731228231]
	TIME [epoch: 0.695 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3340221609799086		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.3340221609799086 | validation: 1.1472080741897546]
	TIME [epoch: 0.697 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3556017590402945		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.3556017590402945 | validation: 1.3325362414895812]
	TIME [epoch: 0.695 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4358061005250817		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.4358061005250817 | validation: 1.1909590370660206]
	TIME [epoch: 0.696 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5477004910101988		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.5477004910101988 | validation: 1.329929340596848]
	TIME [epoch: 0.695 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3474106782089907		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.3474106782089907 | validation: 1.0783453609265092]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2530875610081704		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.2530875610081704 | validation: 1.0278773879906997]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.252324718170638		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.252324718170638 | validation: 1.305820861654911]
	TIME [epoch: 0.7 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3646336388173972		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.3646336388173972 | validation: 1.049032897159455]
	TIME [epoch: 0.699 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4982300683566263		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.4982300683566263 | validation: 1.4603116360819364]
	TIME [epoch: 0.704 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.472738759220855		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.472738759220855 | validation: 1.0061586273419967]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2092020366231886		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.2092020366231886 | validation: 1.0040771266390491]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2152004013990658		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.2152004013990658 | validation: 1.2241349909806574]
	TIME [epoch: 0.699 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2533760593521128		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.2533760593521128 | validation: 0.9750053185585872]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2694758019572912		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.2694758019572912 | validation: 1.3368610429113212]
	TIME [epoch: 0.693 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3265412944010415		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.3265412944010415 | validation: 0.9419354394937027]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3316944474415746		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.3316944474415746 | validation: 1.2178000525150219]
	TIME [epoch: 0.696 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3422065256947167		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.3422065256947167 | validation: 1.0899799213719372]
	TIME [epoch: 0.694 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.314731584312094		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.314731584312094 | validation: 1.1446783315004663]
	TIME [epoch: 0.695 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.521352993698914		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.521352993698914 | validation: 1.4298536554986452]
	TIME [epoch: 0.691 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3987048908374675		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.3987048908374675 | validation: 1.0458594048753114]
	TIME [epoch: 0.691 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2036011612439916		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.2036011612439916 | validation: 0.9421843988396355]
	TIME [epoch: 0.69 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.23095904146065		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.23095904146065 | validation: 1.23278435998997]
	TIME [epoch: 0.691 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2864011625465839		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.2864011625465839 | validation: 0.9305838537799836]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_139.pth
	Model improved!!!
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2211294042421426		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.2211294042421426 | validation: 1.123809868771961]
	TIME [epoch: 0.695 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2083092284310124		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.2083092284310124 | validation: 0.9717765406660697]
	TIME [epoch: 0.694 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2151312318658491		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.2151312318658491 | validation: 1.0425004087020646]
	TIME [epoch: 0.695 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.274249427207769		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.274249427207769 | validation: 1.3481300609542846]
	TIME [epoch: 0.694 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.40953520175219		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.40953520175219 | validation: 1.1476991959599798]
	TIME [epoch: 0.693 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.521714498629185		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.521714498629185 | validation: 1.1761452641883499]
	TIME [epoch: 0.692 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2349013054309708		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.2349013054309708 | validation: 1.142768765277062]
	TIME [epoch: 0.695 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2084397688188993		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.2084397688188993 | validation: 0.9470599921549643]
	TIME [epoch: 0.692 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2553535867187966		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.2553535867187966 | validation: 1.2607675797303648]
	TIME [epoch: 0.693 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2295155707080085		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.2295155707080085 | validation: 0.9353421499053582]
	TIME [epoch: 0.695 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1405653043688089		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.1405653043688089 | validation: 1.0149011634343699]
	TIME [epoch: 0.701 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1323738731726964		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.1323738731726964 | validation: 0.9668686037621916]
	TIME [epoch: 0.699 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1595423372073927		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.1595423372073927 | validation: 1.0288048713272209]
	TIME [epoch: 0.698 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3424296274916094		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.3424296274916094 | validation: 1.1788942279649683]
	TIME [epoch: 0.697 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2850514357992657		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.2850514357992657 | validation: 0.9628744593826545]
	TIME [epoch: 0.7 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2744984612645058		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.2744984612645058 | validation: 1.1651852545466277]
	TIME [epoch: 0.697 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2250154749687217		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.2250154749687217 | validation: 0.9507846770165408]
	TIME [epoch: 0.698 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1887170827062323		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.1887170827062323 | validation: 1.0563402225446594]
	TIME [epoch: 0.696 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.169004938696879		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.169004938696879 | validation: 0.9626266579196958]
	TIME [epoch: 0.698 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1931456056628387		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.1931456056628387 | validation: 1.0289948426913529]
	TIME [epoch: 0.697 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1842319100375156		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.1842319100375156 | validation: 0.9979738911204321]
	TIME [epoch: 0.697 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.204746379299399		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.204746379299399 | validation: 1.0336704884771701]
	TIME [epoch: 0.695 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1437229538472584		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.1437229538472584 | validation: 0.947280496102128]
	TIME [epoch: 0.697 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.192900651050145		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.192900651050145 | validation: 1.4011266184335507]
	TIME [epoch: 0.703 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3141990487876272		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.3141990487876272 | validation: 0.9658711439504792]
	TIME [epoch: 0.698 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.275539689251977		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.275539689251977 | validation: 1.157023815374448]
	TIME [epoch: 0.7 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1455444965371604		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.1455444965371604 | validation: 0.9470456193806259]
	TIME [epoch: 0.698 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0838316944662474		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.0838316944662474 | validation: 0.9407458364925984]
	TIME [epoch: 0.697 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0813921263421202		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.0813921263421202 | validation: 1.0907276368360406]
	TIME [epoch: 0.698 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.131951558439593		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.131951558439593 | validation: 0.8829688411824486]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2679471885435931		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.2679471885435931 | validation: 1.3009602362759283]
	TIME [epoch: 0.696 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2916516975334622		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.2916516975334622 | validation: 0.9194923063916743]
	TIME [epoch: 0.697 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2091919878113404		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.2091919878113404 | validation: 1.3260943153521856]
	TIME [epoch: 0.733 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2160580148578584		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.2160580148578584 | validation: 0.9564742907211745]
	TIME [epoch: 0.699 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1584941672069136		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.1584941672069136 | validation: 0.9489393453772682]
	TIME [epoch: 0.7 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1569759681451666		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.1569759681451666 | validation: 0.950177923926032]
	TIME [epoch: 0.699 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1463647962974763		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.1463647962974763 | validation: 0.9869633129515023]
	TIME [epoch: 0.701 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.075083919765156		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.075083919765156 | validation: 0.8921045062341847]
	TIME [epoch: 0.701 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1460948354206881		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.1460948354206881 | validation: 1.2668728399797256]
	TIME [epoch: 0.7 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2175755504903885		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.2175755504903885 | validation: 0.910440829364405]
	TIME [epoch: 0.699 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1800485809003458		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.1800485809003458 | validation: 1.1068286667652678]
	TIME [epoch: 0.698 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.130618806168578		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.130618806168578 | validation: 0.9486716949330529]
	TIME [epoch: 0.699 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1314120545812876		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.1314120545812876 | validation: 0.994894875297156]
	TIME [epoch: 0.699 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.094122841231385		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.094122841231385 | validation: 0.9393789859487269]
	TIME [epoch: 0.699 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0980472601825677		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.0980472601825677 | validation: 1.0364191125884978]
	TIME [epoch: 0.699 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.097669049663846		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.097669049663846 | validation: 0.8906947557198323]
	TIME [epoch: 0.698 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1569626554484984		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.1569626554484984 | validation: 1.2254983564168707]
	TIME [epoch: 0.697 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1550119757677386		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.1550119757677386 | validation: 0.9086038951537918]
	TIME [epoch: 0.697 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1944966954744525		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.1944966954744525 | validation: 1.3727763934458639]
	TIME [epoch: 0.699 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1852069273976527		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.1852069273976527 | validation: 0.856057530915432]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1316643228926957		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.1316643228926957 | validation: 1.035777045578621]
	TIME [epoch: 0.696 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0991581271220432		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.0991581271220432 | validation: 0.9093123137758865]
	TIME [epoch: 0.695 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.03233151194583		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.03233151194583 | validation: 0.973632539908482]
	TIME [epoch: 0.698 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.022739048615844		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.022739048615844 | validation: 0.9277226519971145]
	TIME [epoch: 0.696 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0206729756823272		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.0206729756823272 | validation: 0.9717362322095785]
	TIME [epoch: 0.699 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0387478684438818		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.0387478684438818 | validation: 0.8679151425563991]
	TIME [epoch: 0.699 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0821012887325099		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.0821012887325099 | validation: 1.205894515188112]
	TIME [epoch: 0.698 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0996202342514532		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.0996202342514532 | validation: 0.9576205591939543]
	TIME [epoch: 0.698 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3578575960238255		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.3578575960238255 | validation: 1.48228650788748]
	TIME [epoch: 0.698 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.226811250221437		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.226811250221437 | validation: 0.9509411361765866]
	TIME [epoch: 0.699 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0282702455444297		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.0282702455444297 | validation: 0.8470581278331132]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.068911393821681		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.068911393821681 | validation: 1.0705217471808723]
	TIME [epoch: 179 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.073797343225763		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.073797343225763 | validation: 0.8694943111801813]
	TIME [epoch: 1.39 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.005991313995753		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.005991313995753 | validation: 0.9579393553602152]
	TIME [epoch: 1.36 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9636081072336933		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.9636081072336933 | validation: 0.9363597324421953]
	TIME [epoch: 1.37 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.973481527288442		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.973481527288442 | validation: 0.8916044116272375]
	TIME [epoch: 1.36 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0073123410095874		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.0073123410095874 | validation: 1.4612058457388437]
	TIME [epoch: 1.36 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3433286828605264		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.3433286828605264 | validation: 1.0538306963318382]
	TIME [epoch: 1.37 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4725306408026864		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.4725306408026864 | validation: 1.1579785047607485]
	TIME [epoch: 1.37 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1224177637946102		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.1224177637946102 | validation: 1.173260900573272]
	TIME [epoch: 1.37 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1679667921180703		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.1679667921180703 | validation: 0.9589724303134656]
	TIME [epoch: 1.37 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1495218044011788		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.1495218044011788 | validation: 1.17610988709377]
	TIME [epoch: 1.37 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.055147798062509		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.055147798062509 | validation: 0.9349214315682949]
	TIME [epoch: 1.37 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.981909254836871		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.981909254836871 | validation: 0.8903896114848291]
	TIME [epoch: 1.37 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9744020120746808		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.9744020120746808 | validation: 1.0216188597977096]
	TIME [epoch: 1.37 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9936832922806371		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.9936832922806371 | validation: 0.8663634199250017]
	TIME [epoch: 1.37 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9892472067288987		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.9892472067288987 | validation: 1.0553546347270066]
	TIME [epoch: 1.36 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9938491323219748		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.9938491323219748 | validation: 0.8533938979600095]
	TIME [epoch: 1.37 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9945840505810746		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.9945840505810746 | validation: 1.2250731729214148]
	TIME [epoch: 1.36 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0599484679844633		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.0599484679844633 | validation: 0.9062342592567447]
	TIME [epoch: 1.36 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2513214365719132		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.2513214365719132 | validation: 1.4008668111934124]
	TIME [epoch: 1.36 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.203496047127742		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 1.203496047127742 | validation: 0.9899837822749694]
	TIME [epoch: 1.36 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0194003847854043		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.0194003847854043 | validation: 0.8803169789331053]
	TIME [epoch: 1.36 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9726366093121882		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.9726366093121882 | validation: 1.01972390142964]
	TIME [epoch: 1.36 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9460193019276382		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.9460193019276382 | validation: 0.8714012884851505]
	TIME [epoch: 1.37 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9489829726672983		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.9489829726672983 | validation: 1.0901849318697]
	TIME [epoch: 1.36 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9869818473812316		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.9869818473812316 | validation: 0.8704901543617535]
	TIME [epoch: 1.37 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0327216753413118		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.0327216753413118 | validation: 1.2598913733801065]
	TIME [epoch: 1.36 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1056025941297116		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.1056025941297116 | validation: 0.9336866121282563]
	TIME [epoch: 1.36 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.066305053945727		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 1.066305053945727 | validation: 1.1192827564353711]
	TIME [epoch: 1.37 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0150195672188487		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.0150195672188487 | validation: 0.8916611031076851]
	TIME [epoch: 1.36 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9593093542766081		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.9593093542766081 | validation: 1.080952657698856]
	TIME [epoch: 1.37 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9493724536557611		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.9493724536557611 | validation: 0.8582978490899112]
	TIME [epoch: 1.36 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9468121839197329		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.9468121839197329 | validation: 1.0658135997223919]
	TIME [epoch: 1.36 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9460835737757306		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.9460835737757306 | validation: 0.8798235749813911]
	TIME [epoch: 1.36 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0151143633541013		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 1.0151143633541013 | validation: 1.355118697151578]
	TIME [epoch: 1.37 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1441754762817544		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.1441754762817544 | validation: 0.9041378017360864]
	TIME [epoch: 1.36 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1142918423311574		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 1.1142918423311574 | validation: 1.006324753928348]
	TIME [epoch: 1.36 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9427370453477204		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.9427370453477204 | validation: 0.9344656823977857]
	TIME [epoch: 1.36 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9114076835981421		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.9114076835981421 | validation: 0.9780425477885104]
	TIME [epoch: 1.37 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9389967880830975		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.9389967880830975 | validation: 0.8969959182236494]
	TIME [epoch: 1.36 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9531640110773181		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.9531640110773181 | validation: 1.0352590236467007]
	TIME [epoch: 1.37 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9274112197450294		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.9274112197450294 | validation: 0.8123150586636904]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9469799032847876		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.9469799032847876 | validation: 1.4693125902652397]
	TIME [epoch: 1.36 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1354709617034553		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 1.1354709617034553 | validation: 0.9160064586862898]
	TIME [epoch: 1.36 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.217114237107116		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 1.217114237107116 | validation: 1.1566377726565555]
	TIME [epoch: 1.37 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9770573845336589		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.9770573845336589 | validation: 0.9686772960489419]
	TIME [epoch: 1.36 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9132619709243346		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.9132619709243346 | validation: 0.8946498096778761]
	TIME [epoch: 1.37 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8848038991825382		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.8848038991825382 | validation: 0.9892843754969495]
	TIME [epoch: 1.36 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8831161029086014		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.8831161029086014 | validation: 0.846275108876821]
	TIME [epoch: 1.36 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8794590326614629		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.8794590326614629 | validation: 1.1131487357839644]
	TIME [epoch: 1.36 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9376065035116125		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.9376065035116125 | validation: 0.8877239954235115]
	TIME [epoch: 1.36 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0770106283552574		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 1.0770106283552574 | validation: 1.2762054120287165]
	TIME [epoch: 1.37 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0545908743101875		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 1.0545908743101875 | validation: 0.918400952758018]
	TIME [epoch: 1.36 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.022389887983805		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 1.022389887983805 | validation: 1.1549746050596195]
	TIME [epoch: 1.36 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9640369407606787		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.9640369407606787 | validation: 0.8776126777377945]
	TIME [epoch: 1.36 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8827547042728864		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.8827547042728864 | validation: 1.047256063385819]
	TIME [epoch: 1.36 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8842622398009023		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.8842622398009023 | validation: 0.8869567142083398]
	TIME [epoch: 1.36 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8779590908787617		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.8779590908787617 | validation: 1.0823630193180627]
	TIME [epoch: 1.36 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8846828410685962		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.8846828410685962 | validation: 0.8215766366800891]
	TIME [epoch: 1.36 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9440506178535719		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.9440506178535719 | validation: 1.4344661395905756]
	TIME [epoch: 1.36 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1068517697665394		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 1.1068517697665394 | validation: 0.8716039124233884]
	TIME [epoch: 1.37 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0686202802048586		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 1.0686202802048586 | validation: 0.9961110526299113]
	TIME [epoch: 1.36 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8993003975757874		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.8993003975757874 | validation: 0.9428433058925594]
	TIME [epoch: 1.36 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8793708287447938		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.8793708287447938 | validation: 0.9565923474470636]
	TIME [epoch: 1.36 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9273154267398778		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.9273154267398778 | validation: 0.9191713513709839]
	TIME [epoch: 1.36 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.904929298347363		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.904929298347363 | validation: 1.1049352583085352]
	TIME [epoch: 1.36 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9196515047723983		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.9196515047723983 | validation: 0.8525102249552863]
	TIME [epoch: 1.36 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9975465418242633		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.9975465418242633 | validation: 1.303066254091819]
	TIME [epoch: 1.36 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0115370358313898		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 1.0115370358313898 | validation: 0.838160786634801]
	TIME [epoch: 1.37 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9203962268338246		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.9203962268338246 | validation: 1.0881569705222458]
	TIME [epoch: 1.36 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9137188357792115		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.9137188357792115 | validation: 0.8586649037023268]
	TIME [epoch: 1.37 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9305952863129445		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.9305952863129445 | validation: 1.0348205765572593]
	TIME [epoch: 1.36 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8947557914762809		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.8947557914762809 | validation: 0.8321464089796905]
	TIME [epoch: 1.36 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8741703171342574		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.8741703171342574 | validation: 1.0505172835926841]
	TIME [epoch: 1.36 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8634408105361767		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.8634408105361767 | validation: 0.8381878914675497]
	TIME [epoch: 1.36 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8771378311368059		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.8771378311368059 | validation: 1.2723204636868481]
	TIME [epoch: 1.36 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9753090131370874		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.9753090131370874 | validation: 0.8775320499648059]
	TIME [epoch: 1.37 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.038751701599503		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 1.038751701599503 | validation: 1.2062681808887075]
	TIME [epoch: 1.36 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.972335532095793		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.972335532095793 | validation: 0.9189653823682149]
	TIME [epoch: 1.36 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8736858657092943		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.8736858657092943 | validation: 0.894469453128905]
	TIME [epoch: 1.36 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8345417621233792		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.8345417621233792 | validation: 0.9694174034547306]
	TIME [epoch: 1.36 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.839034107156206		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.839034107156206 | validation: 0.8977730693893234]
	TIME [epoch: 1.36 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8502240384735613		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.8502240384735613 | validation: 0.9649754635969942]
	TIME [epoch: 1.36 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8426293740093876		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.8426293740093876 | validation: 0.8870178829824993]
	TIME [epoch: 1.36 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8306778663621353		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.8306778663621353 | validation: 0.8918777190839787]
	TIME [epoch: 1.36 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8300944737915242		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.8300944737915242 | validation: 1.1370070896434568]
	TIME [epoch: 1.36 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9898000301656406		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.9898000301656406 | validation: 0.8645094134140152]
	TIME [epoch: 1.36 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0867738473379376		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 1.0867738473379376 | validation: 1.6208089598886462]
	TIME [epoch: 1.36 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.156574455473076		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 1.156574455473076 | validation: 0.874950076963846]
	TIME [epoch: 1.36 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8546607702067303		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.8546607702067303 | validation: 0.8401539447407789]
	TIME [epoch: 1.36 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8433183899653685		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.8433183899653685 | validation: 1.056292800813913]
	TIME [epoch: 1.37 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8585471259218938		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.8585471259218938 | validation: 0.8259308224565569]
	TIME [epoch: 1.36 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8334181863454376		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.8334181863454376 | validation: 1.0353600455306715]
	TIME [epoch: 1.36 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8387690306675363		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.8387690306675363 | validation: 0.8613386380233191]
	TIME [epoch: 1.36 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9222557388676282		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.9222557388676282 | validation: 1.116997952077146]
	TIME [epoch: 1.36 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.894602626190407		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.894602626190407 | validation: 0.8755208566850188]
	TIME [epoch: 1.36 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9604642849496287		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.9604642849496287 | validation: 1.2185876528624906]
	TIME [epoch: 1.36 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9643311221575067		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.9643311221575067 | validation: 0.8656127727315408]
	TIME [epoch: 1.37 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8460142260249432		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.8460142260249432 | validation: 0.9645821619040721]
	TIME [epoch: 1.36 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8031800743273789		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.8031800743273789 | validation: 0.8825459555780303]
	TIME [epoch: 1.36 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7912738047096437		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.7912738047096437 | validation: 0.8846789416699461]
	TIME [epoch: 1.36 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7883169541273213		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.7883169541273213 | validation: 0.9004283872010364]
	TIME [epoch: 1.36 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7847823326166026		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.7847823326166026 | validation: 0.8654481633688124]
	TIME [epoch: 1.36 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7939781889966014		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.7939781889966014 | validation: 0.9673313948818308]
	TIME [epoch: 1.36 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8393545666260337		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.8393545666260337 | validation: 0.9468778739998348]
	TIME [epoch: 1.36 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9048098423724414		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.9048098423724414 | validation: 0.8364378121920275]
	TIME [epoch: 1.36 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.004170853348563		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 1.004170853348563 | validation: 1.5542615194566074]
	TIME [epoch: 1.36 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0945951584560145		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 1.0945951584560145 | validation: 0.7894552000509902]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9178361622799905		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.9178361622799905 | validation: 1.03454208055691]
	TIME [epoch: 1.36 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8230089092044662		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.8230089092044662 | validation: 0.871390960368283]
	TIME [epoch: 1.36 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7873403761351757		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.7873403761351757 | validation: 0.9332738578448858]
	TIME [epoch: 1.36 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8013176564718799		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.8013176564718799 | validation: 0.8447641972590333]
	TIME [epoch: 1.36 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.828917958698498		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.828917958698498 | validation: 1.1186564069368061]
	TIME [epoch: 1.36 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8795464817510084		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.8795464817510084 | validation: 0.8171030997658069]
	TIME [epoch: 1.36 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9417772911854502		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.9417772911854502 | validation: 1.2075328551709277]
	TIME [epoch: 1.36 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8870389271565767		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.8870389271565767 | validation: 0.8529989989912462]
	TIME [epoch: 1.36 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8028342574607971		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.8028342574607971 | validation: 0.9593230941703169]
	TIME [epoch: 1.36 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7705196662709938		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.7705196662709938 | validation: 0.8388652757728203]
	TIME [epoch: 1.36 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.813442835950295		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.813442835950295 | validation: 0.9923510850793722]
	TIME [epoch: 1.36 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7967913664224302		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.7967913664224302 | validation: 0.8142738305024406]
	TIME [epoch: 1.36 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8313735549160542		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.8313735549160542 | validation: 1.1231517101452035]
	TIME [epoch: 1.36 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8484841782235201		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.8484841782235201 | validation: 0.7789112353748525]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_322.pth
	Model improved!!!
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9500777568300336		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.9500777568300336 | validation: 1.2122055221376489]
	TIME [epoch: 1.36 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8739451184496827		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.8739451184496827 | validation: 0.8425157781122231]
	TIME [epoch: 1.36 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8163387731791079		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.8163387731791079 | validation: 0.9520852962467029]
	TIME [epoch: 1.36 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7897171478063593		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.7897171478063593 | validation: 0.8816164703884358]
	TIME [epoch: 1.36 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8004273749825467		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.8004273749825467 | validation: 0.9129423651140969]
	TIME [epoch: 1.36 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7927610470180566		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.7927610470180566 | validation: 0.821235083999698]
	TIME [epoch: 1.36 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8020679769051878		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.8020679769051878 | validation: 1.1021753782690902]
	TIME [epoch: 1.36 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8550855375758313		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.8550855375758313 | validation: 0.7921853366854141]
	TIME [epoch: 1.36 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9589870729374758		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.9589870729374758 | validation: 1.1640515599520487]
	TIME [epoch: 1.36 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8732272395456597		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.8732272395456597 | validation: 0.8240932804134188]
	TIME [epoch: 1.36 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7798822586562196		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.7798822586562196 | validation: 0.9154171944599252]
	TIME [epoch: 1.37 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7544830085275434		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.7544830085275434 | validation: 0.8862757807954451]
	TIME [epoch: 1.36 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7601623787564346		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.7601623787564346 | validation: 0.8757225078577765]
	TIME [epoch: 1.36 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7881713119544079		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.7881713119544079 | validation: 0.8608822552499353]
	TIME [epoch: 1.36 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8108250533993925		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.8108250533993925 | validation: 0.9903698792465385]
	TIME [epoch: 1.36 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8159615640571416		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.8159615640571416 | validation: 0.7509276671565891]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_338.pth
	Model improved!!!
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8612450599503894		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.8612450599503894 | validation: 1.3766899508199342]
	TIME [epoch: 1.36 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9525003184100048		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.9525003184100048 | validation: 0.7955205806478738]
	TIME [epoch: 1.36 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7983662171287886		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.7983662171287886 | validation: 0.9322446135135283]
	TIME [epoch: 1.37 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7491355487774939		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.7491355487774939 | validation: 0.8708417327695444]
	TIME [epoch: 1.36 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7420896827313935		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.7420896827313935 | validation: 0.7865423964170679]
	TIME [epoch: 1.36 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7513968936009134		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.7513968936009134 | validation: 0.9532446926268687]
	TIME [epoch: 1.36 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7573761865349252		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.7573761865349252 | validation: 0.770298978658781]
	TIME [epoch: 1.36 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7898503010941218		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.7898503010941218 | validation: 0.9859172441610892]
	TIME [epoch: 1.36 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7669986234227304		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.7669986234227304 | validation: 0.7487639794789072]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_347.pth
	Model improved!!!
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7866799223053609		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.7866799223053609 | validation: 1.315813858395009]
	TIME [epoch: 1.36 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9669111126568354		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.9669111126568354 | validation: 0.8061809483150104]
	TIME [epoch: 1.36 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0432695080845948		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 1.0432695080845948 | validation: 0.9873263846203422]
	TIME [epoch: 1.36 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7983066134304929		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.7983066134304929 | validation: 0.9170011576495452]
	TIME [epoch: 1.36 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7939423591173757		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.7939423591173757 | validation: 0.7995014956231488]
	TIME [epoch: 1.36 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8439250015867574		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.8439250015867574 | validation: 1.0451165945066319]
	TIME [epoch: 1.36 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7950797799054875		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.7950797799054875 | validation: 0.7693169219086764]
	TIME [epoch: 1.37 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7662251792112191		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.7662251792112191 | validation: 1.0144894357629988]
	TIME [epoch: 1.36 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7756687653103379		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.7756687653103379 | validation: 0.81021810617586]
	TIME [epoch: 1.36 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7459229182310257		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.7459229182310257 | validation: 0.9310347596357608]
	TIME [epoch: 1.36 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7317075821211042		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.7317075821211042 | validation: 0.7965946180682598]
	TIME [epoch: 1.36 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7533811503941504		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.7533811503941504 | validation: 1.0484833591705498]
	TIME [epoch: 1.36 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.855526511688416		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.855526511688416 | validation: 0.7700535122003567]
	TIME [epoch: 1.36 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9374016165847422		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.9374016165847422 | validation: 1.059037879769915]
	TIME [epoch: 1.36 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7975400434325632		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.7975400434325632 | validation: 0.8418508293413137]
	TIME [epoch: 1.36 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7928587603179352		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.7928587603179352 | validation: 0.799090533857784]
	TIME [epoch: 1.36 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7921598402588848		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.7921598402588848 | validation: 1.1238259709205571]
	TIME [epoch: 1.36 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8021169822677638		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.8021169822677638 | validation: 0.7351398733238174]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_365.pth
	Model improved!!!
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8070084153755542		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.8070084153755542 | validation: 1.0120211755965949]
	TIME [epoch: 1.36 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7537016016936159		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.7537016016936159 | validation: 0.8245962238342952]
	TIME [epoch: 1.36 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7119366739054297		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.7119366739054297 | validation: 0.8135267970349173]
	TIME [epoch: 1.35 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7078463072865714		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.7078463072865714 | validation: 0.9218277511787836]
	TIME [epoch: 1.36 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7207740282773879		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.7207740282773879 | validation: 0.7708657826858595]
	TIME [epoch: 1.36 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7448665718419818		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.7448665718419818 | validation: 1.082181434593041]
	TIME [epoch: 1.36 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7844822036592101		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.7844822036592101 | validation: 0.7595127222112567]
	TIME [epoch: 1.36 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8335447429582803		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.8335447429582803 | validation: 1.1516698870463171]
	TIME [epoch: 1.36 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.845182590346245		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.845182590346245 | validation: 0.8020614210146603]
	TIME [epoch: 1.36 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7831486777988446		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.7831486777988446 | validation: 0.8725459031653613]
	TIME [epoch: 1.36 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7304973250984169		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.7304973250984169 | validation: 0.8516279596446776]
	TIME [epoch: 1.36 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7088051379164287		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.7088051379164287 | validation: 0.8137446092515388]
	TIME [epoch: 1.36 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7120060895111294		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.7120060895111294 | validation: 0.8995225930152868]
	TIME [epoch: 1.36 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7076350545194677		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.7076350545194677 | validation: 0.7397863758593598]
	TIME [epoch: 1.36 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7698182140376879		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.7698182140376879 | validation: 1.1714248018611042]
	TIME [epoch: 1.36 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8467087550175267		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.8467087550175267 | validation: 0.746940252153729]
	TIME [epoch: 1.36 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8545963899035018		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.8545963899035018 | validation: 1.0346862887741726]
	TIME [epoch: 1.36 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7716210976575093		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.7716210976575093 | validation: 0.7865919422811979]
	TIME [epoch: 1.36 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7425613770423124		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.7425613770423124 | validation: 0.7890122358112457]
	TIME [epoch: 1.36 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6934135427768205		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.6934135427768205 | validation: 0.9548395566305932]
	TIME [epoch: 1.36 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7154905588861451		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.7154905588861451 | validation: 0.7284810162294724]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_386.pth
	Model improved!!!
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7515407506656712		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.7515407506656712 | validation: 1.0635570198635154]
	TIME [epoch: 1.36 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8563631158347162		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.8563631158347162 | validation: 0.7858322551221303]
	TIME [epoch: 1.36 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8066719240867127		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.8066719240867127 | validation: 0.8447827642589103]
	TIME [epoch: 1.36 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6816725782831585		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.6816725782831585 | validation: 0.8356564436163897]
	TIME [epoch: 1.36 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7152219148312409		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.7152219148312409 | validation: 0.8023850656682012]
	TIME [epoch: 1.36 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7959554033562936		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.7959554033562936 | validation: 0.8385436190264999]
	TIME [epoch: 1.36 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6865947817358918		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.6865947817358918 | validation: 0.765553874800168]
	TIME [epoch: 1.36 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6788908979933657		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.6788908979933657 | validation: 0.8803844159825271]
	TIME [epoch: 1.36 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6916056181666872		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.6916056181666872 | validation: 0.7377151033201461]
	TIME [epoch: 1.36 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7223289257598761		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.7223289257598761 | validation: 1.1498263543576224]
	TIME [epoch: 1.36 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7959719399544513		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.7959719399544513 | validation: 0.6983610495198644]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_397.pth
	Model improved!!!
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.820747059888897		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.820747059888897 | validation: 1.032468204624856]
	TIME [epoch: 1.36 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7508492906375979		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.7508492906375979 | validation: 0.7455946838679964]
	TIME [epoch: 1.36 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7043869926994318		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.7043869926994318 | validation: 0.8252991185711611]
	TIME [epoch: 1.36 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6884453582952804		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.6884453582952804 | validation: 0.8319054100992684]
	TIME [epoch: 1.36 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6685509244538311		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.6685509244538311 | validation: 0.760052490116488]
	TIME [epoch: 1.36 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6750124994808693		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.6750124994808693 | validation: 0.8592132746135562]
	TIME [epoch: 1.36 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6769787296343276		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.6769787296343276 | validation: 0.7489786408696708]
	TIME [epoch: 1.35 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6951538391901702		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.6951538391901702 | validation: 0.9440269382255645]
	TIME [epoch: 1.36 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7184609568832744		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.7184609568832744 | validation: 0.6814643686931927]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_406.pth
	Model improved!!!
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7886716407753775		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.7886716407753775 | validation: 1.2065059346156115]
	TIME [epoch: 1.36 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.878229190151847		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.878229190151847 | validation: 0.7817622252725916]
	TIME [epoch: 1.36 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8277564116956567		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.8277564116956567 | validation: 0.793213857471073]
	TIME [epoch: 1.37 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6955861437223038		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.6955861437223038 | validation: 0.8862554126853799]
	TIME [epoch: 1.36 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6781467584535215		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.6781467584535215 | validation: 0.6693292028191736]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_411.pth
	Model improved!!!
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7299946704428689		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.7299946704428689 | validation: 1.014418253149787]
	TIME [epoch: 1.36 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7556911417253879		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.7556911417253879 | validation: 0.7131800790804004]
	TIME [epoch: 1.36 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7327088200626652		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.7327088200626652 | validation: 0.8988248169675336]
	TIME [epoch: 1.36 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6823082103285565		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.6823082103285565 | validation: 0.7432978716063726]
	TIME [epoch: 1.36 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6490768563254655		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.6490768563254655 | validation: 0.8192189956453367]
	TIME [epoch: 1.36 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.636926370089287		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.636926370089287 | validation: 0.7298403528638245]
	TIME [epoch: 1.37 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6402933070177335		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.6402933070177335 | validation: 0.8392213980229726]
	TIME [epoch: 1.36 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6486119728732291		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.6486119728732291 | validation: 0.6850568583564769]
	TIME [epoch: 1.36 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6635813659280388		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.6635813659280388 | validation: 1.0402603799457617]
	TIME [epoch: 1.36 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7348060901510948		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.7348060901510948 | validation: 0.6967714964385938]
	TIME [epoch: 1.36 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7769405734385967		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.7769405734385967 | validation: 1.0860812737772687]
	TIME [epoch: 1.36 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7714124796893931		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.7714124796893931 | validation: 0.728751302970456]
	TIME [epoch: 1.36 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7237980338417462		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.7237980338417462 | validation: 0.7629118669122348]
	TIME [epoch: 1.36 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6681118924598388		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.6681118924598388 | validation: 0.8682003389350954]
	TIME [epoch: 1.37 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6525603993712901		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.6525603993712901 | validation: 0.642827749783919]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_426.pth
	Model improved!!!
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6700166327326585		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.6700166327326585 | validation: 0.9759754561812122]
	TIME [epoch: 1.36 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7187366292687289		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.7187366292687289 | validation: 0.6887043665884888]
	TIME [epoch: 1.36 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7666375412408746		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.7666375412408746 | validation: 0.846049009623884]
	TIME [epoch: 1.36 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6623183079244835		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.6623183079244835 | validation: 0.7595954452336523]
	TIME [epoch: 1.36 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.63464257479404		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.63464257479404 | validation: 0.7782877224418052]
	TIME [epoch: 1.37 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.629046886938146		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.629046886938146 | validation: 0.7381374832656791]
	TIME [epoch: 1.36 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6480619985225823		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.6480619985225823 | validation: 0.7725257146054418]
	TIME [epoch: 1.36 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.658613824047476		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.658613824047476 | validation: 0.7291356694337072]
	TIME [epoch: 1.36 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6448522394185475		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.6448522394185475 | validation: 0.7508323160719425]
	TIME [epoch: 1.36 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6309743830193787		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.6309743830193787 | validation: 0.7723104782887704]
	TIME [epoch: 1.36 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6130441264539391		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.6130441264539391 | validation: 0.6783313935815767]
	TIME [epoch: 1.36 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6141627778341866		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.6141627778341866 | validation: 0.9226934309405554]
	TIME [epoch: 1.37 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6610204261725774		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.6610204261725774 | validation: 0.6687841206125633]
	TIME [epoch: 1.36 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8387362573731576		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.8387362573731576 | validation: 1.2495458527193557]
	TIME [epoch: 1.37 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9305284083657952		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.9305284083657952 | validation: 0.7660176449328384]
	TIME [epoch: 1.36 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6698501381268097		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.6698501381268097 | validation: 0.655285423059008]
	TIME [epoch: 1.36 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7380814472607057		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.7380814472607057 | validation: 0.9728874529174428]
	TIME [epoch: 1.36 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6911267230570161		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.6911267230570161 | validation: 0.7167820026193257]
	TIME [epoch: 1.36 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6527056503196819		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.6527056503196819 | validation: 0.7219899875669674]
	TIME [epoch: 1.36 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6275588853884844		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.6275588853884844 | validation: 0.7312513544742052]
	TIME [epoch: 1.36 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6208600211954045		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.6208600211954045 | validation: 0.7099369465688444]
	TIME [epoch: 1.36 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6063961540314324		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.6063961540314324 | validation: 0.7300110766986656]
	TIME [epoch: 1.36 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5921021127220477		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.5921021127220477 | validation: 0.6853190044228946]
	TIME [epoch: 1.36 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5950471829399092		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.5950471829399092 | validation: 0.7853346729410368]
	TIME [epoch: 1.36 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6197095613946018		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.6197095613946018 | validation: 0.6238802053371363]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_451.pth
	Model improved!!!
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6837928686944733		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.6837928686944733 | validation: 1.067140733595231]
	TIME [epoch: 1.36 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7722773259063257		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.7722773259063257 | validation: 0.6237532834602276]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_453.pth
	Model improved!!!
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7195562489658539		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.7195562489658539 | validation: 0.8777435570697455]
	TIME [epoch: 1.36 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6341363924181742		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.6341363924181742 | validation: 0.6678944604147935]
	TIME [epoch: 1.36 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6133430690930297		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.6133430690930297 | validation: 0.7466668061219874]
	TIME [epoch: 1.36 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6019789041418458		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.6019789041418458 | validation: 0.6738414644760673]
	TIME [epoch: 1.37 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5984677307471317		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.5984677307471317 | validation: 0.771694662994782]
	TIME [epoch: 1.36 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.592874820752072		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.592874820752072 | validation: 0.6181652143148422]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_459.pth
	Model improved!!!
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6237214052407376		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.6237214052407376 | validation: 0.9327179870493965]
	TIME [epoch: 1.36 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6643809179111871		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.6643809179111871 | validation: 0.6058122248410505]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_461.pth
	Model improved!!!
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7234701728662302		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.7234701728662302 | validation: 0.9598831560854596]
	TIME [epoch: 1.36 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.677558686093638		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.677558686093638 | validation: 0.6326534619447428]
	TIME [epoch: 1.36 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5946117409370288		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.5946117409370288 | validation: 0.768711439060677]
	TIME [epoch: 1.35 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5799554585284292		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.5799554585284292 | validation: 0.6458388846640694]
	TIME [epoch: 1.35 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5713350048530819		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.5713350048530819 | validation: 0.7360887293276815]
	TIME [epoch: 1.35 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5724163091265212		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.5724163091265212 | validation: 0.6055522023970128]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5819918279960491		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.5819918279960491 | validation: 0.8732297753517243]
	TIME [epoch: 1.36 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.644866557071419		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.644866557071419 | validation: 0.5872861924479288]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_469.pth
	Model improved!!!
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7176424140285983		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.7176424140285983 | validation: 0.9815825863784031]
	TIME [epoch: 1.36 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7051943199643422		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.7051943199643422 | validation: 0.6351951136219681]
	TIME [epoch: 1.36 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6062844291099456		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.6062844291099456 | validation: 0.7362699082502351]
	TIME [epoch: 1.36 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5682446415019773		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.5682446415019773 | validation: 0.6531748410149114]
	TIME [epoch: 1.36 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5798377089976073		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.5798377089976073 | validation: 0.6842082918810326]
	TIME [epoch: 1.36 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5666288076763396		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.5666288076763396 | validation: 0.6758331721979443]
	TIME [epoch: 1.36 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6114859889090503		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.6114859889090503 | validation: 0.7813842757778059]
	TIME [epoch: 1.36 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6357677079943018		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.6357677079943018 | validation: 0.585264940967884]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_477.pth
	Model improved!!!
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6402295219702596		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.6402295219702596 | validation: 0.89076160049704]
	TIME [epoch: 1.36 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6243972761352029		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.6243972761352029 | validation: 0.5895523671912789]
	TIME [epoch: 1.37 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6490437306594737		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.6490437306594737 | validation: 0.8781272555596685]
	TIME [epoch: 1.36 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6199593512730242		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.6199593512730242 | validation: 0.5996483806633239]
	TIME [epoch: 1.36 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5638815327077377		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.5638815327077377 | validation: 0.7033173200649168]
	TIME [epoch: 1.36 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5440152238201867		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.5440152238201867 | validation: 0.6472559547049833]
	TIME [epoch: 1.36 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5436665356455869		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.5436665356455869 | validation: 0.6777724406333188]
	TIME [epoch: 1.36 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5433856283004055		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.5433856283004055 | validation: 0.6108260787589253]
	TIME [epoch: 1.36 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.542771397613675		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.542771397613675 | validation: 0.7187776082567812]
	TIME [epoch: 1.36 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5417367289743272		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.5417367289743272 | validation: 0.5550761669423849]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_487.pth
	Model improved!!!
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.589167697248386		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.589167697248386 | validation: 1.0778028163553468]
	TIME [epoch: 1.36 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7388954562474563		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.7388954562474563 | validation: 0.5788740443633229]
	TIME [epoch: 1.36 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6369678769118131		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.6369678769118131 | validation: 0.7543563118559874]
	TIME [epoch: 1.36 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6427192587542621		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.6427192587542621 | validation: 0.63331608892458]
	TIME [epoch: 1.36 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5665659336634254		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.5665659336634254 | validation: 0.6376966464838502]
	TIME [epoch: 1.36 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5230681381704324		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.5230681381704324 | validation: 0.6568614203456076]
	TIME [epoch: 1.36 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5237117709370547		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.5237117709370547 | validation: 0.6085290075032328]
	TIME [epoch: 1.36 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5261809466309775		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.5261809466309775 | validation: 0.7388780998518969]
	TIME [epoch: 1.36 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5506247372860908		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.5506247372860908 | validation: 0.5312666708061092]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_496.pth
	Model improved!!!
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6082769103724686		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.6082769103724686 | validation: 1.0837416876505512]
	TIME [epoch: 1.36 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7493098191380778		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.7493098191380778 | validation: 0.585404237017927]
	TIME [epoch: 1.36 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.559230132648005		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.559230132648005 | validation: 0.6216702049712218]
	TIME [epoch: 1.36 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5176070325723628		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.5176070325723628 | validation: 0.7405266615101368]
	TIME [epoch: 1.37 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5391887776493602		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.5391887776493602 | validation: 0.520266838054129]
	TIME [epoch: 181 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_501.pth
	Model improved!!!
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5605873940157043		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.5605873940157043 | validation: 0.8299144130044109]
	TIME [epoch: 2.7 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5714690634966841		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.5714690634966841 | validation: 0.5210373857826036]
	TIME [epoch: 2.69 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.563663240025729		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.563663240025729 | validation: 0.7619490018000853]
	TIME [epoch: 2.69 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5566247084096924		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.5566247084096924 | validation: 0.5491939829958982]
	TIME [epoch: 2.69 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5537909339861566		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.5537909339861566 | validation: 0.7134450549692413]
	TIME [epoch: 2.69 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5710607622717		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.5710607622717 | validation: 0.5665889732135592]
	TIME [epoch: 2.69 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5737493609834159		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.5737493609834159 | validation: 0.6464823833248734]
	TIME [epoch: 2.69 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5062636869956445		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.5062636869956445 | validation: 0.5922461316990802]
	TIME [epoch: 2.69 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4982208376005628		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.4982208376005628 | validation: 0.6564424418372564]
	TIME [epoch: 2.69 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4945577200468032		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.4945577200468032 | validation: 0.5630884367013651]
	TIME [epoch: 2.69 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5164468772651749		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.5164468772651749 | validation: 0.802948977022182]
	TIME [epoch: 2.69 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5775494694088554		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.5775494694088554 | validation: 0.5297609194033767]
	TIME [epoch: 2.69 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6313345401701074		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.6313345401701074 | validation: 0.8761880330395773]
	TIME [epoch: 2.69 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6198766215273784		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.6198766215273784 | validation: 0.5437859641921238]
	TIME [epoch: 2.69 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5707531604466249		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.5707531604466249 | validation: 0.600143380079477]
	TIME [epoch: 2.69 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5423850530608998		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.5423850530608998 | validation: 0.6816925249172456]
	TIME [epoch: 2.69 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5006077431934933		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.5006077431934933 | validation: 0.5302567986918468]
	TIME [epoch: 2.69 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5022214729343862		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.5022214729343862 | validation: 0.7220247555308341]
	TIME [epoch: 2.69 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.528512289229329		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.528512289229329 | validation: 0.4862779393935867]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_520.pth
	Model improved!!!
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5833285174883155		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.5833285174883155 | validation: 0.7659852892976436]
	TIME [epoch: 2.69 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5359461379727434		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.5359461379727434 | validation: 0.5215020297630459]
	TIME [epoch: 2.69 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5012187877794986		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.5012187877794986 | validation: 0.6686309623838214]
	TIME [epoch: 2.69 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4862086704796333		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.4862086704796333 | validation: 0.5062224534000882]
	TIME [epoch: 2.69 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49014850182477016		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.49014850182477016 | validation: 0.6909853934304646]
	TIME [epoch: 2.69 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49508678290791003		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.49508678290791003 | validation: 0.5012315262035342]
	TIME [epoch: 2.69 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5006373088469523		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.5006373088469523 | validation: 0.7394910326336117]
	TIME [epoch: 2.69 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5251870919975368		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.5251870919975368 | validation: 0.4751346235330005]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_528.pth
	Model improved!!!
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5158994265185146		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.5158994265185146 | validation: 0.7252606902913347]
	TIME [epoch: 2.69 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5307016093765305		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.5307016093765305 | validation: 0.5195545020813775]
	TIME [epoch: 2.69 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5743854692832223		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.5743854692832223 | validation: 0.6322538893075893]
	TIME [epoch: 2.69 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49478182341564253		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.49478182341564253 | validation: 0.5410692607316904]
	TIME [epoch: 2.69 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4598914262786167		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.4598914262786167 | validation: 0.6100762426674179]
	TIME [epoch: 2.69 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46836902481342935		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.46836902481342935 | validation: 0.5118134465336327]
	TIME [epoch: 2.69 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.479393505197421		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.479393505197421 | validation: 0.6009199772944589]
	TIME [epoch: 2.69 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4657219910768529		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.4657219910768529 | validation: 0.5091127148792188]
	TIME [epoch: 2.69 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4762200591036846		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.4762200591036846 | validation: 0.7056641416818983]
	TIME [epoch: 2.69 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49047457303165637		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.49047457303165637 | validation: 0.4555404353975827]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_538.pth
	Model improved!!!
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5656508755950362		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.5656508755950362 | validation: 0.8652141196254631]
	TIME [epoch: 2.69 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5915753721919782		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.5915753721919782 | validation: 0.5003639205442941]
	TIME [epoch: 2.69 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45616356612755754		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.45616356612755754 | validation: 0.5165821841181989]
	TIME [epoch: 2.69 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45334149051554135		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.45334149051554135 | validation: 0.6904603406948553]
	TIME [epoch: 2.69 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49006984712630913		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.49006984712630913 | validation: 0.4779555147063219]
	TIME [epoch: 2.69 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5099517725644108		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.5099517725644108 | validation: 0.6950326585155158]
	TIME [epoch: 2.69 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4877862724530407		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.4877862724530407 | validation: 0.49434060482486325]
	TIME [epoch: 2.69 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44929005525316046		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.44929005525316046 | validation: 0.5316621089529817]
	TIME [epoch: 2.69 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4368649647445308		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.4368649647445308 | validation: 0.5520719577896306]
	TIME [epoch: 2.69 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4341754046231661		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.4341754046231661 | validation: 0.4971202682440506]
	TIME [epoch: 2.69 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4380465004879737		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.4380465004879737 | validation: 0.6889348749291102]
	TIME [epoch: 2.69 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5524826662457824		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.5524826662457824 | validation: 0.42263298302431096]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_550.pth
	Model improved!!!
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6469687140832946		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.6469687140832946 | validation: 0.712597352388181]
	TIME [epoch: 2.69 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5161607270169956		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.5161607270169956 | validation: 0.5631198645255203]
	TIME [epoch: 2.69 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5017991944952533		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.5017991944952533 | validation: 0.4753526779200534]
	TIME [epoch: 2.69 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48251767987840266		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.48251767987840266 | validation: 0.6754843715532513]
	TIME [epoch: 2.69 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4670463510706096		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.4670463510706096 | validation: 0.44692655289450123]
	TIME [epoch: 2.7 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45217904390994973		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.45217904390994973 | validation: 0.5899183457099012]
	TIME [epoch: 2.69 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4339703075684765		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.4339703075684765 | validation: 0.4939552510200964]
	TIME [epoch: 2.69 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42206002964512906		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.42206002964512906 | validation: 0.5498120157129084]
	TIME [epoch: 2.68 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.415823420358946		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.415823420358946 | validation: 0.45479839944576206]
	TIME [epoch: 2.69 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4217153039161891		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.4217153039161891 | validation: 0.5711041207018533]
	TIME [epoch: 2.69 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42759698965051685		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.42759698965051685 | validation: 0.43239214840391044]
	TIME [epoch: 2.69 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4509480832861689		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.4509480832861689 | validation: 0.6906283411965586]
	TIME [epoch: 2.69 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48970550472984054		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.48970550472984054 | validation: 0.41453434394883326]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_563.pth
	Model improved!!!
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4989063414505245		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.4989063414505245 | validation: 0.6125705796709691]
	TIME [epoch: 2.69 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4335216096681412		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.4335216096681412 | validation: 0.45688268430796825]
	TIME [epoch: 2.7 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4055154945656861		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.4055154945656861 | validation: 0.5450329363663168]
	TIME [epoch: 2.69 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39386283882603434		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.39386283882603434 | validation: 0.47149914200636706]
	TIME [epoch: 2.69 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39407263003524257		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.39407263003524257 | validation: 0.5319810536482805]
	TIME [epoch: 2.69 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4051692568168588		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.4051692568168588 | validation: 0.48815367427771916]
	TIME [epoch: 2.69 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4393193675186442		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.4393193675186442 | validation: 0.6245295701452694]
	TIME [epoch: 2.69 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49605070497929915		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.49605070497929915 | validation: 0.4925057550608255]
	TIME [epoch: 2.69 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4735869433584183		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.4735869433584183 | validation: 0.5151376873740219]
	TIME [epoch: 2.69 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38604488577485313		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.38604488577485313 | validation: 0.49269351901625724]
	TIME [epoch: 2.69 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37612819338513503		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.37612819338513503 | validation: 0.45760522427488665]
	TIME [epoch: 2.69 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38603339243255513		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.38603339243255513 | validation: 0.5452304004317838]
	TIME [epoch: 2.7 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3874292156707814		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.3874292156707814 | validation: 0.3814423086577655]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_576.pth
	Model improved!!!
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4560298455774645		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.4560298455774645 | validation: 0.8027124696776458]
	TIME [epoch: 2.69 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.596861070218825		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.596861070218825 | validation: 0.45301653726756524]
	TIME [epoch: 2.69 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4724361941179913		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.4724361941179913 | validation: 0.448332067098589]
	TIME [epoch: 2.69 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41743497147626657		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.41743497147626657 | validation: 0.6390118140907437]
	TIME [epoch: 2.69 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4332677509024546		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.4332677509024546 | validation: 0.3912649860425697]
	TIME [epoch: 2.69 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41645878109177986		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.41645878109177986 | validation: 0.5530420776602467]
	TIME [epoch: 2.68 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4001549126762426		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.4001549126762426 | validation: 0.4075921711849228]
	TIME [epoch: 2.69 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4008436822275883		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.4008436822275883 | validation: 0.529044750135634]
	TIME [epoch: 2.69 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.384315026097117		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.384315026097117 | validation: 0.4111873221923519]
	TIME [epoch: 2.69 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38280655468572194		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.38280655468572194 | validation: 0.5371570846122883]
	TIME [epoch: 2.69 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37529886154384484		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.37529886154384484 | validation: 0.3897942049402403]
	TIME [epoch: 2.69 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3891433235275204		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.3891433235275204 | validation: 0.5883963622273922]
	TIME [epoch: 2.69 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41715181117940375		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.41715181117940375 | validation: 0.3849967246755184]
	TIME [epoch: 2.69 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42128762713557727		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.42128762713557727 | validation: 0.5470217369953779]
	TIME [epoch: 2.69 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40070509844021296		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.40070509844021296 | validation: 0.3998594584410331]
	TIME [epoch: 2.69 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37873504411612274		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.37873504411612274 | validation: 0.46524742363428206]
	TIME [epoch: 2.69 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37248116440575113		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.37248116440575113 | validation: 0.482996256446836]
	TIME [epoch: 2.69 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3775672961619027		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.3775672961619027 | validation: 0.43905349402244775]
	TIME [epoch: 2.69 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3959589034677574		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.3959589034677574 | validation: 0.5538561784843499]
	TIME [epoch: 2.69 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3886516885088207		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.3886516885088207 | validation: 0.37965052094435514]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_596.pth
	Model improved!!!
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38551410965143645		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.38551410965143645 | validation: 0.5633508932863006]
	TIME [epoch: 2.69 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37950579388103123		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.37950579388103123 | validation: 0.381451073146958]
	TIME [epoch: 2.7 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3669752127759177		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.3669752127759177 | validation: 0.534182853486044]
	TIME [epoch: 2.69 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36393994088834597		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.36393994088834597 | validation: 0.38047995519026323]
	TIME [epoch: 2.69 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3714776350847036		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.3714776350847036 | validation: 0.4978804811721892]
	TIME [epoch: 2.68 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38230844299630196		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.38230844299630196 | validation: 0.37177592059231435]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_602.pth
	Model improved!!!
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3605165146321406		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.3605165146321406 | validation: 0.48007844490579626]
	TIME [epoch: 2.69 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3472200903584877		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.3472200903584877 | validation: 0.3659325481776323]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_604.pth
	Model improved!!!
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3661990944026349		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.3661990944026349 | validation: 0.5077263387513141]
	TIME [epoch: 2.69 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35866303510724756		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.35866303510724756 | validation: 0.364700935543436]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_606.pth
	Model improved!!!
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37343490822490155		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.37343490822490155 | validation: 0.5692349819027035]
	TIME [epoch: 2.68 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3987867476722732		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.3987867476722732 | validation: 0.3304383441557552]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_608.pth
	Model improved!!!
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38801900463476896		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.38801900463476896 | validation: 0.47974016390013674]
	TIME [epoch: 2.68 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3406471115407298		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.3406471115407298 | validation: 0.373984001519207]
	TIME [epoch: 2.68 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33103240324405436		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.33103240324405436 | validation: 0.4751482260485704]
	TIME [epoch: 2.68 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3321501009352352		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.3321501009352352 | validation: 0.3843079263065792]
	TIME [epoch: 2.68 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34178049149835843		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.34178049149835843 | validation: 0.4893332473424888]
	TIME [epoch: 2.68 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36401074369767755		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.36401074369767755 | validation: 0.4169665698759924]
	TIME [epoch: 2.69 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38619602030678335		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.38619602030678335 | validation: 0.47524988533237833]
	TIME [epoch: 2.69 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3474445300080828		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.3474445300080828 | validation: 0.3719275224711046]
	TIME [epoch: 2.69 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31540322902851586		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.31540322902851586 | validation: 0.44314096490543975]
	TIME [epoch: 2.69 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30816626069004294		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.30816626069004294 | validation: 0.3449683922938884]
	TIME [epoch: 2.69 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32159389079260275		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.32159389079260275 | validation: 0.48661471954395197]
	TIME [epoch: 2.69 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3418208491760216		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.3418208491760216 | validation: 0.32243725935533185]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_620.pth
	Model improved!!!
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4281450223782454		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.4281450223782454 | validation: 0.5335885224725694]
	TIME [epoch: 2.68 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3672015181325052		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.3672015181325052 | validation: 0.3266761530657847]
	TIME [epoch: 2.68 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32665995350595967		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.32665995350595967 | validation: 0.4309632229930907]
	TIME [epoch: 2.68 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30080334490956534		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.30080334490956534 | validation: 0.3923725391638009]
	TIME [epoch: 2.68 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2949527439840322		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.2949527439840322 | validation: 0.42827370487586625]
	TIME [epoch: 2.68 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2959798063117048		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.2959798063117048 | validation: 0.3663532288715988]
	TIME [epoch: 2.69 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2965343542544135		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.2965343542544135 | validation: 0.4456575683178283]
	TIME [epoch: 2.69 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30749764181543543		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.30749764181543543 | validation: 0.391009685864426]
	TIME [epoch: 2.69 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34101113040391823		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.34101113040391823 | validation: 0.5131976712220264]
	TIME [epoch: 2.69 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3779045954050126		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.3779045954050126 | validation: 0.4104763052263029]
	TIME [epoch: 2.69 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3656058429711817		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.3656058429711817 | validation: 0.3593331153889366]
	TIME [epoch: 2.69 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2917053286001817		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.2917053286001817 | validation: 0.49767003011989364]
	TIME [epoch: 2.69 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31715917290826445		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.31715917290826445 | validation: 0.3106601980166073]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_633.pth
	Model improved!!!
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36284983170207163		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.36284983170207163 | validation: 0.4803431150772795]
	TIME [epoch: 2.68 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32059390617534783		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.32059390617534783 | validation: 0.32811714075025455]
	TIME [epoch: 2.67 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3300060269973969		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.3300060269973969 | validation: 0.41849500694522757]
	TIME [epoch: 2.68 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29968692409732606		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.29968692409732606 | validation: 0.35948550601119894]
	TIME [epoch: 2.68 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.292094276174724		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.292094276174724 | validation: 0.4033217741144041]
	TIME [epoch: 2.68 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27869772930158926		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.27869772930158926 | validation: 0.3484225087979671]
	TIME [epoch: 2.68 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28488973271383766		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.28488973271383766 | validation: 0.41420547297697613]
	TIME [epoch: 2.68 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2842794034459309		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.2842794034459309 | validation: 0.3120991946555802]
	TIME [epoch: 2.68 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3141288790285533		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.3141288790285533 | validation: 0.4517233370801229]
	TIME [epoch: 2.68 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3054609885006056		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.3054609885006056 | validation: 0.30454412285671273]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_643.pth
	Model improved!!!
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32026342477098224		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.32026342477098224 | validation: 0.49154800074631455]
	TIME [epoch: 2.68 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3152990895254724		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.3152990895254724 | validation: 0.28980839070752046]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_645.pth
	Model improved!!!
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.313006109961967		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.313006109961967 | validation: 0.45325480388372985]
	TIME [epoch: 2.69 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3281287340064165		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.3281287340064165 | validation: 0.32267647513815706]
	TIME [epoch: 2.69 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2944631992499908		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.2944631992499908 | validation: 0.35432413184015576]
	TIME [epoch: 2.69 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.280335232934361		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.280335232934361 | validation: 0.4463148407630783]
	TIME [epoch: 2.68 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2935473432846164		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.2935473432846164 | validation: 0.3483821122569187]
	TIME [epoch: 2.68 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2940695173481121		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.2940695173481121 | validation: 0.4062067410049885]
	TIME [epoch: 2.68 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2626096832083152		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.2626096832083152 | validation: 0.34383955834757157]
	TIME [epoch: 2.68 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25250614959557416		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.25250614959557416 | validation: 0.36832341174906863]
	TIME [epoch: 2.68 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25087866608075315		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.25087866608075315 | validation: 0.33568042208333]
	TIME [epoch: 2.68 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2518613591648029		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.2518613591648029 | validation: 0.39831929314315667]
	TIME [epoch: 2.68 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2575459032110158		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.2575459032110158 | validation: 0.2920818336736967]
	TIME [epoch: 2.68 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28384201890049376		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.28384201890049376 | validation: 0.4816935939450021]
	TIME [epoch: 2.67 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32999439518764445		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.32999439518764445 | validation: 0.26991899128705943]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_658.pth
	Model improved!!!
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3595482837342965		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.3595482837342965 | validation: 0.42523051218926194]
	TIME [epoch: 2.69 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30106842397421946		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.30106842397421946 | validation: 0.32544808847269224]
	TIME [epoch: 2.69 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2572273162904819		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.2572273162904819 | validation: 0.33077813639244874]
	TIME [epoch: 2.69 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2603084271275408		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.2603084271275408 | validation: 0.40432161849718523]
	TIME [epoch: 2.68 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2622879360547066		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.2622879360547066 | validation: 0.3191622567029211]
	TIME [epoch: 2.67 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2614852486088855		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.2614852486088855 | validation: 0.3755314663177721]
	TIME [epoch: 2.68 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24596226612492683		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.24596226612492683 | validation: 0.3257356859989477]
	TIME [epoch: 2.67 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2377584602855975		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.2377584602855975 | validation: 0.3790020733138559]
	TIME [epoch: 2.68 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24077890070889738		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.24077890070889738 | validation: 0.3049665475263651]
	TIME [epoch: 2.67 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2477145201491162		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.2477145201491162 | validation: 0.43739620665251444]
	TIME [epoch: 2.67 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2731138244460392		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.2731138244460392 | validation: 0.2644483378643389]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_669.pth
	Model improved!!!
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30198955317621323		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.30198955317621323 | validation: 0.432342178948429]
	TIME [epoch: 2.69 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2846796964598115		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.2846796964598115 | validation: 0.29813024819877637]
	TIME [epoch: 2.69 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2629493997884754		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.2629493997884754 | validation: 0.34892674529271583]
	TIME [epoch: 2.69 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24517422660460442		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.24517422660460442 | validation: 0.3366961054012902]
	TIME [epoch: 2.68 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2742135401039959		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.2742135401039959 | validation: 0.3383930536934814]
	TIME [epoch: 2.67 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.288458674449078		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.288458674449078 | validation: 0.36415674696726497]
	TIME [epoch: 2.68 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2628837690998311		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.2628837690998311 | validation: 0.31877786440172057]
	TIME [epoch: 2.67 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23565578648070207		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.23565578648070207 | validation: 0.3372531816265862]
	TIME [epoch: 2.67 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22520514327697896		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.22520514327697896 | validation: 0.3233948285207604]
	TIME [epoch: 2.69 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22064413978434413		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.22064413978434413 | validation: 0.33798249353392174]
	TIME [epoch: 2.69 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2189841475806377		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.2189841475806377 | validation: 0.28700253763359734]
	TIME [epoch: 2.68 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23349753864047404		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.23349753864047404 | validation: 0.4242502647011394]
	TIME [epoch: 2.68 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3141886217745824		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.3141886217745824 | validation: 0.24687892820257273]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_682.pth
	Model improved!!!
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36454775723749466		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.36454775723749466 | validation: 0.3599642641577252]
	TIME [epoch: 2.67 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25310142921608003		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.25310142921608003 | validation: 0.3457993148223965]
	TIME [epoch: 2.68 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26376234067378124		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.26376234067378124 | validation: 0.3134189155024445]
	TIME [epoch: 2.68 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25569164032547675		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.25569164032547675 | validation: 0.3565988418011866]
	TIME [epoch: 2.68 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22671168971342812		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.22671168971342812 | validation: 0.295981051648664]
	TIME [epoch: 2.67 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2200577327963135		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.2200577327963135 | validation: 0.35984988131437246]
	TIME [epoch: 2.67 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22738233036048738		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.22738233036048738 | validation: 0.2797580064763442]
	TIME [epoch: 2.67 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22744274022597985		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.22744274022597985 | validation: 0.3723391130242531]
	TIME [epoch: 2.67 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23478036784381606		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.23478036784381606 | validation: 0.2870789580467809]
	TIME [epoch: 2.67 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24199094538941687		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.24199094538941687 | validation: 0.37266062786562054]
	TIME [epoch: 2.68 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22780122976779915		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.22780122976779915 | validation: 0.2596998988503508]
	TIME [epoch: 2.67 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21862178720252576		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.21862178720252576 | validation: 0.35293295152449405]
	TIME [epoch: 2.67 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22274715412743706		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.22274715412743706 | validation: 0.2542866295706042]
	TIME [epoch: 2.68 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25880036608918283		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.25880036608918283 | validation: 0.356167184060586]
	TIME [epoch: 2.68 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2613197430244		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.2613197430244 | validation: 0.28041797757483833]
	TIME [epoch: 2.67 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22782703375419094		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.22782703375419094 | validation: 0.3423065520599428]
	TIME [epoch: 2.67 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21775385253952348		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.21775385253952348 | validation: 0.261799596976788]
	TIME [epoch: 2.67 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22203230819513997		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.22203230819513997 | validation: 0.37773274890637]
	TIME [epoch: 2.67 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23369336853388822		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.23369336853388822 | validation: 0.2310096341643095]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_701.pth
	Model improved!!!
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26248847883389187		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.26248847883389187 | validation: 0.4044858653780481]
	TIME [epoch: 2.68 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25351636969208585		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.25351636969208585 | validation: 0.25682655582385844]
	TIME [epoch: 2.69 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22058038889515774		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.22058038889515774 | validation: 0.30971171813402965]
	TIME [epoch: 2.69 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2035446312949629		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.2035446312949629 | validation: 0.31075445878372077]
	TIME [epoch: 2.69 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2054744189998693		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.2054744189998693 | validation: 0.286216958715567]
	TIME [epoch: 2.69 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2100023133157025		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.2100023133157025 | validation: 0.32937935797111423]
	TIME [epoch: 2.69 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22179249209273713		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.22179249209273713 | validation: 0.30173601726558136]
	TIME [epoch: 2.69 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24622791798069457		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.24622791798069457 | validation: 0.31965827261112006]
	TIME [epoch: 2.69 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24152493724094093		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.24152493724094093 | validation: 0.30184125615263846]
	TIME [epoch: 2.69 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.213323837091138		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.213323837091138 | validation: 0.2969332740663408]
	TIME [epoch: 2.69 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19733540412225575		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.19733540412225575 | validation: 0.2769297027161747]
	TIME [epoch: 2.69 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19018473131108254		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.19018473131108254 | validation: 0.32746564180975435]
	TIME [epoch: 2.69 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20607176389429846		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.20607176389429846 | validation: 0.22109016014409022]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_714.pth
	Model improved!!!
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25345465429552305		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.25345465429552305 | validation: 0.4189788096708771]
	TIME [epoch: 2.68 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26676632890939456		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.26676632890939456 | validation: 0.24333147791390886]
	TIME [epoch: 2.67 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23438863286948963		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.23438863286948963 | validation: 0.311668497182205]
	TIME [epoch: 2.68 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20332742643750393		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.20332742643750393 | validation: 0.24522352253442536]
	TIME [epoch: 2.67 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21058857914513326		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.21058857914513326 | validation: 0.2915444438198484]
	TIME [epoch: 2.67 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19463391982987607		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.19463391982987607 | validation: 0.2669145678806175]
	TIME [epoch: 2.67 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19148034869755926		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.19148034869755926 | validation: 0.29081894749594744]
	TIME [epoch: 2.67 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19391895211079566		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.19391895211079566 | validation: 0.24109218832307053]
	TIME [epoch: 2.67 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2019351118631807		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.2019351118631807 | validation: 0.3448082588191286]
	TIME [epoch: 2.67 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2510482239856636		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.2510482239856636 | validation: 0.2239454695307051]
	TIME [epoch: 2.67 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2561523969170366		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.2561523969170366 | validation: 0.3091896467600429]
	TIME [epoch: 2.67 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19949322164006944		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.19949322164006944 | validation: 0.2851261669960297]
	TIME [epoch: 2.67 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19475608708858766		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.19475608708858766 | validation: 0.2995182975266979]
	TIME [epoch: 2.67 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20531892757160403		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.20531892757160403 | validation: 0.26876671868626006]
	TIME [epoch: 2.68 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22153278948692454		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.22153278948692454 | validation: 0.32190581547049457]
	TIME [epoch: 2.67 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20792554740133554		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.20792554740133554 | validation: 0.2524540439547206]
	TIME [epoch: 2.67 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20634632091104393		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.20634632091104393 | validation: 0.31861558087613145]
	TIME [epoch: 2.67 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19522062507002214		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.19522062507002214 | validation: 0.2545806731526935]
	TIME [epoch: 2.67 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19892431526438803		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.19892431526438803 | validation: 0.30583202279088195]
	TIME [epoch: 2.67 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1849778627110783		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.1849778627110783 | validation: 0.2568126749930914]
	TIME [epoch: 2.67 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17870077206517582		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.17870077206517582 | validation: 0.29776181456284506]
	TIME [epoch: 2.67 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1867913775417129		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.1867913775417129 | validation: 0.22080396191232846]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_736.pth
	Model improved!!!
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20697604612810946		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.20697604612810946 | validation: 0.34033598315483937]
	TIME [epoch: 2.69 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22796117989646378		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.22796117989646378 | validation: 0.21433673637021677]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_738.pth
	Model improved!!!
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2209239099178098		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.2209239099178098 | validation: 0.32776786535942154]
	TIME [epoch: 2.68 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19278824432489258		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.19278824432489258 | validation: 0.23517127093335627]
	TIME [epoch: 2.68 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18866686682189915		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.18866686682189915 | validation: 0.3165496322113477]
	TIME [epoch: 2.68 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19479721441306036		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.19479721441306036 | validation: 0.23630901725230657]
	TIME [epoch: 2.68 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20460808098684893		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.20460808098684893 | validation: 0.292311388099986]
	TIME [epoch: 2.68 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20459147513815176		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.20459147513815176 | validation: 0.2715571463202441]
	TIME [epoch: 2.68 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19728442506206315		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.19728442506206315 | validation: 0.2615203500080191]
	TIME [epoch: 2.68 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1907371169336907		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.1907371169336907 | validation: 0.3344906725308329]
	TIME [epoch: 2.69 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20296671926739798		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.20296671926739798 | validation: 0.2291723408772572]
	TIME [epoch: 2.69 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22042822587226518		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.22042822587226518 | validation: 0.31801088467501093]
	TIME [epoch: 2.69 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19929015852487456		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.19929015852487456 | validation: 0.22752282007185257]
	TIME [epoch: 2.7 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18816188426182884		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.18816188426182884 | validation: 0.27795359732325053]
	TIME [epoch: 2.69 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19430314923726832		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.19430314923726832 | validation: 0.29502659500709616]
	TIME [epoch: 2.69 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20007881268217353		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.20007881268217353 | validation: 0.2392269304134485]
	TIME [epoch: 2.69 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18896180394937567		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.18896180394937567 | validation: 0.28792728011162344]
	TIME [epoch: 2.69 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17324841909063005		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.17324841909063005 | validation: 0.24507065435574368]
	TIME [epoch: 2.69 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17193714938491111		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.17193714938491111 | validation: 0.23340229602059198]
	TIME [epoch: 2.69 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1696181021801339		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.1696181021801339 | validation: 0.25844559841003995]
	TIME [epoch: 2.69 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17365792050803044		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.17365792050803044 | validation: 0.28488296876443214]
	TIME [epoch: 2.69 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19421333446529493		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.19421333446529493 | validation: 0.23124861220477452]
	TIME [epoch: 2.69 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21620716373004892		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.21620716373004892 | validation: 0.28829472261847006]
	TIME [epoch: 2.69 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19855657825481993		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.19855657825481993 | validation: 0.2014715398334147]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_760.pth
	Model improved!!!
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18994744271406983		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.18994744271406983 | validation: 0.31752450765888096]
	TIME [epoch: 2.69 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19243918665071624		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.19243918665071624 | validation: 0.22597980468681308]
	TIME [epoch: 2.69 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1975750842500071		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.1975750842500071 | validation: 0.3091589787671707]
	TIME [epoch: 2.69 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17979166563201193		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.17979166563201193 | validation: 0.22197916759456338]
	TIME [epoch: 2.69 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16943982645989333		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.16943982645989333 | validation: 0.24909906507763813]
	TIME [epoch: 2.69 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17194992194648404		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.17194992194648404 | validation: 0.2587361150428033]
	TIME [epoch: 2.69 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17874723254056782		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.17874723254056782 | validation: 0.25118343515073877]
	TIME [epoch: 2.69 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1758445833223279		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.1758445833223279 | validation: 0.27091739547632815]
	TIME [epoch: 2.69 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1803972017904259		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.1803972017904259 | validation: 0.25044228478408326]
	TIME [epoch: 2.69 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18963648768573277		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.18963648768573277 | validation: 0.2839686306835766]
	TIME [epoch: 2.69 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18918293804496641		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.18918293804496641 | validation: 0.21346594934012486]
	TIME [epoch: 2.69 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17738297115731513		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.17738297115731513 | validation: 0.28395863035688745]
	TIME [epoch: 2.69 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16872017266926442		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.16872017266926442 | validation: 0.21246643195548753]
	TIME [epoch: 2.69 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17666701842603416		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.17666701842603416 | validation: 0.28555397356899564]
	TIME [epoch: 2.69 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18773040446331607		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.18773040446331607 | validation: 0.19185057732134694]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_775.pth
	Model improved!!!
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19835090295630525		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.19835090295630525 | validation: 0.2783128095945974]
	TIME [epoch: 2.69 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17632026072544313		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.17632026072544313 | validation: 0.21405292149504496]
	TIME [epoch: 2.69 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16825724029210132		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.16825724029210132 | validation: 0.2715721878185386]
	TIME [epoch: 2.69 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16727155757582793		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.16727155757582793 | validation: 0.21165415144857516]
	TIME [epoch: 2.69 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18035172583854422		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.18035172583854422 | validation: 0.29787612697199883]
	TIME [epoch: 2.69 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1769434710333331		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.1769434710333331 | validation: 0.21377437598681417]
	TIME [epoch: 2.69 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16948211160550858		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.16948211160550858 | validation: 0.258155847698862]
	TIME [epoch: 2.7 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1608319849494984		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.1608319849494984 | validation: 0.22182144321116334]
	TIME [epoch: 2.68 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15792858002195614		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.15792858002195614 | validation: 0.2223971776641238]
	TIME [epoch: 2.69 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15688060044658442		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.15688060044658442 | validation: 0.2618761171910264]
	TIME [epoch: 2.69 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1666703111701827		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.1666703111701827 | validation: 0.19402245060806356]
	TIME [epoch: 2.69 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19221156318833507		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.19221156318833507 | validation: 0.33300477888861163]
	TIME [epoch: 2.69 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22940324459149067		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.22940324459149067 | validation: 0.18868710485384385]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_788.pth
	Model improved!!!
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2194488626879357		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.2194488626879357 | validation: 0.24113122728526118]
	TIME [epoch: 2.69 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16736285213283877		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.16736285213283877 | validation: 0.2843975533901082]
	TIME [epoch: 2.69 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17521843872000678		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.17521843872000678 | validation: 0.19646900702031012]
	TIME [epoch: 2.69 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17387846603132764		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.17387846603132764 | validation: 0.24211729240051347]
	TIME [epoch: 2.69 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1571986970014016		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.1571986970014016 | validation: 0.2331654424078117]
	TIME [epoch: 2.7 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15295203359563073		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.15295203359563073 | validation: 0.23042373790433412]
	TIME [epoch: 2.69 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15158038892472248		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.15158038892472248 | validation: 0.22602434031266286]
	TIME [epoch: 2.69 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1544703966568671		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.1544703966568671 | validation: 0.22389311272602452]
	TIME [epoch: 2.69 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16618505981307968		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.16618505981307968 | validation: 0.23904898467341848]
	TIME [epoch: 2.69 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17864262493362504		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.17864262493362504 | validation: 0.2558507960756153]
	TIME [epoch: 2.69 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1857392720449419		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.1857392720449419 | validation: 0.21978177211959204]
	TIME [epoch: 2.69 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18556155794616658		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.18556155794616658 | validation: 0.2309393726229387]
	TIME [epoch: 2.69 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15896226468418756		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.15896226468418756 | validation: 0.2330866540619493]
	TIME [epoch: 2.69 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15515638041489066		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.15515638041489066 | validation: 0.2023771353209754]
	TIME [epoch: 2.69 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1576112189822947		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.1576112189822947 | validation: 0.27699202574819676]
	TIME [epoch: 2.7 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17278959748145298		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.17278959748145298 | validation: 0.20243453151078203]
	TIME [epoch: 2.69 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19925750724124083		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.19925750724124083 | validation: 0.28987453344536246]
	TIME [epoch: 2.69 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17611187256777633		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.17611187256777633 | validation: 0.20936444713622102]
	TIME [epoch: 2.69 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15869675194980412		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.15869675194980412 | validation: 0.23096656724121412]
	TIME [epoch: 2.69 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1672773485061577		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.1672773485061577 | validation: 0.24333118305384877]
	TIME [epoch: 2.69 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1739417098467822		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.1739417098467822 | validation: 0.23752616802094334]
	TIME [epoch: 2.69 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16909624878344665		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.16909624878344665 | validation: 0.1912077784975153]
	TIME [epoch: 2.69 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1604975090249976		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.1604975090249976 | validation: 0.28039099763427766]
	TIME [epoch: 2.69 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20899274022704503		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.20899274022704503 | validation: 0.17085560116713225]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_812.pth
	Model improved!!!
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.186574850475604		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.186574850475604 | validation: 0.21414397582673522]
	TIME [epoch: 2.69 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15990230450451084		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.15990230450451084 | validation: 0.22753310065985777]
	TIME [epoch: 2.69 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15744765154189627		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.15744765154189627 | validation: 0.1946115266068744]
	TIME [epoch: 2.69 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15676374682402044		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.15676374682402044 | validation: 0.2336492437038169]
	TIME [epoch: 2.69 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15474771925485797		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.15474771925485797 | validation: 0.17934133104309422]
	TIME [epoch: 2.69 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15408418172597574		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.15408418172597574 | validation: 0.2658142463679818]
	TIME [epoch: 2.69 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16174222934300808		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.16174222934300808 | validation: 0.20985730667508903]
	TIME [epoch: 2.69 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15499279521838755		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.15499279521838755 | validation: 0.2309654493478166]
	TIME [epoch: 2.69 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15699437973767094		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.15699437973767094 | validation: 0.20824662910590677]
	TIME [epoch: 2.69 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14918037701002923		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.14918037701002923 | validation: 0.23461715917774148]
	TIME [epoch: 2.69 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1496260651037375		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.1496260651037375 | validation: 0.22550090505466588]
	TIME [epoch: 2.69 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16412509057995256		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.16412509057995256 | validation: 0.18681353744872253]
	TIME [epoch: 2.69 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18950812420519947		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.18950812420519947 | validation: 0.23166167088051967]
	TIME [epoch: 2.69 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16398835880556425		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.16398835880556425 | validation: 0.21620724768756983]
	TIME [epoch: 2.69 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14915603382862574		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.14915603382862574 | validation: 0.1978302892515521]
	TIME [epoch: 2.69 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14961297088456174		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.14961297088456174 | validation: 0.2021355599865105]
	TIME [epoch: 2.69 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15280959824482737		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.15280959824482737 | validation: 0.2516975863756574]
	TIME [epoch: 2.69 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15743168857519912		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.15743168857519912 | validation: 0.196767573399232]
	TIME [epoch: 2.69 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17557031176283872		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.17557031176283872 | validation: 0.27423763217610014]
	TIME [epoch: 2.69 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17461748569937105		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.17461748569937105 | validation: 0.18285945675865004]
	TIME [epoch: 2.69 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1516055988875246		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.1516055988875246 | validation: 0.2034217493493686]
	TIME [epoch: 2.69 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1492542599020363		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.1492542599020363 | validation: 0.21407072304617333]
	TIME [epoch: 2.69 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1527279018545541		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.1527279018545541 | validation: 0.2069288991382564]
	TIME [epoch: 2.7 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.157262140804154		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.157262140804154 | validation: 0.20154599085189853]
	TIME [epoch: 2.7 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15236694828727804		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.15236694828727804 | validation: 0.19901203779915827]
	TIME [epoch: 2.69 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14795586580634884		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.14795586580634884 | validation: 0.17802302725734506]
	TIME [epoch: 2.69 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1622164926802211		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.1622164926802211 | validation: 0.2396775784598794]
	TIME [epoch: 2.69 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17087541005510154		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.17087541005510154 | validation: 0.1704728005541829]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_840.pth
	Model improved!!!
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1629944076012201		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.1629944076012201 | validation: 0.2171737735294671]
	TIME [epoch: 2.69 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14318193269572604		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.14318193269572604 | validation: 0.18441746682311622]
	TIME [epoch: 2.69 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1434959955142885		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.1434959955142885 | validation: 0.20813876964466527]
	TIME [epoch: 2.69 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14152531272955896		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.14152531272955896 | validation: 0.18870798555345392]
	TIME [epoch: 2.69 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14768196847594314		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.14768196847594314 | validation: 0.22169154346201136]
	TIME [epoch: 2.69 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14593825474249977		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.14593825474249977 | validation: 0.17965530594201115]
	TIME [epoch: 2.69 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14346571846369827		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.14346571846369827 | validation: 0.23579829283624645]
	TIME [epoch: 2.69 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15188037586257697		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.15188037586257697 | validation: 0.18157883600712063]
	TIME [epoch: 2.69 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1599612269217172		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.1599612269217172 | validation: 0.23363336948890456]
	TIME [epoch: 2.69 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14933294947435058		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.14933294947435058 | validation: 0.17836649393619647]
	TIME [epoch: 2.69 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14162352858965246		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.14162352858965246 | validation: 0.22421007570702356]
	TIME [epoch: 2.69 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14204087850087918		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.14204087850087918 | validation: 0.18200144472973093]
	TIME [epoch: 2.69 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13916346738887567		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.13916346738887567 | validation: 0.23272152395281634]
	TIME [epoch: 2.69 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15691096400131915		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.15691096400131915 | validation: 0.16529024282092278]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_854.pth
	Model improved!!!
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19878882417126142		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.19878882417126142 | validation: 0.23730532558219944]
	TIME [epoch: 2.69 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1602742818172548		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.1602742818172548 | validation: 0.17889166675268164]
	TIME [epoch: 2.69 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13787172486602722		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.13787172486602722 | validation: 0.19403024201178945]
	TIME [epoch: 2.69 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1413676855267623		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.1413676855267623 | validation: 0.20120531560037036]
	TIME [epoch: 2.7 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14318895457797853		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.14318895457797853 | validation: 0.18348885879873947]
	TIME [epoch: 2.69 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14399611234546497		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.14399611234546497 | validation: 0.19294953546183657]
	TIME [epoch: 2.69 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.140851534444351		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.140851534444351 | validation: 0.2317314549830829]
	TIME [epoch: 2.69 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14818503792219284		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.14818503792219284 | validation: 0.1729119543737463]
	TIME [epoch: 2.69 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1573355980470076		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.1573355980470076 | validation: 0.2224372692830907]
	TIME [epoch: 2.69 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15205924947229077		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.15205924947229077 | validation: 0.1991765074204207]
	TIME [epoch: 2.69 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1450417857042341		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.1450417857042341 | validation: 0.17719097308438073]
	TIME [epoch: 2.69 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14818828111546684		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.14818828111546684 | validation: 0.22197602354887486]
	TIME [epoch: 2.69 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16162568001841138		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.16162568001841138 | validation: 0.1709630443558791]
	TIME [epoch: 2.69 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1566304339058033		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.1566304339058033 | validation: 0.22098020969611276]
	TIME [epoch: 2.69 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14968376414595524		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.14968376414595524 | validation: 0.16805222288722155]
	TIME [epoch: 2.7 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.144035232645374		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.144035232645374 | validation: 0.19030080251591208]
	TIME [epoch: 2.69 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13475466125378818		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.13475466125378818 | validation: 0.1798672348004312]
	TIME [epoch: 2.7 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13649757174597033		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.13649757174597033 | validation: 0.16848468341745085]
	TIME [epoch: 2.69 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14007080925778045		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.14007080925778045 | validation: 0.19391928355330126]
	TIME [epoch: 2.7 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14389041793779256		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.14389041793779256 | validation: 0.18267520233988202]
	TIME [epoch: 2.69 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15203964266566508		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.15203964266566508 | validation: 0.2105656309857533]
	TIME [epoch: 2.69 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14730177342746115		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.14730177342746115 | validation: 0.16834603587071975]
	TIME [epoch: 2.69 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13777020899555734		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.13777020899555734 | validation: 0.20174550415166378]
	TIME [epoch: 2.69 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13918979969951803		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.13918979969951803 | validation: 0.16424756102576185]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_878.pth
	Model improved!!!
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14393022890034302		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.14393022890034302 | validation: 0.240170575399415]
	TIME [epoch: 2.68 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1588960879341966		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.1588960879341966 | validation: 0.17953402136438995]
	TIME [epoch: 2.69 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1521076478026205		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.1521076478026205 | validation: 0.21054010008758217]
	TIME [epoch: 2.68 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13362258924011478		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.13362258924011478 | validation: 0.1737791631229985]
	TIME [epoch: 2.7 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13215098625854726		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.13215098625854726 | validation: 0.17327329601067773]
	TIME [epoch: 2.68 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1380184118793988		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.1380184118793988 | validation: 0.20094992156996125]
	TIME [epoch: 2.69 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14189680911928426		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.14189680911928426 | validation: 0.18746803127816247]
	TIME [epoch: 2.68 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1430866094867368		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.1430866094867368 | validation: 0.18562883777726383]
	TIME [epoch: 2.7 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14896618962809866		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.14896618962809866 | validation: 0.19518294250865606]
	TIME [epoch: 2.68 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14266130993332998		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.14266130993332998 | validation: 0.15115076399283245]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_888.pth
	Model improved!!!
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14202815843104863		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.14202815843104863 | validation: 0.2053586130751683]
	TIME [epoch: 2.69 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1401150776815107		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.1401150776815107 | validation: 0.16504884038933093]
	TIME [epoch: 2.69 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13873712806522506		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.13873712806522506 | validation: 0.19833475607132808]
	TIME [epoch: 2.69 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13639586691102795		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.13639586691102795 | validation: 0.1655549230927143]
	TIME [epoch: 2.69 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13525339627392302		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.13525339627392302 | validation: 0.18395473832984066]
	TIME [epoch: 2.69 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13476688372459739		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.13476688372459739 | validation: 0.16949466569589738]
	TIME [epoch: 2.69 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13585818376050177		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.13585818376050177 | validation: 0.2005978048998819]
	TIME [epoch: 2.69 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13663002887893125		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.13663002887893125 | validation: 0.1739200189065116]
	TIME [epoch: 2.69 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13832054342608144		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.13832054342608144 | validation: 0.1978845955091224]
	TIME [epoch: 2.69 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13595981798734977		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.13595981798734977 | validation: 0.16586158096116607]
	TIME [epoch: 2.69 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13294962774681449		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.13294962774681449 | validation: 0.18321274966054024]
	TIME [epoch: 2.69 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1339876698741632		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.1339876698741632 | validation: 0.19684422437572732]
	TIME [epoch: 2.69 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14406732227596938		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.14406732227596938 | validation: 0.16707591231739133]
	TIME [epoch: 2.69 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16619470917515664		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.16619470917515664 | validation: 0.2158813200648746]
	TIME [epoch: 2.68 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16423096373629137		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.16423096373629137 | validation: 0.16810025051655947]
	TIME [epoch: 2.68 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1332663640530605		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.1332663640530605 | validation: 0.17423934205483707]
	TIME [epoch: 2.68 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1325595278573168		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.1325595278573168 | validation: 0.17153317185506542]
	TIME [epoch: 2.68 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13481765729623546		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.13481765729623546 | validation: 0.16579887773196897]
	TIME [epoch: 2.68 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14644977966328607		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.14644977966328607 | validation: 0.18610086031627102]
	TIME [epoch: 2.68 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13445261093555694		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.13445261093555694 | validation: 0.1692087306855285]
	TIME [epoch: 2.69 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13125224209953942		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.13125224209953942 | validation: 0.18083870988935427]
	TIME [epoch: 2.68 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1300464336147817		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.1300464336147817 | validation: 0.1611779265193189]
	TIME [epoch: 2.68 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13068118442106652		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.13068118442106652 | validation: 0.1791186660286177]
	TIME [epoch: 2.68 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1293481459724526		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.1293481459724526 | validation: 0.19386526272972102]
	TIME [epoch: 2.69 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13154856780699864		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.13154856780699864 | validation: 0.16554144046392186]
	TIME [epoch: 2.68 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1450530363639309		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.1450530363639309 | validation: 0.2361326686707874]
	TIME [epoch: 2.68 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14463690257580072		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.14463690257580072 | validation: 0.16956004928424895]
	TIME [epoch: 2.68 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1269040187465496		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.1269040187465496 | validation: 0.17009628791312537]
	TIME [epoch: 2.68 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13064073724680392		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.13064073724680392 | validation: 0.21347261420093352]
	TIME [epoch: 2.68 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14165927958908872		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.14165927958908872 | validation: 0.17014413127633202]
	TIME [epoch: 2.68 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13353749859340164		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.13353749859340164 | validation: 0.17779738013561913]
	TIME [epoch: 2.68 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13472477122096707		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.13472477122096707 | validation: 0.1858375351319466]
	TIME [epoch: 2.68 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14194536876132793		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.14194536876132793 | validation: 0.1433766899335683]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_921.pth
	Model improved!!!
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15984307917521196		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.15984307917521196 | validation: 0.1977991535962563]
	TIME [epoch: 2.69 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14723452338175708		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.14723452338175708 | validation: 0.15110390657823303]
	TIME [epoch: 2.7 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13006593499476507		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.13006593499476507 | validation: 0.16525063465657858]
	TIME [epoch: 2.69 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13149436145270751		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.13149436145270751 | validation: 0.175704594560411]
	TIME [epoch: 2.7 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13295995943546948		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.13295995943546948 | validation: 0.163480979249935]
	TIME [epoch: 2.7 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13016579470072015		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.13016579470072015 | validation: 0.17219408148108856]
	TIME [epoch: 2.7 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12552577804908416		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.12552577804908416 | validation: 0.16720010798940654]
	TIME [epoch: 2.69 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12421968627727466		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.12421968627727466 | validation: 0.15697030406571746]
	TIME [epoch: 2.7 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1275043897873721		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.1275043897873721 | validation: 0.18511891889922413]
	TIME [epoch: 2.7 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1261475596371457		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.1261475596371457 | validation: 0.16674661143733893]
	TIME [epoch: 2.69 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13046892953590475		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.13046892953590475 | validation: 0.22280460155609108]
	TIME [epoch: 2.69 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1394165354177618		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.1394165354177618 | validation: 0.15839569876067083]
	TIME [epoch: 2.69 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13068872024313322		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.13068872024313322 | validation: 0.1677459121424479]
	TIME [epoch: 2.7 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12350059109182607		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.12350059109182607 | validation: 0.17563547567906268]
	TIME [epoch: 2.69 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12347692033839179		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.12347692033839179 | validation: 0.16033672150785946]
	TIME [epoch: 2.7 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12299422622801068		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.12299422622801068 | validation: 0.17780146180576306]
	TIME [epoch: 2.69 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12717267035949517		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.12717267035949517 | validation: 0.16109979272597708]
	TIME [epoch: 2.7 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13375217705990441		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.13375217705990441 | validation: 0.18280925826821323]
	TIME [epoch: 2.69 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13421013816818372		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.13421013816818372 | validation: 0.1626012516544534]
	TIME [epoch: 2.7 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12769951430358886		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.12769951430358886 | validation: 0.16053042288562774]
	TIME [epoch: 2.7 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1310077694207318		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.1310077694207318 | validation: 0.18569071988548574]
	TIME [epoch: 2.7 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14377928112978608		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.14377928112978608 | validation: 0.14657233785595344]
	TIME [epoch: 2.69 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14667395358911248		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.14667395358911248 | validation: 0.19150882044634776]
	TIME [epoch: 2.7 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1335933106085816		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.1335933106085816 | validation: 0.15118957218682139]
	TIME [epoch: 2.7 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12851500325661508		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.12851500325661508 | validation: 0.17629907895299654]
	TIME [epoch: 2.69 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12743300758372617		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.12743300758372617 | validation: 0.15626639126694752]
	TIME [epoch: 2.69 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12503961786042886		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.12503961786042886 | validation: 0.15763723075481847]
	TIME [epoch: 2.69 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12157456582426758		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.12157456582426758 | validation: 0.18170633009495277]
	TIME [epoch: 2.69 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1260596885956331		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.1260596885956331 | validation: 0.13039343618654572]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_950.pth
	Model improved!!!
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13148046739315353		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.13148046739315353 | validation: 0.17545862749920715]
	TIME [epoch: 2.68 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12945891825185446		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.12945891825185446 | validation: 0.1542263602834677]
	TIME [epoch: 2.68 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12792650318513377		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.12792650318513377 | validation: 0.19623932118700022]
	TIME [epoch: 2.68 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12970132379296254		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.12970132379296254 | validation: 0.15174608175757479]
	TIME [epoch: 2.68 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1247212379471004		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.1247212379471004 | validation: 0.1688336620216882]
	TIME [epoch: 2.68 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1218504221914567		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.1218504221914567 | validation: 0.1462429683126736]
	TIME [epoch: 2.69 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12673429834908184		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.12673429834908184 | validation: 0.16690619259828865]
	TIME [epoch: 2.69 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13192413657378332		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.13192413657378332 | validation: 0.1509906534780151]
	TIME [epoch: 2.68 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1379486841801639		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.1379486841801639 | validation: 0.1912249589896191]
	TIME [epoch: 2.68 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13455133517987355		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.13455133517987355 | validation: 0.13825607573615828]
	TIME [epoch: 2.68 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12766157394784639		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.12766157394784639 | validation: 0.1598193869007313]
	TIME [epoch: 2.68 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12186676831940592		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.12186676831940592 | validation: 0.17189856820871735]
	TIME [epoch: 2.68 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12528512135996764		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.12528512135996764 | validation: 0.1452389804410657]
	TIME [epoch: 2.68 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12521914245851115		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.12521914245851115 | validation: 0.1722777168115433]
	TIME [epoch: 2.68 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1243765272429326		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.1243765272429326 | validation: 0.15618675933450588]
	TIME [epoch: 2.68 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12161845302195215		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.12161845302195215 | validation: 0.17332697977046393]
	TIME [epoch: 2.68 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12408478984635139		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.12408478984635139 | validation: 0.14338229489763413]
	TIME [epoch: 2.68 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12552423935875823		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.12552423935875823 | validation: 0.17455422599793657]
	TIME [epoch: 2.68 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.125722106161176		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.125722106161176 | validation: 0.14673713072608016]
	TIME [epoch: 2.67 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12453662656275998		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.12453662656275998 | validation: 0.17912961279602896]
	TIME [epoch: 2.74 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12085217923119483		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.12085217923119483 | validation: 0.14582308800752433]
	TIME [epoch: 2.67 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12232547996842937		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.12232547996842937 | validation: 0.16850355589026172]
	TIME [epoch: 2.68 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12554125840688715		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.12554125840688715 | validation: 0.17245273562971408]
	TIME [epoch: 2.68 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13147752485396003		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.13147752485396003 | validation: 0.16158250109210287]
	TIME [epoch: 2.68 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12656737893501802		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.12656737893501802 | validation: 0.14793387051864393]
	TIME [epoch: 2.68 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12414180988507392		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.12414180988507392 | validation: 0.16165622588104164]
	TIME [epoch: 2.68 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1207445329471041		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.1207445329471041 | validation: 0.13590257695828453]
	TIME [epoch: 2.68 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12486599054231104		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.12486599054231104 | validation: 0.17562302998195037]
	TIME [epoch: 2.68 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12567655106067363		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.12567655106067363 | validation: 0.14363860615671595]
	TIME [epoch: 2.68 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12202823470815087		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.12202823470815087 | validation: 0.16491593603773516]
	TIME [epoch: 2.68 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12415693346403509		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.12415693346403509 | validation: 0.17056330335686393]
	TIME [epoch: 2.68 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12734194045162106		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.12734194045162106 | validation: 0.15932556498553932]
	TIME [epoch: 2.68 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12325611557053362		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.12325611557053362 | validation: 0.1344391235188561]
	TIME [epoch: 2.67 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16230614617000982		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.16230614617000982 | validation: 0.14797207987642746]
	TIME [epoch: 2.67 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12395935749912677		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.12395935749912677 | validation: 0.17546983905405739]
	TIME [epoch: 2.68 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12505312498506377		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.12505312498506377 | validation: 0.15378287168784122]
	TIME [epoch: 2.67 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12407426079621		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.12407426079621 | validation: 0.1554613826601619]
	TIME [epoch: 2.67 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1178563624862882		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.1178563624862882 | validation: 0.15475249626615067]
	TIME [epoch: 2.68 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11648417041476117		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.11648417041476117 | validation: 0.15618933750307057]
	TIME [epoch: 2.68 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11948670263271816		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.11948670263271816 | validation: 0.14651298869496865]
	TIME [epoch: 2.68 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11517096573206076		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.11517096573206076 | validation: 0.13859858485582208]
	TIME [epoch: 2.68 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11762714512052165		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.11762714512052165 | validation: 0.13838234062909538]
	TIME [epoch: 2.67 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11830226634287506		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.11830226634287506 | validation: 0.16486695799061626]
	TIME [epoch: 2.67 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12189995570522298		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.12189995570522298 | validation: 0.13734045860800118]
	TIME [epoch: 2.68 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1231771971633118		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.1231771971633118 | validation: 0.17728652336176673]
	TIME [epoch: 2.68 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12389605652631595		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.12389605652631595 | validation: 0.14123258606674266]
	TIME [epoch: 2.68 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12147591348321131		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.12147591348321131 | validation: 0.16788607408518647]
	TIME [epoch: 2.68 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11635206155567435		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.11635206155567435 | validation: 0.15137895676544968]
	TIME [epoch: 2.68 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11680690173534297		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.11680690173534297 | validation: 0.14796223469957473]
	TIME [epoch: 2.68 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11597453527487155		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.11597453527487155 | validation: 0.15313366674313075]
	TIME [epoch: 2.68 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1144424894679572		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.1144424894679572 | validation: 0.14003025696297033]
	TIME [epoch: 180 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11691475064789707		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.11691475064789707 | validation: 0.1654106111435093]
	TIME [epoch: 5.75 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11592423234665647		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.11592423234665647 | validation: 0.13883090727745637]
	TIME [epoch: 5.74 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12405592964132518		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.12405592964132518 | validation: 0.17980807645770588]
	TIME [epoch: 5.74 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13293072248924712		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.13293072248924712 | validation: 0.13528921347371745]
	TIME [epoch: 5.73 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12338653029813124		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.12338653029813124 | validation: 0.1682429179892749]
	TIME [epoch: 5.74 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.117903702815838		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.117903702815838 | validation: 0.14167823368861582]
	TIME [epoch: 5.73 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11386870707081315		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.11386870707081315 | validation: 0.1361294756050264]
	TIME [epoch: 5.74 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11492218161772975		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.11492218161772975 | validation: 0.15158261576660959]
	TIME [epoch: 5.74 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11569394529173979		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.11569394529173979 | validation: 0.1463381486740992]
	TIME [epoch: 5.74 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11992135803992851		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.11992135803992851 | validation: 0.1592373727123897]
	TIME [epoch: 5.74 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12284252868685454		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.12284252868685454 | validation: 0.1509974680189588]
	TIME [epoch: 5.75 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12571431596551377		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.12571431596551377 | validation: 0.15879144476386367]
	TIME [epoch: 5.74 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11775671293328874		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.11775671293328874 | validation: 0.13902480482381563]
	TIME [epoch: 5.74 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11770388684937838		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.11770388684937838 | validation: 0.15384675613297968]
	TIME [epoch: 5.74 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11405010712915552		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.11405010712915552 | validation: 0.14047032821776426]
	TIME [epoch: 5.73 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11783436139475682		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.11783436139475682 | validation: 0.13638895322426678]
	TIME [epoch: 5.74 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11719874237949483		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.11719874237949483 | validation: 0.13454988344359806]
	TIME [epoch: 5.73 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1161151245795994		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.1161151245795994 | validation: 0.14083709415145285]
	TIME [epoch: 5.74 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11577623014270688		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.11577623014270688 | validation: 0.13931713472733406]
	TIME [epoch: 5.73 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11311880857086404		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.11311880857086404 | validation: 0.14931463062283687]
	TIME [epoch: 5.74 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11843763897958719		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.11843763897958719 | validation: 0.13004855211576463]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_1022.pth
	Model improved!!!
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1259224781140392		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.1259224781140392 | validation: 0.17275024252296112]
	TIME [epoch: 5.73 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1264349467859593		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.1264349467859593 | validation: 0.13560997218328005]
	TIME [epoch: 5.73 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12317890151917762		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.12317890151917762 | validation: 0.20519612932389475]
	TIME [epoch: 5.74 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18487486360252575		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.18487486360252575 | validation: 0.16529114770472098]
	TIME [epoch: 5.74 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13362503296078457		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.13362503296078457 | validation: 0.134551864586403]
	TIME [epoch: 5.75 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11613853921245049		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.11613853921245049 | validation: 0.12906108606664343]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_1028.pth
	Model improved!!!
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1197254607644325		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.1197254607644325 | validation: 0.14929315834434106]
	TIME [epoch: 5.77 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11908163261093577		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.11908163261093577 | validation: 0.1406114171750336]
	TIME [epoch: 5.77 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11406937291716673		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.11406937291716673 | validation: 0.14255104118668102]
	TIME [epoch: 5.78 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11694778662631734		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.11694778662631734 | validation: 0.13842701806302796]
	TIME [epoch: 5.77 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11289757230278966		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.11289757230278966 | validation: 0.12756596206535567]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_1033.pth
	Model improved!!!
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11421147983997719		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.11421147983997719 | validation: 0.13546684605979153]
	TIME [epoch: 5.77 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11174643272513618		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.11174643272513618 | validation: 0.137932191177265]
	TIME [epoch: 5.77 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11060250062738558		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.11060250062738558 | validation: 0.15995555537799575]
	TIME [epoch: 5.77 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1137204224323356		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.1137204224323356 | validation: 0.13292374066317314]
	TIME [epoch: 5.78 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11570777700344462		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.11570777700344462 | validation: 0.13706072420900678]
	TIME [epoch: 5.77 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10888422716072584		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.10888422716072584 | validation: 0.13851311287640025]
	TIME [epoch: 5.77 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11213422281161814		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.11213422281161814 | validation: 0.1310616719687159]
	TIME [epoch: 5.77 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1145049984942326		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.1145049984942326 | validation: 0.15699872739360987]
	TIME [epoch: 5.77 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11740387356409042		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.11740387356409042 | validation: 0.1377109800952647]
	TIME [epoch: 5.76 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11538748612547477		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.11538748612547477 | validation: 0.13821590226176883]
	TIME [epoch: 5.75 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11254431983805026		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.11254431983805026 | validation: 0.13912417932465335]
	TIME [epoch: 5.74 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11672791568239793		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.11672791568239793 | validation: 0.13841501434707504]
	TIME [epoch: 5.73 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11462877429426119		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.11462877429426119 | validation: 0.1352115560467552]
	TIME [epoch: 5.73 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1133499756311564		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.1133499756311564 | validation: 0.1405085520109199]
	TIME [epoch: 5.73 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11290357155978395		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.11290357155978395 | validation: 0.1353910358157546]
	TIME [epoch: 5.74 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11043682527046692		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.11043682527046692 | validation: 0.13353753986924582]
	TIME [epoch: 5.73 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1126709773903352		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.1126709773903352 | validation: 0.1391534388994724]
	TIME [epoch: 5.73 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11122830174920827		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.11122830174920827 | validation: 0.13719602427862437]
	TIME [epoch: 5.73 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11293401047634148		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.11293401047634148 | validation: 0.14332195912161075]
	TIME [epoch: 5.73 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11114449414137009		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.11114449414137009 | validation: 0.136380267047049]
	TIME [epoch: 5.73 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1120161441752212		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.1120161441752212 | validation: 0.13930058528361733]
	TIME [epoch: 5.73 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11355511990672972		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.11355511990672972 | validation: 0.13654165544913618]
	TIME [epoch: 5.73 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11712111146673064		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.11712111146673064 | validation: 0.14744837010871287]
	TIME [epoch: 5.72 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12667955565482483		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.12667955565482483 | validation: 0.15454049310829257]
	TIME [epoch: 5.73 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11091697200930017		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.11091697200930017 | validation: 0.12925586691051827]
	TIME [epoch: 5.73 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11167238133197475		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.11167238133197475 | validation: 0.13839332967934712]
	TIME [epoch: 5.73 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11383878261315682		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.11383878261315682 | validation: 0.1409408815178309]
	TIME [epoch: 5.73 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11483288418802538		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.11483288418802538 | validation: 0.12776627307875824]
	TIME [epoch: 5.73 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11148818811437879		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.11148818811437879 | validation: 0.1398594511552956]
	TIME [epoch: 5.73 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11060981571801992		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.11060981571801992 | validation: 0.1253991441097369]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_1063.pth
	Model improved!!!
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11010936929950209		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.11010936929950209 | validation: 0.14676546213963432]
	TIME [epoch: 5.73 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1149475089265259		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.1149475089265259 | validation: 0.12038852504878622]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_1065.pth
	Model improved!!!
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12081190481713908		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.12081190481713908 | validation: 0.14545043206674763]
	TIME [epoch: 5.73 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11477707666729137		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.11477707666729137 | validation: 0.1322600763768198]
	TIME [epoch: 5.72 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10778479826982267		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.10778479826982267 | validation: 0.1454476669246544]
	TIME [epoch: 5.73 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1102466901279901		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.1102466901279901 | validation: 0.14505355237273349]
	TIME [epoch: 5.73 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11248983636635018		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.11248983636635018 | validation: 0.12537359389711164]
	TIME [epoch: 5.72 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11140720661691908		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.11140720661691908 | validation: 0.13003218754758553]
	TIME [epoch: 5.73 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1085926020645695		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.1085926020645695 | validation: 0.13168420324537689]
	TIME [epoch: 5.73 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11213281091500661		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.11213281091500661 | validation: 0.14160780184765356]
	TIME [epoch: 5.73 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10838339405435761		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.10838339405435761 | validation: 0.13009315823572742]
	TIME [epoch: 5.73 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10692250122517262		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.10692250122517262 | validation: 0.16123301484986]
	TIME [epoch: 5.72 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11658949902367369		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.11658949902367369 | validation: 0.12589917504834403]
	TIME [epoch: 5.72 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11348410047919143		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.11348410047919143 | validation: 0.14608894482877027]
	TIME [epoch: 5.72 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10982339712579288		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.10982339712579288 | validation: 0.13277100771000325]
	TIME [epoch: 5.73 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1260291799888041		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.1260291799888041 | validation: 0.14041494122469914]
	TIME [epoch: 5.73 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11564320643907369		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.11564320643907369 | validation: 0.1464769670516193]
	TIME [epoch: 5.73 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11214068769354522		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.11214068769354522 | validation: 0.12929735643070783]
	TIME [epoch: 5.73 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11238695003550347		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.11238695003550347 | validation: 0.1433492692865433]
	TIME [epoch: 5.73 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11127553244835359		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.11127553244835359 | validation: 0.12407298163108882]
	TIME [epoch: 5.73 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10803144340182011		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.10803144340182011 | validation: 0.12998394551209166]
	TIME [epoch: 5.73 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11029824808762442		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.11029824808762442 | validation: 0.13059249633172296]
	TIME [epoch: 5.73 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1083038267313647		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.1083038267313647 | validation: 0.13183132827562113]
	TIME [epoch: 5.73 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10629788327030426		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.10629788327030426 | validation: 0.15666561095327505]
	TIME [epoch: 5.76 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12062680086402963		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.12062680086402963 | validation: 0.12466700284536887]
	TIME [epoch: 5.73 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10937649096257403		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.10937649096257403 | validation: 0.12806360465113747]
	TIME [epoch: 5.73 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10711804096968543		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.10711804096968543 | validation: 0.13374438663514485]
	TIME [epoch: 5.73 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10678741750898883		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.10678741750898883 | validation: 0.12921534229623619]
	TIME [epoch: 5.73 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10704959397857039		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.10704959397857039 | validation: 0.1530750839801495]
	TIME [epoch: 5.73 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11290333089401346		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.11290333089401346 | validation: 0.12683156707328128]
	TIME [epoch: 5.73 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1141174871562804		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.1141174871562804 | validation: 0.13681015569156202]
	TIME [epoch: 5.73 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10551394335073637		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.10551394335073637 | validation: 0.1297995723572897]
	TIME [epoch: 5.73 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1061984265907839		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.1061984265907839 | validation: 0.12713990527858893]
	TIME [epoch: 5.73 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10687496798191048		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.10687496798191048 | validation: 0.1261716754976904]
	TIME [epoch: 5.73 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10810950196890078		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.10810950196890078 | validation: 0.12343712982280637]
	TIME [epoch: 5.73 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10949230477727487		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.10949230477727487 | validation: 0.13017226273334623]
	TIME [epoch: 5.73 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10791611321156598		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.10791611321156598 | validation: 0.13232116541744418]
	TIME [epoch: 5.73 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1086596073783121		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.1086596073783121 | validation: 0.13406929441844678]
	TIME [epoch: 5.73 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11282681471741586		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.11282681471741586 | validation: 0.12903092743635744]
	TIME [epoch: 5.73 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11049030247980395		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.11049030247980395 | validation: 0.12321115215649003]
	TIME [epoch: 5.74 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10410080226304573		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.10410080226304573 | validation: 0.12239144531920323]
	TIME [epoch: 5.95 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10781386352709248		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.10781386352709248 | validation: 0.15001988171535474]
	TIME [epoch: 5.73 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11005471481503026		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.11005471481503026 | validation: 0.11905941342265858]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_1106.pth
	Model improved!!!
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11028164057385013		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.11028164057385013 | validation: 0.133768380040083]
	TIME [epoch: 5.74 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10976361604180998		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.10976361604180998 | validation: 0.12671218465758025]
	TIME [epoch: 5.73 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10224043751908121		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.10224043751908121 | validation: 0.11823277374080728]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_1109.pth
	Model improved!!!
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11173542387122173		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.11173542387122173 | validation: 0.1364879354599212]
	TIME [epoch: 5.73 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10931134533224292		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.10931134533224292 | validation: 0.12391566406463408]
	TIME [epoch: 5.73 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10503897312251942		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.10503897312251942 | validation: 0.1252511311460367]
	TIME [epoch: 5.73 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1059997909105779		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.1059997909105779 | validation: 0.12817669522278935]
	TIME [epoch: 5.74 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1056181373183595		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.1056181373183595 | validation: 0.1276929817483309]
	TIME [epoch: 5.74 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1040659392556413		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.1040659392556413 | validation: 0.13634005149140743]
	TIME [epoch: 5.74 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10961642836606032		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.10961642836606032 | validation: 0.13149586136370164]
	TIME [epoch: 5.73 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11104725582758779		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.11104725582758779 | validation: 0.12328642763833653]
	TIME [epoch: 5.74 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11276876065612929		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.11276876065612929 | validation: 0.1353911762998272]
	TIME [epoch: 5.73 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1072377463137521		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.1072377463137521 | validation: 0.11804335373444896]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_1119.pth
	Model improved!!!
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10466527657499601		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.10466527657499601 | validation: 0.12674106789966172]
	TIME [epoch: 5.76 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10319481945004742		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.10319481945004742 | validation: 0.11894893762127282]
	TIME [epoch: 5.76 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10452637833742114		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.10452637833742114 | validation: 0.1266529730251238]
	TIME [epoch: 5.75 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10103287767951132		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.10103287767951132 | validation: 0.1314504989089821]
	TIME [epoch: 5.75 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10261933694297394		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.10261933694297394 | validation: 0.12837808036105341]
	TIME [epoch: 5.75 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10244558895775933		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.10244558895775933 | validation: 0.11190866124481298]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_1125.pth
	Model improved!!!
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10611344005283593		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.10611344005283593 | validation: 0.12498623112799506]
	TIME [epoch: 5.75 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10819641406254285		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.10819641406254285 | validation: 0.15382892717477656]
	TIME [epoch: 5.75 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11536295230263197		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.11536295230263197 | validation: 0.12120590683530574]
	TIME [epoch: 5.76 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10624272649606276		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.10624272649606276 | validation: 0.12699726233628353]
	TIME [epoch: 5.76 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10871946750695294		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.10871946750695294 | validation: 0.12645774380315006]
	TIME [epoch: 5.76 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10601167186013209		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.10601167186013209 | validation: 0.12059353476698945]
	TIME [epoch: 5.75 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10402219062483159		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.10402219062483159 | validation: 0.11942997935738711]
	TIME [epoch: 5.75 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10959139118072918		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.10959139118072918 | validation: 0.12556684266430815]
	TIME [epoch: 5.76 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10286707187490915		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.10286707187490915 | validation: 0.1298620079397671]
	TIME [epoch: 5.75 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10419088406530792		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.10419088406530792 | validation: 0.13108486969173028]
	TIME [epoch: 5.76 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10584631591126135		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.10584631591126135 | validation: 0.12042103336294244]
	TIME [epoch: 5.75 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10299143538225963		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.10299143538225963 | validation: 0.12185039112140901]
	TIME [epoch: 5.75 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10499952793839759		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.10499952793839759 | validation: 0.14793591236579182]
	TIME [epoch: 5.75 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11419551357978282		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.11419551357978282 | validation: 0.11071524712937189]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_1139.pth
	Model improved!!!
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11156135313685646		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.11156135313685646 | validation: 0.12976589186057744]
	TIME [epoch: 5.72 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10374731383165582		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.10374731383165582 | validation: 0.11596956779127528]
	TIME [epoch: 5.72 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10203419326351337		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.10203419326351337 | validation: 0.12453457297134284]
	TIME [epoch: 5.71 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10225745613259143		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.10225745613259143 | validation: 0.13103453882745367]
	TIME [epoch: 5.72 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10246629263942582		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.10246629263942582 | validation: 0.1243096114896868]
	TIME [epoch: 5.71 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10642540648197364		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.10642540648197364 | validation: 0.1320942757756507]
	TIME [epoch: 5.73 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10375269597332824		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.10375269597332824 | validation: 0.13032178923614726]
	TIME [epoch: 5.71 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10282348580507263		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.10282348580507263 | validation: 0.11818732584488326]
	TIME [epoch: 5.72 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10202421034129933		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.10202421034129933 | validation: 0.13027584579371862]
	TIME [epoch: 5.71 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10142952440788701		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.10142952440788701 | validation: 0.11668282334210703]
	TIME [epoch: 5.76 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10366643268428792		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.10366643268428792 | validation: 0.14072656839809233]
	TIME [epoch: 5.76 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1220923876868212		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.1220923876868212 | validation: 0.11269248238278705]
	TIME [epoch: 5.76 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10689297263292469		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.10689297263292469 | validation: 0.11875909469107158]
	TIME [epoch: 5.76 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10540020225783499		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.10540020225783499 | validation: 0.12147902130723338]
	TIME [epoch: 5.76 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10253953353876309		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.10253953353876309 | validation: 0.12280404379154404]
	TIME [epoch: 5.76 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09950929920527013		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.09950929920527013 | validation: 0.11548397372656138]
	TIME [epoch: 5.76 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10086013761805969		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.10086013761805969 | validation: 0.11906357128951868]
	TIME [epoch: 5.76 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10347483237958859		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.10347483237958859 | validation: 0.12231473616907043]
	TIME [epoch: 5.76 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1012981513780433		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.1012981513780433 | validation: 0.12474097783792666]
	TIME [epoch: 5.75 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10067869659540161		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.10067869659540161 | validation: 0.11662549061509533]
	TIME [epoch: 5.76 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10161477250942602		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.10161477250942602 | validation: 0.12005832989111971]
	TIME [epoch: 5.76 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09918468503138106		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.09918468503138106 | validation: 0.1215102794678054]
	TIME [epoch: 5.75 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1004404285626853		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.1004404285626853 | validation: 0.13492955053001363]
	TIME [epoch: 5.76 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10472572014691534		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.10472572014691534 | validation: 0.11512712644694484]
	TIME [epoch: 5.74 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10782258746130605		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.10782258746130605 | validation: 0.12277904826745184]
	TIME [epoch: 5.75 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10602745613900698		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.10602745613900698 | validation: 0.13891967095565416]
	TIME [epoch: 5.75 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10931783917906472		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.10931783917906472 | validation: 0.11406942293493015]
	TIME [epoch: 5.76 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1080997971069717		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.1080997971069717 | validation: 0.12427380093990562]
	TIME [epoch: 5.75 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1038839093830154		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.1038839093830154 | validation: 0.12934916412608888]
	TIME [epoch: 5.75 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10229391416102945		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.10229391416102945 | validation: 0.11058019183515988]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_1169.pth
	Model improved!!!
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10087418522510008		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.10087418522510008 | validation: 0.1169914267187945]
	TIME [epoch: 5.72 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09916710169870716		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.09916710169870716 | validation: 0.11808259752963156]
	TIME [epoch: 5.72 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10017890100522074		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.10017890100522074 | validation: 0.1089994628452202]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_1172.pth
	Model improved!!!
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10031292152590066		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.10031292152590066 | validation: 0.12923806027621954]
	TIME [epoch: 5.72 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10220649477029485		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.10220649477029485 | validation: 0.11745473341694623]
	TIME [epoch: 5.71 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1015100548842696		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.1015100548842696 | validation: 0.11921110937963607]
	TIME [epoch: 5.72 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10512395217162512		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.10512395217162512 | validation: 0.12267503740170015]
	TIME [epoch: 5.72 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10671786986150665		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.10671786986150665 | validation: 0.12263499075722578]
	TIME [epoch: 5.72 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10743312469313253		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.10743312469313253 | validation: 0.11074526514962013]
	TIME [epoch: 5.72 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09918494108012504		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.09918494108012504 | validation: 0.11109412886480166]
	TIME [epoch: 5.72 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10001155273120746		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.10001155273120746 | validation: 0.12187776413552315]
	TIME [epoch: 5.72 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09863914478806446		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.09863914478806446 | validation: 0.12263005725835155]
	TIME [epoch: 5.76 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1015601854168959		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.1015601854168959 | validation: 0.11541184264881017]
	TIME [epoch: 5.72 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10057104639621323		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.10057104639621323 | validation: 0.11310513332858452]
	TIME [epoch: 5.74 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09890752932457635		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.09890752932457635 | validation: 0.12119551315410333]
	TIME [epoch: 5.74 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10153728277048013		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.10153728277048013 | validation: 0.11005515367080441]
	TIME [epoch: 5.74 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1040970333335305		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.1040970333335305 | validation: 0.11895979011698894]
	TIME [epoch: 5.75 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09960189529483912		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.09960189529483912 | validation: 0.11791078388784977]
	TIME [epoch: 5.74 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09983845138486759		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.09983845138486759 | validation: 0.13210362271627468]
	TIME [epoch: 5.75 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10111686371564067		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.10111686371564067 | validation: 0.11176574848854762]
	TIME [epoch: 5.74 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1004336505948682		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.1004336505948682 | validation: 0.112836344608812]
	TIME [epoch: 5.75 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0983068883641062		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.0983068883641062 | validation: 0.11690037601614729]
	TIME [epoch: 5.74 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10163957158356707		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.10163957158356707 | validation: 0.1101770063206054]
	TIME [epoch: 5.75 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10074147412837467		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.10074147412837467 | validation: 0.12148462002633578]
	TIME [epoch: 5.74 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09619094932634234		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.09619094932634234 | validation: 0.1324004915125173]
	TIME [epoch: 5.75 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10487876043217323		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.10487876043217323 | validation: 0.11433293351502494]
	TIME [epoch: 5.75 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10310593040012127		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.10310593040012127 | validation: 0.11613316333350852]
	TIME [epoch: 5.75 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10108515475994707		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.10108515475994707 | validation: 0.11313164399508518]
	TIME [epoch: 5.75 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.097083511474382		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.097083511474382 | validation: 0.11293755479200188]
	TIME [epoch: 5.75 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09732963703745659		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.09732963703745659 | validation: 0.10721761669499781]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_1199.pth
	Model improved!!!
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0995463807693539		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.0995463807693539 | validation: 0.11861674998773915]
	TIME [epoch: 5.73 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10203396814805887		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.10203396814805887 | validation: 0.11397213668395728]
	TIME [epoch: 5.73 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09654645454053924		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.09654645454053924 | validation: 0.11700392989631417]
	TIME [epoch: 5.74 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09869623698571087		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.09869623698571087 | validation: 0.1108523302339556]
	TIME [epoch: 5.73 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10102665636560762		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.10102665636560762 | validation: 0.12209268484746016]
	TIME [epoch: 5.73 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10377191361155541		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.10377191361155541 | validation: 0.1126312913838528]
	TIME [epoch: 5.73 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0986548303629878		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.0986548303629878 | validation: 0.11799350702157843]
	TIME [epoch: 5.73 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10124490051225721		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.10124490051225721 | validation: 0.11596801711825573]
	TIME [epoch: 5.73 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09797974333403643		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.09797974333403643 | validation: 0.1212211263745119]
	TIME [epoch: 5.73 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09881525822719428		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.09881525822719428 | validation: 0.11102174571265609]
	TIME [epoch: 5.73 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09618856582411905		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.09618856582411905 | validation: 0.11863652045091184]
	TIME [epoch: 5.73 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10015038344837539		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.10015038344837539 | validation: 0.11873565938770309]
	TIME [epoch: 5.73 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10083987634965613		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.10083987634965613 | validation: 0.11189797124803672]
	TIME [epoch: 5.74 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0982338740965852		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.0982338740965852 | validation: 0.12895525947147538]
	TIME [epoch: 5.73 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10886992190514647		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.10886992190514647 | validation: 0.10752514517185813]
	TIME [epoch: 5.74 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09946796669994132		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.09946796669994132 | validation: 0.10419241907116694]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_1215.pth
	Model improved!!!
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10474479990856127		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.10474479990856127 | validation: 0.12635538709992514]
	TIME [epoch: 5.74 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09750615391059549		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.09750615391059549 | validation: 0.12519293903289666]
	TIME [epoch: 5.75 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09929962130139856		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.09929962130139856 | validation: 0.11225406104538778]
	TIME [epoch: 5.74 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09708245140535614		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.09708245140535614 | validation: 0.11343046408320633]
	TIME [epoch: 5.74 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09961674729846508		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.09961674729846508 | validation: 0.11843215078558972]
	TIME [epoch: 5.75 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0968304806318923		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.0968304806318923 | validation: 0.11319241516467907]
	TIME [epoch: 5.74 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10054708864771286		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.10054708864771286 | validation: 0.13005965659465915]
	TIME [epoch: 5.75 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09989141372124141		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.09989141372124141 | validation: 0.118466766653364]
	TIME [epoch: 5.74 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09792411550489621		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.09792411550489621 | validation: 0.1088910881250405]
	TIME [epoch: 5.74 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0981843857434091		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.0981843857434091 | validation: 0.10653207396828482]
	TIME [epoch: 5.74 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09588093497916482		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.09588093497916482 | validation: 0.11678441300593193]
	TIME [epoch: 5.74 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09966323526874671		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.09966323526874671 | validation: 0.11004980085308996]
	TIME [epoch: 5.73 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09706369202455233		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.09706369202455233 | validation: 0.11694591869290545]
	TIME [epoch: 5.76 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09737474027933388		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.09737474027933388 | validation: 0.11743686209109519]
	TIME [epoch: 5.75 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09874311836716809		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.09874311836716809 | validation: 0.10310828042243468]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_1230.pth
	Model improved!!!
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09539962913087745		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.09539962913087745 | validation: 0.11290181191379292]
	TIME [epoch: 5.72 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09701387960169601		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.09701387960169601 | validation: 0.12514116809292478]
	TIME [epoch: 5.73 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09889830521539394		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.09889830521539394 | validation: 0.10951434926808504]
	TIME [epoch: 5.73 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09760393668777276		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.09760393668777276 | validation: 0.10744236614705059]
	TIME [epoch: 5.74 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09823177716283386		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.09823177716283386 | validation: 0.10733334927957026]
	TIME [epoch: 5.72 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0954845030109015		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.0954845030109015 | validation: 0.10796556059346166]
	TIME [epoch: 5.72 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10572892057989856		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.10572892057989856 | validation: 0.11667395655401894]
	TIME [epoch: 5.72 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09491266132910184		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.09491266132910184 | validation: 0.11544104014162566]
	TIME [epoch: 5.72 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09847229614339392		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.09847229614339392 | validation: 0.103725871895371]
	TIME [epoch: 5.73 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09843092777023245		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.09843092777023245 | validation: 0.10312358903562582]
	TIME [epoch: 5.72 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0994513003423301		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.0994513003423301 | validation: 0.11839326533259067]
	TIME [epoch: 5.72 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09818335054731496		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.09818335054731496 | validation: 0.10694350199130494]
	TIME [epoch: 5.72 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09821019515003368		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.09821019515003368 | validation: 0.1141732357178112]
	TIME [epoch: 5.72 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09459392642453636		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.09459392642453636 | validation: 0.10727707905351372]
	TIME [epoch: 5.72 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09744038739583202		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.09744038739583202 | validation: 0.10936024375872405]
	TIME [epoch: 5.72 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09515721221370842		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.09515721221370842 | validation: 0.11976218246640205]
	TIME [epoch: 5.72 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09752324181609373		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.09752324181609373 | validation: 0.10704280887145579]
	TIME [epoch: 5.72 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0973644101653555		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.0973644101653555 | validation: 0.10715983698234229]
	TIME [epoch: 5.72 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09659988024826423		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.09659988024826423 | validation: 0.11561143725167629]
	TIME [epoch: 5.73 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.096941932236131		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.096941932236131 | validation: 0.11124727713829766]
	TIME [epoch: 5.72 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09264741420130695		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.09264741420130695 | validation: 0.1084427380547664]
	TIME [epoch: 5.73 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09644310024127999		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.09644310024127999 | validation: 0.11364277425814556]
	TIME [epoch: 5.72 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09807879173490658		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.09807879173490658 | validation: 0.1162500976375359]
	TIME [epoch: 5.72 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09640434721539826		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.09640434721539826 | validation: 0.10162608941851865]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_1254.pth
	Model improved!!!
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09645606978580691		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.09645606978580691 | validation: 0.11433508037625911]
	TIME [epoch: 5.75 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10088877296160735		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.10088877296160735 | validation: 0.10834417821438685]
	TIME [epoch: 5.75 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09763151839272177		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.09763151839272177 | validation: 0.10967725154531993]
	TIME [epoch: 5.75 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09514265705070614		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.09514265705070614 | validation: 0.12771020467999686]
	TIME [epoch: 5.75 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09756599616127408		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.09756599616127408 | validation: 0.11024984292678003]
	TIME [epoch: 5.75 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09683625174067541		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.09683625174067541 | validation: 0.10571188447313395]
	TIME [epoch: 5.75 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09498767420310206		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.09498767420310206 | validation: 0.10596309678294169]
	TIME [epoch: 5.76 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0934974438794145		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.0934974438794145 | validation: 0.10958864667887386]
	TIME [epoch: 5.75 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09585518417374601		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.09585518417374601 | validation: 0.11218777605862346]
	TIME [epoch: 5.75 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09591383177531906		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.09591383177531906 | validation: 0.11386488569312868]
	TIME [epoch: 5.76 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09673942686505115		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.09673942686505115 | validation: 0.10609476948662369]
	TIME [epoch: 5.75 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09596224286211497		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.09596224286211497 | validation: 0.11209759313671347]
	TIME [epoch: 5.76 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09738899931778214		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.09738899931778214 | validation: 0.10983145692658232]
	TIME [epoch: 5.75 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09603368877473076		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.09603368877473076 | validation: 0.11528625984509172]
	TIME [epoch: 5.76 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09432760691757225		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.09432760691757225 | validation: 0.1086141958320137]
	TIME [epoch: 5.76 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09595490201546746		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.09595490201546746 | validation: 0.10618889540879058]
	TIME [epoch: 5.75 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09645253453661927		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.09645253453661927 | validation: 0.09838525230343503]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_1271.pth
	Model improved!!!
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09450177385027654		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.09450177385027654 | validation: 0.11512897298790029]
	TIME [epoch: 5.72 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09579200778980144		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.09579200778980144 | validation: 0.11031326039357614]
	TIME [epoch: 5.72 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09433149948022887		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.09433149948022887 | validation: 0.11215882666826774]
	TIME [epoch: 5.72 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09603043889718821		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.09603043889718821 | validation: 0.1178567659732741]
	TIME [epoch: 5.72 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09600840080193247		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.09600840080193247 | validation: 0.10814921696140672]
	TIME [epoch: 5.72 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0945176050921804		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.0945176050921804 | validation: 0.10592327547648205]
	TIME [epoch: 5.72 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09485719108218119		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.09485719108218119 | validation: 0.11139039332022925]
	TIME [epoch: 5.72 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09576820008093519		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.09576820008093519 | validation: 0.10774175143248846]
	TIME [epoch: 5.73 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0950006916158		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.0950006916158 | validation: 0.10212059520851612]
	TIME [epoch: 5.72 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09594461698614369		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.09594461698614369 | validation: 0.11483198143202658]
	TIME [epoch: 5.72 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09476548563493		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.09476548563493 | validation: 0.10919393295233955]
	TIME [epoch: 5.72 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09671158288379754		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.09671158288379754 | validation: 0.10848413192395752]
	TIME [epoch: 5.72 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0923171432905512		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.0923171432905512 | validation: 0.10765732431478213]
	TIME [epoch: 5.72 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09567467826706828		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.09567467826706828 | validation: 0.11461653551877296]
	TIME [epoch: 5.73 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09511989335444639		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.09511989335444639 | validation: 0.1085560350583358]
	TIME [epoch: 5.72 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0959369525739151		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.0959369525739151 | validation: 0.11699922871056133]
	TIME [epoch: 5.71 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09785312564343464		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.09785312564343464 | validation: 0.10356500557170954]
	TIME [epoch: 5.72 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09159035832860384		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.09159035832860384 | validation: 0.09494896589682095]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_1289.pth
	Model improved!!!
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09573447961039301		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.09573447961039301 | validation: 0.10453965234264884]
	TIME [epoch: 5.73 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09526690948749046		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.09526690948749046 | validation: 0.0980781587998552]
	TIME [epoch: 5.72 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09364228311701524		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.09364228311701524 | validation: 0.1075240260804675]
	TIME [epoch: 5.72 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.093179485157842		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.093179485157842 | validation: 0.11658055404883592]
	TIME [epoch: 5.72 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09355459136254425		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.09355459136254425 | validation: 0.11254420930914913]
	TIME [epoch: 5.72 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09676823730455812		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.09676823730455812 | validation: 0.11246219628281735]
	TIME [epoch: 5.72 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09633692483605391		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.09633692483605391 | validation: 0.0986684861159365]
	TIME [epoch: 5.72 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09233819064044099		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.09233819064044099 | validation: 0.10305582186271658]
	TIME [epoch: 5.72 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09491468732581691		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.09491468732581691 | validation: 0.10208701346196047]
	TIME [epoch: 5.72 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0977383221199548		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.0977383221199548 | validation: 0.10986680317575469]
	TIME [epoch: 5.72 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09321065642828825		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.09321065642828825 | validation: 0.11432948602425119]
	TIME [epoch: 5.73 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09486355063122055		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.09486355063122055 | validation: 0.10510239237165186]
	TIME [epoch: 5.72 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09446223881756574		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.09446223881756574 | validation: 0.10787097342185081]
	TIME [epoch: 5.72 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09343761481014282		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.09343761481014282 | validation: 0.10512965676216815]
	TIME [epoch: 5.72 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09386392307187187		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.09386392307187187 | validation: 0.10196556049854395]
	TIME [epoch: 5.72 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09423881431919938		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.09423881431919938 | validation: 0.11262955603363266]
	TIME [epoch: 5.73 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09566022303111257		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.09566022303111257 | validation: 0.0970755090145396]
	TIME [epoch: 5.72 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09203155500872776		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.09203155500872776 | validation: 0.10685523781221828]
	TIME [epoch: 5.72 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09249549686673293		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.09249549686673293 | validation: 0.10533127384318981]
	TIME [epoch: 5.72 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09413920819701196		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.09413920819701196 | validation: 0.10370823401667822]
	TIME [epoch: 5.72 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09154339844283133		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.09154339844283133 | validation: 0.101330182162762]
	TIME [epoch: 5.73 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09395810366790454		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.09395810366790454 | validation: 0.10652369939572659]
	TIME [epoch: 5.72 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09333632744668723		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.09333632744668723 | validation: 0.10870506114934231]
	TIME [epoch: 5.72 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09416283247787476		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.09416283247787476 | validation: 0.10540043058031578]
	TIME [epoch: 5.72 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09513189234836242		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.09513189234836242 | validation: 0.10312399865075501]
	TIME [epoch: 5.72 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0928780870987258		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.0928780870987258 | validation: 0.10367024078778958]
	TIME [epoch: 5.73 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09082480698131988		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.09082480698131988 | validation: 0.10260536045807943]
	TIME [epoch: 5.72 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09383787076661584		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.09383787076661584 | validation: 0.10495301360108239]
	TIME [epoch: 5.72 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09459251442355776		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.09459251442355776 | validation: 0.09973477874759035]
	TIME [epoch: 5.72 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09111530315109345		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.09111530315109345 | validation: 0.10292207474899305]
	TIME [epoch: 5.73 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.091638421580969		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.091638421580969 | validation: 0.10611341608050875]
	TIME [epoch: 5.73 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09327094504679913		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.09327094504679913 | validation: 0.10404075019366857]
	TIME [epoch: 5.73 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09242779164921952		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.09242779164921952 | validation: 0.10555530511903549]
	TIME [epoch: 5.72 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09226588913861468		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.09226588913861468 | validation: 0.10901032025899156]
	TIME [epoch: 5.72 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09284219164517798		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.09284219164517798 | validation: 0.1049627722107537]
	TIME [epoch: 5.72 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09270511864974994		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.09270511864974994 | validation: 0.09843005999220718]
	TIME [epoch: 5.72 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09169133164045888		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.09169133164045888 | validation: 0.1202633528649773]
	TIME [epoch: 5.72 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09478748225124191		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.09478748225124191 | validation: 0.09952237934604907]
	TIME [epoch: 5.72 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09180445587561341		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.09180445587561341 | validation: 0.10286779141603902]
	TIME [epoch: 5.72 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0929909830744347		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.0929909830744347 | validation: 0.12446842274806309]
	TIME [epoch: 5.72 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11128575454155176		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.11128575454155176 | validation: 0.12033453159737897]
	TIME [epoch: 5.72 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10055553395236184		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.10055553395236184 | validation: 0.09805680093903646]
	TIME [epoch: 5.72 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09229749321343786		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.09229749321343786 | validation: 0.10224521315157697]
	TIME [epoch: 5.72 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0940060221310358		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.0940060221310358 | validation: 0.1053104031408219]
	TIME [epoch: 5.72 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0923152621019004		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.0923152621019004 | validation: 0.09581203816461288]
	TIME [epoch: 5.72 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09282937735215759		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.09282937735215759 | validation: 0.10807227708441616]
	TIME [epoch: 5.72 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09031829088722244		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.09031829088722244 | validation: 0.10016279124703825]
	TIME [epoch: 5.72 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09067666117345528		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.09067666117345528 | validation: 0.100580588473782]
	TIME [epoch: 5.72 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09008510701016931		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.09008510701016931 | validation: 0.10091275502700672]
	TIME [epoch: 5.72 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09066964459708624		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.09066964459708624 | validation: 0.11161756225909869]
	TIME [epoch: 5.72 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09125454066174403		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.09125454066174403 | validation: 0.09761458823310759]
	TIME [epoch: 5.72 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09276952463363917		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.09276952463363917 | validation: 0.10174894132201509]
	TIME [epoch: 5.73 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09308177982462257		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.09308177982462257 | validation: 0.11254095320483565]
	TIME [epoch: 5.72 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08996289904144554		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.08996289904144554 | validation: 0.10388057048515545]
	TIME [epoch: 5.72 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09184339022864603		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.09184339022864603 | validation: 0.10007161254012167]
	TIME [epoch: 5.72 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09119429732230817		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.09119429732230817 | validation: 0.10740327653060754]
	TIME [epoch: 5.72 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09158889713427014		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.09158889713427014 | validation: 0.10003880698284129]
	TIME [epoch: 5.72 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09052791611784557		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.09052791611784557 | validation: 0.10207969032647193]
	TIME [epoch: 5.73 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09210629546377026		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.09210629546377026 | validation: 0.1013146378748504]
	TIME [epoch: 5.72 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09178164505459538		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.09178164505459538 | validation: 0.09839896291822141]
	TIME [epoch: 5.72 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0916207212260822		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.0916207212260822 | validation: 0.096687257929971]
	TIME [epoch: 5.72 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09162195353143739		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.09162195353143739 | validation: 0.1025528564377492]
	TIME [epoch: 5.72 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08931960101650654		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.08931960101650654 | validation: 0.11304543763200288]
	TIME [epoch: 5.72 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.092710028680945		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.092710028680945 | validation: 0.1038904471495874]
	TIME [epoch: 5.72 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09283434963473942		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.09283434963473942 | validation: 0.10307776263738369]
	TIME [epoch: 5.72 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09087179515339053		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.09087179515339053 | validation: 0.10548838644548936]
	TIME [epoch: 5.72 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09000450385970148		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.09000450385970148 | validation: 0.10030686459646235]
	TIME [epoch: 5.72 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09122660932956546		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.09122660932956546 | validation: 0.10158980891207046]
	TIME [epoch: 5.72 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0900666269097625		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.0900666269097625 | validation: 0.10264673441934069]
	TIME [epoch: 5.72 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09116327775245324		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.09116327775245324 | validation: 0.10648102801086781]
	TIME [epoch: 5.72 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08985445630397142		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.08985445630397142 | validation: 0.10107288465358266]
	TIME [epoch: 5.72 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08896406985949401		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.08896406985949401 | validation: 0.10062759038652228]
	TIME [epoch: 5.72 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08850833360167253		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.08850833360167253 | validation: 0.10341880593883579]
	TIME [epoch: 5.72 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09290143139211267		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.09290143139211267 | validation: 0.0960954373894603]
	TIME [epoch: 5.72 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0909556387362359		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.0909556387362359 | validation: 0.10452691517342104]
	TIME [epoch: 5.72 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.088517249999255		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.088517249999255 | validation: 0.11450116721373921]
	TIME [epoch: 5.72 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09048629679122822		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.09048629679122822 | validation: 0.10160834735087805]
	TIME [epoch: 5.72 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08777360772473909		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.08777360772473909 | validation: 0.09909638000926596]
	TIME [epoch: 5.73 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09130139357748236		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.09130139357748236 | validation: 0.09684102879623652]
	TIME [epoch: 5.72 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0922917394780291		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.0922917394780291 | validation: 0.10162162637403833]
	TIME [epoch: 5.73 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08741400082544556		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.08741400082544556 | validation: 0.10841495351070152]
	TIME [epoch: 5.72 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08915216181984555		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.08915216181984555 | validation: 0.10503279451267221]
	TIME [epoch: 5.72 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09013433095288514		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.09013433095288514 | validation: 0.0979557604899063]
	TIME [epoch: 5.72 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09082571583637127		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.09082571583637127 | validation: 0.10517086758650232]
	TIME [epoch: 5.72 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0897624233500159		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.0897624233500159 | validation: 0.0956480196044514]
	TIME [epoch: 5.72 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09257305523414848		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.09257305523414848 | validation: 0.0990063537157698]
	TIME [epoch: 5.72 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09141197946106797		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.09141197946106797 | validation: 0.10151621611301041]
	TIME [epoch: 5.72 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08974593414906783		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.08974593414906783 | validation: 0.1124944333452444]
	TIME [epoch: 5.72 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0939878011990199		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.0939878011990199 | validation: 0.09711640394311992]
	TIME [epoch: 5.72 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09066918291115769		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.09066918291115769 | validation: 0.09833224371097296]
	TIME [epoch: 5.72 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09152234003671673		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.09152234003671673 | validation: 0.09853956739042374]
	TIME [epoch: 5.72 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09036546494310348		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.09036546494310348 | validation: 0.10199488713292099]
	TIME [epoch: 5.72 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08848962160166139		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.08848962160166139 | validation: 0.10681051125348875]
	TIME [epoch: 5.72 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0912483268087619		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.0912483268087619 | validation: 0.0994814190347856]
	TIME [epoch: 5.72 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09016010787596634		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.09016010787596634 | validation: 0.09711533288401292]
	TIME [epoch: 5.72 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08990479570046098		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.08990479570046098 | validation: 0.10050053396335334]
	TIME [epoch: 5.72 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09049164886880115		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.09049164886880115 | validation: 0.10714243892240877]
	TIME [epoch: 5.72 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0898365413236547		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.0898365413236547 | validation: 0.09852648587067481]
	TIME [epoch: 5.72 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09091522228886983		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.09091522228886983 | validation: 0.1012560202282789]
	TIME [epoch: 5.73 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08861313367711332		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.08861313367711332 | validation: 0.0980586408432311]
	TIME [epoch: 5.72 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08946622822950254		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.08946622822950254 | validation: 0.0986576264521778]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_1_v_mmd4_1390.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 4941.718 seconds.
