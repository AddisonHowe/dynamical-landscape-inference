Args:
Namespace(name='model_phi1_4a_distortion_v1_5_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_5/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_5/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.054671332240104675, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2557968378

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.106946572008417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.106946572008417 | validation: 7.282524332332568]
	TIME [epoch: 172 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.0300272138830335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.0300272138830335 | validation: 7.237405122773646]
	TIME [epoch: 0.76 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.082925299462821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.082925299462821 | validation: 7.089229206617205]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.960629777548336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.960629777548336 | validation: 7.044351550518002]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.916203628364833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.916203628364833 | validation: 7.016392472880284]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.849361421684021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.849361421684021 | validation: 6.948583225116081]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.778196315616899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.778196315616899 | validation: 6.8462679776112]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.700995995882347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.700995995882347 | validation: 6.6479354253857315]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.641527027186787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.641527027186787 | validation: 6.42938793323645]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.597666922715988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.597666922715988 | validation: 6.270548828587139]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.528140881415243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.528140881415243 | validation: 6.1258987974908345]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.374115555977378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.374115555977378 | validation: 6.156337188706783]
	TIME [epoch: 0.693 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.510232677200214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.510232677200214 | validation: 6.163292433424958]
	TIME [epoch: 0.692 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.336525253912368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.336525253912368 | validation: 5.839031842328173]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.19154633827977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.19154633827977 | validation: 5.595262279111109]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.085506373760661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.085506373760661 | validation: 5.460697917439012]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.993748466429022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.993748466429022 | validation: 5.664415778410402]
	TIME [epoch: 0.696 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.909763184032177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.909763184032177 | validation: 4.938311005667823]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.6762875779419195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.6762875779419195 | validation: 4.581219761143408]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.5178904153579005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.5178904153579005 | validation: 5.017171648451094]
	TIME [epoch: 0.694 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.7274650891362855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.7274650891362855 | validation: 5.493829858656753]
	TIME [epoch: 0.703 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.969386636296745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.969386636296745 | validation: 5.148049520401994]
	TIME [epoch: 0.692 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.781143704609735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.781143704609735 | validation: 3.463057304779775]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.806924519500418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.806924519500418 | validation: 4.887625705464375]
	TIME [epoch: 0.694 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.600578831155538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.600578831155538 | validation: 4.173781162133082]
	TIME [epoch: 0.692 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.002891573945706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.002891573945706 | validation: 3.107774308994321]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.742907734409352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.742907734409352 | validation: 3.7609810223158804]
	TIME [epoch: 0.695 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.813809793925068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.813809793925068 | validation: 3.7310742160095445]
	TIME [epoch: 0.692 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.793476139528346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.793476139528346 | validation: 2.7409869786506658]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.3308775855454815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3308775855454815 | validation: 3.267194400006239]
	TIME [epoch: 0.693 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.526144089898478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.526144089898478 | validation: 3.618499177354568]
	TIME [epoch: 0.691 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.752877141300959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.752877141300959 | validation: 2.6032190241856825]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.135484647830204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.135484647830204 | validation: 3.5235899542348466]
	TIME [epoch: 0.693 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.792745372943777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.792745372943777 | validation: 2.360481746803516]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002127438880991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.002127438880991 | validation: 2.5644145971341485]
	TIME [epoch: 0.691 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9584065854053274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9584065854053274 | validation: 2.780037722007585]
	TIME [epoch: 0.695 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.386120525098743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.386120525098743 | validation: 2.8604914300673485]
	TIME [epoch: 0.69 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.152752875893865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.152752875893865 | validation: 2.263057240026242]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7897301953463023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7897301953463023 | validation: 2.402255907755744]
	TIME [epoch: 0.692 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.026427039135325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.026427039135325 | validation: 3.006279699103179]
	TIME [epoch: 0.691 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.260674886401118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.260674886401118 | validation: 2.3641672114591383]
	TIME [epoch: 0.69 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9128984142641925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9128984142641925 | validation: 2.1588172817068596]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.785957277422792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.785957277422792 | validation: 2.3265340288415546]
	TIME [epoch: 0.696 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8356456920648943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8356456920648943 | validation: 2.143594708427024]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.697358165458592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.697358165458592 | validation: 2.301512000822525]
	TIME [epoch: 0.697 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9038638677326203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9038638677326203 | validation: 2.434873283415982]
	TIME [epoch: 0.698 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.855654943422089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.855654943422089 | validation: 2.0271472939414075]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7202663059899694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7202663059899694 | validation: 2.16687857506477]
	TIME [epoch: 0.698 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6965085720175512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6965085720175512 | validation: 2.239531616128175]
	TIME [epoch: 0.697 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.805946229539304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.805946229539304 | validation: 1.9931860516928268]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5842119316861223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5842119316861223 | validation: 1.8717725122991358]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.58173817528389		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.58173817528389 | validation: 2.107591449911477]
	TIME [epoch: 0.695 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6273473293851306		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 3.6273473293851306 | validation: 2.052590927381867]
	TIME [epoch: 0.694 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6705698215862834		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 3.6705698215862834 | validation: 1.9046078197178067]
	TIME [epoch: 0.703 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.505914972115939		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 3.505914972115939 | validation: 1.7936813524600304]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.493529947563836		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 3.493529947563836 | validation: 1.952264503656199]
	TIME [epoch: 0.694 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5223392037348673		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 3.5223392037348673 | validation: 2.020076886880221]
	TIME [epoch: 0.697 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5890932559197255		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 3.5890932559197255 | validation: 1.8543859089994683]
	TIME [epoch: 0.696 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4805308578163703		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 3.4805308578163703 | validation: 1.8852394702769246]
	TIME [epoch: 0.694 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.468286207294609		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 3.468286207294609 | validation: 1.739942185663351]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.41950368359025		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 3.41950368359025 | validation: 1.7889793606271203]
	TIME [epoch: 0.693 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.369891286337211		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 3.369891286337211 | validation: 1.6425634026775935]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.332694544476546		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 3.332694544476546 | validation: 1.731628894639615]
	TIME [epoch: 0.693 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3116396817776432		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 3.3116396817776432 | validation: 1.6375580319478276]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.309257159903832		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 3.309257159903832 | validation: 1.7628082773488822]
	TIME [epoch: 0.695 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.301513584794074		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 3.301513584794074 | validation: 1.754933865350688]
	TIME [epoch: 0.694 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3361818635764497		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 3.3361818635764497 | validation: 1.896836456859215]
	TIME [epoch: 0.693 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3801289241514283		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 3.3801289241514283 | validation: 1.9973951592088832]
	TIME [epoch: 0.693 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.394016057071782		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 3.394016057071782 | validation: 1.7964775123418493]
	TIME [epoch: 0.691 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2857348575870367		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 3.2857348575870367 | validation: 1.731477199531815]
	TIME [epoch: 0.693 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.190553552791653		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 3.190553552791653 | validation: 1.6896305779300604]
	TIME [epoch: 0.692 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.166955158944305		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 3.166955158944305 | validation: 1.788283192013185]
	TIME [epoch: 0.692 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2198149147990307		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 3.2198149147990307 | validation: 1.9631749450633564]
	TIME [epoch: 0.693 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.234154516013623		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 3.234154516013623 | validation: 1.6797674681854404]
	TIME [epoch: 0.693 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1749087780089473		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 3.1749087780089473 | validation: 1.5553835517809687]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9931681453068357		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 2.9931681453068357 | validation: 1.581297080121226]
	TIME [epoch: 0.696 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.964229911922339		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 2.964229911922339 | validation: 1.5928684683537782]
	TIME [epoch: 0.692 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.957347190187881		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 2.957347190187881 | validation: 1.5986062892074138]
	TIME [epoch: 0.693 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.895696660488014		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 2.895696660488014 | validation: 1.5367596802496701]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.89324582437319		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 2.89324582437319 | validation: 1.815784067442965]
	TIME [epoch: 0.694 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.93447148260634		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 2.93447148260634 | validation: 1.7781848387115595]
	TIME [epoch: 0.693 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1219597137299364		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 3.1219597137299364 | validation: 1.528042958367279]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7203894668542365		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 2.7203894668542365 | validation: 1.78305576882575]
	TIME [epoch: 0.696 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.795559922532904		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 2.795559922532904 | validation: 1.4368934586552522]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6128803117514203		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 2.6128803117514203 | validation: 1.4404228745005625]
	TIME [epoch: 0.696 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3766822479832417		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 2.3766822479832417 | validation: 1.3395506410297195]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.180157431574279		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 2.180157431574279 | validation: 1.7151954661380722]
	TIME [epoch: 0.696 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0666043055229815		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 2.0666043055229815 | validation: 1.0775619720327227]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9500514718167847		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.9500514718167847 | validation: 1.7187706347382516]
	TIME [epoch: 0.699 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3523403107109515		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 2.3523403107109515 | validation: 2.355252245714003]
	TIME [epoch: 0.695 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6746945272151663		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 2.6746945272151663 | validation: 1.6554432451398449]
	TIME [epoch: 0.695 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8976055160917886		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.8976055160917886 | validation: 1.0968492842550654]
	TIME [epoch: 0.697 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6323717797763835		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.6323717797763835 | validation: 1.2828365906431043]
	TIME [epoch: 0.695 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.568437785833607		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.568437785833607 | validation: 0.9603341610945209]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4492362448232825		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.4492362448232825 | validation: 1.0512405210379825]
	TIME [epoch: 0.696 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3744733654429753		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.3744733654429753 | validation: 1.5100382757747788]
	TIME [epoch: 0.695 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6561727856510557		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.6561727856510557 | validation: 1.0218611376593747]
	TIME [epoch: 0.698 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2869412206046513		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.2869412206046513 | validation: 1.1096273986216216]
	TIME [epoch: 0.695 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.269535194799552		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.269535194799552 | validation: 0.7601372882520138]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2836113833687524		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.2836113833687524 | validation: 1.230093098501994]
	TIME [epoch: 0.696 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4540694273540862		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.4540694273540862 | validation: 0.7813917966656629]
	TIME [epoch: 0.693 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1266738470305484		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.1266738470305484 | validation: 0.8720777324450684]
	TIME [epoch: 0.693 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0667317651305417		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.0667317651305417 | validation: 0.8324195324244983]
	TIME [epoch: 0.692 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0061670026146463		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.0061670026146463 | validation: 0.8502457762100649]
	TIME [epoch: 0.693 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9803398349058203		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.9803398349058203 | validation: 0.71046605597667]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1381697261614045		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.1381697261614045 | validation: 1.3852990383713542]
	TIME [epoch: 0.696 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8343766332871887		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.8343766332871887 | validation: 0.8916836847650554]
	TIME [epoch: 0.692 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0867955286932756		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.0867955286932756 | validation: 0.874553824951323]
	TIME [epoch: 0.691 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.888644631712642		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.888644631712642 | validation: 0.7850036372976726]
	TIME [epoch: 0.693 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0583866353900544		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.0583866353900544 | validation: 1.3332819508841958]
	TIME [epoch: 0.694 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6883401440520647		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.6883401440520647 | validation: 0.9459846464896189]
	TIME [epoch: 0.694 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.162431150067858		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.162431150067858 | validation: 0.7160350159535039]
	TIME [epoch: 0.694 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.298949352493742		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.298949352493742 | validation: 0.8220879767220062]
	TIME [epoch: 0.695 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.878165393874051		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.878165393874051 | validation: 0.9303415540947353]
	TIME [epoch: 0.697 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9567029733530962		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.9567029733530962 | validation: 0.6625451944073764]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.83499071773951		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.83499071773951 | validation: 0.8247914042156266]
	TIME [epoch: 0.696 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.812504111819274		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.812504111819274 | validation: 0.6348916390126086]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8859152463737641		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.8859152463737641 | validation: 1.1390200720973938]
	TIME [epoch: 0.696 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1011532298890334		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.1011532298890334 | validation: 0.5750522020782388]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1045773668592986		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.1045773668592986 | validation: 0.895876531276]
	TIME [epoch: 0.697 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8775296930103781		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.8775296930103781 | validation: 0.5709056756910448]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7729377785476678		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.7729377785476678 | validation: 0.7445484022528823]
	TIME [epoch: 0.698 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7559302144312671		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.7559302144312671 | validation: 0.5909605803735172]
	TIME [epoch: 0.697 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8551275465728716		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.8551275465728716 | validation: 1.1276163256743534]
	TIME [epoch: 0.703 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0309539762548054		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.0309539762548054 | validation: 0.5523365114279412]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.897911646182006		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.897911646182006 | validation: 0.8133291512907882]
	TIME [epoch: 0.699 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.786910786745035		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.786910786745035 | validation: 0.5448752170685472]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8451154284185332		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.8451154284185332 | validation: 0.9633897077886076]
	TIME [epoch: 0.699 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8921293428314865		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.8921293428314865 | validation: 0.49007331841092033]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7870368269967424		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.7870368269967424 | validation: 0.7504515841099929]
	TIME [epoch: 0.695 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7278600810161746		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.7278600810161746 | validation: 0.4809975060178417]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7521725702396684		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.7521725702396684 | validation: 0.8465318593307235]
	TIME [epoch: 0.693 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7946024755371082		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.7946024755371082 | validation: 0.4827999768110785]
	TIME [epoch: 0.693 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7578921339040471		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.7578921339040471 | validation: 0.7397686567713468]
	TIME [epoch: 0.693 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6816997738240482		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.6816997738240482 | validation: 0.4225005045950373]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6734406445983879		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.6734406445983879 | validation: 0.8009336343642567]
	TIME [epoch: 0.693 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7500072972140501		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.7500072972140501 | validation: 0.4597679640016596]
	TIME [epoch: 0.693 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7041321324587672		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.7041321324587672 | validation: 0.6638803246895614]
	TIME [epoch: 0.691 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.621660993473997		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.621660993473997 | validation: 0.4652791475831979]
	TIME [epoch: 0.691 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6386312622787804		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.6386312622787804 | validation: 1.017680895794821]
	TIME [epoch: 0.691 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.911377326271766		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.911377326271766 | validation: 0.48735347757285413]
	TIME [epoch: 0.691 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7806290314083398		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.7806290314083398 | validation: 0.8365346558045096]
	TIME [epoch: 0.691 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6609240137142303		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.6609240137142303 | validation: 0.5402876307952007]
	TIME [epoch: 0.691 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5950953478590665		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.5950953478590665 | validation: 0.5493529389019641]
	TIME [epoch: 0.695 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5167976241959725		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.5167976241959725 | validation: 0.37519947303419876]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5450236292466312		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.5450236292466312 | validation: 0.8138306063047835]
	TIME [epoch: 0.695 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7801808423822376		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.7801808423822376 | validation: 0.5210225951055131]
	TIME [epoch: 0.693 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9333687950612626		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.9333687950612626 | validation: 0.8453735605673011]
	TIME [epoch: 0.694 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6823549195984258		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.6823549195984258 | validation: 0.38780457693711495]
	TIME [epoch: 0.693 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4933388838817432		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.4933388838817432 | validation: 0.5236052037353244]
	TIME [epoch: 0.692 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5125575278361932		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.5125575278361932 | validation: 0.3636247458013475]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_151.pth
	Model improved!!!
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4504600397599085		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.4504600397599085 | validation: 0.579021010736232]
	TIME [epoch: 0.696 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49221792418036536		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.49221792418036536 | validation: 0.36804513775191006]
	TIME [epoch: 0.693 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6669394126658245		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.6669394126658245 | validation: 0.7933969338144153]
	TIME [epoch: 0.695 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7860173134578665		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.7860173134578665 | validation: 0.4147021780024913]
	TIME [epoch: 0.696 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.673828814680291		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.673828814680291 | validation: 0.5933212762678938]
	TIME [epoch: 0.694 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47004057830491036		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.47004057830491036 | validation: 0.5307925337970293]
	TIME [epoch: 0.693 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4843982212012049		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.4843982212012049 | validation: 0.48276968796066994]
	TIME [epoch: 0.694 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4217410458374977		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.4217410458374977 | validation: 0.31329330916668496]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_159.pth
	Model improved!!!
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44089194576317553		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.44089194576317553 | validation: 0.6779704086267663]
	TIME [epoch: 0.7 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.653743754411465		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.653743754411465 | validation: 0.4440321976146227]
	TIME [epoch: 0.691 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.710269428619736		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.710269428619736 | validation: 0.6912094878343773]
	TIME [epoch: 0.693 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5964023518575486		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.5964023518575486 | validation: 0.3351197459316319]
	TIME [epoch: 0.692 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5342725211715811		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.5342725211715811 | validation: 0.6523439445321515]
	TIME [epoch: 0.692 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.522811779333789		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.522811779333789 | validation: 0.4358618607954894]
	TIME [epoch: 0.691 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.496947102503279		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.496947102503279 | validation: 0.5683224989383493]
	TIME [epoch: 0.693 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48990187255197426		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.48990187255197426 | validation: 0.34942585157603023]
	TIME [epoch: 0.692 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5697226349546441		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.5697226349546441 | validation: 0.5555987890072672]
	TIME [epoch: 0.693 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4767068552071868		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.4767068552071868 | validation: 0.28888287921489575]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4401017963343234		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.4401017963343234 | validation: 0.5231181552891223]
	TIME [epoch: 0.695 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45865943142446336		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.45865943142446336 | validation: 0.2885044858736913]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_171.pth
	Model improved!!!
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47157001839402496		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.47157001839402496 | validation: 0.569049772765462]
	TIME [epoch: 0.696 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4856105834161727		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.4856105834161727 | validation: 0.3098380789774921]
	TIME [epoch: 0.694 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4494893790232719		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.4494893790232719 | validation: 0.5455562707494984]
	TIME [epoch: 0.693 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43780413917492184		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.43780413917492184 | validation: 0.3243436150251532]
	TIME [epoch: 0.694 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4889228584387497		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.4889228584387497 | validation: 0.6127485059643201]
	TIME [epoch: 0.693 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4874760833017471		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.4874760833017471 | validation: 0.33616102618404425]
	TIME [epoch: 0.694 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46283742231687824		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.46283742231687824 | validation: 0.4832756306732952]
	TIME [epoch: 0.695 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3971432282666863		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.3971432282666863 | validation: 0.2556936526864519]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39702744991681005		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.39702744991681005 | validation: 0.5420521782932127]
	TIME [epoch: 0.694 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45663110346557156		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.45663110346557156 | validation: 0.48241807431489064]
	TIME [epoch: 0.694 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6025311232225452		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.6025311232225452 | validation: 0.7632457690488694]
	TIME [epoch: 0.694 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5969737841636884		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.5969737841636884 | validation: 0.3004927249716204]
	TIME [epoch: 0.694 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38240623924626294		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.38240623924626294 | validation: 0.4291679109560531]
	TIME [epoch: 0.693 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3718582246501135		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.3718582246501135 | validation: 0.31342062249367153]
	TIME [epoch: 0.695 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.355794581892353		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.355794581892353 | validation: 0.42062474128959687]
	TIME [epoch: 0.697 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3414149100862915		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.3414149100862915 | validation: 0.27825047148146315]
	TIME [epoch: 0.696 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.310327672902939		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.310327672902939 | validation: 0.4419669802324556]
	TIME [epoch: 0.695 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3504477290503851		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.3504477290503851 | validation: 0.26612042330311025]
	TIME [epoch: 0.693 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5041733535047938		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.5041733535047938 | validation: 0.5289460433068104]
	TIME [epoch: 0.695 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5130163022289208		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.5130163022289208 | validation: 0.3121862389285826]
	TIME [epoch: 0.694 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.497250688845095		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.497250688845095 | validation: 0.4829953089621426]
	TIME [epoch: 0.695 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3574455782010314		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.3574455782010314 | validation: 0.24446709859497695]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33458137371367735		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.33458137371367735 | validation: 0.423845757345961]
	TIME [epoch: 0.697 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3783082547119925		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.3783082547119925 | validation: 0.2248899215378616]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34888126194940633		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.34888126194940633 | validation: 0.44206930761581004]
	TIME [epoch: 0.694 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38565005053817997		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.38565005053817997 | validation: 0.3924520316737916]
	TIME [epoch: 0.694 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48333180484948957		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.48333180484948957 | validation: 0.6579708451316096]
	TIME [epoch: 0.704 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4942229594829439		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.4942229594829439 | validation: 0.26983998944023163]
	TIME [epoch: 0.694 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3392584686742157		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.3392584686742157 | validation: 0.3376033465734308]
	TIME [epoch: 0.694 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3048751324685774		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.3048751324685774 | validation: 0.26603806559923504]
	TIME [epoch: 177 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2906154630117828		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.2906154630117828 | validation: 0.3071534540055157]
	TIME [epoch: 1.38 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30575522035308045		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.30575522035308045 | validation: 0.30170970286992516]
	TIME [epoch: 1.36 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30505310732368907		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.30505310732368907 | validation: 0.3788148838040457]
	TIME [epoch: 1.36 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3302339533978356		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.3302339533978356 | validation: 0.38825400574807567]
	TIME [epoch: 1.36 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3691187998799403		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.3691187998799403 | validation: 0.4956683845888618]
	TIME [epoch: 1.36 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.487966529984443		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.487966529984443 | validation: 0.2587671479487524]
	TIME [epoch: 1.36 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4597069116036077		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.4597069116036077 | validation: 0.5011540075576638]
	TIME [epoch: 1.36 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3552058104099266		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.3552058104099266 | validation: 0.3222003556698598]
	TIME [epoch: 1.36 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3638984669144587		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.3638984669144587 | validation: 0.4271248755711147]
	TIME [epoch: 1.36 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35851600055548877		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.35851600055548877 | validation: 0.216226162243656]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34524614232007933		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.34524614232007933 | validation: 0.3520995336859886]
	TIME [epoch: 1.36 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31206811329620204		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.31206811329620204 | validation: 0.2226823345542294]
	TIME [epoch: 1.36 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30909647764628384		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.30909647764628384 | validation: 0.3659310141068696]
	TIME [epoch: 1.36 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29397046760762907		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.29397046760762907 | validation: 0.2513434454100014]
	TIME [epoch: 1.36 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33365010223094		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.33365010223094 | validation: 0.44561992590788313]
	TIME [epoch: 1.36 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37510028538810763		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.37510028538810763 | validation: 0.2441832916471885]
	TIME [epoch: 1.36 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33792125459015426		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.33792125459015426 | validation: 0.3109594173695036]
	TIME [epoch: 1.36 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27136583813251514		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.27136583813251514 | validation: 0.1967113663210328]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26319394980159105		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.26319394980159105 | validation: 0.3219798256832842]
	TIME [epoch: 1.36 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2919212161216878		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.2919212161216878 | validation: 0.18664115969179573]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3134121406918348		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.3134121406918348 | validation: 0.4472776352621887]
	TIME [epoch: 1.36 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3369177786838241		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.3369177786838241 | validation: 0.3811412583257209]
	TIME [epoch: 1.37 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42764657215035795		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.42764657215035795 | validation: 0.4309528227016996]
	TIME [epoch: 1.36 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33614926872441686		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.33614926872441686 | validation: 0.2019311578947452]
	TIME [epoch: 1.36 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.291562510591978		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.291562510591978 | validation: 0.26138669774219503]
	TIME [epoch: 1.36 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23168656007283095		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.23168656007283095 | validation: 0.24944738048506343]
	TIME [epoch: 1.36 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24127185647383884		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.24127185647383884 | validation: 0.47356750758153227]
	TIME [epoch: 1.36 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.349239768045135		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.349239768045135 | validation: 0.3504245260185355]
	TIME [epoch: 1.36 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3947113885794876		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.3947113885794876 | validation: 0.3828618410681661]
	TIME [epoch: 1.36 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3653561182788953		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.3653561182788953 | validation: 0.1717869489296801]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26719288508941097		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.26719288508941097 | validation: 0.27363695056882575]
	TIME [epoch: 1.36 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24068419940543545		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.24068419940543545 | validation: 0.1907034044996127]
	TIME [epoch: 1.36 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23310982856676485		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.23310982856676485 | validation: 0.3040427940643273]
	TIME [epoch: 1.36 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2473431868244077		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.2473431868244077 | validation: 0.2009839295476641]
	TIME [epoch: 1.36 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.276336560093594		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.276336560093594 | validation: 0.41101877818407645]
	TIME [epoch: 1.36 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3359362858457033		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.3359362858457033 | validation: 0.25665841762332825]
	TIME [epoch: 1.36 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3848501175859174		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.3848501175859174 | validation: 0.4096709857391286]
	TIME [epoch: 1.35 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30933983807214693		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.30933983807214693 | validation: 0.21759583432554602]
	TIME [epoch: 1.36 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2534365299504709		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.2534365299504709 | validation: 0.3202216274907027]
	TIME [epoch: 1.36 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26544996403480237		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.26544996403480237 | validation: 0.29313702500471656]
	TIME [epoch: 1.35 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30996738136217417		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.30996738136217417 | validation: 0.3381844192331873]
	TIME [epoch: 1.36 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.281348235601072		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.281348235601072 | validation: 0.18919985548392948]
	TIME [epoch: 1.36 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23756884391971525		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.23756884391971525 | validation: 0.25898244201687154]
	TIME [epoch: 1.36 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21979514794926033		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.21979514794926033 | validation: 0.15998011766439893]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_245.pth
	Model improved!!!
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22266407431405916		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.22266407431405916 | validation: 0.29218504556579133]
	TIME [epoch: 1.36 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24690127987945573		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.24690127987945573 | validation: 0.19337481264120782]
	TIME [epoch: 1.36 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29849772394564894		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.29849772394564894 | validation: 0.3748229059569159]
	TIME [epoch: 1.36 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3033963486106671		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.3033963486106671 | validation: 0.2021458954871402]
	TIME [epoch: 1.36 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27169903823000996		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.27169903823000996 | validation: 0.2676822585487771]
	TIME [epoch: 1.36 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20816552521069362		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.20816552521069362 | validation: 0.15055495869469548]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_251.pth
	Model improved!!!
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19088054571615087		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.19088054571615087 | validation: 0.25752148135372915]
	TIME [epoch: 1.36 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2007134690258496		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.2007134690258496 | validation: 0.17037190616212572]
	TIME [epoch: 1.36 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2559289383734048		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.2559289383734048 | validation: 0.39129552032328796]
	TIME [epoch: 1.36 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3089064505748136		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.3089064505748136 | validation: 0.20551145684904226]
	TIME [epoch: 1.36 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27766318272188284		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.27766318272188284 | validation: 0.2591092136491106]
	TIME [epoch: 1.36 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2329362452935137		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.2329362452935137 | validation: 0.1528693586352036]
	TIME [epoch: 1.36 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2005516634469062		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.2005516634469062 | validation: 0.2102887686839437]
	TIME [epoch: 1.35 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1944855052065369		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.1944855052065369 | validation: 0.22180402050150327]
	TIME [epoch: 1.36 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19846013522690645		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.19846013522690645 | validation: 0.2488313890916577]
	TIME [epoch: 1.36 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19587439931291198		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.19587439931291198 | validation: 0.2118241133063842]
	TIME [epoch: 1.36 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18512181978495104		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.18512181978495104 | validation: 0.1747854774019448]
	TIME [epoch: 1.36 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1644582620630373		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.1644582620630373 | validation: 0.2620950290730765]
	TIME [epoch: 1.36 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23930753439280672		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.23930753439280672 | validation: 0.4057890141545176]
	TIME [epoch: 1.36 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4921183932268186		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.4921183932268186 | validation: 0.5487446759058058]
	TIME [epoch: 1.36 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47746009165930114		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.47746009165930114 | validation: 0.17141250348549253]
	TIME [epoch: 1.36 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21465502116747032		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.21465502116747032 | validation: 0.18202101800548431]
	TIME [epoch: 1.36 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15063719623819627		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.15063719623819627 | validation: 0.14756667527167208]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14071477676485672		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.14071477676485672 | validation: 0.17775077469706996]
	TIME [epoch: 1.36 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1349013410663504		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.1349013410663504 | validation: 0.12943363034387775]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19935955385764337		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.19935955385764337 | validation: 0.35165013841533077]
	TIME [epoch: 1.36 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3627103800244314		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.3627103800244314 | validation: 0.27474498854609386]
	TIME [epoch: 1.36 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37166917523276327		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.37166917523276327 | validation: 0.32054495739404554]
	TIME [epoch: 1.36 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24332414632609528		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.24332414632609528 | validation: 0.13718311021287005]
	TIME [epoch: 1.36 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17638913819671934		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.17638913819671934 | validation: 0.20699559453179256]
	TIME [epoch: 1.36 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1929351900920017		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.1929351900920017 | validation: 0.23459078768096495]
	TIME [epoch: 1.36 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2400450480469756		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.2400450480469756 | validation: 0.3201842659908885]
	TIME [epoch: 1.36 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27124593195281466		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.27124593195281466 | validation: 0.2333032039422975]
	TIME [epoch: 1.36 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24055974741705421		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.24055974741705421 | validation: 0.21911581160014615]
	TIME [epoch: 1.36 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18262606506183265		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.18262606506183265 | validation: 0.132817029809338]
	TIME [epoch: 1.36 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1709511503416077		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.1709511503416077 | validation: 0.19709237264856092]
	TIME [epoch: 1.36 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1833052727563701		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.1833052727563701 | validation: 0.1275977319391983]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_282.pth
	Model improved!!!
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20445591954872824		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.20445591954872824 | validation: 0.24115120128187845]
	TIME [epoch: 1.35 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19405118405257862		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.19405118405257862 | validation: 0.17265880325521799]
	TIME [epoch: 1.35 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20869906932565654		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.20869906932565654 | validation: 0.2616677959610183]
	TIME [epoch: 1.35 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20772078301165728		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.20772078301165728 | validation: 0.12404935831421421]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20234529200219656		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.20234529200219656 | validation: 0.21991693764410006]
	TIME [epoch: 1.35 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19116941231977802		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.19116941231977802 | validation: 0.13149750923924428]
	TIME [epoch: 1.35 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19535475214605533		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.19535475214605533 | validation: 0.2540209335774212]
	TIME [epoch: 1.35 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22272948711811172		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.22272948711811172 | validation: 0.30143778474485844]
	TIME [epoch: 1.35 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26276539811711785		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.26276539811711785 | validation: 0.2747797482434881]
	TIME [epoch: 1.35 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23043821608076742		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.23043821608076742 | validation: 0.14851221011647006]
	TIME [epoch: 1.35 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1640006188693181		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.1640006188693181 | validation: 0.15622322146890846]
	TIME [epoch: 1.35 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1340269922415458		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.1340269922415458 | validation: 0.10129901190235442]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_294.pth
	Model improved!!!
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13162390420279596		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.13162390420279596 | validation: 0.1548160039276646]
	TIME [epoch: 1.36 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14225408979609253		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.14225408979609253 | validation: 0.0965483903700652]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15429475139440782		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.15429475139440782 | validation: 0.20528309224477292]
	TIME [epoch: 1.36 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19845962370220235		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.19845962370220235 | validation: 0.2102792133864303]
	TIME [epoch: 1.35 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2898199324559107		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.2898199324559107 | validation: 0.37826165843634807]
	TIME [epoch: 1.35 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3035876832877224		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.3035876832877224 | validation: 0.14027323259991356]
	TIME [epoch: 1.35 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1534109236620325		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.1534109236620325 | validation: 0.135503514855968]
	TIME [epoch: 1.36 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11215669279829815		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.11215669279829815 | validation: 0.1329450687371835]
	TIME [epoch: 1.36 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11923173000036609		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.11923173000036609 | validation: 0.18949105746208003]
	TIME [epoch: 1.36 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15467440460158902		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.15467440460158902 | validation: 0.26946791500445383]
	TIME [epoch: 1.36 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25813335366268836		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.25813335366268836 | validation: 0.28847857892106077]
	TIME [epoch: 1.36 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26968631881753113		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.26968631881753113 | validation: 0.14900742641416184]
	TIME [epoch: 1.36 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17216886489840674		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.17216886489840674 | validation: 0.14118663698854134]
	TIME [epoch: 1.36 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1315838396073898		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.1315838396073898 | validation: 0.09587073837817389]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12783111723264606		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.12783111723264606 | validation: 0.16227525818499827]
	TIME [epoch: 1.36 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1345314312261908		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.1345314312261908 | validation: 0.11458257326641563]
	TIME [epoch: 1.36 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1590301801557389		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.1590301801557389 | validation: 0.2653265885948673]
	TIME [epoch: 1.36 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21318965085384556		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.21318965085384556 | validation: 0.18275705276689347]
	TIME [epoch: 1.36 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25881565636647147		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.25881565636647147 | validation: 0.2583185721727408]
	TIME [epoch: 1.36 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23416043224285218		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.23416043224285218 | validation: 0.16079725233719844]
	TIME [epoch: 1.36 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16354568449169107		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.16354568449169107 | validation: 0.20478273455771123]
	TIME [epoch: 1.36 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16081777966545308		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.16081777966545308 | validation: 0.18447677626906886]
	TIME [epoch: 1.36 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17350543771490587		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.17350543771490587 | validation: 0.1836289190640469]
	TIME [epoch: 1.36 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1734780678120549		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.1734780678120549 | validation: 0.12325257638835413]
	TIME [epoch: 1.36 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1633829431298148		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.1633829431298148 | validation: 0.1565875022565105]
	TIME [epoch: 1.36 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1452393885078925		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.1452393885078925 | validation: 0.10573410346759869]
	TIME [epoch: 1.36 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1320463324616644		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.1320463324616644 | validation: 0.15726497505380332]
	TIME [epoch: 1.36 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13171540222049263		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.13171540222049263 | validation: 0.10489035901071944]
	TIME [epoch: 1.36 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13537201507514918		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.13537201507514918 | validation: 0.17756473651780855]
	TIME [epoch: 1.36 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16695198617896842		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.16695198617896842 | validation: 0.15342881603029276]
	TIME [epoch: 1.36 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18977366688599703		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.18977366688599703 | validation: 0.2052503380253402]
	TIME [epoch: 1.36 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18405297035457416		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.18405297035457416 | validation: 0.14390359413602893]
	TIME [epoch: 1.36 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16319895373315155		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.16319895373315155 | validation: 0.13735818247073647]
	TIME [epoch: 1.37 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12431865394616783		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.12431865394616783 | validation: 0.10975291949768584]
	TIME [epoch: 1.36 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10193078792143952		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.10193078792143952 | validation: 0.11799556646718086]
	TIME [epoch: 1.36 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10787225137184084		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.10787225137184084 | validation: 0.11592794669750116]
	TIME [epoch: 1.36 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11407898619029169		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.11407898619029169 | validation: 0.14381650712867572]
	TIME [epoch: 1.36 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13129535809324389		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.13129535809324389 | validation: 0.1331938893840398]
	TIME [epoch: 1.36 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16791729992459778		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.16791729992459778 | validation: 0.30450417267810215]
	TIME [epoch: 1.36 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2391531311893875		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.2391531311893875 | validation: 0.1568288219851276]
	TIME [epoch: 1.36 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24218452423632122		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.24218452423632122 | validation: 0.16517723556013436]
	TIME [epoch: 1.36 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1504477064788005		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.1504477064788005 | validation: 0.0919101016014095]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_336.pth
	Model improved!!!
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11329665001647378		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.11329665001647378 | validation: 0.1104699121511184]
	TIME [epoch: 1.36 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10460290055862252		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.10460290055862252 | validation: 0.07645248976173805]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_338.pth
	Model improved!!!
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09292918987188944		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.09292918987188944 | validation: 0.10763239408436097]
	TIME [epoch: 1.36 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1019419846872246		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.1019419846872246 | validation: 0.15980623339184624]
	TIME [epoch: 1.36 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1675732433811384		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.1675732433811384 | validation: 0.30281237416395584]
	TIME [epoch: 1.36 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27065889494744716		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.27065889494744716 | validation: 0.2390949380257985]
	TIME [epoch: 1.36 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22601920476283532		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.22601920476283532 | validation: 0.2154505580459518]
	TIME [epoch: 1.36 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17763825125515909		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.17763825125515909 | validation: 0.11601584131415964]
	TIME [epoch: 1.36 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15050600062490357		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.15050600062490357 | validation: 0.1371995016217895]
	TIME [epoch: 1.36 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11173699826279153		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.11173699826279153 | validation: 0.07811383536578698]
	TIME [epoch: 1.36 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10092474438973316		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.10092474438973316 | validation: 0.11470512258692328]
	TIME [epoch: 1.36 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09473485699931815		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.09473485699931815 | validation: 0.0693530763094188]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_348.pth
	Model improved!!!
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1056853733575339		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.1056853733575339 | validation: 0.1229107404513063]
	TIME [epoch: 1.36 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12160475632625939		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.12160475632625939 | validation: 0.10555586353468122]
	TIME [epoch: 1.36 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1435640886095504		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.1435640886095504 | validation: 0.20664542565236982]
	TIME [epoch: 1.36 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17260707041890744		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.17260707041890744 | validation: 0.1958486321501114]
	TIME [epoch: 1.36 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1719959902497628		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.1719959902497628 | validation: 0.16885973881136726]
	TIME [epoch: 1.36 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.146442456234512		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.146442456234512 | validation: 0.13729092731002815]
	TIME [epoch: 1.36 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13209228202859044		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.13209228202859044 | validation: 0.13347113755238368]
	TIME [epoch: 1.36 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11945660673153033		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.11945660673153033 | validation: 0.0972564742133128]
	TIME [epoch: 1.36 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11500294604589946		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.11500294604589946 | validation: 0.10242983874691283]
	TIME [epoch: 1.36 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10418664635520594		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.10418664635520594 | validation: 0.09313135247415645]
	TIME [epoch: 1.36 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10767933151110044		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.10767933151110044 | validation: 0.14948075170380634]
	TIME [epoch: 1.36 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12431004609256523		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.12431004609256523 | validation: 0.1379119160304399]
	TIME [epoch: 1.36 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15862147947865685		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.15862147947865685 | validation: 0.18113511562582585]
	TIME [epoch: 1.36 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.157039493506282		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.157039493506282 | validation: 0.07184017509098499]
	TIME [epoch: 1.36 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12512064707911358		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.12512064707911358 | validation: 0.12143824772711508]
	TIME [epoch: 1.36 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10523219703242638		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.10523219703242638 | validation: 0.08479242072909267]
	TIME [epoch: 1.36 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11941832870388186		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.11941832870388186 | validation: 0.15361980268434922]
	TIME [epoch: 1.36 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1420489619647511		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.1420489619647511 | validation: 0.10937033895023028]
	TIME [epoch: 1.36 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12134550971657573		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.12134550971657573 | validation: 0.14327626895135423]
	TIME [epoch: 1.36 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09879093999069909		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.09879093999069909 | validation: 0.12513716768664238]
	TIME [epoch: 1.36 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09463027364743994		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.09463027364743994 | validation: 0.1304743385602831]
	TIME [epoch: 1.36 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12799988041076843		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.12799988041076843 | validation: 0.18115533989637056]
	TIME [epoch: 1.37 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19221203805509737		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.19221203805509737 | validation: 0.14365913236095101]
	TIME [epoch: 1.36 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15519003285753474		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.15519003285753474 | validation: 0.12116570102877273]
	TIME [epoch: 1.36 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11464747328176919		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.11464747328176919 | validation: 0.09235201033460641]
	TIME [epoch: 1.36 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10800390844684493		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.10800390844684493 | validation: 0.125477649209568]
	TIME [epoch: 1.36 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11439751015616731		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.11439751015616731 | validation: 0.08819423589536211]
	TIME [epoch: 1.36 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11531568009350719		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.11531568009350719 | validation: 0.1508230643132783]
	TIME [epoch: 1.36 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1254238172983634		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.1254238172983634 | validation: 0.09857127404932593]
	TIME [epoch: 1.36 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12244709401534097		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.12244709401534097 | validation: 0.12429479689183931]
	TIME [epoch: 1.36 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10076410806477964		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.10076410806477964 | validation: 0.06147379531306107]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_379.pth
	Model improved!!!
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08306990165641823		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.08306990165641823 | validation: 0.08714288099645177]
	TIME [epoch: 1.36 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07650225239212499		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.07650225239212499 | validation: 0.057799926525561575]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_381.pth
	Model improved!!!
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08374102107320741		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.08374102107320741 | validation: 0.11991258036277347]
	TIME [epoch: 1.35 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11334875673698576		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.11334875673698576 | validation: 0.1404827349874583]
	TIME [epoch: 1.36 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1559075610897517		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.1559075610897517 | validation: 0.23120205596237176]
	TIME [epoch: 1.36 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19333842120088698		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.19333842120088698 | validation: 0.16667010871536173]
	TIME [epoch: 1.35 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1451193990456811		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.1451193990456811 | validation: 0.10995837460899383]
	TIME [epoch: 1.36 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09621024586407934		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.09621024586407934 | validation: 0.0845955554561565]
	TIME [epoch: 1.36 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09149223303017079		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.09149223303017079 | validation: 0.09442678118144449]
	TIME [epoch: 1.36 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10110116104914571		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.10110116104914571 | validation: 0.0877315770667332]
	TIME [epoch: 1.36 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09667308631044258		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.09667308631044258 | validation: 0.08793554433298355]
	TIME [epoch: 1.36 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08510961809713948		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.08510961809713948 | validation: 0.09440750158450506]
	TIME [epoch: 1.37 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09029574817653246		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.09029574817653246 | validation: 0.08666302785963617]
	TIME [epoch: 1.36 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11984055798466925		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.11984055798466925 | validation: 0.14316395034109194]
	TIME [epoch: 1.36 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15526771158893068		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.15526771158893068 | validation: 0.09542971027407288]
	TIME [epoch: 1.36 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12585988673011878		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.12585988673011878 | validation: 0.10972488898910711]
	TIME [epoch: 1.36 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09316593572068443		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.09316593572068443 | validation: 0.11599182680406286]
	TIME [epoch: 1.36 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08928624214615949		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.08928624214615949 | validation: 0.1369539511734387]
	TIME [epoch: 1.36 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10345900509294718		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.10345900509294718 | validation: 0.13388387491878925]
	TIME [epoch: 1.36 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10132915270475298		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.10132915270475298 | validation: 0.09800227174859677]
	TIME [epoch: 1.36 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09078535939109154		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.09078535939109154 | validation: 0.06272068228691788]
	TIME [epoch: 1.36 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0965123822225209		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.0965123822225209 | validation: 0.09237158769462855]
	TIME [epoch: 1.35 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09156425758580149		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.09156425758580149 | validation: 0.06524709611662313]
	TIME [epoch: 1.35 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08333859734924964		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.08333859734924964 | validation: 0.10379005880091477]
	TIME [epoch: 1.35 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09006092509877256		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.09006092509877256 | validation: 0.09805010228752968]
	TIME [epoch: 1.35 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10926629442683579		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.10926629442683579 | validation: 0.14512482592910367]
	TIME [epoch: 1.35 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12053510779393065		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.12053510779393065 | validation: 0.08520458885074121]
	TIME [epoch: 1.35 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10503819580108376		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.10503819580108376 | validation: 0.0881234049059797]
	TIME [epoch: 1.35 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10115439115939757		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.10115439115939757 | validation: 0.07661297378231921]
	TIME [epoch: 1.35 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11076936017347916		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.11076936017347916 | validation: 0.0920054857836175]
	TIME [epoch: 1.35 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08673601717418414		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.08673601717418414 | validation: 0.08821153417711536]
	TIME [epoch: 1.35 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07204435407074669		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.07204435407074669 | validation: 0.08406560946146842]
	TIME [epoch: 1.35 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07474769777459654		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.07474769777459654 | validation: 0.1132146696113007]
	TIME [epoch: 1.36 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09722193898446428		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.09722193898446428 | validation: 0.16887502164609788]
	TIME [epoch: 1.35 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1357113200014918		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.1357113200014918 | validation: 0.14754632875744997]
	TIME [epoch: 1.35 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13230021114529006		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.13230021114529006 | validation: 0.09629343021281991]
	TIME [epoch: 1.35 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10897743920220009		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.10897743920220009 | validation: 0.10035555220372246]
	TIME [epoch: 1.35 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09551941709690719		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.09551941709690719 | validation: 0.06942278923189574]
	TIME [epoch: 1.35 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09346870866209717		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.09346870866209717 | validation: 0.08736443971545632]
	TIME [epoch: 1.35 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08684326910281726		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.08684326910281726 | validation: 0.0582629528195763]
	TIME [epoch: 1.35 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07375634711497149		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.07375634711497149 | validation: 0.07883476447554422]
	TIME [epoch: 1.35 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06624878365910376		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.06624878365910376 | validation: 0.062074345893699916]
	TIME [epoch: 1.35 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06555417603531881		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.06555417603531881 | validation: 0.08322047880198999]
	TIME [epoch: 1.35 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0746096903696447		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.0746096903696447 | validation: 0.08716445858876165]
	TIME [epoch: 1.35 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10479474294072773		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.10479474294072773 | validation: 0.14057580244253062]
	TIME [epoch: 1.35 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1316986078523087		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.1316986078523087 | validation: 0.15585584808433067]
	TIME [epoch: 1.35 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12590657399825222		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.12590657399825222 | validation: 0.10829215847035883]
	TIME [epoch: 1.35 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08881908912585416		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.08881908912585416 | validation: 0.06727052449722597]
	TIME [epoch: 1.35 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06025465157945461		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.06025465157945461 | validation: 0.060627620001705435]
	TIME [epoch: 1.36 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053142536435560786		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.053142536435560786 | validation: 0.049148152716522855]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_429.pth
	Model improved!!!
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05895252405281589		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.05895252405281589 | validation: 0.07562727602569233]
	TIME [epoch: 1.35 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07834501676799463		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.07834501676799463 | validation: 0.06204125624208434]
	TIME [epoch: 1.35 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08451096819253062		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.08451096819253062 | validation: 0.07992489285381765]
	TIME [epoch: 1.35 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0724927128831164		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.0724927128831164 | validation: 0.07550153632738789]
	TIME [epoch: 1.35 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06954865337358351		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.06954865337358351 | validation: 0.09161058091245906]
	TIME [epoch: 1.36 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08113628662801155		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.08113628662801155 | validation: 0.12387193693032215]
	TIME [epoch: 1.36 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13366291036629144		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.13366291036629144 | validation: 0.10433159866152507]
	TIME [epoch: 1.35 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15593982794953304		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.15593982794953304 | validation: 0.08582866882344815]
	TIME [epoch: 1.35 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08432872583295621		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.08432872583295621 | validation: 0.06164403095521236]
	TIME [epoch: 1.35 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05460095737841135		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.05460095737841135 | validation: 0.07512969494921809]
	TIME [epoch: 1.35 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062178726611025754		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.062178726611025754 | validation: 0.13237364699099005]
	TIME [epoch: 1.35 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08630168838369674		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.08630168838369674 | validation: 0.09778191545131194]
	TIME [epoch: 1.35 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0755422049442816		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.0755422049442816 | validation: 0.05772010253050998]
	TIME [epoch: 1.35 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05875036206260749		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.05875036206260749 | validation: 0.05803448624743951]
	TIME [epoch: 1.35 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05519789807205783		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.05519789807205783 | validation: 0.04374650471798211]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_444.pth
	Model improved!!!
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06477486529476144		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.06477486529476144 | validation: 0.07947661903423124]
	TIME [epoch: 1.36 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08593800475864374		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.08593800475864374 | validation: 0.07518199172346057]
	TIME [epoch: 1.36 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10955956530301483		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.10955956530301483 | validation: 0.13626397832405854]
	TIME [epoch: 1.36 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12815368655013376		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.12815368655013376 | validation: 0.1425305538354661]
	TIME [epoch: 1.36 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12081771592288285		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.12081771592288285 | validation: 0.08504605964449702]
	TIME [epoch: 1.36 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07629173049714959		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.07629173049714959 | validation: 0.07955999540217674]
	TIME [epoch: 1.36 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055298773482311346		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.055298773482311346 | validation: 0.06428963687297178]
	TIME [epoch: 1.36 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05001874105953004		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.05001874105953004 | validation: 0.059743584249649796]
	TIME [epoch: 1.36 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048090010416151796		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.048090010416151796 | validation: 0.0517682207372952]
	TIME [epoch: 1.36 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04872755020974411		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.04872755020974411 | validation: 0.05310966960372704]
	TIME [epoch: 1.36 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06814264168835667		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.06814264168835667 | validation: 0.08770172920902841]
	TIME [epoch: 1.37 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10262848782791695		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.10262848782791695 | validation: 0.07675283650630144]
	TIME [epoch: 1.36 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09227888093687638		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.09227888093687638 | validation: 0.0807637674370078]
	TIME [epoch: 1.36 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07572798080780688		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.07572798080780688 | validation: 0.08667816566324522]
	TIME [epoch: 1.36 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08075601362449453		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.08075601362449453 | validation: 0.10650019141477025]
	TIME [epoch: 1.36 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11030032260467547		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.11030032260467547 | validation: 0.10556922556718146]
	TIME [epoch: 1.36 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11591747868988077		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.11591747868988077 | validation: 0.08081847821001456]
	TIME [epoch: 1.36 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07752277400902799		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.07752277400902799 | validation: 0.03986673931721009]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_462.pth
	Model improved!!!
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049710001710046095		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.049710001710046095 | validation: 0.05784271037454522]
	TIME [epoch: 1.35 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0424876005197032		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.0424876005197032 | validation: 0.043179435364613473]
	TIME [epoch: 1.35 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045555897034712176		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.045555897034712176 | validation: 0.06814577081173878]
	TIME [epoch: 1.35 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0513398810217758		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.0513398810217758 | validation: 0.046673154138122924]
	TIME [epoch: 1.35 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05887376833626994		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.05887376833626994 | validation: 0.08071349413183133]
	TIME [epoch: 1.36 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06820937150479985		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.06820937150479985 | validation: 0.041924462958073894]
	TIME [epoch: 1.36 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06426322831969582		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.06426322831969582 | validation: 0.06396885907993809]
	TIME [epoch: 1.36 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06036694134828231		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.06036694134828231 | validation: 0.03943121380056955]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_470.pth
	Model improved!!!
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05951049422976664		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.05951049422976664 | validation: 0.08792773898672182]
	TIME [epoch: 1.35 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0822209141763279		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.0822209141763279 | validation: 0.1278619473257749]
	TIME [epoch: 1.35 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13373429091813954		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.13373429091813954 | validation: 0.12495266188854091]
	TIME [epoch: 1.35 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1144041391813056		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.1144041391813056 | validation: 0.07549513679359784]
	TIME [epoch: 1.36 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06402546103386561		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.06402546103386561 | validation: 0.06309894474129162]
	TIME [epoch: 1.36 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047293781531666		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.047293781531666 | validation: 0.05944965933928367]
	TIME [epoch: 1.36 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052379819426597696		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.052379819426597696 | validation: 0.06152667919900398]
	TIME [epoch: 1.36 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06799446553897502		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.06799446553897502 | validation: 0.05746016268808925]
	TIME [epoch: 1.36 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0674301337707229		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.0674301337707229 | validation: 0.05545527152161389]
	TIME [epoch: 1.36 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05513391257738061		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.05513391257738061 | validation: 0.058976967902871685]
	TIME [epoch: 1.36 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05803936369601275		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.05803936369601275 | validation: 0.07862016630033665]
	TIME [epoch: 1.36 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07335299478855703		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.07335299478855703 | validation: 0.09172836180905895]
	TIME [epoch: 1.36 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09458988040662046		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.09458988040662046 | validation: 0.09045157300352008]
	TIME [epoch: 1.36 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09035870493641661		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.09035870493641661 | validation: 0.06232835092730015]
	TIME [epoch: 1.36 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06545399650259849		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.06545399650259849 | validation: 0.05709084045965568]
	TIME [epoch: 1.36 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049699852154624136		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.049699852154624136 | validation: 0.039099650050985785]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_486.pth
	Model improved!!!
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04188152379217813		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.04188152379217813 | validation: 0.0412455669596381]
	TIME [epoch: 1.36 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03881792283572115		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.03881792283572115 | validation: 0.034652575483256454]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_488.pth
	Model improved!!!
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039914249847402154		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.039914249847402154 | validation: 0.04381260565909605]
	TIME [epoch: 1.36 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0424913413185044		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.0424913413185044 | validation: 0.04475384000015793]
	TIME [epoch: 1.36 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05312674626889801		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.05312674626889801 | validation: 0.08128323428516754]
	TIME [epoch: 1.36 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07621966780834122		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.07621966780834122 | validation: 0.08322428660997411]
	TIME [epoch: 1.36 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09657633604589587		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.09657633604589587 | validation: 0.07770975058523563]
	TIME [epoch: 1.36 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07656413007616145		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.07656413007616145 | validation: 0.04634224915245958]
	TIME [epoch: 1.36 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05167674748505797		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.05167674748505797 | validation: 0.04885094582108899]
	TIME [epoch: 1.36 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043570186343545786		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.043570186343545786 | validation: 0.09249658593675171]
	TIME [epoch: 1.36 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06233976747082224		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.06233976747082224 | validation: 0.10103869377033546]
	TIME [epoch: 1.37 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08874294489948255		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.08874294489948255 | validation: 0.08247015665458178]
	TIME [epoch: 1.36 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11069917301813713		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.11069917301813713 | validation: 0.05770668467267788]
	TIME [epoch: 1.36 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060810528530944784		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.060810528530944784 | validation: 0.03278724745412372]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_500.pth
	Model improved!!!
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03312753059860116		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.03312753059860116 | validation: 0.043298435598741504]
	TIME [epoch: 184 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031913892431516884		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.031913892431516884 | validation: 0.04085524666750287]
	TIME [epoch: 2.7 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03557603752089043		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.03557603752089043 | validation: 0.03997808356754932]
	TIME [epoch: 2.67 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03488817080045555		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.03488817080045555 | validation: 0.043732262053717656]
	TIME [epoch: 2.67 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038571849690385795		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.038571849690385795 | validation: 0.061247523733907364]
	TIME [epoch: 2.67 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05482907288241094		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.05482907288241094 | validation: 0.07580047451820661]
	TIME [epoch: 2.67 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08002717319566884		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.08002717319566884 | validation: 0.101888151605007]
	TIME [epoch: 2.68 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10255007604762523		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.10255007604762523 | validation: 0.08619451512909017]
	TIME [epoch: 2.67 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.083534565357044		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.083534565357044 | validation: 0.04311196685325868]
	TIME [epoch: 2.68 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048110682256188045		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.048110682256188045 | validation: 0.03971313014267869]
	TIME [epoch: 2.67 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035231937944258856		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.035231937944258856 | validation: 0.03442834652393839]
	TIME [epoch: 2.68 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0326915780745963		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.0326915780745963 | validation: 0.042162066364509566]
	TIME [epoch: 2.67 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03541448327015525		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.03541448327015525 | validation: 0.04677891808251641]
	TIME [epoch: 2.67 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0499413604298166		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.0499413604298166 | validation: 0.0636941430922531]
	TIME [epoch: 2.67 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07619489474669058		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.07619489474669058 | validation: 0.06766269727951955]
	TIME [epoch: 2.68 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07839387075869594		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.07839387075869594 | validation: 0.04892683463864361]
	TIME [epoch: 2.67 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045294769052974644		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.045294769052974644 | validation: 0.03989499639208876]
	TIME [epoch: 2.67 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03390653956240442		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.03390653956240442 | validation: 0.03684360468567628]
	TIME [epoch: 2.67 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03316119589777493		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.03316119589777493 | validation: 0.04381348122441012]
	TIME [epoch: 2.67 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04409682477173251		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.04409682477173251 | validation: 0.068259818296017]
	TIME [epoch: 2.68 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07661082106538354		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.07661082106538354 | validation: 0.12490711400872075]
	TIME [epoch: 2.67 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11789911353693779		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.11789911353693779 | validation: 0.08683919984648764]
	TIME [epoch: 2.67 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0805427648562831		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.0805427648562831 | validation: 0.041532978671263314]
	TIME [epoch: 2.67 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04111885166896503		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.04111885166896503 | validation: 0.032232403814696]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_524.pth
	Model improved!!!
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035546549600336486		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.035546549600336486 | validation: 0.04146021456768112]
	TIME [epoch: 2.67 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03764134475258147		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.03764134475258147 | validation: 0.03912501799623211]
	TIME [epoch: 2.67 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038490640915581034		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.038490640915581034 | validation: 0.046051861022329034]
	TIME [epoch: 2.67 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040678698896886606		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.040678698896886606 | validation: 0.048244750600416636]
	TIME [epoch: 2.67 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042688311338378075		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.042688311338378075 | validation: 0.046225017542297735]
	TIME [epoch: 2.67 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045693150831186725		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.045693150831186725 | validation: 0.04534761621663391]
	TIME [epoch: 2.68 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04179545550368884		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.04179545550368884 | validation: 0.039410934703333234]
	TIME [epoch: 2.68 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03868476914067445		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.03868476914067445 | validation: 0.04060203354136768]
	TIME [epoch: 2.67 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036540185076829095		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.036540185076829095 | validation: 0.046450681589081444]
	TIME [epoch: 2.67 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049797479108082535		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.049797479108082535 | validation: 0.08671188872819027]
	TIME [epoch: 2.67 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09643441267341418		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.09643441267341418 | validation: 0.1215944517721199]
	TIME [epoch: 2.67 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11875500254871042		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.11875500254871042 | validation: 0.066145420483501]
	TIME [epoch: 2.67 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05839753446580536		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.05839753446580536 | validation: 0.030238164698857862]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_537.pth
	Model improved!!!
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030450187070790422		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.030450187070790422 | validation: 0.035935149061934527]
	TIME [epoch: 2.67 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029278215274007504		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.029278215274007504 | validation: 0.032837928066578474]
	TIME [epoch: 2.68 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0341351137804807		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.0341351137804807 | validation: 0.04038613891759821]
	TIME [epoch: 2.67 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039801910218196714		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.039801910218196714 | validation: 0.044158402908173404]
	TIME [epoch: 2.68 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043165537662326545		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.043165537662326545 | validation: 0.04736167627515145]
	TIME [epoch: 2.68 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04453584662527515		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.04453584662527515 | validation: 0.051487972766218894]
	TIME [epoch: 2.68 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04504646809734274		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.04504646809734274 | validation: 0.046943041948564856]
	TIME [epoch: 2.67 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04453608400456001		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.04453608400456001 | validation: 0.05403913913287994]
	TIME [epoch: 2.68 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046547970581910805		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.046547970581910805 | validation: 0.057744616721984235]
	TIME [epoch: 2.67 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0569867514475631		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.0569867514475631 | validation: 0.06539148812171718]
	TIME [epoch: 2.67 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07290993531999185		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.07290993531999185 | validation: 0.06628077754733926]
	TIME [epoch: 2.67 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07163145722864536		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.07163145722864536 | validation: 0.04026258903261903]
	TIME [epoch: 2.67 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04294346927471581		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.04294346927471581 | validation: 0.03248874618373593]
	TIME [epoch: 2.67 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02648936038547197		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.02648936038547197 | validation: 0.03215037123769648]
	TIME [epoch: 2.67 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02637804361750927		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.02637804361750927 | validation: 0.02725324453546323]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_552.pth
	Model improved!!!
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034955785054696216		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.034955785054696216 | validation: 0.03909816876160621]
	TIME [epoch: 2.68 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044167422262285844		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.044167422262285844 | validation: 0.041393799160520564]
	TIME [epoch: 2.67 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05243827133930834		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.05243827133930834 | validation: 0.06191535974586977]
	TIME [epoch: 2.67 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05562592140536026		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.05562592140536026 | validation: 0.07040214488860286]
	TIME [epoch: 2.67 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053386739958501704		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.053386739958501704 | validation: 0.0582352060276355]
	TIME [epoch: 2.68 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04697549926291973		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.04697549926291973 | validation: 0.04053997800944025]
	TIME [epoch: 2.68 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03637463519100214		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.03637463519100214 | validation: 0.031990463603339674]
	TIME [epoch: 2.68 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029958517404930537		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.029958517404930537 | validation: 0.0266525655723601]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_560.pth
	Model improved!!!
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028949798761344358		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.028949798761344358 | validation: 0.037432346350869526]
	TIME [epoch: 2.68 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03919004405283375		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.03919004405283375 | validation: 0.05305267183393767]
	TIME [epoch: 2.68 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060982279640463384		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.060982279640463384 | validation: 0.06810293218498213]
	TIME [epoch: 2.69 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07032716544566832		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.07032716544566832 | validation: 0.0639971321168205]
	TIME [epoch: 2.68 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047680152404212174		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.047680152404212174 | validation: 0.024914924410724085]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_565.pth
	Model improved!!!
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028745089626627324		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.028745089626627324 | validation: 0.024738669655978287]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_566.pth
	Model improved!!!
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024178030389657997		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.024178030389657997 | validation: 0.03372092183196691]
	TIME [epoch: 2.67 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02473387681441282		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.02473387681441282 | validation: 0.031834908613702716]
	TIME [epoch: 2.67 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02872223984816097		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.02872223984816097 | validation: 0.03913045575296097]
	TIME [epoch: 2.67 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04930444649392175		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.04930444649392175 | validation: 0.07658922264979286]
	TIME [epoch: 2.67 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09354474385817974		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.09354474385817974 | validation: 0.0943648653025411]
	TIME [epoch: 2.67 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07678031175057153		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.07678031175057153 | validation: 0.048113771063772015]
	TIME [epoch: 2.67 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03998142257405214		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.03998142257405214 | validation: 0.026073705378436774]
	TIME [epoch: 2.67 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022741265924126505		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.022741265924126505 | validation: 0.024919560787873475]
	TIME [epoch: 2.68 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021576689751165033		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.021576689751165033 | validation: 0.026869248616567976]
	TIME [epoch: 2.67 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024924832564004018		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.024924832564004018 | validation: 0.027612613367260286]
	TIME [epoch: 2.67 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025009518264937105		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.025009518264937105 | validation: 0.02854727909949846]
	TIME [epoch: 2.68 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02802592174059004		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.02802592174059004 | validation: 0.03491546469447465]
	TIME [epoch: 2.67 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04166333085059628		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.04166333085059628 | validation: 0.05803143966407376]
	TIME [epoch: 2.67 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058402323691768426		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.058402323691768426 | validation: 0.055098687189138865]
	TIME [epoch: 2.67 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05654197728211606		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.05654197728211606 | validation: 0.03985614180008926]
	TIME [epoch: 2.68 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038719757650441874		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.038719757650441874 | validation: 0.04049629086475899]
	TIME [epoch: 2.67 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04172599785670538		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.04172599785670538 | validation: 0.06495474919631367]
	TIME [epoch: 2.67 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06663120191910973		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.06663120191910973 | validation: 0.07330930519503819]
	TIME [epoch: 2.68 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06806781041392096		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.06806781041392096 | validation: 0.04566527336172276]
	TIME [epoch: 2.68 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03829595402318689		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.03829595402318689 | validation: 0.027033353079461378]
	TIME [epoch: 2.67 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026930430166932997		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.026930430166932997 | validation: 0.022199855202168262]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_587.pth
	Model improved!!!
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021601606204828715		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.021601606204828715 | validation: 0.02328746254003418]
	TIME [epoch: 2.68 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021674872310094934		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.021674872310094934 | validation: 0.023316469446325084]
	TIME [epoch: 2.68 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02086474816021514		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.02086474816021514 | validation: 0.0295574712337635]
	TIME [epoch: 2.68 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021592836112600905		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.021592836112600905 | validation: 0.029473676852769595]
	TIME [epoch: 2.68 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025716805966992693		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.025716805966992693 | validation: 0.040818082625809764]
	TIME [epoch: 2.68 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033947590719357205		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.033947590719357205 | validation: 0.05764027130208506]
	TIME [epoch: 2.68 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05583318911320854		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.05583318911320854 | validation: 0.07105544513364656]
	TIME [epoch: 2.68 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07474282749643033		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.07474282749643033 | validation: 0.05182691180696071]
	TIME [epoch: 2.68 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05099886840807358		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.05099886840807358 | validation: 0.031263554956623654]
	TIME [epoch: 2.68 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03180306568619268		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.03180306568619268 | validation: 0.033465274027434426]
	TIME [epoch: 2.68 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032935721495293356		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.032935721495293356 | validation: 0.040223861569891266]
	TIME [epoch: 2.68 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048079450302756045		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.048079450302756045 | validation: 0.04373365197876118]
	TIME [epoch: 2.68 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050832113013694724		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.050832113013694724 | validation: 0.03951265547316114]
	TIME [epoch: 2.68 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034816425165663877		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.034816425165663877 | validation: 0.025507395777805142]
	TIME [epoch: 2.67 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025837951143638637		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.025837951143638637 | validation: 0.03507139623742185]
	TIME [epoch: 2.68 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026210869279995018		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.026210869279995018 | validation: 0.034339262538376325]
	TIME [epoch: 2.67 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028974593735144882		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.028974593735144882 | validation: 0.038465908944693365]
	TIME [epoch: 2.67 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03392847632811136		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.03392847632811136 | validation: 0.0365755873292007]
	TIME [epoch: 2.67 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037330995320775426		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.037330995320775426 | validation: 0.039689893909469726]
	TIME [epoch: 2.67 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04118020618305477		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.04118020618305477 | validation: 0.04229165799329307]
	TIME [epoch: 2.68 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041120500249066126		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.041120500249066126 | validation: 0.04390667823341762]
	TIME [epoch: 2.67 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035951358912288185		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.035951358912288185 | validation: 0.03328656654474368]
	TIME [epoch: 2.67 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0326106108388549		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.0326106108388549 | validation: 0.03339495771731793]
	TIME [epoch: 2.67 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03148652399660624		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.03148652399660624 | validation: 0.0333730373710713]
	TIME [epoch: 2.68 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029313567241287527		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.029313567241287527 | validation: 0.031005657849109403]
	TIME [epoch: 2.68 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02425877349319567		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.02425877349319567 | validation: 0.02733585660112341]
	TIME [epoch: 2.68 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027048787523318504		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.027048787523318504 | validation: 0.03075764703018489]
	TIME [epoch: 2.68 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02845814256560181		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.02845814256560181 | validation: 0.034039264877795516]
	TIME [epoch: 2.67 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028663868469764837		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.028663868469764837 | validation: 0.03523342267392481]
	TIME [epoch: 2.67 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03172171039186218		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.03172171039186218 | validation: 0.04002323017494963]
	TIME [epoch: 2.67 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03528531339164816		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.03528531339164816 | validation: 0.04153104774047638]
	TIME [epoch: 2.68 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04232161030460405		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.04232161030460405 | validation: 0.057631983893101396]
	TIME [epoch: 2.67 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050057725047990956		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.050057725047990956 | validation: 0.049152557344926254]
	TIME [epoch: 2.67 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05134850086622812		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.05134850086622812 | validation: 0.050782424085396176]
	TIME [epoch: 2.67 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0411000945635295		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.0411000945635295 | validation: 0.029029027708284163]
	TIME [epoch: 2.67 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033240911091545707		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.033240911091545707 | validation: 0.02874818169562049]
	TIME [epoch: 2.67 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025061386368818283		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.025061386368818283 | validation: 0.022000965626622904]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_624.pth
	Model improved!!!
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021069524582484258		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.021069524582484258 | validation: 0.019947868283532868]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_625.pth
	Model improved!!!
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021679772211984333		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.021679772211984333 | validation: 0.02242340488670702]
	TIME [epoch: 2.68 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019647347826902608		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.019647347826902608 | validation: 0.023440777557220904]
	TIME [epoch: 2.68 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02031187382015214		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.02031187382015214 | validation: 0.025029460873763554]
	TIME [epoch: 2.67 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024261500011049535		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.024261500011049535 | validation: 0.03175016493749202]
	TIME [epoch: 2.68 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03231284305449464		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.03231284305449464 | validation: 0.04667633998417878]
	TIME [epoch: 2.67 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04451080015930198		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.04451080015930198 | validation: 0.05274418311638991]
	TIME [epoch: 2.67 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056670843466974916		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.056670843466974916 | validation: 0.04669078881646363]
	TIME [epoch: 2.67 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05079886871013642		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.05079886871013642 | validation: 0.03789083549063904]
	TIME [epoch: 2.67 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04624690539633455		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.04624690539633455 | validation: 0.034857615804065926]
	TIME [epoch: 2.67 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03516204105039571		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.03516204105039571 | validation: 0.02610808048943172]
	TIME [epoch: 2.67 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025047823732119095		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.025047823732119095 | validation: 0.02507705253230168]
	TIME [epoch: 2.67 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021328309870362272		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.021328309870362272 | validation: 0.023950254215253264]
	TIME [epoch: 2.67 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02114689984870462		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.02114689984870462 | validation: 0.021415229762026478]
	TIME [epoch: 2.67 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02176490735073214		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.02176490735073214 | validation: 0.023739470923420727]
	TIME [epoch: 2.67 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021772285517690403		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.021772285517690403 | validation: 0.02225476830254669]
	TIME [epoch: 2.68 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022821344387371205		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.022821344387371205 | validation: 0.036226128390164314]
	TIME [epoch: 2.67 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02645585843511145		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.02645585843511145 | validation: 0.03421879962382523]
	TIME [epoch: 2.67 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0351290483029207		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.0351290483029207 | validation: 0.0521512365085166]
	TIME [epoch: 2.68 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05031641882296281		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.05031641882296281 | validation: 0.04278022549372656]
	TIME [epoch: 2.67 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050309566588110255		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.050309566588110255 | validation: 0.033698542251098196]
	TIME [epoch: 2.67 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0306855832609333		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.0306855832609333 | validation: 0.017617020704041666]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_646.pth
	Model improved!!!
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019338619938146142		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.019338619938146142 | validation: 0.02109303711771551]
	TIME [epoch: 2.68 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018185253971165546		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.018185253971165546 | validation: 0.020359885017723137]
	TIME [epoch: 2.68 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020891007610574067		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.020891007610574067 | validation: 0.027404260860043474]
	TIME [epoch: 2.68 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02664394062770181		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.02664394062770181 | validation: 0.027269453430225632]
	TIME [epoch: 2.68 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03438321188150674		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.03438321188150674 | validation: 0.044071829708525426]
	TIME [epoch: 2.69 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040084510135166465		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.040084510135166465 | validation: 0.043684535249471135]
	TIME [epoch: 2.68 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03526986517816114		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.03526986517816114 | validation: 0.021082023430555874]
	TIME [epoch: 2.68 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022513802537711288		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.022513802537711288 | validation: 0.017653906345198644]
	TIME [epoch: 2.68 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018604521155904993		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.018604521155904993 | validation: 0.02701756393978232]
	TIME [epoch: 2.69 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02075488045243399		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.02075488045243399 | validation: 0.03035934041545927]
	TIME [epoch: 2.68 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029303524651841793		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.029303524651841793 | validation: 0.04908899212395082]
	TIME [epoch: 2.68 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038049307267959014		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.038049307267959014 | validation: 0.04172751424540874]
	TIME [epoch: 2.68 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0458002185162441		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.0458002185162441 | validation: 0.03292069158227673]
	TIME [epoch: 2.68 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035784339033756364		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.035784339033756364 | validation: 0.023422648556190386]
	TIME [epoch: 2.68 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025357839030188983		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.025357839030188983 | validation: 0.018968260960189533]
	TIME [epoch: 2.68 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016700439477427158		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.016700439477427158 | validation: 0.017825528354668097]
	TIME [epoch: 2.69 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016813894508189424		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.016813894508189424 | validation: 0.017857156069541757]
	TIME [epoch: 2.68 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01747070995702415		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.01747070995702415 | validation: 0.016659959222940224]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_664.pth
	Model improved!!!
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017377943622008854		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.017377943622008854 | validation: 0.018261613870309234]
	TIME [epoch: 2.67 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019779667526874576		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.019779667526874576 | validation: 0.02655830284259273]
	TIME [epoch: 2.67 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026043931928097192		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.026043931928097192 | validation: 0.030481168531749193]
	TIME [epoch: 2.67 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04668354886318033		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.04668354886318033 | validation: 0.04904042719753933]
	TIME [epoch: 2.68 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04433609232623591		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.04433609232623591 | validation: 0.0253604141415962]
	TIME [epoch: 2.68 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024337757918196777		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.024337757918196777 | validation: 0.02025830383093016]
	TIME [epoch: 2.68 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016091902418195858		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.016091902418195858 | validation: 0.01738021598288029]
	TIME [epoch: 2.68 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02028053532672862		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.02028053532672862 | validation: 0.02002254611666874]
	TIME [epoch: 2.68 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022231962601864545		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.022231962601864545 | validation: 0.026957748527591842]
	TIME [epoch: 2.69 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04007521512613382		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.04007521512613382 | validation: 0.05756621789377208]
	TIME [epoch: 2.69 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05688341164018534		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.05688341164018534 | validation: 0.05495795858899904]
	TIME [epoch: 2.68 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04598353231160569		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.04598353231160569 | validation: 0.026336502134912367]
	TIME [epoch: 2.68 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019416642662199002		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.019416642662199002 | validation: 0.02339514764633113]
	TIME [epoch: 2.68 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01667821885980452		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.01667821885980452 | validation: 0.021816976277736433]
	TIME [epoch: 2.69 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018545737290959943		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.018545737290959943 | validation: 0.016697548591198264]
	TIME [epoch: 2.68 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019219373418967636		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.019219373418967636 | validation: 0.017459688626953584]
	TIME [epoch: 2.67 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01666182132023457		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.01666182132023457 | validation: 0.02169645355439677]
	TIME [epoch: 2.67 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01854791374648562		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.01854791374648562 | validation: 0.017812934854525175]
	TIME [epoch: 2.67 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019565765048198347		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.019565765048198347 | validation: 0.02269588287364759]
	TIME [epoch: 2.67 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02727607638952107		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.02727607638952107 | validation: 0.032686029067251675]
	TIME [epoch: 2.68 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03460091467158843		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.03460091467158843 | validation: 0.038682448413911844]
	TIME [epoch: 2.67 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035769134308674025		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.035769134308674025 | validation: 0.028387780568962774]
	TIME [epoch: 2.67 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02769379213090474		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.02769379213090474 | validation: 0.02169137644534043]
	TIME [epoch: 2.67 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018794950870656316		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.018794950870656316 | validation: 0.01951866945916323]
	TIME [epoch: 2.69 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021451394316715916		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.021451394316715916 | validation: 0.02064879959216781]
	TIME [epoch: 2.69 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02252128050024887		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.02252128050024887 | validation: 0.032119018345333074]
	TIME [epoch: 2.68 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026447472014293413		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.026447472014293413 | validation: 0.035740195952153865]
	TIME [epoch: 2.68 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03513897181296444		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.03513897181296444 | validation: 0.046439626360451014]
	TIME [epoch: 2.69 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03658946159705904		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.03658946159705904 | validation: 0.027314766714579743]
	TIME [epoch: 2.68 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025770346070242898		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.025770346070242898 | validation: 0.01738366554315185]
	TIME [epoch: 2.69 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018760524558883503		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.018760524558883503 | validation: 0.017767427172744366]
	TIME [epoch: 2.69 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016708426521109147		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.016708426521109147 | validation: 0.021304480820953042]
	TIME [epoch: 2.69 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015662096131561852		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.015662096131561852 | validation: 0.01781012511611114]
	TIME [epoch: 2.68 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01576442894121122		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.01576442894121122 | validation: 0.021452154113519042]
	TIME [epoch: 2.69 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022904362306378358		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.022904362306378358 | validation: 0.025749975116173274]
	TIME [epoch: 2.69 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030132499608984228		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.030132499608984228 | validation: 0.02901601608490223]
	TIME [epoch: 2.69 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03493308220026357		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.03493308220026357 | validation: 0.030473993188354444]
	TIME [epoch: 2.67 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026554049245122543		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.026554049245122543 | validation: 0.021438066265750268]
	TIME [epoch: 2.67 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019233798712499087		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.019233798712499087 | validation: 0.020665258047616143]
	TIME [epoch: 2.67 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024388287591837025		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.024388287591837025 | validation: 0.03482302623666366]
	TIME [epoch: 2.67 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03325152946323242		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.03325152946323242 | validation: 0.031282243766912125]
	TIME [epoch: 2.67 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03696246757522845		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.03696246757522845 | validation: 0.029637119085786826]
	TIME [epoch: 2.69 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026399036275696935		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.026399036275696935 | validation: 0.019488090882437616]
	TIME [epoch: 2.68 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01865454608105624		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.01865454608105624 | validation: 0.018320225053871843]
	TIME [epoch: 2.68 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016665668834997413		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.016665668834997413 | validation: 0.018780879940863283]
	TIME [epoch: 2.68 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016257751554384123		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.016257751554384123 | validation: 0.015481939811051637]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_710.pth
	Model improved!!!
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01626595085353728		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.01626595085353728 | validation: 0.018529750787726073]
	TIME [epoch: 2.67 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01627171884911501		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.01627171884911501 | validation: 0.01585431438590641]
	TIME [epoch: 2.67 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018710666875254037		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.018710666875254037 | validation: 0.02139899738538044]
	TIME [epoch: 2.67 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020434599608148307		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.020434599608148307 | validation: 0.027814698789599093]
	TIME [epoch: 2.67 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026650501605611897		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.026650501605611897 | validation: 0.03243370940134806]
	TIME [epoch: 2.67 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0331640936025793		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.0331640936025793 | validation: 0.03508861447854059]
	TIME [epoch: 2.67 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028168797721584937		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.028168797721584937 | validation: 0.020257684756648343]
	TIME [epoch: 2.68 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019524886595074422		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.019524886595074422 | validation: 0.018477048619225556]
	TIME [epoch: 2.67 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020657884654104137		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.020657884654104137 | validation: 0.020910792683531965]
	TIME [epoch: 2.67 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02566848936279575		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.02566848936279575 | validation: 0.031743403072412725]
	TIME [epoch: 2.67 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0265178297523786		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.0265178297523786 | validation: 0.01970998135509924]
	TIME [epoch: 2.67 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02161703548926612		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.02161703548926612 | validation: 0.027076040701335992]
	TIME [epoch: 2.67 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01767217055499311		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.01767217055499311 | validation: 0.017451815251414184]
	TIME [epoch: 2.67 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017718172741666097		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.017718172741666097 | validation: 0.01768378235234205]
	TIME [epoch: 2.67 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017344929439347085		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.017344929439347085 | validation: 0.01691037917689462]
	TIME [epoch: 2.67 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019030605726857046		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.019030605726857046 | validation: 0.01836674598680036]
	TIME [epoch: 2.67 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019101865298176323		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.019101865298176323 | validation: 0.017981422901163748]
	TIME [epoch: 2.67 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019089231231723326		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.019089231231723326 | validation: 0.018377496963052666]
	TIME [epoch: 2.68 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019423833191498287		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.019423833191498287 | validation: 0.02556146388035744]
	TIME [epoch: 2.67 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022438611720593377		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.022438611720593377 | validation: 0.02507321881148914]
	TIME [epoch: 2.67 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025462608540328353		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.025462608540328353 | validation: 0.03477071761615622]
	TIME [epoch: 2.67 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02599769319392756		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.02599769319392756 | validation: 0.02695011513674164]
	TIME [epoch: 2.67 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03223025335957447		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.03223025335957447 | validation: 0.026881031546429825]
	TIME [epoch: 2.67 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03515157934035386		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.03515157934035386 | validation: 0.01856716311441453]
	TIME [epoch: 2.67 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020196010719684604		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.020196010719684604 | validation: 0.016538969508133505]
	TIME [epoch: 2.67 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016811587920083488		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.016811587920083488 | validation: 0.02159134832048214]
	TIME [epoch: 2.67 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01858650452143252		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.01858650452143252 | validation: 0.023039079076150017]
	TIME [epoch: 2.67 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020241271174660014		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.020241271174660014 | validation: 0.025068248493671397]
	TIME [epoch: 2.67 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02046937383625897		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.02046937383625897 | validation: 0.01746996402555926]
	TIME [epoch: 2.67 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017891977264894714		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.017891977264894714 | validation: 0.019550597482818545]
	TIME [epoch: 2.67 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01812311609889535		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.01812311609889535 | validation: 0.01906198050477466]
	TIME [epoch: 2.67 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019679586690732158		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.019679586690732158 | validation: 0.025848381848956416]
	TIME [epoch: 2.67 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020373624064589643		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.020373624064589643 | validation: 0.018206422992444904]
	TIME [epoch: 2.67 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020002296930497102		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.020002296930497102 | validation: 0.020638936203507187]
	TIME [epoch: 2.67 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020423463176172035		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.020423463176172035 | validation: 0.021655804559040204]
	TIME [epoch: 2.67 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02149087873372382		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.02149087873372382 | validation: 0.02317736247909337]
	TIME [epoch: 2.67 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024196333882814305		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.024196333882814305 | validation: 0.019731912033732493]
	TIME [epoch: 2.67 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02062999540778391		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.02062999540778391 | validation: 0.018147551875521317]
	TIME [epoch: 2.67 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018417496026013523		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.018417496026013523 | validation: 0.015983428593505468]
	TIME [epoch: 2.67 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01365764176112433		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.01365764176112433 | validation: 0.01669603818919432]
	TIME [epoch: 2.68 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015664873152428333		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.015664873152428333 | validation: 0.021542539550433105]
	TIME [epoch: 2.67 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0178259096112494		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.0178259096112494 | validation: 0.019393608419645103]
	TIME [epoch: 2.67 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022428085659009746		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.022428085659009746 | validation: 0.03067728193850079]
	TIME [epoch: 2.67 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028813805617567665		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.028813805617567665 | validation: 0.02473263948920891]
	TIME [epoch: 2.67 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02796441612391864		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.02796441612391864 | validation: 0.0264063615209766]
	TIME [epoch: 2.67 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020959749165225396		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.020959749165225396 | validation: 0.016821073212030746]
	TIME [epoch: 2.66 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01736496489283708		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.01736496489283708 | validation: 0.01916891409606444]
	TIME [epoch: 2.68 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017771695106571835		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.017771695106571835 | validation: 0.01479148225083269]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_758.pth
	Model improved!!!
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016532452781440048		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.016532452781440048 | validation: 0.013561474985295308]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_759.pth
	Model improved!!!
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016000190686840977		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.016000190686840977 | validation: 0.01381786854913305]
	TIME [epoch: 2.67 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016648394867347947		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.016648394867347947 | validation: 0.0168119405113737]
	TIME [epoch: 2.68 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016643333453703658		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.016643333453703658 | validation: 0.018584485032634823]
	TIME [epoch: 2.67 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02316060315390768		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.02316060315390768 | validation: 0.016731569571357307]
	TIME [epoch: 2.67 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021055931341662105		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.021055931341662105 | validation: 0.017891860462269406]
	TIME [epoch: 2.67 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01851999013572427		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.01851999013572427 | validation: 0.0193594666149329]
	TIME [epoch: 2.67 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0158166239119425		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.0158166239119425 | validation: 0.013461390116662066]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_766.pth
	Model improved!!!
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015429596786457531		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.015429596786457531 | validation: 0.025335097352520375]
	TIME [epoch: 2.68 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018406392705444242		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.018406392705444242 | validation: 0.03149616164999976]
	TIME [epoch: 2.68 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030499456571815332		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.030499456571815332 | validation: 0.03631643954568219]
	TIME [epoch: 2.68 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03407197648110939		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.03407197648110939 | validation: 0.017945491421062942]
	TIME [epoch: 2.68 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019894113140074613		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.019894113140074613 | validation: 0.010321541841317117]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_771.pth
	Model improved!!!
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01414753507276743		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.01414753507276743 | validation: 0.013114321820832386]
	TIME [epoch: 2.67 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014890064577110453		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.014890064577110453 | validation: 0.012108545976769126]
	TIME [epoch: 2.67 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013731659831738333		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.013731659831738333 | validation: 0.01617745410333901]
	TIME [epoch: 2.67 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013799161655262855		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.013799161655262855 | validation: 0.012505726370849457]
	TIME [epoch: 2.67 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012946677724011289		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.012946677724011289 | validation: 0.019055442460605768]
	TIME [epoch: 2.67 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013767339245308204		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.013767339245308204 | validation: 0.015233194566351272]
	TIME [epoch: 2.67 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015979316740511958		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.015979316740511958 | validation: 0.022183104503474493]
	TIME [epoch: 2.67 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018726887905987224		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.018726887905987224 | validation: 0.018975515599667495]
	TIME [epoch: 2.67 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020753209048582112		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.020753209048582112 | validation: 0.017274871563350835]
	TIME [epoch: 2.67 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0199664853877773		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.0199664853877773 | validation: 0.01501395980748358]
	TIME [epoch: 2.68 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019410048428691165		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.019410048428691165 | validation: 0.022089370822254872]
	TIME [epoch: 2.69 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021394317375990003		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.021394317375990003 | validation: 0.02243154745015016]
	TIME [epoch: 2.67 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020807218820601145		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.020807218820601145 | validation: 0.015262499869052915]
	TIME [epoch: 2.67 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01807175237787171		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.01807175237787171 | validation: 0.01863899147360344]
	TIME [epoch: 2.67 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0189554241979024		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.0189554241979024 | validation: 0.019366898050580752]
	TIME [epoch: 2.67 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02318208615390688		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.02318208615390688 | validation: 0.017794219745075203]
	TIME [epoch: 2.67 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023252661723078844		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.023252661723078844 | validation: 0.031896156999174606]
	TIME [epoch: 2.67 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023441569020493477		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.023441569020493477 | validation: 0.016116609317525]
	TIME [epoch: 2.67 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016865383887691127		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.016865383887691127 | validation: 0.012199213637413941]
	TIME [epoch: 2.67 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013700403167196012		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.013700403167196012 | validation: 0.014645989849346664]
	TIME [epoch: 2.67 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01316413078861721		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.01316413078861721 | validation: 0.010958047828211649]
	TIME [epoch: 2.67 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013764097446262778		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.013764097446262778 | validation: 0.014067113361703555]
	TIME [epoch: 2.68 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014043382536472575		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.014043382536472575 | validation: 0.015507002043701168]
	TIME [epoch: 2.67 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011848225047871476		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.011848225047871476 | validation: 0.016320464443674087]
	TIME [epoch: 2.67 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012611735736831218		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.012611735736831218 | validation: 0.014142440163015147]
	TIME [epoch: 2.67 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014910195386541045		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.014910195386541045 | validation: 0.017911409846878935]
	TIME [epoch: 2.67 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01780444862329743		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.01780444862329743 | validation: 0.02272837147485096]
	TIME [epoch: 2.67 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026217522611342057		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.026217522611342057 | validation: 0.03270597594566709]
	TIME [epoch: 2.67 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0340927729987017		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.0340927729987017 | validation: 0.0191027828341505]
	TIME [epoch: 2.67 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021321743027156823		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.021321743027156823 | validation: 0.017086566168226548]
	TIME [epoch: 2.69 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015282579622860329		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.015282579622860329 | validation: 0.015820031136765757]
	TIME [epoch: 2.68 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01822201890550794		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.01822201890550794 | validation: 0.01774426481179229]
	TIME [epoch: 2.68 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021064571172485353		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.021064571172485353 | validation: 0.018809504440854087]
	TIME [epoch: 2.69 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016874456681966642		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.016874456681966642 | validation: 0.012666681516247414]
	TIME [epoch: 2.68 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01205852292051831		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.01205852292051831 | validation: 0.01057715276380471]
	TIME [epoch: 2.68 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012426581987826544		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.012426581987826544 | validation: 0.01175995082076139]
	TIME [epoch: 2.68 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014859640789932339		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.014859640789932339 | validation: 0.013650545043150886]
	TIME [epoch: 2.69 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015058213441559434		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.015058213441559434 | validation: 0.015205780669090951]
	TIME [epoch: 2.68 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014341425933480804		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.014341425933480804 | validation: 0.013487448406165559]
	TIME [epoch: 2.68 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0130208236591325		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.0130208236591325 | validation: 0.016157584698360206]
	TIME [epoch: 2.69 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015373360506971299		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.015373360506971299 | validation: 0.012287830650525368]
	TIME [epoch: 2.69 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01449573984136693		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.01449573984136693 | validation: 0.019651018885549446]
	TIME [epoch: 2.68 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01697069429077871		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.01697069429077871 | validation: 0.014054187879376534]
	TIME [epoch: 2.68 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017025807603558377		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.017025807603558377 | validation: 0.01706132339442821]
	TIME [epoch: 2.69 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019074311005568154		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.019074311005568154 | validation: 0.01746827905581925]
	TIME [epoch: 2.68 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017678644916448768		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.017678644916448768 | validation: 0.018646643555852615]
	TIME [epoch: 2.68 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021265513591614806		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.021265513591614806 | validation: 0.01889047736433354]
	TIME [epoch: 2.69 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026879165123565578		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.026879165123565578 | validation: 0.02385097642906684]
	TIME [epoch: 2.69 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022783442602246327		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.022783442602246327 | validation: 0.016065272133599408]
	TIME [epoch: 2.69 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016621644211979213		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.016621644211979213 | validation: 0.014860329647620164]
	TIME [epoch: 2.69 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012638374494025318		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.012638374494025318 | validation: 0.010580576730161828]
	TIME [epoch: 2.68 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012083080616419122		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.012083080616419122 | validation: 0.011962869506284658]
	TIME [epoch: 2.69 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012011323731901813		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.012011323731901813 | validation: 0.01374312898145158]
	TIME [epoch: 2.68 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012842411207953566		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.012842411207953566 | validation: 0.014461839765618156]
	TIME [epoch: 2.69 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012882197443325524		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.012882197443325524 | validation: 0.014094682273214067]
	TIME [epoch: 2.69 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012504275765914525		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.012504275765914525 | validation: 0.015230221856003014]
	TIME [epoch: 2.69 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013458630517364588		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.013458630517364588 | validation: 0.013955390869882501]
	TIME [epoch: 2.68 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012637529770325185		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.012637529770325185 | validation: 0.012278525892446525]
	TIME [epoch: 2.69 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01392807551078442		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.01392807551078442 | validation: 0.01893132123656546]
	TIME [epoch: 2.69 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017669727403012862		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.017669727403012862 | validation: 0.015881025969421248]
	TIME [epoch: 2.69 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021974912588976145		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.021974912588976145 | validation: 0.027686957988255403]
	TIME [epoch: 2.68 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017802560372083238		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.017802560372083238 | validation: 0.01576154016644823]
	TIME [epoch: 2.69 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016914846984341772		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.016914846984341772 | validation: 0.016325648286806937]
	TIME [epoch: 2.68 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018658683049380582		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.018658683049380582 | validation: 0.014613948406036182]
	TIME [epoch: 2.69 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019628798508480844		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.019628798508480844 | validation: 0.00894216897943302]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_836.pth
	Model improved!!!
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014340484020365958		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.014340484020365958 | validation: 0.010417801310730324]
	TIME [epoch: 2.69 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011791863805536491		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.011791863805536491 | validation: 0.01668489745402445]
	TIME [epoch: 2.69 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013404624250672289		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.013404624250672289 | validation: 0.010734618075915149]
	TIME [epoch: 2.68 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015602116450126307		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.015602116450126307 | validation: 0.015437529307480592]
	TIME [epoch: 2.69 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016246487469938		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.016246487469938 | validation: 0.013090231850998314]
	TIME [epoch: 2.69 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015846818548240743		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.015846818548240743 | validation: 0.014485094631639484]
	TIME [epoch: 2.68 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015581739520793723		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.015581739520793723 | validation: 0.021571437890631706]
	TIME [epoch: 2.68 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015851291159679315		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.015851291159679315 | validation: 0.012490559145967018]
	TIME [epoch: 2.68 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013066436922020593		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.013066436922020593 | validation: 0.02092189684094723]
	TIME [epoch: 2.69 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016131344725818567		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.016131344725818567 | validation: 0.014801477596367863]
	TIME [epoch: 2.69 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019176411734877396		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.019176411734877396 | validation: 0.01900630648388707]
	TIME [epoch: 2.68 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01750571707451841		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.01750571707451841 | validation: 0.013453565770544185]
	TIME [epoch: 2.69 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013368556199023315		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.013368556199023315 | validation: 0.012821433410855643]
	TIME [epoch: 2.68 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011319143961766897		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.011319143961766897 | validation: 0.012244301562546689]
	TIME [epoch: 2.68 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011666806899076025		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.011666806899076025 | validation: 0.011405805046513152]
	TIME [epoch: 2.68 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012620053183854854		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.012620053183854854 | validation: 0.013145620893549826]
	TIME [epoch: 2.68 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012955837560736955		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.012955837560736955 | validation: 0.016815716529280555]
	TIME [epoch: 2.68 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01354469579574589		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.01354469579574589 | validation: 0.010456842254987643]
	TIME [epoch: 2.68 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013288088553372528		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.013288088553372528 | validation: 0.010753002130091383]
	TIME [epoch: 2.68 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013372668023608774		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.013372668023608774 | validation: 0.021687887967616594]
	TIME [epoch: 2.68 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019287627135598905		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.019287627135598905 | validation: 0.014786793232013463]
	TIME [epoch: 2.68 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018375786357903268		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.018375786357903268 | validation: 0.015769120354348343]
	TIME [epoch: 2.68 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014064744342241007		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.014064744342241007 | validation: 0.01157232341477571]
	TIME [epoch: 2.69 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013270866218024709		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.013270866218024709 | validation: 0.021526027091355385]
	TIME [epoch: 2.68 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015394128556462255		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.015394128556462255 | validation: 0.016998810238067565]
	TIME [epoch: 2.68 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018819881130963743		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.018819881130963743 | validation: 0.0177567523950637]
	TIME [epoch: 2.68 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020777376258625677		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.020777376258625677 | validation: 0.010158835639848174]
	TIME [epoch: 2.68 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015534275985873789		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.015534275985873789 | validation: 0.012221758821374697]
	TIME [epoch: 2.68 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011154274269420113		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.011154274269420113 | validation: 0.01498965780319369]
	TIME [epoch: 2.68 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012050402968626393		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.012050402968626393 | validation: 0.011328613514784404]
	TIME [epoch: 2.68 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012286110360480045		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.012286110360480045 | validation: 0.011616696545422646]
	TIME [epoch: 2.68 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010581575365259313		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.010581575365259313 | validation: 0.010517523446161149]
	TIME [epoch: 2.68 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01201636667431588		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.01201636667431588 | validation: 0.019299639491842647]
	TIME [epoch: 2.68 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013978490738861229		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.013978490738861229 | validation: 0.013022689498164498]
	TIME [epoch: 2.69 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017447621078853198		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.017447621078853198 | validation: 0.02583891265110243]
	TIME [epoch: 2.68 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02002413476562562		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.02002413476562562 | validation: 0.012009771554343074]
	TIME [epoch: 2.68 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015695382778718004		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.015695382778718004 | validation: 0.013563066384331624]
	TIME [epoch: 2.68 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011089471157747026		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.011089471157747026 | validation: 0.01234944215961048]
	TIME [epoch: 2.68 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011022934197489854		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.011022934197489854 | validation: 0.011488598776983439]
	TIME [epoch: 2.68 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014606780507147255		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.014606780507147255 | validation: 0.014709011331772416]
	TIME [epoch: 2.68 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016646920292903124		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.016646920292903124 | validation: 0.014763521442496808]
	TIME [epoch: 2.68 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014674904500849255		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.014674904500849255 | validation: 0.010179866844578889]
	TIME [epoch: 2.68 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013898034303361761		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.013898034303361761 | validation: 0.017077218619988222]
	TIME [epoch: 2.68 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015092193751284039		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.015092193751284039 | validation: 0.014175304672992174]
	TIME [epoch: 2.68 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013098415684368163		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.013098415684368163 | validation: 0.01273394963286665]
	TIME [epoch: 2.69 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011975044651352414		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.011975044651352414 | validation: 0.008464395104456469]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_882.pth
	Model improved!!!
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011371292545310335		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.011371292545310335 | validation: 0.008499484583062922]
	TIME [epoch: 2.68 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011381645658414588		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.011381645658414588 | validation: 0.011875969495422257]
	TIME [epoch: 2.68 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011611392620529655		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.011611392620529655 | validation: 0.011081237449829674]
	TIME [epoch: 2.68 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010961642112452017		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.010961642112452017 | validation: 0.009393562781372844]
	TIME [epoch: 2.68 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01253430891623544		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.01253430891623544 | validation: 0.013568238560339797]
	TIME [epoch: 2.68 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014248240194579416		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.014248240194579416 | validation: 0.014739053782886858]
	TIME [epoch: 2.68 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01756631465746648		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.01756631465746648 | validation: 0.01860017146676173]
	TIME [epoch: 2.68 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019531666675023564		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.019531666675023564 | validation: 0.01422043729231971]
	TIME [epoch: 2.68 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01566153721750026		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.01566153721750026 | validation: 0.015472598185934118]
	TIME [epoch: 2.68 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014503957446428807		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.014503957446428807 | validation: 0.012880842960512274]
	TIME [epoch: 2.69 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013254822298186326		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.013254822298186326 | validation: 0.010509972798408885]
	TIME [epoch: 2.69 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014888439970161938		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.014888439970161938 | validation: 0.01306718653873016]
	TIME [epoch: 2.68 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01641017937729489		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.01641017937729489 | validation: 0.015799964488962936]
	TIME [epoch: 2.68 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014418967137806252		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.014418967137806252 | validation: 0.00869507674757769]
	TIME [epoch: 2.68 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011875745657149217		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.011875745657149217 | validation: 0.01398205158075081]
	TIME [epoch: 2.69 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012068497860162482		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.012068497860162482 | validation: 0.013984178809367765]
	TIME [epoch: 2.68 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01336365514632652		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.01336365514632652 | validation: 0.01537330408094334]
	TIME [epoch: 2.68 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01271022731896828		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.01271022731896828 | validation: 0.01290575320321854]
	TIME [epoch: 2.68 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012159515238436858		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.012159515238436858 | validation: 0.013989118530852185]
	TIME [epoch: 2.68 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011354207241227455		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.011354207241227455 | validation: 0.015330371896182594]
	TIME [epoch: 2.68 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014062663784956795		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.014062663784956795 | validation: 0.009936544003615223]
	TIME [epoch: 2.69 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012103788125232037		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.012103788125232037 | validation: 0.007995730943443037]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_904.pth
	Model improved!!!
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011754130703197774		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.011754130703197774 | validation: 0.012624887376935345]
	TIME [epoch: 2.68 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012315948160874503		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.012315948160874503 | validation: 0.016155693508847803]
	TIME [epoch: 2.68 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01785587515398736		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.01785587515398736 | validation: 0.02085320301488123]
	TIME [epoch: 2.68 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019229436164323013		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.019229436164323013 | validation: 0.010932785978044635]
	TIME [epoch: 2.68 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013335389172101158		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.013335389172101158 | validation: 0.009800185838450826]
	TIME [epoch: 2.68 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01198039527720502		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.01198039527720502 | validation: 0.012928859061257859]
	TIME [epoch: 2.69 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012584489278579922		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.012584489278579922 | validation: 0.011704860288651142]
	TIME [epoch: 2.69 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012516287908794109		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.012516287908794109 | validation: 0.012062053315850996]
	TIME [epoch: 2.69 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010975710949402475		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.010975710949402475 | validation: 0.011181543641574133]
	TIME [epoch: 2.69 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011536273752775977		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.011536273752775977 | validation: 0.010405849180666948]
	TIME [epoch: 2.7 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012079459026126843		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.012079459026126843 | validation: 0.011768508328102912]
	TIME [epoch: 2.69 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011962261361687565		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.011962261361687565 | validation: 0.009690340734268622]
	TIME [epoch: 2.69 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011629559016828557		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.011629559016828557 | validation: 0.01644906630169828]
	TIME [epoch: 2.68 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013230470662612888		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.013230470662612888 | validation: 0.010532367986623305]
	TIME [epoch: 2.69 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01341023472984342		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.01341023472984342 | validation: 0.013221872037240623]
	TIME [epoch: 2.68 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01361042941280603		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.01361042941280603 | validation: 0.010448176576473824]
	TIME [epoch: 2.68 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014896719457576296		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.014896719457576296 | validation: 0.013456169940411234]
	TIME [epoch: 2.68 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013851903151798716		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.013851903151798716 | validation: 0.010051626459804942]
	TIME [epoch: 2.68 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01290242508262981		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.01290242508262981 | validation: 0.012365753939504287]
	TIME [epoch: 2.78 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012920071613479429		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.012920071613479429 | validation: 0.012721273923595523]
	TIME [epoch: 2.68 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014657375491879652		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.014657375491879652 | validation: 0.015563002064295062]
	TIME [epoch: 2.69 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011508855995972551		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.011508855995972551 | validation: 0.009380044426718737]
	TIME [epoch: 2.68 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01131005819802867		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.01131005819802867 | validation: 0.012568051534986358]
	TIME [epoch: 2.68 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011675440347214149		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.011675440347214149 | validation: 0.008522781272055047]
	TIME [epoch: 2.68 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012586866937144486		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.012586866937144486 | validation: 0.011464648951190716]
	TIME [epoch: 2.68 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013177189176164061		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.013177189176164061 | validation: 0.010694013210861398]
	TIME [epoch: 2.68 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012643533104801966		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.012643533104801966 | validation: 0.019246382043079692]
	TIME [epoch: 2.68 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013721421196427615		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.013721421196427615 | validation: 0.009326808916125319]
	TIME [epoch: 2.68 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013524829426169402		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.013524829426169402 | validation: 0.014492452056344985]
	TIME [epoch: 2.69 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011416533238491917		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.011416533238491917 | validation: 0.010803159207930714]
	TIME [epoch: 2.69 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011486945632672003		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.011486945632672003 | validation: 0.007863564331303908]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_935.pth
	Model improved!!!
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011793202520936963		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.011793202520936963 | validation: 0.012019539209765229]
	TIME [epoch: 2.68 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012253169119023114		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.012253169119023114 | validation: 0.008844167974754092]
	TIME [epoch: 2.68 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013537999872936569		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.013537999872936569 | validation: 0.012932227322431922]
	TIME [epoch: 2.68 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013997080819736994		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.013997080819736994 | validation: 0.011476300583870025]
	TIME [epoch: 2.68 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011995375316537526		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.011995375316537526 | validation: 0.013858693296668823]
	TIME [epoch: 2.68 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009583255878076602		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.009583255878076602 | validation: 0.010890828181269907]
	TIME [epoch: 2.68 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011735544115539962		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.011735544115539962 | validation: 0.012261830644212036]
	TIME [epoch: 2.69 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0124921018659591		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.0124921018659591 | validation: 0.011960511633126126]
	TIME [epoch: 2.68 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01137225801059279		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.01137225801059279 | validation: 0.010195550525138453]
	TIME [epoch: 2.69 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01236747402323015		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.01236747402323015 | validation: 0.016970483236769952]
	TIME [epoch: 2.68 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01196344227759329		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.01196344227759329 | validation: 0.012919571984894485]
	TIME [epoch: 2.69 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014245860287630157		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.014245860287630157 | validation: 0.016306902480324913]
	TIME [epoch: 2.68 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016455318258798403		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.016455318258798403 | validation: 0.013935949699018346]
	TIME [epoch: 2.68 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011496652325785717		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.011496652325785717 | validation: 0.011921961184060727]
	TIME [epoch: 2.68 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010865173916745512		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.010865173916745512 | validation: 0.011476709872599567]
	TIME [epoch: 2.68 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011482411368097686		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.011482411368097686 | validation: 0.01415497356794019]
	TIME [epoch: 2.68 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013315791578589895		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.013315791578589895 | validation: 0.015808510436701607]
	TIME [epoch: 2.68 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013087942189609994		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.013087942189609994 | validation: 0.007491915443926378]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_953.pth
	Model improved!!!
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011478091926759996		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.011478091926759996 | validation: 0.009826543943229404]
	TIME [epoch: 2.68 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012139067150237421		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.012139067150237421 | validation: 0.0102995882997605]
	TIME [epoch: 2.68 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011006315524193915		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.011006315524193915 | validation: 0.008367150005084811]
	TIME [epoch: 2.68 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010989280107681658		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.010989280107681658 | validation: 0.013107675240108597]
	TIME [epoch: 2.69 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01093320020553953		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.01093320020553953 | validation: 0.012047130005705266]
	TIME [epoch: 2.68 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011359933814142931		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.011359933814142931 | validation: 0.009488181379240757]
	TIME [epoch: 2.68 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011680737977451883		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.011680737977451883 | validation: 0.011628295505159703]
	TIME [epoch: 2.68 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011783455326484834		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.011783455326484834 | validation: 0.010686261671554965]
	TIME [epoch: 2.68 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013265431664302163		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.013265431664302163 | validation: 0.008922340993517108]
	TIME [epoch: 2.68 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011755620069449782		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.011755620069449782 | validation: 0.020055185452323113]
	TIME [epoch: 2.68 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017252067748825643		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.017252067748825643 | validation: 0.0095127674566048]
	TIME [epoch: 2.68 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011059226414781728		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.011059226414781728 | validation: 0.009607970516531394]
	TIME [epoch: 2.68 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010818243381181593		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.010818243381181593 | validation: 0.014083355773080631]
	TIME [epoch: 2.68 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010769006066908516		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.010769006066908516 | validation: 0.008866048128289472]
	TIME [epoch: 2.68 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010906628448605411		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.010906628448605411 | validation: 0.007950061618932492]
	TIME [epoch: 2.69 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012113667212960349		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.012113667212960349 | validation: 0.008740546813869765]
	TIME [epoch: 2.68 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011923743535513193		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.011923743535513193 | validation: 0.009814918201901057]
	TIME [epoch: 2.68 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011723405368894749		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.011723405368894749 | validation: 0.008723612649934908]
	TIME [epoch: 2.68 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010372719188045934		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.010372719188045934 | validation: 0.009183991634031108]
	TIME [epoch: 2.68 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010644545262804629		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.010644545262804629 | validation: 0.008883630307817992]
	TIME [epoch: 2.68 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011174758104742949		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.011174758104742949 | validation: 0.011020260788407277]
	TIME [epoch: 2.68 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012811704466808719		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.012811704466808719 | validation: 0.014850038170756065]
	TIME [epoch: 2.68 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016665899586673312		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.016665899586673312 | validation: 0.011662197333371828]
	TIME [epoch: 2.68 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013341609567551798		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.013341609567551798 | validation: 0.01459859015224514]
	TIME [epoch: 2.68 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01011128891691413		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.01011128891691413 | validation: 0.010156495158637603]
	TIME [epoch: 2.68 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012334263607501797		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.012334263607501797 | validation: 0.010932399399554683]
	TIME [epoch: 2.69 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010979968521684263		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.010979968521684263 | validation: 0.008810496773151677]
	TIME [epoch: 2.68 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009796471237797621		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.009796471237797621 | validation: 0.0084336046857025]
	TIME [epoch: 2.68 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01134013283387249		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.01134013283387249 | validation: 0.013656745326174037]
	TIME [epoch: 2.68 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010874261285508639		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.010874261285508639 | validation: 0.015411744264801975]
	TIME [epoch: 2.68 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012290195215436991		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.012290195215436991 | validation: 0.009273959535550524]
	TIME [epoch: 2.68 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011468138057878878		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.011468138057878878 | validation: 0.00992905856869334]
	TIME [epoch: 2.68 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011070248610307774		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.011070248610307774 | validation: 0.009851487141221638]
	TIME [epoch: 2.68 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011009718025324067		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.011009718025324067 | validation: 0.008610138342909412]
	TIME [epoch: 2.68 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010735105151929117		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.010735105151929117 | validation: 0.007876546530812023]
	TIME [epoch: 2.68 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009964476659865374		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.009964476659865374 | validation: 0.011754235023409922]
	TIME [epoch: 2.68 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01069817761021717		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.01069817761021717 | validation: 0.012176136189027865]
	TIME [epoch: 2.69 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011937000016464607		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.011937000016464607 | validation: 0.010849625713147027]
	TIME [epoch: 2.68 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011869598540036512		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.011869598540036512 | validation: 0.00797424269752346]
	TIME [epoch: 2.68 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011719145935157617		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.011719145935157617 | validation: 0.009756615797054325]
	TIME [epoch: 2.68 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012863295193919677		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.012863295193919677 | validation: 0.008235984442462463]
	TIME [epoch: 2.68 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010881614618397943		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.010881614618397943 | validation: 0.010897798592471053]
	TIME [epoch: 2.68 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010286576815367675		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.010286576815367675 | validation: 0.008106661315714537]
	TIME [epoch: 2.68 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010754344405765526		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.010754344405765526 | validation: 0.01054118807293718]
	TIME [epoch: 2.68 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010760089402212306		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.010760089402212306 | validation: 0.007204512466272395]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_998.pth
	Model improved!!!
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010419441228514159		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.010419441228514159 | validation: 0.008670747841082538]
	TIME [epoch: 2.68 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01078145130314387		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.01078145130314387 | validation: 0.012546624948341013]
	TIME [epoch: 2.68 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012224604790478101		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.012224604790478101 | validation: 0.009325279641599727]
	TIME [epoch: 186 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010857878682027566		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.010857878682027566 | validation: 0.008851424374979334]
	TIME [epoch: 5.72 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011909638799828686		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.011909638799828686 | validation: 0.011351476464040068]
	TIME [epoch: 5.72 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008510222401350688		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.008510222401350688 | validation: 0.012309799300643444]
	TIME [epoch: 5.71 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009994806332417418		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.009994806332417418 | validation: 0.012284432217073772]
	TIME [epoch: 5.72 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011518774287142226		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.011518774287142226 | validation: 0.0076218504820803395]
	TIME [epoch: 5.72 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011069348216714698		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.011069348216714698 | validation: 0.011243536791264209]
	TIME [epoch: 5.72 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011924900807472083		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.011924900807472083 | validation: 0.009017301161699255]
	TIME [epoch: 5.71 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012696647457938887		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.012696647457938887 | validation: 0.009154843158199878]
	TIME [epoch: 5.71 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012104188092653604		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.012104188092653604 | validation: 0.012463626482714762]
	TIME [epoch: 5.71 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011718229197388748		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.011718229197388748 | validation: 0.009140683349652378]
	TIME [epoch: 5.72 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011785495324183057		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.011785495324183057 | validation: 0.013138878349780558]
	TIME [epoch: 5.71 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009612500215544438		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.009612500215544438 | validation: 0.011508037731418043]
	TIME [epoch: 5.71 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010585066278420808		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.010585066278420808 | validation: 0.011174086651225535]
	TIME [epoch: 5.71 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011202787988361896		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.011202787988361896 | validation: 0.010614048565550727]
	TIME [epoch: 5.71 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010748547152906655		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.010748547152906655 | validation: 0.013040062374562011]
	TIME [epoch: 5.71 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013218928012308458		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.013218928012308458 | validation: 0.010797080457762332]
	TIME [epoch: 5.71 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01187724892744721		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.01187724892744721 | validation: 0.012242148780411488]
	TIME [epoch: 5.71 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010802386149281312		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.010802386149281312 | validation: 0.012932534597379919]
	TIME [epoch: 5.71 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009786705413433764		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.009786705413433764 | validation: 0.01010421510675329]
	TIME [epoch: 5.71 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010310364544130306		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.010310364544130306 | validation: 0.010050950582516994]
	TIME [epoch: 5.71 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010207111291445106		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.010207111291445106 | validation: 0.013617114294758948]
	TIME [epoch: 5.71 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011766705899109527		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.011766705899109527 | validation: 0.012893636873413918]
	TIME [epoch: 5.71 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011033499668351059		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.011033499668351059 | validation: 0.009386290998773217]
	TIME [epoch: 5.71 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009736946605121751		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.009736946605121751 | validation: 0.011067447068979398]
	TIME [epoch: 5.71 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009713706491713828		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.009713706491713828 | validation: 0.0085666194062061]
	TIME [epoch: 5.71 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009978114158447616		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.009978114158447616 | validation: 0.00958398081174079]
	TIME [epoch: 5.72 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011123875824430012		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.011123875824430012 | validation: 0.008219871933007706]
	TIME [epoch: 5.71 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01098657746378096		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.01098657746378096 | validation: 0.011925571695190984]
	TIME [epoch: 5.71 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012394540092509537		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.012394540092509537 | validation: 0.010460004007982571]
	TIME [epoch: 5.71 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011791163766004443		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.011791163766004443 | validation: 0.01289052939619536]
	TIME [epoch: 5.71 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010590233341776925		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.010590233341776925 | validation: 0.00838347957753839]
	TIME [epoch: 5.72 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010588430308019921		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.010588430308019921 | validation: 0.009996651041648808]
	TIME [epoch: 5.71 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010685402049700306		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.010685402049700306 | validation: 0.006624677845980232]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_1034.pth
	Model improved!!!
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011592050483693047		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.011592050483693047 | validation: 0.016001318988049996]
	TIME [epoch: 5.71 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011712406884563155		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.011712406884563155 | validation: 0.01009169264775588]
	TIME [epoch: 5.71 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010321856066269071		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.010321856066269071 | validation: 0.009596423392984377]
	TIME [epoch: 5.71 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01118828304577137		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.01118828304577137 | validation: 0.010677361904959437]
	TIME [epoch: 5.71 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010060222123739889		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.010060222123739889 | validation: 0.008291905409391809]
	TIME [epoch: 5.71 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010225846842683213		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.010225846842683213 | validation: 0.008568282504272384]
	TIME [epoch: 5.71 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011801974664715761		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.011801974664715761 | validation: 0.009175346513001682]
	TIME [epoch: 5.71 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009262702773590781		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.009262702773590781 | validation: 0.009801890523172298]
	TIME [epoch: 5.72 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010076783187167298		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.010076783187167298 | validation: 0.010621396411482676]
	TIME [epoch: 5.71 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011375958971153595		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.011375958971153595 | validation: 0.010974970829396725]
	TIME [epoch: 5.71 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011034083872008979		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.011034083872008979 | validation: 0.011669275657888313]
	TIME [epoch: 5.71 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010465593992937464		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.010465593992937464 | validation: 0.006942048124844458]
	TIME [epoch: 5.71 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009176693930331365		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.009176693930331365 | validation: 0.010989417181613437]
	TIME [epoch: 5.72 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009640042970025388		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.009640042970025388 | validation: 0.012466447326158092]
	TIME [epoch: 5.71 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01183850640743086		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.01183850640743086 | validation: 0.007436367794265464]
	TIME [epoch: 5.71 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011881236968433961		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.011881236968433961 | validation: 0.010274461549558501]
	TIME [epoch: 5.71 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009942538488766356		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.009942538488766356 | validation: 0.012750311413362237]
	TIME [epoch: 5.71 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010840129075445477		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.010840129075445477 | validation: 0.008276568836027431]
	TIME [epoch: 5.72 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010282183889880867		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.010282183889880867 | validation: 0.007405971296059688]
	TIME [epoch: 5.73 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009436733759556386		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.009436733759556386 | validation: 0.008564457716227658]
	TIME [epoch: 5.72 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0089109931612702		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.0089109931612702 | validation: 0.011457100626233252]
	TIME [epoch: 5.71 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00906398993046562		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.00906398993046562 | validation: 0.009983779422124528]
	TIME [epoch: 5.71 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010426669811145053		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.010426669811145053 | validation: 0.0073051299135773245]
	TIME [epoch: 5.71 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009916701803120731		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.009916701803120731 | validation: 0.009518046863964813]
	TIME [epoch: 5.73 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01076027462949046		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.01076027462949046 | validation: 0.00832044554661039]
	TIME [epoch: 5.71 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011020891584289014		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.011020891584289014 | validation: 0.008154913085880256]
	TIME [epoch: 5.72 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012249622430776102		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.012249622430776102 | validation: 0.009274210948817353]
	TIME [epoch: 5.72 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01065336082078344		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.01065336082078344 | validation: 0.006875426314615374]
	TIME [epoch: 5.72 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010155677208295188		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.010155677208295188 | validation: 0.014647140629438693]
	TIME [epoch: 5.72 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010208558016525487		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.010208558016525487 | validation: 0.009508852849061444]
	TIME [epoch: 5.72 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011239044907363263		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.011239044907363263 | validation: 0.011375972723322792]
	TIME [epoch: 5.72 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012256346932975228		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.012256346932975228 | validation: 0.008430438672220463]
	TIME [epoch: 5.72 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010959838632601226		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.010959838632601226 | validation: 0.009765426439641811]
	TIME [epoch: 5.72 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009366732588390924		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.009366732588390924 | validation: 0.0069533873562905085]
	TIME [epoch: 5.73 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009270422077109106		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.009270422077109106 | validation: 0.009815840840434431]
	TIME [epoch: 5.72 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010221721672853986		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.010221721672853986 | validation: 0.0073216603042031815]
	TIME [epoch: 5.72 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009829192943141786		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.009829192943141786 | validation: 0.012009375652543154]
	TIME [epoch: 5.72 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01078039374063009		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.01078039374063009 | validation: 0.010639323355748243]
	TIME [epoch: 5.72 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01066797064924678		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.01066797064924678 | validation: 0.010007835327829463]
	TIME [epoch: 5.72 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010216294938741082		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.010216294938741082 | validation: 0.011417482274728287]
	TIME [epoch: 5.73 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010146412955663967		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.010146412955663967 | validation: 0.008594969956339205]
	TIME [epoch: 5.72 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009661708049161943		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.009661708049161943 | validation: 0.010481693756200495]
	TIME [epoch: 5.71 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010462465047941608		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.010462465047941608 | validation: 0.008725365800739215]
	TIME [epoch: 5.74 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010736527610729253		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.010736527610729253 | validation: 0.01001070974018984]
	TIME [epoch: 5.72 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011490924884250485		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.011490924884250485 | validation: 0.011811560572488]
	TIME [epoch: 5.73 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009743681709995152		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.009743681709995152 | validation: 0.01305545067948103]
	TIME [epoch: 5.72 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010279226280607878		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.010279226280607878 | validation: 0.0075126150463577625]
	TIME [epoch: 5.72 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009280515934946935		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.009280515934946935 | validation: 0.010597789431287108]
	TIME [epoch: 5.72 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010455754992348316		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.010455754992348316 | validation: 0.00997447669071122]
	TIME [epoch: 5.71 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00893086375243439		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.00893086375243439 | validation: 0.006175951467065833]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_1084.pth
	Model improved!!!
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00982203423417209		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.00982203423417209 | validation: 0.010999212670600424]
	TIME [epoch: 5.72 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010289440773663845		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.010289440773663845 | validation: 0.00693082517846837]
	TIME [epoch: 5.72 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009447381942449295		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.009447381942449295 | validation: 0.012108927021093896]
	TIME [epoch: 5.71 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01157097577357583		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.01157097577357583 | validation: 0.008991728296367107]
	TIME [epoch: 5.72 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010482936586156604		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.010482936586156604 | validation: 0.008583484117541718]
	TIME [epoch: 5.72 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011605054497801848		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.011605054497801848 | validation: 0.009869537187248301]
	TIME [epoch: 5.72 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009516505344262108		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.009516505344262108 | validation: 0.007897060929429267]
	TIME [epoch: 5.71 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008723002826939209		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.008723002826939209 | validation: 0.008786962190817605]
	TIME [epoch: 5.71 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009321175121420756		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.009321175121420756 | validation: 0.012171816213597942]
	TIME [epoch: 5.73 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009661479666704192		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.009661479666704192 | validation: 0.009310510752654167]
	TIME [epoch: 5.72 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010496026484930105		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.010496026484930105 | validation: 0.008399047701686913]
	TIME [epoch: 5.71 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01092570971569745		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.01092570971569745 | validation: 0.008339403050263816]
	TIME [epoch: 5.72 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010958856450365873		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.010958856450365873 | validation: 0.010328082531725858]
	TIME [epoch: 5.71 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009188112747486987		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.009188112747486987 | validation: 0.007194257219529154]
	TIME [epoch: 5.71 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009793270110706047		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.009793270110706047 | validation: 0.008886256528569614]
	TIME [epoch: 5.72 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01131732692600152		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.01131732692600152 | validation: 0.009372677195001611]
	TIME [epoch: 5.71 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011006168133425418		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.011006168133425418 | validation: 0.009223124287043572]
	TIME [epoch: 5.71 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009383730807304345		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.009383730807304345 | validation: 0.01055602441886523]
	TIME [epoch: 5.72 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010037868169523376		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.010037868169523376 | validation: 0.0076413535323629714]
	TIME [epoch: 5.71 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010456745376489033		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.010456745376489033 | validation: 0.008439814763260334]
	TIME [epoch: 5.73 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010906897844667522		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.010906897844667522 | validation: 0.007484663194128727]
	TIME [epoch: 5.72 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009543215447104889		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.009543215447104889 | validation: 0.008379576844690496]
	TIME [epoch: 5.72 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008957128470864493		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.008957128470864493 | validation: 0.00831446089396839]
	TIME [epoch: 5.71 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00968538201085541		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.00968538201085541 | validation: 0.007957417801458333]
	TIME [epoch: 5.72 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009109093310859994		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.009109093310859994 | validation: 0.012709857612489883]
	TIME [epoch: 5.71 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01068513626580758		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.01068513626580758 | validation: 0.00970736087229477]
	TIME [epoch: 5.73 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012006843718443157		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.012006843718443157 | validation: 0.008768600688359374]
	TIME [epoch: 5.71 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008579894410919112		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.008579894410919112 | validation: 0.010713316798232432]
	TIME [epoch: 5.72 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010402713230626803		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.010402713230626803 | validation: 0.008758408057824818]
	TIME [epoch: 5.71 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009970990855437144		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.009970990855437144 | validation: 0.010782306528351694]
	TIME [epoch: 5.72 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010075764926754221		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.010075764926754221 | validation: 0.007164298016265136]
	TIME [epoch: 5.72 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008820427217823546		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.008820427217823546 | validation: 0.006371026876284836]
	TIME [epoch: 5.72 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008771569551072425		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.008771569551072425 | validation: 0.010937167099911039]
	TIME [epoch: 5.71 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008654450872110782		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.008654450872110782 | validation: 0.012967085576176651]
	TIME [epoch: 5.71 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010524679260110468		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.010524679260110468 | validation: 0.0065054013523076605]
	TIME [epoch: 5.71 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010437604590614585		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.010437604590614585 | validation: 0.009424145584931277]
	TIME [epoch: 5.72 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009070369764516617		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.009070369764516617 | validation: 0.008527654896101867]
	TIME [epoch: 5.71 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009987583925824559		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.009987583925824559 | validation: 0.007529848830242103]
	TIME [epoch: 5.71 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010194023793073534		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.010194023793073534 | validation: 0.013664774965475046]
	TIME [epoch: 5.72 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011076695173759287		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.011076695173759287 | validation: 0.005585970496457815]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_1124.pth
	Model improved!!!
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00894299238454844		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.00894299238454844 | validation: 0.007976023975264979]
	TIME [epoch: 5.72 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008506653735858603		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.008506653735858603 | validation: 0.007855700114290088]
	TIME [epoch: 5.71 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008687824603549813		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.008687824603549813 | validation: 0.00853971870617074]
	TIME [epoch: 5.72 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009439174762782594		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.009439174762782594 | validation: 0.009978113767243713]
	TIME [epoch: 5.71 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009120817161765672		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.009120817161765672 | validation: 0.012045493722250101]
	TIME [epoch: 5.72 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009870994179629175		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.009870994179629175 | validation: 0.007240059129062304]
	TIME [epoch: 5.72 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010862533020323055		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.010862533020323055 | validation: 0.01077175353624379]
	TIME [epoch: 5.72 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009270257594345048		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.009270257594345048 | validation: 0.00695455885703713]
	TIME [epoch: 5.71 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009846426121887353		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.009846426121887353 | validation: 0.011144060464053786]
	TIME [epoch: 5.71 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009550936416284683		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.009550936416284683 | validation: 0.005862694208954944]
	TIME [epoch: 5.71 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009186935841626657		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.009186935841626657 | validation: 0.009398765251583431]
	TIME [epoch: 5.72 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00960549808814918		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.00960549808814918 | validation: 0.008401985246926725]
	TIME [epoch: 5.73 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010135559151182206		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.010135559151182206 | validation: 0.0101986959019992]
	TIME [epoch: 5.71 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010391887792655834		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.010391887792655834 | validation: 0.008384622888518701]
	TIME [epoch: 5.71 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010506270007994365		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.010506270007994365 | validation: 0.008152229840294489]
	TIME [epoch: 5.71 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008101931533691183		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.008101931533691183 | validation: 0.009619544529621927]
	TIME [epoch: 5.71 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008049673309419459		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.008049673309419459 | validation: 0.012474263183537239]
	TIME [epoch: 5.72 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009403906767307499		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.009403906767307499 | validation: 0.008823434493815941]
	TIME [epoch: 5.71 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009501861247453575		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.009501861247453575 | validation: 0.008856139023108356]
	TIME [epoch: 5.71 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008157722593902055		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.008157722593902055 | validation: 0.008040897979554885]
	TIME [epoch: 5.71 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010426532059899634		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.010426532059899634 | validation: 0.010112167436072496]
	TIME [epoch: 5.72 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009522329782029519		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.009522329782029519 | validation: 0.009349681017858958]
	TIME [epoch: 5.72 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009309570935501242		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.009309570935501242 | validation: 0.007088817235083667]
	TIME [epoch: 5.71 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008131846880697255		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.008131846880697255 | validation: 0.008738150552426839]
	TIME [epoch: 5.72 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009445920368365035		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.009445920368365035 | validation: 0.01185339227878739]
	TIME [epoch: 5.71 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011113224532306832		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.011113224532306832 | validation: 0.008372117825289438]
	TIME [epoch: 5.72 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01024618107944401		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.01024618107944401 | validation: 0.007742163440841776]
	TIME [epoch: 5.72 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009559487757583914		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.009559487757583914 | validation: 0.010431405906662139]
	TIME [epoch: 5.72 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010771805755257624		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.010771805755257624 | validation: 0.008649585741772593]
	TIME [epoch: 5.72 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009796305917364946		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.009796305917364946 | validation: 0.0077928523020804885]
	TIME [epoch: 5.71 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011039129173672876		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.011039129173672876 | validation: 0.00812199872892121]
	TIME [epoch: 5.72 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009854448173097585		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.009854448173097585 | validation: 0.008006797200799243]
	TIME [epoch: 5.72 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00979596415317874		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.00979596415317874 | validation: 0.01049766151526731]
	TIME [epoch: 5.72 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00885199221320012		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.00885199221320012 | validation: 0.0076342423907720175]
	TIME [epoch: 5.71 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01052591919126274		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.01052591919126274 | validation: 0.00842530879485688]
	TIME [epoch: 5.72 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010192471007160213		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.010192471007160213 | validation: 0.007635557835421625]
	TIME [epoch: 5.72 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00819876523105199		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.00819876523105199 | validation: 0.009797214686676759]
	TIME [epoch: 5.72 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009018473097738825		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.009018473097738825 | validation: 0.006326530799025965]
	TIME [epoch: 5.73 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009139156939608189		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.009139156939608189 | validation: 0.009377117984279693]
	TIME [epoch: 5.72 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008722990589120348		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.008722990589120348 | validation: 0.009294132398137667]
	TIME [epoch: 5.72 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010310474496589662		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.010310474496589662 | validation: 0.008820746656020107]
	TIME [epoch: 5.71 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011756988680971452		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.011756988680971452 | validation: 0.011142774094108422]
	TIME [epoch: 5.72 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009532144578897082		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.009532144578897082 | validation: 0.007899466122390874]
	TIME [epoch: 5.72 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009628282620191692		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.009628282620191692 | validation: 0.008071490482176659]
	TIME [epoch: 5.72 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010056781054242441		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.010056781054242441 | validation: 0.006868856227014931]
	TIME [epoch: 5.71 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009835254828394273		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.009835254828394273 | validation: 0.010712675104359016]
	TIME [epoch: 5.72 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009721211183510724		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.009721211183510724 | validation: 0.0074701577416845695]
	TIME [epoch: 5.71 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009945892959838032		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.009945892959838032 | validation: 0.007577001133579686]
	TIME [epoch: 5.73 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011008635326074967		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.011008635326074967 | validation: 0.01190241813868661]
	TIME [epoch: 5.72 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008433444614670545		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.008433444614670545 | validation: 0.010545922155353405]
	TIME [epoch: 5.72 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010780645100627786		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.010780645100627786 | validation: 0.008258661435510484]
	TIME [epoch: 5.72 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009089265548071072		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.009089265548071072 | validation: 0.008397980990542587]
	TIME [epoch: 5.72 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00944149981902646		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.00944149981902646 | validation: 0.012483669884540128]
	TIME [epoch: 5.73 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008511825665109664		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.008511825665109664 | validation: 0.009303934501521328]
	TIME [epoch: 5.72 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009863466302609624		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.009863466302609624 | validation: 0.007348051735923489]
	TIME [epoch: 5.72 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009564390405724637		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.009564390405724637 | validation: 0.008249013009446094]
	TIME [epoch: 5.72 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009100296874179157		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.009100296874179157 | validation: 0.009634182730014318]
	TIME [epoch: 5.72 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008401909433396328		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.008401909433396328 | validation: 0.006695204453820203]
	TIME [epoch: 5.72 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009271596418661916		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.009271596418661916 | validation: 0.009294476900645622]
	TIME [epoch: 5.73 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009945352344484768		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.009945352344484768 | validation: 0.007451449362444918]
	TIME [epoch: 5.72 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008828324289734292		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.008828324289734292 | validation: 0.008049011946706198]
	TIME [epoch: 5.71 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008662668721488536		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.008662668721488536 | validation: 0.007216381024068463]
	TIME [epoch: 5.72 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009137407499021352		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.009137407499021352 | validation: 0.006734981517754902]
	TIME [epoch: 5.71 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009419289816530771		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.009419289816530771 | validation: 0.008098690277916055]
	TIME [epoch: 5.73 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009272280672717514		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.009272280672717514 | validation: 0.008394746041983148]
	TIME [epoch: 5.71 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010317045877897715		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.010317045877897715 | validation: 0.005223264909684411]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_1190.pth
	Model improved!!!
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008629546714226905		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.008629546714226905 | validation: 0.006368326259594293]
	TIME [epoch: 5.71 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009697083357605203		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.009697083357605203 | validation: 0.006240042039974559]
	TIME [epoch: 5.72 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008051326626834485		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.008051326626834485 | validation: 0.007303268403166996]
	TIME [epoch: 5.72 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009043169726798013		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.009043169726798013 | validation: 0.006198161510447853]
	TIME [epoch: 5.71 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009016395817104098		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.009016395817104098 | validation: 0.009507865680509176]
	TIME [epoch: 5.71 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009500829563180007		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.009500829563180007 | validation: 0.009964791157638032]
	TIME [epoch: 5.71 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009676036966754585		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.009676036966754585 | validation: 0.008387237038589802]
	TIME [epoch: 5.72 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010182397397483918		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.010182397397483918 | validation: 0.006888049753989701]
	TIME [epoch: 5.73 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00977465896846019		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.00977465896846019 | validation: 0.01033048983790742]
	TIME [epoch: 5.71 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00844965383311695		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.00844965383311695 | validation: 0.007806396815021744]
	TIME [epoch: 5.71 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008232822181789931		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.008232822181789931 | validation: 0.005135633417827346]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_1201.pth
	Model improved!!!
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009514025731819207		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.009514025731819207 | validation: 0.007908522714505274]
	TIME [epoch: 5.71 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00877422269255075		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.00877422269255075 | validation: 0.007506783873911605]
	TIME [epoch: 5.73 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008803601641795456		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.008803601641795456 | validation: 0.005759277194096579]
	TIME [epoch: 5.71 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008935671587765962		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.008935671587765962 | validation: 0.008788766559524442]
	TIME [epoch: 5.72 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008280556687866288		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.008280556687866288 | validation: 0.005497939960092135]
	TIME [epoch: 5.71 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008715745123694616		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.008715745123694616 | validation: 0.011484861536225145]
	TIME [epoch: 5.71 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010909621383401817		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.010909621383401817 | validation: 0.009244333265680128]
	TIME [epoch: 5.72 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008279670327540902		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.008279670327540902 | validation: 0.0055218579100385305]
	TIME [epoch: 5.71 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009380943704325398		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.009380943704325398 | validation: 0.009540716821950825]
	TIME [epoch: 5.71 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009404773551350396		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.009404773551350396 | validation: 0.007341292438646441]
	TIME [epoch: 5.71 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008530716701302489		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.008530716701302489 | validation: 0.009807264467574673]
	TIME [epoch: 5.71 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009593047341260265		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.009593047341260265 | validation: 0.010627326452206277]
	TIME [epoch: 5.72 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010038610918736233		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.010038610918736233 | validation: 0.010221964182178534]
	TIME [epoch: 5.71 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009894912333283825		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.009894912333283825 | validation: 0.0093853100167405]
	TIME [epoch: 5.71 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007871665695070642		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.007871665695070642 | validation: 0.007341286911409762]
	TIME [epoch: 5.71 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00901424327072805		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.00901424327072805 | validation: 0.008507265629565131]
	TIME [epoch: 5.71 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009785073355727938		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.009785073355727938 | validation: 0.006413047347426315]
	TIME [epoch: 5.71 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008520768290509795		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.008520768290509795 | validation: 0.010858031786640465]
	TIME [epoch: 5.72 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008761445532441159		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.008761445532441159 | validation: 0.009362870156478477]
	TIME [epoch: 5.71 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00788328578833779		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.00788328578833779 | validation: 0.007332996430032746]
	TIME [epoch: 5.72 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007966530189108225		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.007966530189108225 | validation: 0.009141820370769493]
	TIME [epoch: 5.71 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010351408832787934		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.010351408832787934 | validation: 0.008265561193578031]
	TIME [epoch: 5.71 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008108383949641581		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.008108383949641581 | validation: 0.005769565666120103]
	TIME [epoch: 5.72 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00945887994919359		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.00945887994919359 | validation: 0.007520363814288345]
	TIME [epoch: 5.71 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008568792300874322		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.008568792300874322 | validation: 0.008705308791262334]
	TIME [epoch: 5.71 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008821370923702405		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.008821370923702405 | validation: 0.007007686289822579]
	TIME [epoch: 5.71 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008883821195819855		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.008883821195819855 | validation: 0.00961029965739002]
	TIME [epoch: 5.71 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009697933842824052		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.009697933842824052 | validation: 0.008991184467593916]
	TIME [epoch: 5.72 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008816699559270232		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.008816699559270232 | validation: 0.007432971038875191]
	TIME [epoch: 5.71 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00859604650323937		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.00859604650323937 | validation: 0.008553363037899187]
	TIME [epoch: 5.71 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00896553553991542		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.00896553553991542 | validation: 0.010192449277414929]
	TIME [epoch: 5.71 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009050440953986133		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.009050440953986133 | validation: 0.008437735072641117]
	TIME [epoch: 5.72 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008665356499358243		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.008665356499358243 | validation: 0.006481228696215669]
	TIME [epoch: 5.72 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009322713474029585		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.009322713474029585 | validation: 0.007961967272896142]
	TIME [epoch: 5.72 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008168242366819508		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.008168242366819508 | validation: 0.00915222316958071]
	TIME [epoch: 5.71 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008691081937529472		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.008691081937529472 | validation: 0.00860883925115592]
	TIME [epoch: 5.72 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008753253821839254		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.008753253821839254 | validation: 0.00831647308322303]
	TIME [epoch: 5.71 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008348908480895551		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.008348908480895551 | validation: 0.007366087711999625]
	TIME [epoch: 5.72 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009317727683383974		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.009317727683383974 | validation: 0.007407583856181211]
	TIME [epoch: 5.72 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007885003351274575		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.007885003351274575 | validation: 0.009787027379644188]
	TIME [epoch: 5.72 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009281282837418555		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.009281282837418555 | validation: 0.00729032708637517]
	TIME [epoch: 5.71 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009164017258430013		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.009164017258430013 | validation: 0.01045739648372942]
	TIME [epoch: 5.72 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00861692169176262		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.00861692169176262 | validation: 0.0067786479412725205]
	TIME [epoch: 5.71 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00851935053322276		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.00851935053322276 | validation: 0.010612195177364149]
	TIME [epoch: 5.73 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00908242062989572		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.00908242062989572 | validation: 0.00791353896554965]
	TIME [epoch: 5.71 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008151447175835292		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.008151447175835292 | validation: 0.010211169758324291]
	TIME [epoch: 5.72 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007922024022405733		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.007922024022405733 | validation: 0.007195486994431977]
	TIME [epoch: 5.71 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009348693308958298		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.009348693308958298 | validation: 0.008334715022581263]
	TIME [epoch: 5.72 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009050714137890449		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.009050714137890449 | validation: 0.007979867580816914]
	TIME [epoch: 5.72 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008753506634316963		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.008753506634316963 | validation: 0.0071962900882544985]
	TIME [epoch: 5.71 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00769873148476703		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.00769873148476703 | validation: 0.007031359079798516]
	TIME [epoch: 5.71 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009852467766062818		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.009852467766062818 | validation: 0.01212734004632351]
	TIME [epoch: 5.71 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008488853652671201		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.008488853652671201 | validation: 0.007162709654635369]
	TIME [epoch: 5.71 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009616228937858587		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.009616228937858587 | validation: 0.00824386264699506]
	TIME [epoch: 5.72 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009731617589955322		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.009731617589955322 | validation: 0.00896472935130358]
	TIME [epoch: 5.71 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008450105322998227		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.008450105322998227 | validation: 0.005242520436664566]
	TIME [epoch: 5.71 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008769872647231666		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.008769872647231666 | validation: 0.010808426317186304]
	TIME [epoch: 5.7 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008690827844221781		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.008690827844221781 | validation: 0.007995711269138195]
	TIME [epoch: 5.7 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00900788651657866		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.00900788651657866 | validation: 0.012166404023306689]
	TIME [epoch: 5.75 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00898914102931271		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.00898914102931271 | validation: 0.007454260030656934]
	TIME [epoch: 5.73 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00985336987795152		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.00985336987795152 | validation: 0.006193508971491968]
	TIME [epoch: 5.74 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008483728027304334		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.008483728027304334 | validation: 0.0072993652217626]
	TIME [epoch: 5.74 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009630001725087824		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.009630001725087824 | validation: 0.008743744918797104]
	TIME [epoch: 5.73 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008768320030983313		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.008768320030983313 | validation: 0.007125184880431224]
	TIME [epoch: 5.74 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008773902594877205		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.008773902594877205 | validation: 0.008563481523353445]
	TIME [epoch: 5.75 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008188790466377915		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.008188790466377915 | validation: 0.006466584347839189]
	TIME [epoch: 5.73 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007291021033781788		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.007291021033781788 | validation: 0.006057589861253454]
	TIME [epoch: 5.74 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009197885207043068		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.009197885207043068 | validation: 0.006193028858705119]
	TIME [epoch: 5.74 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008783239268290163		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.008783239268290163 | validation: 0.008477307899902564]
	TIME [epoch: 5.74 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0091334365965944		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.0091334365965944 | validation: 0.00860013793348673]
	TIME [epoch: 5.74 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007368823772522534		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.007368823772522534 | validation: 0.006629225849558973]
	TIME [epoch: 5.74 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009068257958531875		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.009068257958531875 | validation: 0.007640893786700531]
	TIME [epoch: 5.7 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008193548855966917		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.008193548855966917 | validation: 0.005352099889159645]
	TIME [epoch: 5.72 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008628131691186945		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.008628131691186945 | validation: 0.00743554927493213]
	TIME [epoch: 5.7 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008396307959865125		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.008396307959865125 | validation: 0.008934839102906534]
	TIME [epoch: 5.72 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008956241944749402		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.008956241944749402 | validation: 0.006301897989246453]
	TIME [epoch: 5.7 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008814447334039958		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.008814447334039958 | validation: 0.007994647945653433]
	TIME [epoch: 5.71 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010567459635716023		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.010567459635716023 | validation: 0.006185617746041006]
	TIME [epoch: 5.7 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009459766387578415		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.009459766387578415 | validation: 0.007901394506335258]
	TIME [epoch: 5.71 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008287721891119645		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.008287721891119645 | validation: 0.007324965543346918]
	TIME [epoch: 5.71 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008758350711402615		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.008758350711402615 | validation: 0.006892967429831654]
	TIME [epoch: 5.72 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009539967074568343		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.009539967074568343 | validation: 0.008647444120544068]
	TIME [epoch: 5.7 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008631994492949449		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.008631994492949449 | validation: 0.005929453980371424]
	TIME [epoch: 5.7 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008224720955098557		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.008224720955098557 | validation: 0.005490070024065175]
	TIME [epoch: 5.71 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009418469117486237		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.009418469117486237 | validation: 0.006421556854267052]
	TIME [epoch: 5.72 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00824549532500388		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.00824549532500388 | validation: 0.006638149900238055]
	TIME [epoch: 5.71 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009648616689468084		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.009648616689468084 | validation: 0.006870532065307211]
	TIME [epoch: 5.71 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0080528755830625		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.0080528755830625 | validation: 0.006713885854100998]
	TIME [epoch: 5.71 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00914790874701966		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.00914790874701966 | validation: 0.008522251643633882]
	TIME [epoch: 5.74 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00849902529823101		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.00849902529823101 | validation: 0.006910052717875138]
	TIME [epoch: 5.74 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008527505252830768		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.008527505252830768 | validation: 0.006480596702007447]
	TIME [epoch: 5.74 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008813551569224044		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.008813551569224044 | validation: 0.005967381180470844]
	TIME [epoch: 5.73 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009440970448048345		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.009440970448048345 | validation: 0.006221533438083649]
	TIME [epoch: 5.73 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008515071549539031		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.008515071549539031 | validation: 0.007841450974953146]
	TIME [epoch: 5.74 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008978191967822234		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.008978191967822234 | validation: 0.005752338349684827]
	TIME [epoch: 5.73 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00954511944809224		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.00954511944809224 | validation: 0.006091059663626686]
	TIME [epoch: 5.74 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007193428751598458		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.007193428751598458 | validation: 0.009777038801105455]
	TIME [epoch: 5.73 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0091506842528823		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.0091506842528823 | validation: 0.009395891469608031]
	TIME [epoch: 5.73 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00906372325981104		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.00906372325981104 | validation: 0.007521664692826547]
	TIME [epoch: 5.73 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008808992986391711		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.008808992986391711 | validation: 0.008101144937532301]
	TIME [epoch: 5.71 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00800287018319448		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.00800287018319448 | validation: 0.008460895940320257]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_5_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_5_v_mmd4_1302.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 4430.231 seconds.
