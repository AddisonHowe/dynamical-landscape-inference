Args:
Namespace(name='model_phi1_4a_distortion_v1_9_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_9/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_9/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.051844362, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1553467475

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.895150464058294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.895150464058294 | validation: 6.260132821425161]
	TIME [epoch: 163 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.808070809776266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.808070809776266 | validation: 4.923122230454842]
	TIME [epoch: 1.02 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.614815763563844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.614815763563844 | validation: 6.39693659084426]
	TIME [epoch: 0.695 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.222834935193373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.222834935193373 | validation: 6.013420711894739]
	TIME [epoch: 0.693 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.983405733072344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.983405733072344 | validation: 5.332869905711381]
	TIME [epoch: 0.695 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.580318118284297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.580318118284297 | validation: 4.66930331960431]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.25701373899258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.25701373899258 | validation: 4.072557130936113]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.069978839274476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.069978839274476 | validation: 3.7377558497904406]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.807439384364087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.807439384364087 | validation: 3.0327886184531]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.504015895682038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.504015895682038 | validation: 4.106399661779881]
	TIME [epoch: 0.692 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.135649785618816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.135649785618816 | validation: 2.8822197572900983]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.348233508150633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.348233508150633 | validation: 3.8172475082117687]
	TIME [epoch: 0.699 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.059072688435511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.059072688435511 | validation: 2.8524592358379373]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.283746512968871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.283746512968871 | validation: 3.167013248104494]
	TIME [epoch: 0.693 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.432057285937932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.432057285937932 | validation: 2.5740257302372784]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.125375069011686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.125375069011686 | validation: 3.0895197611628973]
	TIME [epoch: 0.692 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.635966001052862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.635966001052862 | validation: 2.5250732110807634]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.09085343773394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.09085343773394 | validation: 2.343343291309235]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.984779007539456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.984779007539456 | validation: 2.3477333895121393]
	TIME [epoch: 0.697 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0487761709658745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0487761709658745 | validation: 2.532006576329926]
	TIME [epoch: 0.694 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.074181428167424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.074181428167424 | validation: 2.1782250684861815]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8830004010803214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8830004010803214 | validation: 2.170353330837915]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9193299768341796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9193299768341796 | validation: 2.3897764721822337]
	TIME [epoch: 0.694 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9872698080337448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9872698080337448 | validation: 2.101309877037099]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8323557117112013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8323557117112013 | validation: 2.0477239363225705]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8093772724863912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8093772724863912 | validation: 2.1251828863276794]
	TIME [epoch: 0.696 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8151148106992423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8151148106992423 | validation: 2.0679173125884263]
	TIME [epoch: 0.695 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.799664864348138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.799664864348138 | validation: 2.3497816100045514]
	TIME [epoch: 0.693 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8173722934555157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8173722934555157 | validation: 2.0268253680754733]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.674064818645776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.674064818645776 | validation: 2.058586704336376]
	TIME [epoch: 0.699 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6039761357564504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6039761357564504 | validation: 2.0651351783158183]
	TIME [epoch: 0.694 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5477204530582256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5477204530582256 | validation: 2.082268059331006]
	TIME [epoch: 0.692 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5703287498283207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5703287498283207 | validation: 2.7061788955204094]
	TIME [epoch: 0.689 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8523616911965908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8523616911965908 | validation: 3.1916535986478074]
	TIME [epoch: 0.691 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.46916308507099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.46916308507099 | validation: 1.9714049574483186]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6006655965463943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6006655965463943 | validation: 2.3044597783444134]
	TIME [epoch: 0.695 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.58107623230401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.58107623230401 | validation: 2.0942739293751056]
	TIME [epoch: 0.693 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5016505129274345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5016505129274345 | validation: 1.9785913533747612]
	TIME [epoch: 0.693 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4260985620071085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4260985620071085 | validation: 2.1243884788542124]
	TIME [epoch: 0.694 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.413840839720324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.413840839720324 | validation: 2.0209761655276903]
	TIME [epoch: 0.694 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.424670924188047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.424670924188047 | validation: 2.5482644323546837]
	TIME [epoch: 0.692 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6349652467728344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6349652467728344 | validation: 1.9717784874451032]
	TIME [epoch: 0.692 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.497663492500354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.497663492500354 | validation: 2.117962054388131]
	TIME [epoch: 0.693 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3747158340244314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3747158340244314 | validation: 1.9842192025101746]
	TIME [epoch: 0.695 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2932962486677333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2932962486677333 | validation: 1.9395751971504216]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.286035490715186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.286035490715186 | validation: 2.0967354188838985]
	TIME [epoch: 0.692 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2901027298965984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2901027298965984 | validation: 1.9224664186322025]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2950234320928757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2950234320928757 | validation: 2.2456733596962417]
	TIME [epoch: 0.694 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.34018284275021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.34018284275021 | validation: 1.893406085231331]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2687278286101877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2687278286101877 | validation: 2.096450374484187]
	TIME [epoch: 0.696 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.237846316798632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.237846316798632 | validation: 1.8714144355545537]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.18576239222315		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.18576239222315 | validation: 2.0246868678554217]
	TIME [epoch: 0.693 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1793368634646355		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 3.1793368634646355 | validation: 1.9079333478656126]
	TIME [epoch: 0.693 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.188475199917174		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 3.188475199917174 | validation: 2.2844906618621024]
	TIME [epoch: 0.695 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.275707005006666		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 3.275707005006666 | validation: 1.851524312192153]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.196057073295807		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 3.196057073295807 | validation: 2.153241537349834]
	TIME [epoch: 0.695 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.169027331170464		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 3.169027331170464 | validation: 1.877728114414925]
	TIME [epoch: 0.694 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.115170242407675		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 3.115170242407675 | validation: 2.0410748904127405]
	TIME [epoch: 0.693 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0787774491176094		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 3.0787774491176094 | validation: 1.889680897013129]
	TIME [epoch: 0.693 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0724697518854316		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 3.0724697518854316 | validation: 2.122965917383855]
	TIME [epoch: 0.694 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0860693346869046		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 3.0860693346869046 | validation: 1.859436164520772]
	TIME [epoch: 0.696 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.152654457391321		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 3.152654457391321 | validation: 2.5844928499286937]
	TIME [epoch: 0.694 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3031965869636806		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 3.3031965869636806 | validation: 1.8794258281251954]
	TIME [epoch: 0.693 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.030239134485445		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 3.030239134485445 | validation: 1.885087086553731]
	TIME [epoch: 0.694 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.013026708522186		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 3.013026708522186 | validation: 2.1991807781565806]
	TIME [epoch: 0.693 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0517678738586205		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 3.0517678738586205 | validation: 1.8739547265821976]
	TIME [epoch: 0.693 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.049499683321743		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 3.049499683321743 | validation: 2.2582255468766963]
	TIME [epoch: 0.693 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.061165137103619		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 3.061165137103619 | validation: 1.863386580605514]
	TIME [epoch: 0.703 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9826825083489843		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 2.9826825083489843 | validation: 2.0889743296006715]
	TIME [epoch: 0.693 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.951189346687812		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 2.951189346687812 | validation: 1.8999119651554286]
	TIME [epoch: 0.693 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.922378465820764		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 2.922378465820764 | validation: 2.1112005670547354]
	TIME [epoch: 0.692 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9333155759496505		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 2.9333155759496505 | validation: 1.8593700247531537]
	TIME [epoch: 0.694 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.966841047750281		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 2.966841047750281 | validation: 2.425989369898124]
	TIME [epoch: 0.694 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.11781843287729		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 3.11781843287729 | validation: 1.8830851575429208]
	TIME [epoch: 0.693 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.924083534688744		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 2.924083534688744 | validation: 2.0240174203558237]
	TIME [epoch: 0.692 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8683453480857257		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 2.8683453480857257 | validation: 1.971000475017932]
	TIME [epoch: 0.693 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.846935291540711		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 2.846935291540711 | validation: 1.9793899401721626]
	TIME [epoch: 0.694 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.845196180892231		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 2.845196180892231 | validation: 1.9987001434280882]
	TIME [epoch: 0.693 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.818793069262034		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 2.818793069262034 | validation: 1.946990060234705]
	TIME [epoch: 0.693 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8259670634699456		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 2.8259670634699456 | validation: 2.076692923778182]
	TIME [epoch: 0.694 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.836674031919068		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 2.836674031919068 | validation: 1.9120976069163333]
	TIME [epoch: 0.693 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9383468761873313		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 2.9383468761873313 | validation: 2.752394604292468]
	TIME [epoch: 0.693 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.218094800244187		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 3.218094800244187 | validation: 1.9738823023721086]
	TIME [epoch: 0.691 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8088976737882962		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 2.8088976737882962 | validation: 1.9823448046284746]
	TIME [epoch: 0.693 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.931712235072016		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 2.931712235072016 | validation: 2.3890666460599643]
	TIME [epoch: 0.694 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.034444716936866		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 3.034444716936866 | validation: 2.027970414001295]
	TIME [epoch: 0.694 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8665968537799267		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 2.8665968537799267 | validation: 1.930330954251121]
	TIME [epoch: 0.698 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8716540272627276		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 2.8716540272627276 | validation: 2.190820949738685]
	TIME [epoch: 0.695 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.858265983564493		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 2.858265983564493 | validation: 1.9733909404045882]
	TIME [epoch: 0.694 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.770265795122089		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 2.770265795122089 | validation: 1.9301106446171479]
	TIME [epoch: 0.693 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.805226854769058		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 2.805226854769058 | validation: 2.136496225296541]
	TIME [epoch: 0.692 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.814151718808484		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 2.814151718808484 | validation: 2.0653222627099157]
	TIME [epoch: 0.695 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8412379627865527		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 2.8412379627865527 | validation: 2.009018919487274]
	TIME [epoch: 0.694 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.756684533955257		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 2.756684533955257 | validation: 2.064319045021294]
	TIME [epoch: 0.693 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7895645720788265		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 2.7895645720788265 | validation: 1.9959995584784338]
	TIME [epoch: 0.694 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7768299054889436		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 2.7768299054889436 | validation: 2.0265094463473967]
	TIME [epoch: 0.694 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7422762875213667		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 2.7422762875213667 | validation: 1.9811830566617035]
	TIME [epoch: 0.694 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7443537766474724		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 2.7443537766474724 | validation: 2.044082857069289]
	TIME [epoch: 0.695 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7454836481932228		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 2.7454836481932228 | validation: 2.1723917565884663]
	TIME [epoch: 0.693 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8631742831785947		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 2.8631742831785947 | validation: 2.2692243822529314]
	TIME [epoch: 0.692 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.941453484900013		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 2.941453484900013 | validation: 2.1992906848652387]
	TIME [epoch: 0.696 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.868515074331709		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 2.868515074331709 | validation: 2.0218478567278892]
	TIME [epoch: 0.695 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8398512954836876		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 2.8398512954836876 | validation: 2.2498179742115396]
	TIME [epoch: 0.694 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8438493715137816		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 2.8438493715137816 | validation: 2.0492709477584286]
	TIME [epoch: 0.694 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7503382250846182		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 2.7503382250846182 | validation: 2.021152018848737]
	TIME [epoch: 0.693 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.759485194927502		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 2.759485194927502 | validation: 2.140958833215643]
	TIME [epoch: 0.695 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7501030109133935		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 2.7501030109133935 | validation: 2.008236843920437]
	TIME [epoch: 0.694 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.729598971867794		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 2.729598971867794 | validation: 2.1019667713352477]
	TIME [epoch: 0.694 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7130367763692336		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 2.7130367763692336 | validation: 2.05662850123317]
	TIME [epoch: 0.695 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7019677789931134		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 2.7019677789931134 | validation: 2.0808097540555286]
	TIME [epoch: 0.703 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6769341858173727		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 2.6769341858173727 | validation: 2.066768067100718]
	TIME [epoch: 0.696 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.677872354962949		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 2.677872354962949 | validation: 2.058882920756635]
	TIME [epoch: 0.694 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.664695712953766		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 2.664695712953766 | validation: 2.0365581619395736]
	TIME [epoch: 0.694 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.659758380713928		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 2.659758380713928 | validation: 2.136123359438232]
	TIME [epoch: 0.696 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.709741219761221		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 2.709741219761221 | validation: 2.27662978081137]
	TIME [epoch: 0.696 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.820939294758176		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 2.820939294758176 | validation: 2.0612307557411116]
	TIME [epoch: 0.696 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.693463531280709		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 2.693463531280709 | validation: 2.3782096583479237]
	TIME [epoch: 0.694 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7972878518294726		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 2.7972878518294726 | validation: 2.084185330076566]
	TIME [epoch: 0.694 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7098114913454676		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 2.7098114913454676 | validation: 2.0776514271767725]
	TIME [epoch: 0.696 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6224278063995006		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 2.6224278063995006 | validation: 2.0862148366269047]
	TIME [epoch: 0.694 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.613114814600325		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 2.613114814600325 | validation: 2.130835259875675]
	TIME [epoch: 0.693 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6267744654079275		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 2.6267744654079275 | validation: 2.071191198873575]
	TIME [epoch: 0.693 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.573134368750297		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 2.573134368750297 | validation: 2.0606180352469163]
	TIME [epoch: 0.693 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.53696525710752		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 2.53696525710752 | validation: 2.135024623777873]
	TIME [epoch: 0.694 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5503869621478077		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 2.5503869621478077 | validation: 2.2690574312363654]
	TIME [epoch: 0.694 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5842491425702985		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 2.5842491425702985 | validation: 2.1621054199412293]
	TIME [epoch: 0.695 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.541996889811482		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 2.541996889811482 | validation: 2.244904899508144]
	TIME [epoch: 0.694 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5147144525809195		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 2.5147144525809195 | validation: 2.149819490320376]
	TIME [epoch: 0.695 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.425215370011151		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 2.425215370011151 | validation: 2.2286882672468287]
	TIME [epoch: 0.694 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.494842421264617		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 2.494842421264617 | validation: 2.3138631754712673]
	TIME [epoch: 0.696 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.35639836421229		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 2.35639836421229 | validation: 2.024997310894404]
	TIME [epoch: 0.694 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3464595543990003		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 2.3464595543990003 | validation: 2.331571350219286]
	TIME [epoch: 0.693 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.580131831685009		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 2.580131831685009 | validation: 2.0197458488616475]
	TIME [epoch: 0.697 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1439344494194716		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 2.1439344494194716 | validation: 2.039308762544935]
	TIME [epoch: 0.696 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.157307328990244		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 2.157307328990244 | validation: 2.5509007946615614]
	TIME [epoch: 0.694 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4536545196018626		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 2.4536545196018626 | validation: 2.3596973224458653]
	TIME [epoch: 0.694 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4996578860716245		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 2.4996578860716245 | validation: 1.98246729479976]
	TIME [epoch: 0.693 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.044657075755809		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 2.044657075755809 | validation: 2.2033672586910407]
	TIME [epoch: 0.696 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2961222596333135		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 2.2961222596333135 | validation: 1.9945316967817268]
	TIME [epoch: 0.693 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.019200096782692		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 2.019200096782692 | validation: 1.909675051100194]
	TIME [epoch: 0.696 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9167431656382052		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.9167431656382052 | validation: 1.9757789170516447]
	TIME [epoch: 0.695 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8670090637835302		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.8670090637835302 | validation: 1.8467351071893683]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8348305565782932		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.8348305565782932 | validation: 1.757167482142566]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8545849157884733		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.8545849157884733 | validation: 1.95919874925903]
	TIME [epoch: 0.697 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9768794704262582		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.9768794704262582 | validation: 1.672319077242849]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.771223333268149		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.771223333268149 | validation: 1.6943491239546469]
	TIME [epoch: 0.695 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6937519440096005		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.6937519440096005 | validation: 1.5974007542065136]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6530630782457194		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.6530630782457194 | validation: 1.6510132022330137]
	TIME [epoch: 0.695 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6994874087992877		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.6994874087992877 | validation: 1.8163608078675466]
	TIME [epoch: 0.698 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9622771753482904		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.9622771753482904 | validation: 2.1270701553400637]
	TIME [epoch: 0.698 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.125337042988584		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 2.125337042988584 | validation: 1.573652182090516]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_151.pth
	Model improved!!!
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6273921881044424		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.6273921881044424 | validation: 2.157963316930178]
	TIME [epoch: 0.695 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0736046245251294		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 2.0736046245251294 | validation: 1.7979963025760595]
	TIME [epoch: 0.697 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7842986594209271		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.7842986594209271 | validation: 1.628387678121127]
	TIME [epoch: 0.695 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6467359335794272		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.6467359335794272 | validation: 1.623768001937794]
	TIME [epoch: 0.694 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.695021227899935		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.695021227899935 | validation: 1.575361615485117]
	TIME [epoch: 0.695 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6237331332354823		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.6237331332354823 | validation: 1.4104853706147513]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5261733924100775		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.5261733924100775 | validation: 1.4417667987051173]
	TIME [epoch: 0.695 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.506084492449903		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.506084492449903 | validation: 1.4075932394840303]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_159.pth
	Model improved!!!
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.499413696412231		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.499413696412231 | validation: 1.3886640671661112]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4884665769954666		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.4884665769954666 | validation: 1.4286266141998516]
	TIME [epoch: 0.694 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4729642355838024		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.4729642355838024 | validation: 1.3627132385010985]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4756993956605902		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.4756993956605902 | validation: 1.3202045619410505]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4164880420227286		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.4164880420227286 | validation: 1.3213298719245847]
	TIME [epoch: 0.696 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.429156291395338		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.429156291395338 | validation: 1.4421587274730567]
	TIME [epoch: 0.697 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6101180586550226		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.6101180586550226 | validation: 2.025737615509405]
	TIME [epoch: 0.694 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.956709027985911		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.956709027985911 | validation: 1.3442841933731475]
	TIME [epoch: 0.696 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4400113303271485		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.4400113303271485 | validation: 1.5651113060907427]
	TIME [epoch: 0.694 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6057346698977728		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.6057346698977728 | validation: 1.7912836358504487]
	TIME [epoch: 0.696 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.770422525361796		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.770422525361796 | validation: 1.2897461345709196]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4068427085562982		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.4068427085562982 | validation: 1.4795868223822437]
	TIME [epoch: 0.697 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5954038361490341		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.5954038361490341 | validation: 1.6040663572451734]
	TIME [epoch: 0.696 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.590605745549234		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.590605745549234 | validation: 1.2470394455526106]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.367425540694929		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.367425540694929 | validation: 1.321665660406157]
	TIME [epoch: 0.696 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4437415446023483		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.4437415446023483 | validation: 1.3891370393556408]
	TIME [epoch: 0.696 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4570496420257528		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.4570496420257528 | validation: 1.1500139800555014]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3170936692029471		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.3170936692029471 | validation: 1.1918600868402078]
	TIME [epoch: 0.696 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3076738572596278		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.3076738572596278 | validation: 1.1994767319850799]
	TIME [epoch: 0.695 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3178770386147445		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.3178770386147445 | validation: 1.0996566833147898]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.319642544929878		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.319642544929878 | validation: 1.1845154184763573]
	TIME [epoch: 0.692 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3224049117432586		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.3224049117432586 | validation: 1.0459919864955685]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.261595393300144		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.261595393300144 | validation: 1.130305006489435]
	TIME [epoch: 0.692 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2501846848904563		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.2501846848904563 | validation: 1.0490092952151895]
	TIME [epoch: 0.697 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.266112179725116		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.266112179725116 | validation: 1.5769784318314235]
	TIME [epoch: 0.69 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5007225060604408		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.5007225060604408 | validation: 1.5376900110103238]
	TIME [epoch: 0.692 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6296713958263422		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.6296713958263422 | validation: 1.1621169080908158]
	TIME [epoch: 0.693 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.284476109412332		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.284476109412332 | validation: 1.3396702991216776]
	TIME [epoch: 0.696 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.333760826892583		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.333760826892583 | validation: 1.0247674983247446]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2262715104196145		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.2262715104196145 | validation: 1.0054382031308273]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1696794397081889		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.1696794397081889 | validation: 1.0133309899046843]
	TIME [epoch: 0.693 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.163323230588631		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.163323230588631 | validation: 1.0571026404009893]
	TIME [epoch: 0.691 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2033908919120797		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.2033908919120797 | validation: 1.2600418428118254]
	TIME [epoch: 0.691 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3331204507565333		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.3331204507565333 | validation: 1.720342127146023]
	TIME [epoch: 0.691 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.646646950811794		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.646646950811794 | validation: 1.0179179405972307]
	TIME [epoch: 0.69 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2460710573219937		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.2460710573219937 | validation: 1.322278691307458]
	TIME [epoch: 0.692 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2594858358498915		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.2594858358498915 | validation: 1.0007012943940534]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1508626756140725		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.1508626756140725 | validation: 1.0073117530248792]
	TIME [epoch: 0.693 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2249434831662844		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.2249434831662844 | validation: 1.3739609827801786]
	TIME [epoch: 0.692 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3298635363359208		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.3298635363359208 | validation: 1.033281405760005]
	TIME [epoch: 0.691 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.244851366884542		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.244851366884542 | validation: 1.107802115417504]
	TIME [epoch: 0.691 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2210489854001108		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.2210489854001108 | validation: 0.981977322154585]
	TIME [epoch: 171 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1306169856943098		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.1306169856943098 | validation: 0.964422754646741]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1219275282927565		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.1219275282927565 | validation: 0.9021889977811803]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.10975892385649		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.10975892385649 | validation: 1.1208034642394573]
	TIME [epoch: 1.36 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1894624244430494		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.1894624244430494 | validation: 1.1186655623518698]
	TIME [epoch: 1.36 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3193320653682237		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.3193320653682237 | validation: 1.1638487871958494]
	TIME [epoch: 1.36 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.189022485450238		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.189022485450238 | validation: 0.9025242465308694]
	TIME [epoch: 1.36 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1089626247700124		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.1089626247700124 | validation: 0.9709992177848543]
	TIME [epoch: 1.36 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.075520350290537		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.075520350290537 | validation: 0.8268267993884664]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_209.pth
	Model improved!!!
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1184866673309493		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.1184866673309493 | validation: 1.171111150694961]
	TIME [epoch: 1.36 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1938936547334966		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.1938936547334966 | validation: 1.0053926497794008]
	TIME [epoch: 1.36 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.217997480200316		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.217997480200316 | validation: 1.1843681234536365]
	TIME [epoch: 1.36 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2034013802322097		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 1.2034013802322097 | validation: 0.9692049194873896]
	TIME [epoch: 1.36 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.138715239306969		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.138715239306969 | validation: 0.8968669012197528]
	TIME [epoch: 1.36 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0317138467751537		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 1.0317138467751537 | validation: 0.800128528810976]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_215.pth
	Model improved!!!
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0016856843904685		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.0016856843904685 | validation: 0.9257817584368987]
	TIME [epoch: 1.36 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0267420071506639		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.0267420071506639 | validation: 0.9402923311372972]
	TIME [epoch: 1.36 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1273721457502632		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.1273721457502632 | validation: 1.310385642329321]
	TIME [epoch: 1.36 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1924027908572536		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.1924027908572536 | validation: 0.8306085269276017]
	TIME [epoch: 1.36 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0479837988686107		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.0479837988686107 | validation: 0.9057869220095404]
	TIME [epoch: 1.36 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0051455244639973		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 1.0051455244639973 | validation: 0.9945858401685789]
	TIME [epoch: 1.36 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1391152485903862		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.1391152485903862 | validation: 1.3924809352728844]
	TIME [epoch: 1.36 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.389106885603		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 1.389106885603 | validation: 0.7973057688393469]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0136802661427122		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.0136802661427122 | validation: 0.9020919986213857]
	TIME [epoch: 1.35 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9750046382685722		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.9750046382685722 | validation: 0.7767493739309201]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_225.pth
	Model improved!!!
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9880321147250212		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.9880321147250212 | validation: 0.96949403968034]
	TIME [epoch: 1.36 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.987815630418351		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.987815630418351 | validation: 0.852734528314887]
	TIME [epoch: 1.36 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0243790036225653		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.0243790036225653 | validation: 1.1297533291074096]
	TIME [epoch: 1.36 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.116944244109793		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 1.116944244109793 | validation: 1.0628579007892789]
	TIME [epoch: 1.36 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.241853276706585		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.241853276706585 | validation: 1.0617232572439268]
	TIME [epoch: 1.36 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1107710306353473		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 1.1107710306353473 | validation: 0.7353030188034277]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9570502758560884		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.9570502758560884 | validation: 0.7679555344663105]
	TIME [epoch: 1.36 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8920352041953882		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.8920352041953882 | validation: 0.7927852344404687]
	TIME [epoch: 1.36 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8972646766904347		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.8972646766904347 | validation: 0.7899429177338254]
	TIME [epoch: 1.36 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9422544775032318		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.9422544775032318 | validation: 1.2271512545250152]
	TIME [epoch: 1.36 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.151015860519212		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.151015860519212 | validation: 1.0531049232245617]
	TIME [epoch: 1.36 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2892332764250243		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 1.2892332764250243 | validation: 0.9533512020711781]
	TIME [epoch: 1.36 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.956318832203784		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.956318832203784 | validation: 0.6814166113297948]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_238.pth
	Model improved!!!
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8983848992214232		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.8983848992214232 | validation: 0.7353239229078183]
	TIME [epoch: 1.36 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8553961306370468		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.8553961306370468 | validation: 0.8280678850634751]
	TIME [epoch: 1.36 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8788051004543291		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.8788051004543291 | validation: 0.8175855431609153]
	TIME [epoch: 1.36 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9947435276189305		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.9947435276189305 | validation: 1.3062484879994913]
	TIME [epoch: 1.36 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1719694275872288		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 1.1719694275872288 | validation: 0.8184361499386515]
	TIME [epoch: 1.36 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.068201129908793		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 1.068201129908793 | validation: 0.8390776943049443]
	TIME [epoch: 1.36 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8886818512033381		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.8886818512033381 | validation: 0.6635215194124005]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_245.pth
	Model improved!!!
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8249470595556894		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.8249470595556894 | validation: 0.7302905892959572]
	TIME [epoch: 1.36 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.807037645576826		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.807037645576826 | validation: 0.7253421979405532]
	TIME [epoch: 1.36 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8172726897934786		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.8172726897934786 | validation: 0.8416797621900195]
	TIME [epoch: 1.36 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9009042053662847		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.9009042053662847 | validation: 1.0071218474581995]
	TIME [epoch: 1.36 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1027138081954126		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 1.1027138081954126 | validation: 1.0547102354235136]
	TIME [epoch: 1.36 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0319835989190895		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 1.0319835989190895 | validation: 0.712415213559048]
	TIME [epoch: 1.36 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9029303290336362		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.9029303290336362 | validation: 1.255449848242366]
	TIME [epoch: 1.36 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0237631897858293		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 1.0237631897858293 | validation: 0.6736602763908568]
	TIME [epoch: 1.36 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8166051410966741		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.8166051410966741 | validation: 0.7504172790280705]
	TIME [epoch: 1.36 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7922597019834065		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.7922597019834065 | validation: 0.729434292042874]
	TIME [epoch: 1.36 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8374763215645843		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.8374763215645843 | validation: 0.9396691946780309]
	TIME [epoch: 1.36 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9023880724731081		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.9023880724731081 | validation: 0.8237575505136525]
	TIME [epoch: 1.36 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.001465879444461		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 1.001465879444461 | validation: 1.0548914160026956]
	TIME [epoch: 1.36 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9033515437033839		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.9033515437033839 | validation: 0.6183376511967076]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_259.pth
	Model improved!!!
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7947614640420784		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.7947614640420784 | validation: 0.7335812207393669]
	TIME [epoch: 1.36 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7634392750057717		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.7634392750057717 | validation: 0.6797135911068125]
	TIME [epoch: 1.36 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7736789757349937		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.7736789757349937 | validation: 0.8546299767585197]
	TIME [epoch: 1.36 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8280688559010995		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.8280688559010995 | validation: 0.7473317870674294]
	TIME [epoch: 1.36 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8855293497570965		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.8855293497570965 | validation: 1.0087644277013366]
	TIME [epoch: 1.36 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.89925034507726		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.89925034507726 | validation: 0.7029653351858793]
	TIME [epoch: 1.36 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.871314538015971		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.871314538015971 | validation: 0.8826714077303915]
	TIME [epoch: 1.36 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.793340606095505		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.793340606095505 | validation: 0.6278550626602863]
	TIME [epoch: 1.36 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7316923085058566		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.7316923085058566 | validation: 0.7090384728857595]
	TIME [epoch: 1.36 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.695246756809089		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.695246756809089 | validation: 0.6364180761316413]
	TIME [epoch: 1.36 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6984244455373496		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.6984244455373496 | validation: 0.7565534140121539]
	TIME [epoch: 1.36 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7434781675317993		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.7434781675317993 | validation: 0.7714455869172555]
	TIME [epoch: 1.36 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8781875975923785		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.8781875975923785 | validation: 1.0717350411175228]
	TIME [epoch: 1.36 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9301792557228354		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.9301792557228354 | validation: 0.6396384404239129]
	TIME [epoch: 1.36 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.854686436032992		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.854686436032992 | validation: 0.7899552769191724]
	TIME [epoch: 1.36 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7349774488911186		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.7349774488911186 | validation: 0.5439092337403867]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_275.pth
	Model improved!!!
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.662878692208451		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.662878692208451 | validation: 0.7257520686652688]
	TIME [epoch: 1.36 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6534862009829513		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.6534862009829513 | validation: 0.6328538353580547]
	TIME [epoch: 1.36 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6951715379489727		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.6951715379489727 | validation: 0.8655736265057792]
	TIME [epoch: 1.36 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7625851899703944		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.7625851899703944 | validation: 0.7003021807661215]
	TIME [epoch: 1.36 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8075379375606758		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.8075379375606758 | validation: 0.8596920827388426]
	TIME [epoch: 1.36 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7762788815916377		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.7762788815916377 | validation: 0.6094535519335884]
	TIME [epoch: 1.35 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7570896351132076		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.7570896351132076 | validation: 0.7569422192370858]
	TIME [epoch: 1.36 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7238397518539487		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.7238397518539487 | validation: 0.5899415863557155]
	TIME [epoch: 1.36 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6593212352163964		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.6593212352163964 | validation: 0.6485321877936658]
	TIME [epoch: 1.36 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6534212634890332		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.6534212634890332 | validation: 0.6242730616212239]
	TIME [epoch: 1.36 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6842358490039503		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.6842358490039503 | validation: 0.7199322873697179]
	TIME [epoch: 1.36 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6872603634751303		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.6872603634751303 | validation: 0.6168757797726407]
	TIME [epoch: 1.36 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7161995215481209		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.7161995215481209 | validation: 0.9942508352270285]
	TIME [epoch: 1.36 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7691667160111786		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.7691667160111786 | validation: 0.5664689829086139]
	TIME [epoch: 1.36 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6969330958246933		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.6969330958246933 | validation: 0.7302046234966572]
	TIME [epoch: 1.36 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6271288171351593		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.6271288171351593 | validation: 0.5370338907745972]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5885507131025378		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.5885507131025378 | validation: 0.6392563394212765]
	TIME [epoch: 1.36 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6008401717345647		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.6008401717345647 | validation: 0.6133731350781126]
	TIME [epoch: 1.36 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6787835210967886		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.6787835210967886 | validation: 0.7263362168301604]
	TIME [epoch: 1.36 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7224288479475129		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.7224288479475129 | validation: 0.5778328060877821]
	TIME [epoch: 1.36 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.650650196897094		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.650650196897094 | validation: 0.7076812804266945]
	TIME [epoch: 1.36 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.623931849472987		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.623931849472987 | validation: 0.5677844203675102]
	TIME [epoch: 1.36 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6517279347857107		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.6517279347857107 | validation: 0.9281230667837003]
	TIME [epoch: 1.36 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7142596368800059		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.7142596368800059 | validation: 0.5817498233549748]
	TIME [epoch: 1.35 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6149221038325469		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.6149221038325469 | validation: 0.6154563835801659]
	TIME [epoch: 1.36 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5821367608294049		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.5821367608294049 | validation: 0.5656419674305131]
	TIME [epoch: 1.35 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.594940077483132		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.594940077483132 | validation: 0.6373867456358218]
	TIME [epoch: 1.35 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6401622379264685		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.6401622379264685 | validation: 0.5635484460876483]
	TIME [epoch: 1.35 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6100764416445357		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.6100764416445357 | validation: 0.6453272285570085]
	TIME [epoch: 1.35 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5930140521005683		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.5930140521005683 | validation: 0.5116814540723235]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6048097011116405		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.6048097011116405 | validation: 0.7884924134018856]
	TIME [epoch: 1.35 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6356833909504689		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.6356833909504689 | validation: 0.5349310056083251]
	TIME [epoch: 1.35 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6287550440177665		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.6287550440177665 | validation: 0.8107209142976727]
	TIME [epoch: 1.35 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6302798566079163		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.6302798566079163 | validation: 0.5361263460059071]
	TIME [epoch: 1.35 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5345934194068653		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.5345934194068653 | validation: 0.5203269712542752]
	TIME [epoch: 1.35 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5327969853432317		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.5327969853432317 | validation: 0.5401388011406507]
	TIME [epoch: 1.35 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5304627654527605		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.5304627654527605 | validation: 0.5330365381398406]
	TIME [epoch: 1.35 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5357305436150628		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.5357305436150628 | validation: 0.5509293905672531]
	TIME [epoch: 1.35 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.568231832420438		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.568231832420438 | validation: 0.7340659753856449]
	TIME [epoch: 1.35 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6454729388117		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.6454729388117 | validation: 0.5469154013742242]
	TIME [epoch: 1.35 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.625497193301585		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.625497193301585 | validation: 0.7000926861888526]
	TIME [epoch: 1.36 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5786331510864743		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.5786331510864743 | validation: 0.49295571573980657]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_317.pth
	Model improved!!!
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.537231916859241		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.537231916859241 | validation: 0.6349644598998851]
	TIME [epoch: 1.35 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5241306206809937		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.5241306206809937 | validation: 0.5056998486833351]
	TIME [epoch: 1.35 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49980384836879693		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.49980384836879693 | validation: 0.5585559077942117]
	TIME [epoch: 1.35 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4953541282755838		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.4953541282755838 | validation: 0.49671860962937536]
	TIME [epoch: 1.35 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4959881140434619		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.4959881140434619 | validation: 0.5728647607532601]
	TIME [epoch: 1.35 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5331458277981745		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.5331458277981745 | validation: 0.5344135147854285]
	TIME [epoch: 1.35 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6033198393007415		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.6033198393007415 | validation: 0.7221611395768961]
	TIME [epoch: 1.35 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6361230269088568		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.6361230269088568 | validation: 0.4723587100593517]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_325.pth
	Model improved!!!
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5275682757792697		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.5275682757792697 | validation: 0.5042562771890893]
	TIME [epoch: 1.36 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4636147224707574		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.4636147224707574 | validation: 0.4196280313379]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_327.pth
	Model improved!!!
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4436501108237931		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.4436501108237931 | validation: 0.5709707089935397]
	TIME [epoch: 1.36 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4622243908513312		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.4622243908513312 | validation: 0.5410788834712706]
	TIME [epoch: 1.36 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5550941261725032		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.5550941261725032 | validation: 0.8244152372266541]
	TIME [epoch: 1.36 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6206749470963528		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.6206749470963528 | validation: 0.4492284546746487]
	TIME [epoch: 1.36 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49886541421810954		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.49886541421810954 | validation: 0.5005839400130533]
	TIME [epoch: 1.36 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4820271048090573		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.4820271048090573 | validation: 0.5112881714086186]
	TIME [epoch: 1.35 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4986798956088006		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.4986798956088006 | validation: 0.5349216456040035]
	TIME [epoch: 1.35 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5157590332305155		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.5157590332305155 | validation: 0.4622239124576717]
	TIME [epoch: 1.35 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4635618455247716		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.4635618455247716 | validation: 0.47674214194278375]
	TIME [epoch: 1.35 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4430721454264295		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.4430721454264295 | validation: 0.4599517280350302]
	TIME [epoch: 1.35 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44928007408898296		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.44928007408898296 | validation: 0.5866063259291852]
	TIME [epoch: 1.35 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48023865133407856		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.48023865133407856 | validation: 0.4970909725794832]
	TIME [epoch: 1.35 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.509712337910241		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.509712337910241 | validation: 0.6318562536921648]
	TIME [epoch: 1.35 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5072469864866413		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.5072469864866413 | validation: 0.43897643885279125]
	TIME [epoch: 1.35 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4773043705386213		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.4773043705386213 | validation: 0.6156332597293795]
	TIME [epoch: 1.35 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46765714661738533		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.46765714661738533 | validation: 0.38862808885989697]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44980865855319024		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.44980865855319024 | validation: 0.5701967476307123]
	TIME [epoch: 1.36 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44589653533113616		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.44589653533113616 | validation: 0.4346448036431012]
	TIME [epoch: 1.36 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43354016139080137		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.43354016139080137 | validation: 0.4770578143551343]
	TIME [epoch: 1.36 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44692204360517623		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.44692204360517623 | validation: 0.4742137811433881]
	TIME [epoch: 1.36 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46113368795938897		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.46113368795938897 | validation: 0.4443438892089696]
	TIME [epoch: 1.36 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45264701260050877		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.45264701260050877 | validation: 0.44323369276120045]
	TIME [epoch: 1.36 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4053250632219709		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.4053250632219709 | validation: 0.4639821555259931]
	TIME [epoch: 1.36 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40028011718074846		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.40028011718074846 | validation: 0.4361202054614876]
	TIME [epoch: 1.36 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4369717749053953		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.4369717749053953 | validation: 0.6449018059442718]
	TIME [epoch: 1.36 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5100271542655243		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.5100271542655243 | validation: 0.4659861985300926]
	TIME [epoch: 1.36 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.490679964949693		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.490679964949693 | validation: 0.5782897108233381]
	TIME [epoch: 1.35 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4365131632915759		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.4365131632915759 | validation: 0.3718335742998616]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37767409555548775		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.37767409555548775 | validation: 0.47675236153712874]
	TIME [epoch: 1.36 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3740681556547467		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.3740681556547467 | validation: 0.4137453872048904]
	TIME [epoch: 1.36 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4037701754604305		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.4037701754604305 | validation: 0.5034056545467154]
	TIME [epoch: 1.36 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4684428006549942		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.4684428006549942 | validation: 0.4754957306840181]
	TIME [epoch: 1.36 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4590118664225433		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.4590118664225433 | validation: 0.4292498337229577]
	TIME [epoch: 1.36 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39327472827042953		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.39327472827042953 | validation: 0.39777250046811485]
	TIME [epoch: 1.36 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.351293928479714		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.351293928479714 | validation: 0.42076176060418674]
	TIME [epoch: 1.36 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3512537959610781		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.3512537959610781 | validation: 0.42120421216869597]
	TIME [epoch: 1.35 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38863958095452034		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.38863958095452034 | validation: 0.6336823421370581]
	TIME [epoch: 1.36 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49546334155711197		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.49546334155711197 | validation: 0.4791484317006055]
	TIME [epoch: 1.36 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4996135036664572		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.4996135036664572 | validation: 0.508883831884933]
	TIME [epoch: 1.36 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39119709768010025		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.39119709768010025 | validation: 0.3497300101107874]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_367.pth
	Model improved!!!
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33111760579035204		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.33111760579035204 | validation: 0.45251312741194716]
	TIME [epoch: 1.36 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3446404344458314		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.3446404344458314 | validation: 0.3911963724439623]
	TIME [epoch: 1.36 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38962611709513456		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.38962611709513456 | validation: 0.5507369995452408]
	TIME [epoch: 1.36 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41089101278014634		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.41089101278014634 | validation: 0.42629355098126454]
	TIME [epoch: 1.36 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4324158606476439		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.4324158606476439 | validation: 0.42106540119147395]
	TIME [epoch: 1.35 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.423467546812649		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.423467546812649 | validation: 0.37894381871749605]
	TIME [epoch: 1.36 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33730076819237426		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.33730076819237426 | validation: 0.372292339199804]
	TIME [epoch: 1.36 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31092037175717285		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.31092037175717285 | validation: 0.38787176215752206]
	TIME [epoch: 1.36 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31285852993082974		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.31285852993082974 | validation: 0.458488264562952]
	TIME [epoch: 1.36 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35614743871613613		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.35614743871613613 | validation: 0.4481383982418967]
	TIME [epoch: 1.36 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4415482390079172		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.4415482390079172 | validation: 0.6353695041619432]
	TIME [epoch: 1.35 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46355725382435053		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.46355725382435053 | validation: 0.3221288018446298]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_379.pth
	Model improved!!!
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3628697424063607		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.3628697424063607 | validation: 0.4050275168280588]
	TIME [epoch: 1.36 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31550021103688175		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.31550021103688175 | validation: 0.3803871988555236]
	TIME [epoch: 1.36 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3336502166444403		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.3336502166444403 | validation: 0.4143212240018186]
	TIME [epoch: 1.36 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3699582044833602		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.3699582044833602 | validation: 0.4362120511613025]
	TIME [epoch: 1.36 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3993886416720154		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.3993886416720154 | validation: 0.41022635826445053]
	TIME [epoch: 1.35 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38648020291373864		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.38648020291373864 | validation: 0.3574690836658363]
	TIME [epoch: 1.36 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3180839866182725		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.3180839866182725 | validation: 0.4105221717734477]
	TIME [epoch: 1.36 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3037221189257043		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.3037221189257043 | validation: 0.35961797483687]
	TIME [epoch: 1.36 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32620777789193883		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.32620777789193883 | validation: 0.5231679309736198]
	TIME [epoch: 1.36 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37489431290661646		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.37489431290661646 | validation: 0.3742932808077386]
	TIME [epoch: 1.36 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36463130178503617		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.36463130178503617 | validation: 0.43844148492763824]
	TIME [epoch: 1.36 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3559671872830725		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.3559671872830725 | validation: 0.3741794684853954]
	TIME [epoch: 1.36 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3573547020938634		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.3573547020938634 | validation: 0.38702484562177525]
	TIME [epoch: 1.36 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3281492783929319		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.3281492783929319 | validation: 0.36665744565733993]
	TIME [epoch: 1.36 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31056066215009065		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.31056066215009065 | validation: 0.34153497958360746]
	TIME [epoch: 1.36 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31677103668383916		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.31677103668383916 | validation: 0.3847668136074173]
	TIME [epoch: 1.36 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30213976564656414		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.30213976564656414 | validation: 0.39069791213710786]
	TIME [epoch: 1.36 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3183410712377117		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.3183410712377117 | validation: 0.3858114662530035]
	TIME [epoch: 1.36 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33868931334947106		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.33868931334947106 | validation: 0.45277283514387356]
	TIME [epoch: 1.36 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34295325400425825		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.34295325400425825 | validation: 0.34825432399771716]
	TIME [epoch: 1.36 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3319430664584727		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.3319430664584727 | validation: 0.46398328660997756]
	TIME [epoch: 1.36 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3227488315123791		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.3227488315123791 | validation: 0.3025703950436289]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_401.pth
	Model improved!!!
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32564782241334006		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.32564782241334006 | validation: 0.4263991113802494]
	TIME [epoch: 1.36 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3053891975770119		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.3053891975770119 | validation: 0.35296556207971863]
	TIME [epoch: 1.36 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3062372496880326		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.3062372496880326 | validation: 0.36037662282492156]
	TIME [epoch: 1.36 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3069965653332509		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.3069965653332509 | validation: 0.3577532967077014]
	TIME [epoch: 1.36 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30014090420231404		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.30014090420231404 | validation: 0.34069840235890236]
	TIME [epoch: 1.36 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31363723877998245		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.31363723877998245 | validation: 0.3563311971881401]
	TIME [epoch: 1.36 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2911072548095984		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.2911072548095984 | validation: 0.4233192724831207]
	TIME [epoch: 1.36 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30167253238390707		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.30167253238390707 | validation: 0.3175798845093164]
	TIME [epoch: 1.36 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3169736836049218		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.3169736836049218 | validation: 0.4545917141071558]
	TIME [epoch: 1.36 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3278978039241916		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.3278978039241916 | validation: 0.32764353897600557]
	TIME [epoch: 1.36 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31162961947478374		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.31162961947478374 | validation: 0.35674545341433445]
	TIME [epoch: 1.36 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27195178438940276		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.27195178438940276 | validation: 0.3197077447092685]
	TIME [epoch: 1.36 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2568701419334337		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.2568701419334337 | validation: 0.31832280085320463]
	TIME [epoch: 1.36 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2645248734839457		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.2645248734839457 | validation: 0.3626155975281273]
	TIME [epoch: 1.36 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2860259906594837		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.2860259906594837 | validation: 0.3379946418060403]
	TIME [epoch: 1.36 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3095296724440667		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.3095296724440667 | validation: 0.33582419238822364]
	TIME [epoch: 1.36 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2856971502405385		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.2856971502405385 | validation: 0.381608548563585]
	TIME [epoch: 1.36 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2767266786203959		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.2767266786203959 | validation: 0.33514711155040744]
	TIME [epoch: 1.36 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30386860347368916		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.30386860347368916 | validation: 0.39794595176230135]
	TIME [epoch: 1.36 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2811264121727703		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.2811264121727703 | validation: 0.3041690347226593]
	TIME [epoch: 1.36 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27042624145580446		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.27042624145580446 | validation: 0.3906027675981778]
	TIME [epoch: 1.36 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2539842360771778		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.2539842360771778 | validation: 0.3150969694249506]
	TIME [epoch: 1.36 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2511656753082044		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.2511656753082044 | validation: 0.3510444252400102]
	TIME [epoch: 1.36 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24599948167067115		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.24599948167067115 | validation: 0.2940118277270933]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_425.pth
	Model improved!!!
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2674858836308495		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.2674858836308495 | validation: 0.3884364118719802]
	TIME [epoch: 1.35 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2987449601289949		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.2987449601289949 | validation: 0.3319369411496068]
	TIME [epoch: 1.36 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32583419787403245		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.32583419787403245 | validation: 0.3419782517353131]
	TIME [epoch: 1.36 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26636251431914915		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.26636251431914915 | validation: 0.2810823215184121]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_429.pth
	Model improved!!!
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21794269167524383		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.21794269167524383 | validation: 0.30595890601189146]
	TIME [epoch: 1.36 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20621727367175993		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.20621727367175993 | validation: 0.31781438729262174]
	TIME [epoch: 1.35 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22585277234321596		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.22585277234321596 | validation: 0.32136679047904054]
	TIME [epoch: 1.35 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26937732723652935		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.26937732723652935 | validation: 0.34562984161924054]
	TIME [epoch: 1.35 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.307160726750488		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.307160726750488 | validation: 0.3871905721749593]
	TIME [epoch: 1.35 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29246414325685427		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.29246414325685427 | validation: 0.28600435977567784]
	TIME [epoch: 1.35 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25425684892159117		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.25425684892159117 | validation: 0.36794110267510705]
	TIME [epoch: 1.35 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2246883383535347		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.2246883383535347 | validation: 0.25865015558398985]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_437.pth
	Model improved!!!
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21685724589328448		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.21685724589328448 | validation: 0.36139998486765484]
	TIME [epoch: 1.36 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23529140825442355		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.23529140825442355 | validation: 0.2910494011384025]
	TIME [epoch: 1.36 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26830402203000775		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.26830402203000775 | validation: 0.35081894798258717]
	TIME [epoch: 1.36 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2704530014673605		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.2704530014673605 | validation: 0.303702759467103]
	TIME [epoch: 1.35 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27467531413510166		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.27467531413510166 | validation: 0.2584247663564342]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_442.pth
	Model improved!!!
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27018458978788107		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.27018458978788107 | validation: 0.29382559597884217]
	TIME [epoch: 1.36 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21124000645142996		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.21124000645142996 | validation: 0.2694204355622515]
	TIME [epoch: 1.36 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18509602279900902		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.18509602279900902 | validation: 0.2661933750237497]
	TIME [epoch: 1.36 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18877319059643		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.18877319059643 | validation: 0.3233692645153125]
	TIME [epoch: 1.36 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21793644876968693		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.21793644876968693 | validation: 0.3103202229871153]
	TIME [epoch: 1.35 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28365013921387183		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.28365013921387183 | validation: 0.38538135695765635]
	TIME [epoch: 1.36 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27544627931858834		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.27544627931858834 | validation: 0.2531304049042004]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_449.pth
	Model improved!!!
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2522148253380372		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.2522148253380372 | validation: 0.289763462753147]
	TIME [epoch: 1.35 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21636148402727948		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.21636148402727948 | validation: 0.3088282644521941]
	TIME [epoch: 1.36 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22892259299312995		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.22892259299312995 | validation: 0.29427780240159745]
	TIME [epoch: 1.35 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24782209684126721		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.24782209684126721 | validation: 0.2693431630700205]
	TIME [epoch: 1.35 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2266339160672188		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.2266339160672188 | validation: 0.3023093218552415]
	TIME [epoch: 1.35 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21809439668762776		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.21809439668762776 | validation: 0.26326291345820296]
	TIME [epoch: 1.35 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22433434219024356		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.22433434219024356 | validation: 0.33901721215715097]
	TIME [epoch: 1.35 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22953521659257514		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.22953521659257514 | validation: 0.2602895600556412]
	TIME [epoch: 1.35 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2308441145898987		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.2308441145898987 | validation: 0.28976196891516887]
	TIME [epoch: 1.35 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2079049108830089		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.2079049108830089 | validation: 0.25599143454128226]
	TIME [epoch: 1.35 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20048594143491763		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.20048594143491763 | validation: 0.2745827482849135]
	TIME [epoch: 1.35 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19071897519933154		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.19071897519933154 | validation: 0.26487141614846704]
	TIME [epoch: 1.35 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2159079830639612		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.2159079830639612 | validation: 0.2749054850900598]
	TIME [epoch: 1.35 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2423466543984729		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.2423466543984729 | validation: 0.2957523856902748]
	TIME [epoch: 1.35 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21769000168270217		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.21769000168270217 | validation: 0.25474462616701654]
	TIME [epoch: 1.35 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20480746418089724		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.20480746418089724 | validation: 0.27311000479423925]
	TIME [epoch: 1.35 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20782498968409258		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.20782498968409258 | validation: 0.2746549899143599]
	TIME [epoch: 1.35 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19232723340427918		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.19232723340427918 | validation: 0.23840946646543926]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19982669487989987		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.19982669487989987 | validation: 0.3983097637327002]
	TIME [epoch: 1.36 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23737071060193093		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.23737071060193093 | validation: 0.23258126799718748]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_469.pth
	Model improved!!!
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22503531219110126		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.22503531219110126 | validation: 0.28480999602609514]
	TIME [epoch: 1.36 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19091807196382038		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.19091807196382038 | validation: 0.2457177959431096]
	TIME [epoch: 1.36 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19985382111198885		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.19985382111198885 | validation: 0.248947609378057]
	TIME [epoch: 1.36 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2081374340955385		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.2081374340955385 | validation: 0.2537718286703539]
	TIME [epoch: 1.36 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20409942969913872		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.20409942969913872 | validation: 0.25874945115808184]
	TIME [epoch: 1.36 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1939977597579262		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.1939977597579262 | validation: 0.24617223570237387]
	TIME [epoch: 1.36 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19674997022638976		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.19674997022638976 | validation: 0.2678871726664503]
	TIME [epoch: 1.36 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18628521212409702		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.18628521212409702 | validation: 0.23294507146998536]
	TIME [epoch: 1.36 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19639126484874248		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.19639126484874248 | validation: 0.271497320907369]
	TIME [epoch: 1.36 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19255975038165532		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.19255975038165532 | validation: 0.2499979813779036]
	TIME [epoch: 1.36 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20332011548464834		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.20332011548464834 | validation: 0.28844951861981727]
	TIME [epoch: 1.36 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1944091774649975		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.1944091774649975 | validation: 0.2298365955594436]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_481.pth
	Model improved!!!
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19851015096221972		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.19851015096221972 | validation: 0.2683332159248853]
	TIME [epoch: 1.35 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1654923232369657		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.1654923232369657 | validation: 0.21111932862185703]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_483.pth
	Model improved!!!
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1584272041725524		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.1584272041725524 | validation: 0.2757935188882558]
	TIME [epoch: 1.36 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1500638347272711		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.1500638347272711 | validation: 0.19298041060567808]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_485.pth
	Model improved!!!
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16190341197014554		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.16190341197014554 | validation: 0.2752703890415648]
	TIME [epoch: 1.36 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1663692578316821		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.1663692578316821 | validation: 0.22846385703180128]
	TIME [epoch: 1.36 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22077310020755497		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.22077310020755497 | validation: 0.31707575861448734]
	TIME [epoch: 1.36 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28580304028884307		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.28580304028884307 | validation: 0.27967901299784903]
	TIME [epoch: 1.36 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24304735006770373		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.24304735006770373 | validation: 0.2343737267718372]
	TIME [epoch: 1.36 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14187886307765668		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.14187886307765668 | validation: 0.20352521450574002]
	TIME [epoch: 1.36 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13196358733793756		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.13196358733793756 | validation: 0.2225183549277263]
	TIME [epoch: 1.36 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12927079513488018		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.12927079513488018 | validation: 0.209791550310546]
	TIME [epoch: 1.36 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15093825875530242		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.15093825875530242 | validation: 0.2479672386179591]
	TIME [epoch: 1.36 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19082505114850445		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.19082505114850445 | validation: 0.27203799495665826]
	TIME [epoch: 1.36 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2528877263272085		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.2528877263272085 | validation: 0.2728706493736472]
	TIME [epoch: 1.36 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24738103672640244		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.24738103672640244 | validation: 0.22561149388399812]
	TIME [epoch: 1.36 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2046543803208739		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.2046543803208739 | validation: 0.25252241946945125]
	TIME [epoch: 1.36 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.149801377209537		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.149801377209537 | validation: 0.17627100174222282]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_499.pth
	Model improved!!!
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13891564535576584		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.13891564535576584 | validation: 0.22018884474808262]
	TIME [epoch: 1.35 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14206038909465638		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.14206038909465638 | validation: 0.20424622740790982]
	TIME [epoch: 173 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18016948967849739		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.18016948967849739 | validation: 0.27829646605193464]
	TIME [epoch: 2.69 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18549013309883286		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.18549013309883286 | validation: 0.19614060088928256]
	TIME [epoch: 2.69 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1823001118514143		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.1823001118514143 | validation: 0.22848658001075928]
	TIME [epoch: 2.68 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16949286143083928		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.16949286143083928 | validation: 0.22775390968488987]
	TIME [epoch: 2.69 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17703616884065773		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.17703616884065773 | validation: 0.220848115153388]
	TIME [epoch: 2.68 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18298099605975104		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.18298099605975104 | validation: 0.20089480585318992]
	TIME [epoch: 2.69 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16413403768269752		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.16413403768269752 | validation: 0.2514223871802374]
	TIME [epoch: 2.68 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1596743870615929		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.1596743870615929 | validation: 0.2032898032574873]
	TIME [epoch: 2.69 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16626164069863983		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.16626164069863983 | validation: 0.26980938465666526]
	TIME [epoch: 2.68 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17994307776331128		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.17994307776331128 | validation: 0.2123087375052072]
	TIME [epoch: 2.69 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16675772098022804		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.16675772098022804 | validation: 0.1740438551425725]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_512.pth
	Model improved!!!
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14217984312741108		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.14217984312741108 | validation: 0.2106631846326224]
	TIME [epoch: 2.69 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1415725223571388		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.1415725223571388 | validation: 0.16757213370345309]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_514.pth
	Model improved!!!
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14379056561710832		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.14379056561710832 | validation: 0.22182822664193858]
	TIME [epoch: 2.68 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14199866210793716		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.14199866210793716 | validation: 0.19062813097470588]
	TIME [epoch: 2.68 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14989854506160222		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.14989854506160222 | validation: 0.21104674353917263]
	TIME [epoch: 2.68 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1746137890387105		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.1746137890387105 | validation: 0.293178803273647]
	TIME [epoch: 2.68 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2095147086112346		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.2095147086112346 | validation: 0.20243847294296097]
	TIME [epoch: 2.69 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22097223401587662		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.22097223401587662 | validation: 0.23997973006563625]
	TIME [epoch: 2.68 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14959441077688537		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.14959441077688537 | validation: 0.18178689840849888]
	TIME [epoch: 2.68 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12336266482022451		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.12336266482022451 | validation: 0.19668779161770955]
	TIME [epoch: 2.68 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12413579626585389		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.12413579626585389 | validation: 0.19880013725570317]
	TIME [epoch: 2.68 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1425856474161701		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.1425856474161701 | validation: 0.20400583808241965]
	TIME [epoch: 2.68 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15519921036676002		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.15519921036676002 | validation: 0.21601262761593484]
	TIME [epoch: 2.68 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19716157942395618		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.19716157942395618 | validation: 0.2362360285316016]
	TIME [epoch: 2.68 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.183308283305318		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.183308283305318 | validation: 0.2052477720285047]
	TIME [epoch: 2.68 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17372585138919866		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.17372585138919866 | validation: 0.1894882930073785]
	TIME [epoch: 2.68 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13168562841559686		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.13168562841559686 | validation: 0.17285516973937404]
	TIME [epoch: 2.68 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1191561205755658		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.1191561205755658 | validation: 0.19664643872758447]
	TIME [epoch: 2.68 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12052358325575728		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.12052358325575728 | validation: 0.1758602244608335]
	TIME [epoch: 2.68 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16009032416149435		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.16009032416149435 | validation: 0.2717688956836511]
	TIME [epoch: 2.68 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1730281885987737		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.1730281885987737 | validation: 0.18574456249863316]
	TIME [epoch: 2.68 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1671848519779983		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.1671848519779983 | validation: 0.18162934936228423]
	TIME [epoch: 2.68 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12925899812719402		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.12925899812719402 | validation: 0.18055927648256587]
	TIME [epoch: 2.68 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12173436624197055		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.12173436624197055 | validation: 0.19329632245861655]
	TIME [epoch: 2.68 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1444544365098011		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.1444544365098011 | validation: 0.21036253752455758]
	TIME [epoch: 2.68 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18838362655527924		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.18838362655527924 | validation: 0.22863442289735023]
	TIME [epoch: 2.68 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1696696647290404		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.1696696647290404 | validation: 0.15889245542078156]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_539.pth
	Model improved!!!
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13324003923958158		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.13324003923958158 | validation: 0.2036301355971849]
	TIME [epoch: 2.68 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1207687556775289		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.1207687556775289 | validation: 0.15694834241451808]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_541.pth
	Model improved!!!
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13014642797241904		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.13014642797241904 | validation: 0.24545385618411483]
	TIME [epoch: 2.68 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14373801184796833		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.14373801184796833 | validation: 0.17411458504425825]
	TIME [epoch: 2.68 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15580643705352523		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.15580643705352523 | validation: 0.17797184759628568]
	TIME [epoch: 2.68 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1426787335064343		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.1426787335064343 | validation: 0.17838493380640266]
	TIME [epoch: 2.68 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13566366883333544		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.13566366883333544 | validation: 0.18021246609386044]
	TIME [epoch: 2.68 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1281669510130301		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.1281669510130301 | validation: 0.18345573473970153]
	TIME [epoch: 2.68 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14343540823994963		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.14343540823994963 | validation: 0.20703507210936262]
	TIME [epoch: 2.68 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15748209069907862		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.15748209069907862 | validation: 0.18045572947623523]
	TIME [epoch: 2.68 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16041204757050423		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.16041204757050423 | validation: 0.19707456336954277]
	TIME [epoch: 2.68 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12279993411126362		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.12279993411126362 | validation: 0.1407512486076888]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_551.pth
	Model improved!!!
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11318913132861629		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.11318913132861629 | validation: 0.21186532667504807]
	TIME [epoch: 2.69 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11903965137111142		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.11903965137111142 | validation: 0.15359155102474134]
	TIME [epoch: 2.68 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12408002916708398		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.12408002916708398 | validation: 0.22653800660813933]
	TIME [epoch: 2.68 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13219743919673121		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.13219743919673121 | validation: 0.1641960610516618]
	TIME [epoch: 2.68 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1583275651927765		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.1583275651927765 | validation: 0.20869933864902493]
	TIME [epoch: 2.68 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17489449314375513		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.17489449314375513 | validation: 0.1787029009294555]
	TIME [epoch: 2.69 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14998435889143483		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.14998435889143483 | validation: 0.16846178312422044]
	TIME [epoch: 2.68 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11581887276119854		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.11581887276119854 | validation: 0.1585908233369521]
	TIME [epoch: 2.68 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10993681438657796		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.10993681438657796 | validation: 0.16160225710508613]
	TIME [epoch: 2.68 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1170186850043973		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.1170186850043973 | validation: 0.17299532165687637]
	TIME [epoch: 2.68 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12972801664755965		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.12972801664755965 | validation: 0.20032841664418913]
	TIME [epoch: 2.68 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14323722286042925		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.14323722286042925 | validation: 0.18542911300611653]
	TIME [epoch: 2.69 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16574005858336224		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.16574005858336224 | validation: 0.17558176708202358]
	TIME [epoch: 2.68 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13495133070441034		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.13495133070441034 | validation: 0.1673834276082748]
	TIME [epoch: 2.68 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11838997704760712		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.11838997704760712 | validation: 0.16335423176872774]
	TIME [epoch: 2.68 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10922495422758836		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.10922495422758836 | validation: 0.13976278552959884]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_567.pth
	Model improved!!!
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10364332318387096		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.10364332318387096 | validation: 0.2002207399247638]
	TIME [epoch: 2.68 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11915342627384852		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.11915342627384852 | validation: 0.14871984701849533]
	TIME [epoch: 2.69 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14266130995516255		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.14266130995516255 | validation: 0.2382801571895092]
	TIME [epoch: 2.68 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15496155565181485		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.15496155565181485 | validation: 0.16488447291605285]
	TIME [epoch: 2.68 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13203773148539533		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.13203773148539533 | validation: 0.16355903670891736]
	TIME [epoch: 2.68 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12088534636125374		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.12088534636125374 | validation: 0.1688369494382966]
	TIME [epoch: 2.68 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11204257043241433		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.11204257043241433 | validation: 0.15410377130643066]
	TIME [epoch: 2.69 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11599644920882476		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.11599644920882476 | validation: 0.1728019742133088]
	TIME [epoch: 2.68 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12898252516102648		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.12898252516102648 | validation: 0.18598242227109213]
	TIME [epoch: 2.68 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13617963197685984		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.13617963197685984 | validation: 0.15728222568493738]
	TIME [epoch: 2.68 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1291567246649877		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.1291567246649877 | validation: 0.189290907423268]
	TIME [epoch: 2.69 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11088505071659523		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.11088505071659523 | validation: 0.12409147634849536]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_579.pth
	Model improved!!!
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1048578640405785		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.1048578640405785 | validation: 0.20039748660743797]
	TIME [epoch: 2.68 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11523981907798968		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.11523981907798968 | validation: 0.14665653877351606]
	TIME [epoch: 2.68 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1195920654456941		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.1195920654456941 | validation: 0.19830832166743603]
	TIME [epoch: 2.68 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12731572846674374		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.12731572846674374 | validation: 0.13878754183942288]
	TIME [epoch: 2.68 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11748859136782497		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.11748859136782497 | validation: 0.15108068751646808]
	TIME [epoch: 2.68 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11131694261250619		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.11131694261250619 | validation: 0.16871829063446292]
	TIME [epoch: 2.68 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12618505294580423		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.12618505294580423 | validation: 0.16523100294996196]
	TIME [epoch: 2.69 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13073424046124849		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.13073424046124849 | validation: 0.14980748382218742]
	TIME [epoch: 2.68 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1168484536010114		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.1168484536010114 | validation: 0.1622590345572454]
	TIME [epoch: 2.69 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10660953082644781		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.10660953082644781 | validation: 0.13268348531634264]
	TIME [epoch: 2.68 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10366401163557966		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.10366401163557966 | validation: 0.16938031216100302]
	TIME [epoch: 2.68 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10663781354780927		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.10663781354780927 | validation: 0.1478252815438788]
	TIME [epoch: 2.68 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10835541281699407		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.10835541281699407 | validation: 0.18665447335240037]
	TIME [epoch: 2.68 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1292845265671747		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.1292845265671747 | validation: 0.15641260804301543]
	TIME [epoch: 2.68 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12679847589279106		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.12679847589279106 | validation: 0.14845806143662443]
	TIME [epoch: 2.68 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11625295036323202		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.11625295036323202 | validation: 0.14314786372599173]
	TIME [epoch: 2.68 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10956876855503658		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.10956876855503658 | validation: 0.13871427428755478]
	TIME [epoch: 2.69 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09394678901123978		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.09394678901123978 | validation: 0.1450839835051895]
	TIME [epoch: 2.68 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09536742486900135		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.09536742486900135 | validation: 0.1606403656742088]
	TIME [epoch: 2.68 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10034662771793432		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.10034662771793432 | validation: 0.1490449504128784]
	TIME [epoch: 2.68 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.117887663195534		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.117887663195534 | validation: 0.1895669086149484]
	TIME [epoch: 2.68 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13912811567568267		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.13912811567568267 | validation: 0.16658459918066976]
	TIME [epoch: 2.68 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13342477491930793		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.13342477491930793 | validation: 0.17010086827212845]
	TIME [epoch: 2.68 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10816828487268892		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.10816828487268892 | validation: 0.12604842379711836]
	TIME [epoch: 2.68 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09251090825561402		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.09251090825561402 | validation: 0.13974255708642558]
	TIME [epoch: 2.68 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09225606620827857		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.09225606620827857 | validation: 0.12141723696163993]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_605.pth
	Model improved!!!
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08739580959216443		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.08739580959216443 | validation: 0.1311532156666666]
	TIME [epoch: 2.68 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0968004370824296		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.0968004370824296 | validation: 0.14329327835927094]
	TIME [epoch: 2.67 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10743921255551296		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.10743921255551296 | validation: 0.1618164189781077]
	TIME [epoch: 2.67 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1397709680631868		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.1397709680631868 | validation: 0.16093279615871228]
	TIME [epoch: 2.67 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13885849993727115		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.13885849993727115 | validation: 0.16742412373587867]
	TIME [epoch: 2.67 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10703561221495371		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.10703561221495371 | validation: 0.11606995900297089]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_611.pth
	Model improved!!!
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0960568541134165		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.0960568541134165 | validation: 0.17845475949172468]
	TIME [epoch: 2.68 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08931924495593084		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.08931924495593084 | validation: 0.11855155909205158]
	TIME [epoch: 2.68 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08245734844563148		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.08245734844563148 | validation: 0.13128921768829419]
	TIME [epoch: 2.69 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07899092810259178		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.07899092810259178 | validation: 0.117812641679647]
	TIME [epoch: 2.68 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08180879492325246		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.08180879492325246 | validation: 0.1419873713646373]
	TIME [epoch: 2.69 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09035660069001328		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.09035660069001328 | validation: 0.15096066711000777]
	TIME [epoch: 2.69 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13934027406614852		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.13934027406614852 | validation: 0.18841906603309655]
	TIME [epoch: 2.68 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1627038972213331		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.1627038972213331 | validation: 0.14826198592904985]
	TIME [epoch: 2.69 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13526936672897755		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.13526936672897755 | validation: 0.15076558480909413]
	TIME [epoch: 2.68 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09428350123700892		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.09428350123700892 | validation: 0.11382629908003664]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_621.pth
	Model improved!!!
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07731808100479326		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.07731808100479326 | validation: 0.12756219223717843]
	TIME [epoch: 2.69 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06993796615801687		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.06993796615801687 | validation: 0.11054097427196843]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_623.pth
	Model improved!!!
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06836005165082802		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.06836005165082802 | validation: 0.12012892049374405]
	TIME [epoch: 2.67 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0704218616387058		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.0704218616387058 | validation: 0.1108725880049522]
	TIME [epoch: 2.67 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07072809848689095		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.07072809848689095 | validation: 0.13691905292151407]
	TIME [epoch: 2.67 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06946219968023548		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.06946219968023548 | validation: 0.12223467584360166]
	TIME [epoch: 2.69 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07149627766572822		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.07149627766572822 | validation: 0.1311073468576278]
	TIME [epoch: 2.68 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07706158339461962		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.07706158339461962 | validation: 0.16225957057344267]
	TIME [epoch: 2.68 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1205035200768619		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.1205035200768619 | validation: 0.20448548116663431]
	TIME [epoch: 2.68 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2417148184707079		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.2417148184707079 | validation: 0.15017233430353188]
	TIME [epoch: 2.69 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11289342248253938		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.11289342248253938 | validation: 0.0911909621147471]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_632.pth
	Model improved!!!
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07338454018895045		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.07338454018895045 | validation: 0.13805338117014182]
	TIME [epoch: 2.68 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07020989774788017		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.07020989774788017 | validation: 0.10627799804349643]
	TIME [epoch: 2.67 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07003474243525049		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.07003474243525049 | validation: 0.10841266357393185]
	TIME [epoch: 2.67 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07121315042435096		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.07121315042435096 | validation: 0.10932505608878099]
	TIME [epoch: 2.67 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08790625073038878		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.08790625073038878 | validation: 0.187099231217869]
	TIME [epoch: 2.68 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1473132450761178		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.1473132450761178 | validation: 0.15770911354951878]
	TIME [epoch: 2.67 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1646650494407507		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.1646650494407507 | validation: 0.13565803010082855]
	TIME [epoch: 2.68 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09549162469980038		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.09549162469980038 | validation: 0.09599902687413295]
	TIME [epoch: 2.67 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07176973518440402		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.07176973518440402 | validation: 0.13274021733578634]
	TIME [epoch: 2.67 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06693149451084004		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.06693149451084004 | validation: 0.10158186291621858]
	TIME [epoch: 2.67 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06659544375023975		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.06659544375023975 | validation: 0.13335277795811587]
	TIME [epoch: 2.68 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07295182723490584		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.07295182723490584 | validation: 0.11210536400956952]
	TIME [epoch: 2.67 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08904256189738291		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.08904256189738291 | validation: 0.17866499220838408]
	TIME [epoch: 2.67 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11397680189807119		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.11397680189807119 | validation: 0.1328063775603364]
	TIME [epoch: 2.67 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12669602674275088		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.12669602674275088 | validation: 0.1506235816428301]
	TIME [epoch: 2.67 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11841898167699152		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.11841898167699152 | validation: 0.11990797810343277]
	TIME [epoch: 2.67 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09136635581578478		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.09136635581578478 | validation: 0.10423453439489738]
	TIME [epoch: 2.67 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07508500108462768		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.07508500108462768 | validation: 0.1237728697942401]
	TIME [epoch: 2.67 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06799269020381664		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.06799269020381664 | validation: 0.10368970721969824]
	TIME [epoch: 2.67 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06533122757839867		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.06533122757839867 | validation: 0.10545818464547058]
	TIME [epoch: 2.68 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06554687854606776		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.06554687854606776 | validation: 0.1431350706334782]
	TIME [epoch: 2.67 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0823879504310245		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.0823879504310245 | validation: 0.12002360748585271]
	TIME [epoch: 2.67 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10946063187489365		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.10946063187489365 | validation: 0.18134895647256233]
	TIME [epoch: 2.67 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13853788225761643		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.13853788225761643 | validation: 0.13420205393507795]
	TIME [epoch: 2.67 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10656581320781455		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.10656581320781455 | validation: 0.09646487176411661]
	TIME [epoch: 2.67 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07529569002060146		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.07529569002060146 | validation: 0.10558437033991996]
	TIME [epoch: 2.67 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062394985087854744		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.062394985087854744 | validation: 0.1087888245504713]
	TIME [epoch: 2.67 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06362841949855141		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.06362841949855141 | validation: 0.1120360640428839]
	TIME [epoch: 2.67 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05960976837844816		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.05960976837844816 | validation: 0.09953835482949508]
	TIME [epoch: 2.68 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060318771798977745		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.060318771798977745 | validation: 0.10253737635576604]
	TIME [epoch: 2.67 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059854245675303126		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.059854245675303126 | validation: 0.10316426336500553]
	TIME [epoch: 2.67 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061598061733785354		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.061598061733785354 | validation: 0.10499552670271024]
	TIME [epoch: 2.67 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06744124847545878		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.06744124847545878 | validation: 0.146565037291554]
	TIME [epoch: 2.67 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12960635442523794		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.12960635442523794 | validation: 0.25602582042268773]
	TIME [epoch: 2.67 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2172317990604371		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.2172317990604371 | validation: 0.12303751108226711]
	TIME [epoch: 2.67 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11771618599218857		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.11771618599218857 | validation: 0.13859708103763965]
	TIME [epoch: 2.67 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07618236987686239		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.07618236987686239 | validation: 0.08389927434610817]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_669.pth
	Model improved!!!
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0621449465447268		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.0621449465447268 | validation: 0.10910419515020937]
	TIME [epoch: 2.68 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0647851797057172		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.0647851797057172 | validation: 0.11212531544937183]
	TIME [epoch: 2.68 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07272904163682671		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.07272904163682671 | validation: 0.09516623033859845]
	TIME [epoch: 2.69 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08010818339128653		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.08010818339128653 | validation: 0.14441493364919977]
	TIME [epoch: 2.68 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09599803523542845		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.09599803523542845 | validation: 0.11887553966551037]
	TIME [epoch: 2.68 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09579374814886746		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.09579374814886746 | validation: 0.11288947056418462]
	TIME [epoch: 2.68 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0825500448460771		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.0825500448460771 | validation: 0.09239472316083708]
	TIME [epoch: 2.68 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07403690875745983		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.07403690875745983 | validation: 0.12168762831221805]
	TIME [epoch: 2.68 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06650675984726469		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.06650675984726469 | validation: 0.08563294362962345]
	TIME [epoch: 2.68 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0645425235936768		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.0645425235936768 | validation: 0.10517344762840507]
	TIME [epoch: 2.68 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06491710844966012		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.06491710844966012 | validation: 0.08557216107236942]
	TIME [epoch: 2.68 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06407829958703574		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.06407829958703574 | validation: 0.10455350586295786]
	TIME [epoch: 2.68 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07318816518157696		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.07318816518157696 | validation: 0.11442184461855574]
	TIME [epoch: 2.68 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09799465296585037		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.09799465296585037 | validation: 0.14901583072617425]
	TIME [epoch: 2.69 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12197762140181663		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.12197762140181663 | validation: 0.10898741371812314]
	TIME [epoch: 2.68 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09896112763889825		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.09896112763889825 | validation: 0.10438588030910348]
	TIME [epoch: 2.68 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07043720094058824		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.07043720094058824 | validation: 0.09341328732306627]
	TIME [epoch: 2.68 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05995518785622636		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.05995518785622636 | validation: 0.09750030004324968]
	TIME [epoch: 2.68 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05928438913256244		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.05928438913256244 | validation: 0.09360425919539858]
	TIME [epoch: 2.68 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05567078751846914		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.05567078751846914 | validation: 0.1065904336771658]
	TIME [epoch: 2.68 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05810657471622454		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.05810657471622454 | validation: 0.09688045726587141]
	TIME [epoch: 2.68 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05998798685549934		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.05998798685549934 | validation: 0.09568339898069472]
	TIME [epoch: 2.68 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06513707972254473		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.06513707972254473 | validation: 0.1034914152678971]
	TIME [epoch: 2.68 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09245064356704781		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.09245064356704781 | validation: 0.18953172102149532]
	TIME [epoch: 2.68 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1479863822327855		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.1479863822327855 | validation: 0.12284384854919637]
	TIME [epoch: 2.69 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11195619406617438		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.11195619406617438 | validation: 0.10195386228286019]
	TIME [epoch: 2.68 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07854416339676323		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.07854416339676323 | validation: 0.1059201425943121]
	TIME [epoch: 2.68 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057300322215355304		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.057300322215355304 | validation: 0.07800109289316005]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_697.pth
	Model improved!!!
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056032649346631325		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.056032649346631325 | validation: 0.08866654063742298]
	TIME [epoch: 2.67 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05626628661157772		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.05626628661157772 | validation: 0.08785863594571867]
	TIME [epoch: 2.67 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06122165772547681		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.06122165772547681 | validation: 0.10758891773331974]
	TIME [epoch: 2.67 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06851127815706956		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.06851127815706956 | validation: 0.10246140454981818]
	TIME [epoch: 2.68 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0761526629616827		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.0761526629616827 | validation: 0.12982293304244413]
	TIME [epoch: 2.69 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09994306924487564		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.09994306924487564 | validation: 0.11013228435363849]
	TIME [epoch: 2.68 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09790361927254262		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.09790361927254262 | validation: 0.1284465973588565]
	TIME [epoch: 2.68 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08971969278712759		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.08971969278712759 | validation: 0.10966625863991003]
	TIME [epoch: 2.69 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06390829217175502		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.06390829217175502 | validation: 0.08771341604404509]
	TIME [epoch: 2.68 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05245666384595933		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.05245666384595933 | validation: 0.09653414295304123]
	TIME [epoch: 2.68 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054533160451639126		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.054533160451639126 | validation: 0.08178145496250794]
	TIME [epoch: 2.69 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051846392231294516		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.051846392231294516 | validation: 0.10073289598506785]
	TIME [epoch: 2.69 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05421298091969088		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.05421298091969088 | validation: 0.08052643982713809]
	TIME [epoch: 2.67 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060694977642220344		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.060694977642220344 | validation: 0.1337501012835553]
	TIME [epoch: 2.67 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08115470656319876		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.08115470656319876 | validation: 0.11080573102297704]
	TIME [epoch: 2.67 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11091154955405066		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.11091154955405066 | validation: 0.13215790706776295]
	TIME [epoch: 2.67 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10537138560016977		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.10537138560016977 | validation: 0.09516616395718389]
	TIME [epoch: 2.67 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06903439876562109		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.06903439876562109 | validation: 0.09032691092402595]
	TIME [epoch: 2.67 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053029634134819696		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.053029634134819696 | validation: 0.07714979361831276]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_716.pth
	Model improved!!!
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049906072367093754		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.049906072367093754 | validation: 0.08581302980164687]
	TIME [epoch: 2.68 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051877576801263245		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.051877576801263245 | validation: 0.07505532361072205]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_718.pth
	Model improved!!!
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04921287476655488		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.04921287476655488 | validation: 0.07690900451662222]
	TIME [epoch: 2.67 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04884456116431597		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.04884456116431597 | validation: 0.09874677814071459]
	TIME [epoch: 2.67 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05309720355352033		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.05309720355352033 | validation: 0.08085078823806091]
	TIME [epoch: 2.67 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06679109634329967		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.06679109634329967 | validation: 0.12227538170433278]
	TIME [epoch: 2.67 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11007523758568914		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.11007523758568914 | validation: 0.12724437272852593]
	TIME [epoch: 2.67 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15134661931327714		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.15134661931327714 | validation: 0.10971229252834509]
	TIME [epoch: 2.67 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08620035416891263		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.08620035416891263 | validation: 0.08687327498371139]
	TIME [epoch: 2.67 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04971667545505415		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.04971667545505415 | validation: 0.07168915889416226]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_726.pth
	Model improved!!!
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05272059623140264		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.05272059623140264 | validation: 0.10090661220982203]
	TIME [epoch: 2.67 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054937823245055084		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.054937823245055084 | validation: 0.07014993704980299]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_728.pth
	Model improved!!!
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052813180595150694		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.052813180595150694 | validation: 0.08703522239455518]
	TIME [epoch: 2.67 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05029845077117974		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.05029845077117974 | validation: 0.08121854545180014]
	TIME [epoch: 2.67 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05584859362202579		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.05584859362202579 | validation: 0.10974282995952045]
	TIME [epoch: 2.67 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07291055624341648		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.07291055624341648 | validation: 0.10943599948184933]
	TIME [epoch: 2.67 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1043399107014611		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.1043399107014611 | validation: 0.11926226588554599]
	TIME [epoch: 2.67 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08842226700313706		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.08842226700313706 | validation: 0.08945770426909264]
	TIME [epoch: 2.67 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06330225217396698		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.06330225217396698 | validation: 0.08562224063081368]
	TIME [epoch: 2.67 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05265018602775388		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.05265018602775388 | validation: 0.08843705096884219]
	TIME [epoch: 2.67 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051440500682124544		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.051440500682124544 | validation: 0.08101610834454841]
	TIME [epoch: 2.67 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04969729432921088		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.04969729432921088 | validation: 0.09295889331116669]
	TIME [epoch: 2.67 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05803594587801149		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.05803594587801149 | validation: 0.08238978139361441]
	TIME [epoch: 2.67 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06862246109650082		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.06862246109650082 | validation: 0.12385114359591864]
	TIME [epoch: 2.67 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09717873152370884		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.09717873152370884 | validation: 0.09466877376025877]
	TIME [epoch: 2.67 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08821126824775767		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.08821126824775767 | validation: 0.10928707941831864]
	TIME [epoch: 2.67 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07978545754971754		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.07978545754971754 | validation: 0.07215500156537268]
	TIME [epoch: 2.67 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05511655327614878		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.05511655327614878 | validation: 0.08015002503229615]
	TIME [epoch: 2.68 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05237541564386174		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.05237541564386174 | validation: 0.07976127012772023]
	TIME [epoch: 2.67 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048061842284344305		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.048061842284344305 | validation: 0.08751125612387269]
	TIME [epoch: 2.67 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0466878387091654		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.0466878387091654 | validation: 0.07446744677234174]
	TIME [epoch: 2.67 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048591666123427066		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.048591666123427066 | validation: 0.0813050183017589]
	TIME [epoch: 2.68 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0511345459177695		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.0511345459177695 | validation: 0.07590327041527274]
	TIME [epoch: 2.67 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051833150696562524		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.051833150696562524 | validation: 0.09925015285980321]
	TIME [epoch: 2.68 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05786029082461924		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.05786029082461924 | validation: 0.08872510018202981]
	TIME [epoch: 2.67 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07237623992463484		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.07237623992463484 | validation: 0.1240828112339551]
	TIME [epoch: 2.67 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1047584358017209		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.1047584358017209 | validation: 0.10458611177765564]
	TIME [epoch: 2.67 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09626185424352598		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.09626185424352598 | validation: 0.10372516450477112]
	TIME [epoch: 2.67 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07577503993315535		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.07577503993315535 | validation: 0.0839422079310873]
	TIME [epoch: 2.67 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05339859148729981		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.05339859148729981 | validation: 0.08216952375680618]
	TIME [epoch: 2.67 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04738411399519166		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.04738411399519166 | validation: 0.07720857280812611]
	TIME [epoch: 2.67 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04631619760016662		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.04631619760016662 | validation: 0.0729370412794097]
	TIME [epoch: 2.67 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04617758627427586		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.04617758627427586 | validation: 0.07700924225337963]
	TIME [epoch: 2.67 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046007212514521244		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.046007212514521244 | validation: 0.08106849863476762]
	TIME [epoch: 2.67 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04639612760554111		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.04639612760554111 | validation: 0.07118234506118995]
	TIME [epoch: 2.67 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053769151850058206		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.053769151850058206 | validation: 0.09722507411329001]
	TIME [epoch: 2.67 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06251529674813762		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.06251529674813762 | validation: 0.09349193630604508]
	TIME [epoch: 2.67 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09032442569007658		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.09032442569007658 | validation: 0.12661402805396005]
	TIME [epoch: 2.67 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11825877261662315		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.11825877261662315 | validation: 0.09967405714762664]
	TIME [epoch: 2.67 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07237937015212521		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.07237937015212521 | validation: 0.07814512199644678]
	TIME [epoch: 2.67 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045379113718082155		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.045379113718082155 | validation: 0.06855850153109427]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_767.pth
	Model improved!!!
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04576312860933658		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.04576312860933658 | validation: 0.07987777075788231]
	TIME [epoch: 2.67 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051340907567341734		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.051340907567341734 | validation: 0.07508950802384912]
	TIME [epoch: 2.67 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04864476437275553		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.04864476437275553 | validation: 0.07881248255674018]
	TIME [epoch: 2.68 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05161452751946096		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.05161452751946096 | validation: 0.09546592271210706]
	TIME [epoch: 2.67 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05971590270971572		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.05971590270971572 | validation: 0.07941185896932562]
	TIME [epoch: 2.67 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0708787268944001		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.0708787268944001 | validation: 0.1087525657198225]
	TIME [epoch: 2.67 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08096184696234585		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.08096184696234585 | validation: 0.07968665093156894]
	TIME [epoch: 2.67 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0642116121111321		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.0642116121111321 | validation: 0.07848364450263828]
	TIME [epoch: 2.67 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04930917965945248		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.04930917965945248 | validation: 0.08290196471720011]
	TIME [epoch: 2.67 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04545952365942671		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.04545952365942671 | validation: 0.06817067515961432]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_777.pth
	Model improved!!!
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04692455236738113		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.04692455236738113 | validation: 0.06815395076720934]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_778.pth
	Model improved!!!
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04603074585524297		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.04603074585524297 | validation: 0.08378450219054472]
	TIME [epoch: 2.67 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0493262630422994		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.0493262630422994 | validation: 0.07721994279130669]
	TIME [epoch: 2.68 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05360235246766069		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.05360235246766069 | validation: 0.10277467085110824]
	TIME [epoch: 2.67 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07742803495394551		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.07742803495394551 | validation: 0.10005089476229961]
	TIME [epoch: 2.67 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08916932025755499		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.08916932025755499 | validation: 0.08361914619143836]
	TIME [epoch: 2.67 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062318704664657804		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.062318704664657804 | validation: 0.07687699992440143]
	TIME [epoch: 2.67 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04674078731707182		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.04674078731707182 | validation: 0.0834703938839302]
	TIME [epoch: 2.67 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04951927873640455		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.04951927873640455 | validation: 0.06318558883179497]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_786.pth
	Model improved!!!
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046381978797716085		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.046381978797716085 | validation: 0.08256013610427163]
	TIME [epoch: 2.67 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05131895710740591		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.05131895710740591 | validation: 0.07837053272354916]
	TIME [epoch: 2.67 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052500272021295605		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.052500272021295605 | validation: 0.09254238280915776]
	TIME [epoch: 2.67 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059035996888963674		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.059035996888963674 | validation: 0.08527294612104079]
	TIME [epoch: 2.67 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061279701295802874		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.061279701295802874 | validation: 0.08374826983451346]
	TIME [epoch: 2.68 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06231606706262367		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.06231606706262367 | validation: 0.07623586816437435]
	TIME [epoch: 2.67 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06491733880791321		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.06491733880791321 | validation: 0.08069458817630654]
	TIME [epoch: 2.67 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05860714312615924		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.05860714312615924 | validation: 0.06787392466621878]
	TIME [epoch: 2.67 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04833329615817717		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.04833329615817717 | validation: 0.07667555696459197]
	TIME [epoch: 2.67 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045606061131112256		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.045606061131112256 | validation: 0.0753742697529691]
	TIME [epoch: 2.67 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052950863335270146		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.052950863335270146 | validation: 0.07930589770228459]
	TIME [epoch: 2.67 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05689627239853765		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.05689627239853765 | validation: 0.08692702930246349]
	TIME [epoch: 2.67 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06351080796822756		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.06351080796822756 | validation: 0.10150466298120163]
	TIME [epoch: 2.67 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07530740277133545		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.07530740277133545 | validation: 0.07046367124434676]
	TIME [epoch: 2.67 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06557194773115545		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.06557194773115545 | validation: 0.08079196336185968]
	TIME [epoch: 2.67 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046139390440069546		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.046139390440069546 | validation: 0.06423761510171605]
	TIME [epoch: 2.68 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0432069093788621		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.0432069093788621 | validation: 0.07922099364583732]
	TIME [epoch: 2.67 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047562675890318415		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.047562675890318415 | validation: 0.06600948941692346]
	TIME [epoch: 2.67 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04517926220109796		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.04517926220109796 | validation: 0.07295086144529037]
	TIME [epoch: 2.67 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0467240717238803		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.0467240717238803 | validation: 0.08028093766946164]
	TIME [epoch: 2.67 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05311794105009723		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.05311794105009723 | validation: 0.07860517610423567]
	TIME [epoch: 2.67 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06519054813680898		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.06519054813680898 | validation: 0.08765783712696022]
	TIME [epoch: 2.67 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07211813678094905		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.07211813678094905 | validation: 0.07238210259798841]
	TIME [epoch: 2.67 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05214591506958663		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.05214591506958663 | validation: 0.07030166191220476]
	TIME [epoch: 2.67 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046311521511599806		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.046311521511599806 | validation: 0.07047797254268047]
	TIME [epoch: 2.67 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047408185452847765		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.047408185452847765 | validation: 0.07274642035482352]
	TIME [epoch: 2.67 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04586324365901016		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.04586324365901016 | validation: 0.07608847924596883]
	TIME [epoch: 2.67 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044575052188840954		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.044575052188840954 | validation: 0.06788251547845393]
	TIME [epoch: 2.67 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04128241855397612		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.04128241855397612 | validation: 0.0689272836090764]
	TIME [epoch: 2.67 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04575771128820401		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.04575771128820401 | validation: 0.07471964873456792]
	TIME [epoch: 2.67 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04611508534885232		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.04611508534885232 | validation: 0.07767498454179114]
	TIME [epoch: 2.67 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05099907729074371		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.05099907729074371 | validation: 0.09121352215783818]
	TIME [epoch: 2.67 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05988296548997198		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.05988296548997198 | validation: 0.08217022892010732]
	TIME [epoch: 2.67 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06975681094916057		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.06975681094916057 | validation: 0.09049806031404257]
	TIME [epoch: 2.67 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06998035811193314		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.06998035811193314 | validation: 0.08303614039213908]
	TIME [epoch: 2.67 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0546529060032009		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.0546529060032009 | validation: 0.06341424206947194]
	TIME [epoch: 2.67 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044177578042053564		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.044177578042053564 | validation: 0.06785544676658228]
	TIME [epoch: 2.67 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03989709794017936		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.03989709794017936 | validation: 0.06462901105369064]
	TIME [epoch: 2.68 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03943508516173083		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.03943508516173083 | validation: 0.06588747628139667]
	TIME [epoch: 2.67 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041510723287821974		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.041510723287821974 | validation: 0.07275707957704274]
	TIME [epoch: 2.67 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04337274196063836		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.04337274196063836 | validation: 0.07447413149711668]
	TIME [epoch: 2.67 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05186063676333269		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.05186063676333269 | validation: 0.07538773045868508]
	TIME [epoch: 2.67 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06317005313109593		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.06317005313109593 | validation: 0.10367505213018663]
	TIME [epoch: 2.67 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08332410846889145		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.08332410846889145 | validation: 0.08869043710583802]
	TIME [epoch: 2.67 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05754587438381858		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.05754587438381858 | validation: 0.06906068126454638]
	TIME [epoch: 2.67 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04350108748814265		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.04350108748814265 | validation: 0.0631591645481244]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_832.pth
	Model improved!!!
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039858749437142765		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.039858749437142765 | validation: 0.06729848809484962]
	TIME [epoch: 2.67 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039674232437939244		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.039674232437939244 | validation: 0.06272393456174259]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_834.pth
	Model improved!!!
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04127232617131295		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.04127232617131295 | validation: 0.06715378211739524]
	TIME [epoch: 2.67 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04434750741502791		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.04434750741502791 | validation: 0.07918127834649105]
	TIME [epoch: 2.67 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05417518919933981		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.05417518919933981 | validation: 0.09087188149530491]
	TIME [epoch: 2.67 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06761832012214562		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.06761832012214562 | validation: 0.0838226802742062]
	TIME [epoch: 2.67 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06729920179375141		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.06729920179375141 | validation: 0.08261290524256848]
	TIME [epoch: 2.67 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054605148784659524		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.054605148784659524 | validation: 0.06636931254602639]
	TIME [epoch: 2.67 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040209413950334644		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.040209413950334644 | validation: 0.06686390332055965]
	TIME [epoch: 2.67 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04259360799423994		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.04259360799423994 | validation: 0.06731161854918839]
	TIME [epoch: 2.67 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043781268370115714		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.043781268370115714 | validation: 0.06816395427234721]
	TIME [epoch: 2.67 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041656908463921276		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.041656908463921276 | validation: 0.07311662263613822]
	TIME [epoch: 2.67 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04466772481990208		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.04466772481990208 | validation: 0.06204719547334701]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_845.pth
	Model improved!!!
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04384190315326179		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.04384190315326179 | validation: 0.07767228244779123]
	TIME [epoch: 2.68 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045993016098882206		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.045993016098882206 | validation: 0.06544781569262338]
	TIME [epoch: 2.67 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05076736005549545		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.05076736005549545 | validation: 0.08338588471231333]
	TIME [epoch: 2.67 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053701674552719764		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.053701674552719764 | validation: 0.08421913806948816]
	TIME [epoch: 2.67 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05740854963846987		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.05740854963846987 | validation: 0.06774276018402094]
	TIME [epoch: 2.67 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05490966155520823		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.05490966155520823 | validation: 0.06731239978371732]
	TIME [epoch: 2.67 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04545032459419776		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.04545032459419776 | validation: 0.06696347026800474]
	TIME [epoch: 2.67 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044148708351081145		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.044148708351081145 | validation: 0.0584546656052069]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_853.pth
	Model improved!!!
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04019278341022869		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.04019278341022869 | validation: 0.06571253645325108]
	TIME [epoch: 2.67 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03859789260621388		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.03859789260621388 | validation: 0.06589760304694701]
	TIME [epoch: 2.67 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037629642536673995		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.037629642536673995 | validation: 0.05753958415403772]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_856.pth
	Model improved!!!
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03996548559049369		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.03996548559049369 | validation: 0.06931215495186445]
	TIME [epoch: 2.67 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04190276745580386		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.04190276745580386 | validation: 0.06759420416152004]
	TIME [epoch: 2.67 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04236545971609381		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.04236545971609381 | validation: 0.07357838218072649]
	TIME [epoch: 2.67 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04061953210383491		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.04061953210383491 | validation: 0.0634096303083288]
	TIME [epoch: 2.67 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04388083382875866		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.04388083382875866 | validation: 0.08343207044602191]
	TIME [epoch: 2.67 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06598085577650074		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.06598085577650074 | validation: 0.12982780552512527]
	TIME [epoch: 2.67 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0954804061180054		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.0954804061180054 | validation: 0.07765944274548581]
	TIME [epoch: 2.67 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05000207200114609		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.05000207200114609 | validation: 0.062283502049403056]
	TIME [epoch: 2.67 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04036886179011758		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.04036886179011758 | validation: 0.06818392574189344]
	TIME [epoch: 2.67 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046285143201740174		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.046285143201740174 | validation: 0.060331957144163666]
	TIME [epoch: 2.67 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0461227219147867		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.0461227219147867 | validation: 0.06859398867744468]
	TIME [epoch: 2.67 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04276449263903935		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.04276449263903935 | validation: 0.06517787322954828]
	TIME [epoch: 2.67 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03891319965899908		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.03891319965899908 | validation: 0.060957338699011446]
	TIME [epoch: 2.67 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04134819762658171		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.04134819762658171 | validation: 0.06709135785127496]
	TIME [epoch: 2.67 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03702481318241453		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.03702481318241453 | validation: 0.05852842932063495]
	TIME [epoch: 2.67 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036527553423475315		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.036527553423475315 | validation: 0.06539272204845302]
	TIME [epoch: 2.67 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03645168879732466		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.03645168879732466 | validation: 0.06173817931304091]
	TIME [epoch: 2.67 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03987082728455218		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.03987082728455218 | validation: 0.0741891029602679]
	TIME [epoch: 2.67 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044524722830384535		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.044524722830384535 | validation: 0.07722359401236717]
	TIME [epoch: 2.67 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06361652506673725		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.06361652506673725 | validation: 0.07559161768157639]
	TIME [epoch: 2.67 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07125783761166536		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.07125783761166536 | validation: 0.06368541449652354]
	TIME [epoch: 2.67 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048012705044815356		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.048012705044815356 | validation: 0.06641503278150901]
	TIME [epoch: 2.67 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03754443053011592		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.03754443053011592 | validation: 0.06317719291108927]
	TIME [epoch: 2.67 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040675951662890675		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.040675951662890675 | validation: 0.06758410960172624]
	TIME [epoch: 2.67 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0384837454839727		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.0384837454839727 | validation: 0.06003394917782935]
	TIME [epoch: 2.67 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039105468740982745		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.039105468740982745 | validation: 0.068367242038222]
	TIME [epoch: 2.67 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03986098872362625		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.03986098872362625 | validation: 0.07144585430202947]
	TIME [epoch: 2.67 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04288310948740767		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.04288310948740767 | validation: 0.06145806783229604]
	TIME [epoch: 2.67 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041564174411425264		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.041564174411425264 | validation: 0.06746719407677561]
	TIME [epoch: 2.67 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04597122199990941		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.04597122199990941 | validation: 0.06649937294210632]
	TIME [epoch: 2.67 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05187575133313712		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.05187575133313712 | validation: 0.07602796230459924]
	TIME [epoch: 2.67 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050013776991723165		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.050013776991723165 | validation: 0.058096415423761566]
	TIME [epoch: 2.67 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042208542671075905		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.042208542671075905 | validation: 0.06307061546800456]
	TIME [epoch: 2.67 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03753138584788092		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.03753138584788092 | validation: 0.062090938188827986]
	TIME [epoch: 2.67 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035890750480552895		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.035890750480552895 | validation: 0.061567468343484456]
	TIME [epoch: 2.67 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03682942842714603		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.03682942842714603 | validation: 0.06283666795071861]
	TIME [epoch: 2.67 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03890630599937123		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.03890630599937123 | validation: 0.059825690099939426]
	TIME [epoch: 2.67 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036732881174160396		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.036732881174160396 | validation: 0.05943404435066594]
	TIME [epoch: 2.67 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03749862622460957		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.03749862622460957 | validation: 0.07034611415373365]
	TIME [epoch: 2.67 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05237931617128348		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.05237931617128348 | validation: 0.08445584599331364]
	TIME [epoch: 2.66 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06228896158540431		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.06228896158540431 | validation: 0.07807655345829861]
	TIME [epoch: 2.67 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05372258082822328		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.05372258082822328 | validation: 0.05256646662286735]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_898.pth
	Model improved!!!
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03677857936048392		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.03677857936048392 | validation: 0.05930738790364107]
	TIME [epoch: 2.67 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03593922489830436		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.03593922489830436 | validation: 0.06462508356843248]
	TIME [epoch: 2.67 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03524073755225124		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.03524073755225124 | validation: 0.059204304076984016]
	TIME [epoch: 2.69 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035590285185818485		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.035590285185818485 | validation: 0.06003960137518621]
	TIME [epoch: 2.68 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03890860815402915		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.03890860815402915 | validation: 0.05856198539553337]
	TIME [epoch: 2.68 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03925200970428716		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.03925200970428716 | validation: 0.05979942213990539]
	TIME [epoch: 2.68 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04159946722218418		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.04159946722218418 | validation: 0.05954909327298857]
	TIME [epoch: 2.69 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04110450210235151		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.04110450210235151 | validation: 0.06281699058685605]
	TIME [epoch: 2.68 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04295311373795865		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.04295311373795865 | validation: 0.06453088376379143]
	TIME [epoch: 2.69 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04383437533013062		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.04383437533013062 | validation: 0.06369201541360607]
	TIME [epoch: 2.68 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047982442482246965		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.047982442482246965 | validation: 0.06199534356792491]
	TIME [epoch: 2.69 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04318458754060085		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.04318458754060085 | validation: 0.06570786920752632]
	TIME [epoch: 2.68 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042135944822251005		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.042135944822251005 | validation: 0.05640609274277397]
	TIME [epoch: 2.69 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03662778315549755		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.03662778315549755 | validation: 0.055309821467045654]
	TIME [epoch: 2.69 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03641001269539732		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.03641001269539732 | validation: 0.06275488901923824]
	TIME [epoch: 2.68 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03436552501603855		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.03436552501603855 | validation: 0.06340537138488286]
	TIME [epoch: 2.68 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03616715546318596		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.03616715546318596 | validation: 0.05467359338031939]
	TIME [epoch: 2.68 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03574858964652756		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.03574858964652756 | validation: 0.05893797488832767]
	TIME [epoch: 2.68 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044831516419910226		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.044831516419910226 | validation: 0.07857005891534335]
	TIME [epoch: 2.68 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058584499592749764		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.058584499592749764 | validation: 0.07264698969709521]
	TIME [epoch: 2.68 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0494914808318932		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.0494914808318932 | validation: 0.057222650669650627]
	TIME [epoch: 2.68 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04068582496418914		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.04068582496418914 | validation: 0.0542693069774024]
	TIME [epoch: 2.68 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035473467723085295		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.035473467723085295 | validation: 0.058767297880135606]
	TIME [epoch: 2.68 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03375985144109062		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.03375985144109062 | validation: 0.05804529654359924]
	TIME [epoch: 2.68 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03825654216134753		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.03825654216134753 | validation: 0.06084248961735807]
	TIME [epoch: 2.69 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03497330414845745		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.03497330414845745 | validation: 0.053084446074241266]
	TIME [epoch: 2.68 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03835617307212322		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.03835617307212322 | validation: 0.06792532545186526]
	TIME [epoch: 2.68 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04050021711243905		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.04050021711243905 | validation: 0.056267937408067736]
	TIME [epoch: 2.68 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04322208372676798		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.04322208372676798 | validation: 0.06779706140411636]
	TIME [epoch: 2.68 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04691613489739397		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.04691613489739397 | validation: 0.07739678763193965]
	TIME [epoch: 2.68 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052976670337513986		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.052976670337513986 | validation: 0.0624715267958469]
	TIME [epoch: 2.68 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049298968133592296		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.049298968133592296 | validation: 0.060488497804921104]
	TIME [epoch: 2.68 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040034540273926285		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.040034540273926285 | validation: 0.05602153249345492]
	TIME [epoch: 2.68 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03465183229569092		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.03465183229569092 | validation: 0.0626442489364598]
	TIME [epoch: 2.68 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03383634866112182		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.03383634866112182 | validation: 0.055560487085646565]
	TIME [epoch: 2.68 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039251320572656255		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.039251320572656255 | validation: 0.05876218163437052]
	TIME [epoch: 2.69 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03865900356299958		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.03865900356299958 | validation: 0.0573732642126433]
	TIME [epoch: 2.68 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04072465873814295		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.04072465873814295 | validation: 0.0714727883003858]
	TIME [epoch: 2.68 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040030158115174776		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.040030158115174776 | validation: 0.06055175927021286]
	TIME [epoch: 2.69 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03676621881604265		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.03676621881604265 | validation: 0.06039783722518158]
	TIME [epoch: 2.68 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037557304383498726		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.037557304383498726 | validation: 0.05581270383012415]
	TIME [epoch: 2.68 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040697138607502376		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.040697138607502376 | validation: 0.06827525518433672]
	TIME [epoch: 2.68 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05167071221090323		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.05167071221090323 | validation: 0.07497705044289797]
	TIME [epoch: 2.68 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05083468461980019		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.05083468461980019 | validation: 0.05732201159346959]
	TIME [epoch: 2.69 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037220053248375976		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.037220053248375976 | validation: 0.04794402754981643]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_943.pth
	Model improved!!!
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0358227515140962		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.0358227515140962 | validation: 0.05950072022810996]
	TIME [epoch: 2.67 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03495169622321594		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.03495169622321594 | validation: 0.05782108261141551]
	TIME [epoch: 2.67 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03569400178431702		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.03569400178431702 | validation: 0.05915907554030843]
	TIME [epoch: 2.67 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034072734560494304		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.034072734560494304 | validation: 0.05702732136355351]
	TIME [epoch: 2.67 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03791922117620622		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.03791922117620622 | validation: 0.05997619623134324]
	TIME [epoch: 2.67 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03632020636036464		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.03632020636036464 | validation: 0.0574844626149491]
	TIME [epoch: 2.67 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038503597636486094		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.038503597636486094 | validation: 0.056521032716999924]
	TIME [epoch: 2.67 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04084337007162952		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.04084337007162952 | validation: 0.060292841713626845]
	TIME [epoch: 2.67 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03815277235790918		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.03815277235790918 | validation: 0.05324626356894764]
	TIME [epoch: 2.67 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038800815076890115		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.038800815076890115 | validation: 0.05408178562423972]
	TIME [epoch: 2.67 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03458465192773896		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.03458465192773896 | validation: 0.059019025453409195]
	TIME [epoch: 2.67 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03687861774469377		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.03687861774469377 | validation: 0.05597640809279944]
	TIME [epoch: 2.67 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03613388854044273		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.03613388854044273 | validation: 0.051514252149945476]
	TIME [epoch: 2.68 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03762974669209714		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.03762974669209714 | validation: 0.060858947962210586]
	TIME [epoch: 2.67 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039152856392554364		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.039152856392554364 | validation: 0.05421672838276387]
	TIME [epoch: 2.67 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04089019599660894		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.04089019599660894 | validation: 0.05776592091276758]
	TIME [epoch: 2.67 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044834748819458634		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.044834748819458634 | validation: 0.06370861855709152]
	TIME [epoch: 2.67 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03959176224720691		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.03959176224720691 | validation: 0.0593701731690717]
	TIME [epoch: 2.67 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03606212398950743		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.03606212398950743 | validation: 0.06033534509277897]
	TIME [epoch: 2.67 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03391441400183056		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.03391441400183056 | validation: 0.057187569743746974]
	TIME [epoch: 2.67 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03222866093334934		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.03222866093334934 | validation: 0.059786847763640155]
	TIME [epoch: 2.67 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03373408314323285		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.03373408314323285 | validation: 0.057007372917990996]
	TIME [epoch: 2.67 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033194069895401246		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.033194069895401246 | validation: 0.06200624940276123]
	TIME [epoch: 2.68 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03502483902623038		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.03502483902623038 | validation: 0.05527177206611397]
	TIME [epoch: 2.68 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03556276942930361		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.03556276942930361 | validation: 0.05084874094669456]
	TIME [epoch: 2.67 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042181657649831505		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.042181657649831505 | validation: 0.0799565304437362]
	TIME [epoch: 2.67 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05193881396584447		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.05193881396584447 | validation: 0.06076023483159698]
	TIME [epoch: 2.67 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04256844898939564		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.04256844898939564 | validation: 0.05262369141325294]
	TIME [epoch: 2.67 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036134281131174444		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.036134281131174444 | validation: 0.05604258014922269]
	TIME [epoch: 2.67 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035065128720463495		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.035065128720463495 | validation: 0.05987254916463047]
	TIME [epoch: 2.67 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03613059492845716		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.03613059492845716 | validation: 0.06171019132307029]
	TIME [epoch: 2.67 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03853431612078202		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.03853431612078202 | validation: 0.06298565625248517]
	TIME [epoch: 2.67 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03622334690347302		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.03622334690347302 | validation: 0.05334765118318725]
	TIME [epoch: 2.67 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031743896483067864		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.031743896483067864 | validation: 0.052868612759222514]
	TIME [epoch: 2.67 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03538471727239287		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.03538471727239287 | validation: 0.05223657614700614]
	TIME [epoch: 2.68 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0353397550511654		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.0353397550511654 | validation: 0.06152503645945243]
	TIME [epoch: 2.67 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03635214289295841		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.03635214289295841 | validation: 0.05737976385872168]
	TIME [epoch: 2.67 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0345092193977018		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.0345092193977018 | validation: 0.05431906088222657]
	TIME [epoch: 2.67 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03780250039474237		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.03780250039474237 | validation: 0.06567433666411955]
	TIME [epoch: 2.67 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039866719633954224		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.039866719633954224 | validation: 0.0536868673376953]
	TIME [epoch: 2.67 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03635674267110679		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.03635674267110679 | validation: 0.05067945549924651]
	TIME [epoch: 2.67 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03253843654916791		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.03253843654916791 | validation: 0.05313123435968323]
	TIME [epoch: 2.67 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03499405809163188		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.03499405809163188 | validation: 0.057606848083464834]
	TIME [epoch: 2.67 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03950230695217419		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.03950230695217419 | validation: 0.05891356281392421]
	TIME [epoch: 2.67 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04180706881357334		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.04180706881357334 | validation: 0.055509067384635895]
	TIME [epoch: 2.67 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03911572424486927		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.03911572424486927 | validation: 0.06052076621361266]
	TIME [epoch: 2.67 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03520609743672246		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.03520609743672246 | validation: 0.049420466972604854]
	TIME [epoch: 2.67 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03292541203860698		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.03292541203860698 | validation: 0.046791193515441955]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_991.pth
	Model improved!!!
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031791370522798845		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.031791370522798845 | validation: 0.048506657396017464]
	TIME [epoch: 2.68 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033682336643301024		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.033682336643301024 | validation: 0.05299216894338549]
	TIME [epoch: 2.68 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032160821664496235		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.032160821664496235 | validation: 0.06380246601225885]
	TIME [epoch: 2.68 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033102778010140915		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.033102778010140915 | validation: 0.057484016174475516]
	TIME [epoch: 2.67 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035832100781955915		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.035832100781955915 | validation: 0.054194124794426694]
	TIME [epoch: 2.67 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03524521169206363		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.03524521169206363 | validation: 0.06296544957040487]
	TIME [epoch: 2.67 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03867164735105716		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.03867164735105716 | validation: 0.06690616295198576]
	TIME [epoch: 2.67 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05043237861775574		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.05043237861775574 | validation: 0.06348523698593228]
	TIME [epoch: 2.67 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044717972031907305		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.044717972031907305 | validation: 0.052353288519059786]
	TIME [epoch: 2.67 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032303494943281275		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.032303494943281275 | validation: 0.05570266825046075]
	TIME [epoch: 179 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03455689144190749		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.03455689144190749 | validation: 0.05642111080093522]
	TIME [epoch: 5.73 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03055406168631708		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.03055406168631708 | validation: 0.05252312653295277]
	TIME [epoch: 5.71 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0331672877227758		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.0331672877227758 | validation: 0.053946860311325506]
	TIME [epoch: 5.72 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031534959397212486		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.031534959397212486 | validation: 0.04843194236909553]
	TIME [epoch: 5.71 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033028862957602635		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.033028862957602635 | validation: 0.05161342559833722]
	TIME [epoch: 5.73 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034438528844716564		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.034438528844716564 | validation: 0.05013823348100373]
	TIME [epoch: 5.71 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03331870432384116		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.03331870432384116 | validation: 0.06526917175873247]
	TIME [epoch: 5.72 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03414070947239505		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.03414070947239505 | validation: 0.05892582879381506]
	TIME [epoch: 5.71 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04408693401843619		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.04408693401843619 | validation: 0.05935090252804553]
	TIME [epoch: 5.72 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0436153902455275		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.0436153902455275 | validation: 0.055197580575930216]
	TIME [epoch: 5.7 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03871284387232043		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.03871284387232043 | validation: 0.05098584662143945]
	TIME [epoch: 5.73 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03298801351038179		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.03298801351038179 | validation: 0.048933038829762646]
	TIME [epoch: 5.71 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03448968489676287		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.03448968489676287 | validation: 0.04697559794342511]
	TIME [epoch: 5.72 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035670612073891		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.035670612073891 | validation: 0.053408841756990594]
	TIME [epoch: 5.71 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03393822707438586		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.03393822707438586 | validation: 0.05044743325837934]
	TIME [epoch: 5.72 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03256724960737336		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.03256724960737336 | validation: 0.054389115695853446]
	TIME [epoch: 5.71 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03310251543878681		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.03310251543878681 | validation: 0.0520070628839425]
	TIME [epoch: 5.72 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0346900567151941		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.0346900567151941 | validation: 0.04997475661807176]
	TIME [epoch: 5.72 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03276847436502669		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.03276847436502669 | validation: 0.05543615223341069]
	TIME [epoch: 5.72 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03203120284553384		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.03203120284553384 | validation: 0.057480365670515225]
	TIME [epoch: 5.71 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03131852473063001		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.03131852473063001 | validation: 0.051363115902053086]
	TIME [epoch: 5.72 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032261977921168986		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.032261977921168986 | validation: 0.05905720055502972]
	TIME [epoch: 5.71 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03263992376780533		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.03263992376780533 | validation: 0.05461064509014949]
	TIME [epoch: 5.72 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03530033748290076		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.03530033748290076 | validation: 0.06380798714001167]
	TIME [epoch: 5.71 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05125021879193554		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.05125021879193554 | validation: 0.061990485224777805]
	TIME [epoch: 5.72 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045134826963363274		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.045134826963363274 | validation: 0.05598467465045196]
	TIME [epoch: 5.71 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03051624352731452		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.03051624352731452 | validation: 0.058536275874049554]
	TIME [epoch: 5.72 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03508885372135245		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.03508885372135245 | validation: 0.05321653647140032]
	TIME [epoch: 5.71 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03562112397635309		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.03562112397635309 | validation: 0.04966300593776221]
	TIME [epoch: 5.72 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03162670119648729		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.03162670119648729 | validation: 0.04290033469204756]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_1031.pth
	Model improved!!!
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030352848803925952		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.030352848803925952 | validation: 0.046420021348896695]
	TIME [epoch: 5.73 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030045773050453986		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.030045773050453986 | validation: 0.05129581652962248]
	TIME [epoch: 5.72 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03008499861346238		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.03008499861346238 | validation: 0.04934407209493355]
	TIME [epoch: 5.72 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03141888765919548		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.03141888765919548 | validation: 0.05209030997444748]
	TIME [epoch: 5.7 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03201476820448561		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.03201476820448561 | validation: 0.05625869700311963]
	TIME [epoch: 5.71 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032600564399115836		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.032600564399115836 | validation: 0.059184612388777674]
	TIME [epoch: 5.7 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03602525651836947		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.03602525651836947 | validation: 0.054745196794084454]
	TIME [epoch: 5.72 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03815115225051195		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.03815115225051195 | validation: 0.05697677682586828]
	TIME [epoch: 5.71 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03629132146319019		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.03629132146319019 | validation: 0.05865749633435755]
	TIME [epoch: 5.71 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033154380719787414		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.033154380719787414 | validation: 0.04083488776549809]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_1041.pth
	Model improved!!!
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0311394724005993		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.0311394724005993 | validation: 0.057511181289675854]
	TIME [epoch: 5.71 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033096773820446866		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.033096773820446866 | validation: 0.04969712135597731]
	TIME [epoch: 5.71 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032663954748035635		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.032663954748035635 | validation: 0.05850336104468199]
	TIME [epoch: 5.72 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03112490153166016		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.03112490153166016 | validation: 0.050292349659654206]
	TIME [epoch: 5.7 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030787577930718815		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.030787577930718815 | validation: 0.048306035076019375]
	TIME [epoch: 5.72 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03178989103391694		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.03178989103391694 | validation: 0.05034931007696213]
	TIME [epoch: 5.71 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031420535106369954		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.031420535106369954 | validation: 0.05201030307738752]
	TIME [epoch: 5.72 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031410909487304454		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.031410909487304454 | validation: 0.05019531732999715]
	TIME [epoch: 5.7 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03701884249404711		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.03701884249404711 | validation: 0.052395800657357695]
	TIME [epoch: 5.71 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03818860136329012		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.03818860136329012 | validation: 0.05999884513414712]
	TIME [epoch: 5.71 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03696218295985939		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.03696218295985939 | validation: 0.05721413075322373]
	TIME [epoch: 5.71 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03317777295859034		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.03317777295859034 | validation: 0.053991593307629995]
	TIME [epoch: 5.71 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029824645457230445		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.029824645457230445 | validation: 0.054715041873363957]
	TIME [epoch: 5.71 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02969831739885058		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.02969831739885058 | validation: 0.05458643529891913]
	TIME [epoch: 5.71 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03422205989099024		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.03422205989099024 | validation: 0.057468013793753076]
	TIME [epoch: 5.71 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03532618318741109		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.03532618318741109 | validation: 0.05526698073740123]
	TIME [epoch: 5.7 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03906876621388018		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.03906876621388018 | validation: 0.05629383427279865]
	TIME [epoch: 5.72 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03620775948988688		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.03620775948988688 | validation: 0.05258824335563959]
	TIME [epoch: 5.71 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03092706098319244		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.03092706098319244 | validation: 0.05264217833548701]
	TIME [epoch: 5.71 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03346115687119348		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.03346115687119348 | validation: 0.05162896429638535]
	TIME [epoch: 5.71 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029642052096904327		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.029642052096904327 | validation: 0.055377016205812805]
	TIME [epoch: 5.71 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03259419191094981		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.03259419191094981 | validation: 0.050439572589031284]
	TIME [epoch: 5.71 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033814310871090277		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.033814310871090277 | validation: 0.053351626501894046]
	TIME [epoch: 5.72 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033058300569022765		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.033058300569022765 | validation: 0.05071662421278052]
	TIME [epoch: 5.71 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03237370960969033		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.03237370960969033 | validation: 0.05290187301643398]
	TIME [epoch: 5.72 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03266235856900909		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.03266235856900909 | validation: 0.04827225442904445]
	TIME [epoch: 5.71 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034439607226282545		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.034439607226282545 | validation: 0.057405545855013465]
	TIME [epoch: 5.72 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03334159507660192		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.03334159507660192 | validation: 0.04829845964372512]
	TIME [epoch: 5.71 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03254350737034316		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.03254350737034316 | validation: 0.049821578689954564]
	TIME [epoch: 5.72 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031655910522032024		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.031655910522032024 | validation: 0.05047304162342097]
	TIME [epoch: 5.71 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030746599863797155		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.030746599863797155 | validation: 0.05293949136285351]
	TIME [epoch: 5.71 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03168756741718155		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.03168756741718155 | validation: 0.047405914492683124]
	TIME [epoch: 5.71 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029510495211707694		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.029510495211707694 | validation: 0.04654515306095578]
	TIME [epoch: 5.71 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02954887072628473		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.02954887072628473 | validation: 0.044095355259259665]
	TIME [epoch: 5.71 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02841629815270833		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.02841629815270833 | validation: 0.04094816805985521]
	TIME [epoch: 5.72 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029960575894881193		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.029960575894881193 | validation: 0.05572296498687487]
	TIME [epoch: 5.71 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03637894621218453		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.03637894621218453 | validation: 0.05562317724843249]
	TIME [epoch: 5.72 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04624630424896257		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.04624630424896257 | validation: 0.044626398455595045]
	TIME [epoch: 5.71 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03062572724057035		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.03062572724057035 | validation: 0.052251220555831104]
	TIME [epoch: 5.71 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029844087383863602		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.029844087383863602 | validation: 0.04617719031818821]
	TIME [epoch: 5.71 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030626908219150213		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.030626908219150213 | validation: 0.04764905386426217]
	TIME [epoch: 5.72 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029965073419052253		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.029965073419052253 | validation: 0.047843706582524605]
	TIME [epoch: 5.72 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03077274071167613		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.03077274071167613 | validation: 0.05739676194098824]
	TIME [epoch: 5.72 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03328219305271838		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.03328219305271838 | validation: 0.050876978621124384]
	TIME [epoch: 5.71 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03073547451636441		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.03073547451636441 | validation: 0.049613638477669135]
	TIME [epoch: 5.72 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030148500712717578		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.030148500712717578 | validation: 0.0518040202994911]
	TIME [epoch: 5.72 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030520852142956856		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.030520852142956856 | validation: 0.05376895903340794]
	TIME [epoch: 5.72 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02924636494240044		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.02924636494240044 | validation: 0.05066421175038519]
	TIME [epoch: 5.71 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027943913963248246		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.027943913963248246 | validation: 0.05254278525530895]
	TIME [epoch: 5.72 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03108125319120456		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.03108125319120456 | validation: 0.04457739652783765]
	TIME [epoch: 5.71 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03443982888997265		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.03443982888997265 | validation: 0.057294055921557785]
	TIME [epoch: 5.72 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035391647633560085		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.035391647633560085 | validation: 0.059541756659774625]
	TIME [epoch: 5.71 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034422811702521254		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.034422811702521254 | validation: 0.04819278655796949]
	TIME [epoch: 5.71 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031171360257557578		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.031171360257557578 | validation: 0.0515669005519549]
	TIME [epoch: 5.72 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027234668433654213		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.027234668433654213 | validation: 0.04413847865769626]
	TIME [epoch: 5.72 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02938897634953369		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.02938897634953369 | validation: 0.057139689454437696]
	TIME [epoch: 5.72 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02828656355060743		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.02828656355060743 | validation: 0.0493813658669168]
	TIME [epoch: 5.72 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03288205663522935		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.03288205663522935 | validation: 0.05045504170519791]
	TIME [epoch: 5.71 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033330809101082555		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.033330809101082555 | validation: 0.0522089202592052]
	TIME [epoch: 5.72 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036151735728395765		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.036151735728395765 | validation: 0.04943756484694189]
	TIME [epoch: 5.71 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032142794771687536		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.032142794771687536 | validation: 0.053673540676775815]
	TIME [epoch: 5.71 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029149513422893056		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.029149513422893056 | validation: 0.04720979765452942]
	TIME [epoch: 5.72 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03341544582857775		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.03341544582857775 | validation: 0.05635947533841549]
	TIME [epoch: 5.71 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03448101234357895		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.03448101234357895 | validation: 0.0455352114202559]
	TIME [epoch: 5.73 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0314522728874378		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.0314522728874378 | validation: 0.04195682371066402]
	TIME [epoch: 5.71 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03084138658482438		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.03084138658482438 | validation: 0.05324656917414964]
	TIME [epoch: 5.71 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028673040046244394		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.028673040046244394 | validation: 0.056133639032865223]
	TIME [epoch: 5.71 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02855130053110485		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.02855130053110485 | validation: 0.04548124566367086]
	TIME [epoch: 5.71 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02963342792061952		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.02963342792061952 | validation: 0.049502255124917766]
	TIME [epoch: 5.71 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030946351687239283		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.030946351687239283 | validation: 0.048981225906596954]
	TIME [epoch: 5.72 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03198489305347352		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.03198489305347352 | validation: 0.05495747994941376]
	TIME [epoch: 5.71 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0334797555627437		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.0334797555627437 | validation: 0.05374785758559907]
	TIME [epoch: 5.73 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034399872509384276		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.034399872509384276 | validation: 0.054139176200626686]
	TIME [epoch: 5.71 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02917340571065022		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.02917340571065022 | validation: 0.045548011877564666]
	TIME [epoch: 5.72 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026773712183269106		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.026773712183269106 | validation: 0.05077028606052195]
	TIME [epoch: 5.71 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029302677243382657		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.029302677243382657 | validation: 0.050994443445236196]
	TIME [epoch: 5.73 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03121689345681589		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.03121689345681589 | validation: 0.05662659416866911]
	TIME [epoch: 5.71 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03773522938606268		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.03773522938606268 | validation: 0.05259487077058159]
	TIME [epoch: 5.72 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03273469993685747		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.03273469993685747 | validation: 0.046814918681629875]
	TIME [epoch: 5.71 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027720359729574567		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.027720359729574567 | validation: 0.04975172734101873]
	TIME [epoch: 5.72 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030078868999132098		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.030078868999132098 | validation: 0.04663782762482065]
	TIME [epoch: 5.71 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03130884441557929		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.03130884441557929 | validation: 0.05152187737141271]
	TIME [epoch: 5.72 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031249810829456406		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.031249810829456406 | validation: 0.0553840706512127]
	TIME [epoch: 5.71 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03141620693365793		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.03141620693365793 | validation: 0.05260232006716397]
	TIME [epoch: 5.72 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029526747195903584		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.029526747195903584 | validation: 0.04576057338962661]
	TIME [epoch: 5.71 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028420709875877713		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.028420709875877713 | validation: 0.045508342666430585]
	TIME [epoch: 5.72 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03104598010296297		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.03104598010296297 | validation: 0.03713321900600047]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_1128.pth
	Model improved!!!
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02713731007013839		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.02713731007013839 | validation: 0.05474887088340158]
	TIME [epoch: 5.72 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030804803149460866		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.030804803149460866 | validation: 0.05061729925841183]
	TIME [epoch: 5.71 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02900872052361135		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.02900872052361135 | validation: 0.05081559708011355]
	TIME [epoch: 5.72 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030012760501689592		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.030012760501689592 | validation: 0.046239777153866485]
	TIME [epoch: 5.71 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029752409144146404		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.029752409144146404 | validation: 0.044635725944929]
	TIME [epoch: 5.71 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02858495117365691		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.02858495117365691 | validation: 0.045944283797640165]
	TIME [epoch: 5.71 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030833622452377557		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.030833622452377557 | validation: 0.04448188940030147]
	TIME [epoch: 5.71 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03159385583021174		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.03159385583021174 | validation: 0.04898802502482378]
	TIME [epoch: 5.72 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03579776486359978		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.03579776486359978 | validation: 0.05163953549661376]
	TIME [epoch: 5.71 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03522612071928975		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.03522612071928975 | validation: 0.04408068943491178]
	TIME [epoch: 5.71 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027740752597726976		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.027740752597726976 | validation: 0.04666020297121629]
	TIME [epoch: 5.71 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028086562966542116		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.028086562966542116 | validation: 0.045199515207331654]
	TIME [epoch: 5.71 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028707490207894714		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.028707490207894714 | validation: 0.04085476922508058]
	TIME [epoch: 5.71 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027150916072801765		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.027150916072801765 | validation: 0.04706784979057832]
	TIME [epoch: 5.71 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029950500546597418		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.029950500546597418 | validation: 0.04587358845282785]
	TIME [epoch: 5.72 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029306262580556953		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.029306262580556953 | validation: 0.048563090444207146]
	TIME [epoch: 5.71 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028484189621049715		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.028484189621049715 | validation: 0.04999668512724684]
	TIME [epoch: 5.71 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027625133568854418		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.027625133568854418 | validation: 0.046129387840259675]
	TIME [epoch: 5.71 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028275279999869056		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.028275279999869056 | validation: 0.05537062541438255]
	TIME [epoch: 5.71 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0318531065737265		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.0318531065737265 | validation: 0.051572419622526025]
	TIME [epoch: 5.71 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03241093588864053		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.03241093588864053 | validation: 0.05293842744549507]
	TIME [epoch: 5.71 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03109582726000058		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.03109582726000058 | validation: 0.049476187881422766]
	TIME [epoch: 5.72 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030150080543145552		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.030150080543145552 | validation: 0.04359192449062824]
	TIME [epoch: 5.71 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028950051038433112		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.028950051038433112 | validation: 0.04969650812280896]
	TIME [epoch: 5.71 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02815977911178239		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.02815977911178239 | validation: 0.0477496037824248]
	TIME [epoch: 5.71 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027377054858997045		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.027377054858997045 | validation: 0.04047035765844742]
	TIME [epoch: 5.71 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029115955291650487		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.029115955291650487 | validation: 0.04631697374583038]
	TIME [epoch: 5.71 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028126287295163702		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.028126287295163702 | validation: 0.046365194956407556]
	TIME [epoch: 5.71 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02820934356626736		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.02820934356626736 | validation: 0.0457864592136755]
	TIME [epoch: 5.72 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028654616161997045		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.028654616161997045 | validation: 0.04634488184147587]
	TIME [epoch: 5.71 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028597859465374402		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.028597859465374402 | validation: 0.049951652767576155]
	TIME [epoch: 5.71 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03154637763334374		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.03154637763334374 | validation: 0.04765720531528751]
	TIME [epoch: 5.71 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034553271806888726		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.034553271806888726 | validation: 0.048738677212225905]
	TIME [epoch: 5.71 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028966635251860228		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.028966635251860228 | validation: 0.046183552022556175]
	TIME [epoch: 5.71 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027075891455138935		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.027075891455138935 | validation: 0.048097630865977375]
	TIME [epoch: 5.71 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028966612052736335		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.028966612052736335 | validation: 0.04685540632973231]
	TIME [epoch: 5.71 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027335120167942206		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.027335120167942206 | validation: 0.042385853398671625]
	TIME [epoch: 5.71 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027454186633643136		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.027454186633643136 | validation: 0.04436528884590819]
	TIME [epoch: 5.71 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028329901883366232		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.028329901883366232 | validation: 0.05135033256185484]
	TIME [epoch: 5.71 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028925414496712147		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.028925414496712147 | validation: 0.050898342105247]
	TIME [epoch: 5.72 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027937422715713724		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.027937422715713724 | validation: 0.043587851298562325]
	TIME [epoch: 5.71 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030108091545156618		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.030108091545156618 | validation: 0.049293725816416736]
	TIME [epoch: 5.71 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027090756755334407		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.027090756755334407 | validation: 0.04336100039657767]
	TIME [epoch: 5.71 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02921513432605777		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.02921513432605777 | validation: 0.0456361572321214]
	TIME [epoch: 5.72 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028894631480608588		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.028894631480608588 | validation: 0.0441358786475258]
	TIME [epoch: 5.71 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02939213488981826		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.02939213488981826 | validation: 0.04700906823965835]
	TIME [epoch: 5.72 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028890011573920262		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.028890011573920262 | validation: 0.047827482956349934]
	TIME [epoch: 5.71 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028312332202759586		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.028312332202759586 | validation: 0.05409871019015107]
	TIME [epoch: 5.71 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025673213346087825		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.025673213346087825 | validation: 0.04727650911999853]
	TIME [epoch: 5.71 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027543503841280303		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.027543503841280303 | validation: 0.05286100470239498]
	TIME [epoch: 5.71 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02893053528460576		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.02893053528460576 | validation: 0.05589493569940465]
	TIME [epoch: 5.72 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03198543574874114		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.03198543574874114 | validation: 0.04817663664311576]
	TIME [epoch: 5.71 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03243570760741084		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.03243570760741084 | validation: 0.04387306534289337]
	TIME [epoch: 5.71 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027166767946787358		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.027166767946787358 | validation: 0.046096876213572696]
	TIME [epoch: 5.71 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02865457651750658		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.02865457651750658 | validation: 0.05073776831222696]
	TIME [epoch: 5.72 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02959417104493861		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.02959417104493861 | validation: 0.05259701202277847]
	TIME [epoch: 5.71 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025951784789372635		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.025951784789372635 | validation: 0.0426443851766826]
	TIME [epoch: 5.71 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027735613406582482		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.027735613406582482 | validation: 0.04371480694133922]
	TIME [epoch: 5.71 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027584997348811983		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.027584997348811983 | validation: 0.04038516181439466]
	TIME [epoch: 5.71 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02764695181084084		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.02764695181084084 | validation: 0.05234450407508723]
	TIME [epoch: 5.71 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027750656690405887		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.027750656690405887 | validation: 0.053491380827047325]
	TIME [epoch: 5.71 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026961646727024988		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.026961646727024988 | validation: 0.04845735924916619]
	TIME [epoch: 5.71 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03034803208907418		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.03034803208907418 | validation: 0.052762739234796]
	TIME [epoch: 5.71 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033959207822636936		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.033959207822636936 | validation: 0.05419134864263963]
	TIME [epoch: 5.71 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028011232639114522		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.028011232639114522 | validation: 0.04900655248193691]
	TIME [epoch: 5.71 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028295031503781907		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.028295031503781907 | validation: 0.04219681858623271]
	TIME [epoch: 5.71 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026733103677121704		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.026733103677121704 | validation: 0.047784103619904796]
	TIME [epoch: 5.71 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026206881722688223		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.026206881722688223 | validation: 0.045778169122801304]
	TIME [epoch: 5.71 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02805034356538075		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.02805034356538075 | validation: 0.04913050520735623]
	TIME [epoch: 5.71 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0263522513868502		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.0263522513868502 | validation: 0.04913050408325658]
	TIME [epoch: 5.71 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02736050970185951		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.02736050970185951 | validation: 0.04634498360729675]
	TIME [epoch: 5.72 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02797856029158455		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.02797856029158455 | validation: 0.0508298539217646]
	TIME [epoch: 5.71 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026842495720728794		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.026842495720728794 | validation: 0.04185576577177766]
	TIME [epoch: 5.71 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02634361633299464		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.02634361633299464 | validation: 0.042156501966614426]
	TIME [epoch: 5.71 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02557145522851764		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.02557145522851764 | validation: 0.04570158295788164]
	TIME [epoch: 5.72 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026350778037228108		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.026350778037228108 | validation: 0.041430040377951165]
	TIME [epoch: 5.71 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02841960005882247		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.02841960005882247 | validation: 0.0443961390120338]
	TIME [epoch: 5.72 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027675506898121396		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.027675506898121396 | validation: 0.04375303173294044]
	TIME [epoch: 5.71 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033394976137663984		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.033394976137663984 | validation: 0.04525040960333704]
	TIME [epoch: 5.72 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026654011991851523		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.026654011991851523 | validation: 0.04230274004624839]
	TIME [epoch: 5.71 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026626211262229943		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.026626211262229943 | validation: 0.04270678551191766]
	TIME [epoch: 5.72 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025814139604292224		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.025814139604292224 | validation: 0.043904668001060014]
	TIME [epoch: 5.71 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026695634853937226		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.026695634853937226 | validation: 0.05509312630886679]
	TIME [epoch: 5.71 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026811867141361728		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.026811867141361728 | validation: 0.04347614838608326]
	TIME [epoch: 5.71 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026404923710685875		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.026404923710685875 | validation: 0.04749855948133183]
	TIME [epoch: 5.71 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02746095476219773		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.02746095476219773 | validation: 0.04982111956499624]
	TIME [epoch: 5.72 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027200219998267033		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.027200219998267033 | validation: 0.04427167146674965]
	TIME [epoch: 5.71 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027104011237607462		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.027104011237607462 | validation: 0.050187613228135544]
	TIME [epoch: 5.72 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02741355582020602		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.02741355582020602 | validation: 0.04513982145756184]
	TIME [epoch: 5.71 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026432443745336674		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.026432443745336674 | validation: 0.049192889955316316]
	TIME [epoch: 5.71 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02558873522839332		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.02558873522839332 | validation: 0.049731480454573096]
	TIME [epoch: 5.71 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026950807049576826		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.026950807049576826 | validation: 0.05439829077718068]
	TIME [epoch: 5.71 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029915790120089207		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.029915790120089207 | validation: 0.043311644602727975]
	TIME [epoch: 5.71 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030347079709524837		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.030347079709524837 | validation: 0.05137448493998851]
	TIME [epoch: 5.71 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027531706786483687		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.027531706786483687 | validation: 0.040590636567920696]
	TIME [epoch: 5.72 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027026151792088386		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.027026151792088386 | validation: 0.04864473200280728]
	TIME [epoch: 5.71 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027482096245381706		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.027482096245381706 | validation: 0.05218190268796816]
	TIME [epoch: 5.72 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027502966468506234		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.027502966468506234 | validation: 0.042807590745309335]
	TIME [epoch: 5.71 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027086950940273358		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.027086950940273358 | validation: 0.044050660986593604]
	TIME [epoch: 5.71 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025046356351785138		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.025046356351785138 | validation: 0.04935207714311788]
	TIME [epoch: 5.71 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024907974366637983		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.024907974366637983 | validation: 0.04243242482428222]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_9_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_9_v_mmd4_1229.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 3972.072 seconds.
