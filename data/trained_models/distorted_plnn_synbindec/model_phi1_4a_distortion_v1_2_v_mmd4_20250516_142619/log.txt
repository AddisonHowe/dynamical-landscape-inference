Args:
Namespace(name='model_phi1_4a_distortion_v1_2_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_2/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_2/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.04682813, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3738088065

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.633971104283114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.633971104283114 | validation: 6.0274473620548825]
	TIME [epoch: 157 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.293301775158422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.293301775158422 | validation: 5.797282962960523]
	TIME [epoch: 0.77 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.2102455576239075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.2102455576239075 | validation: 5.520392481838813]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.198066494242652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.198066494242652 | validation: 5.140778679719026]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.854321628381921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.854321628381921 | validation: 5.111055438297563]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.765260435862537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.765260435862537 | validation: 4.928021451430981]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.580255195919205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.580255195919205 | validation: 4.390206014775608]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.442780716433016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.442780716433016 | validation: 5.5136932778009795]
	TIME [epoch: 0.689 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.542113868548702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.542113868548702 | validation: 4.266004036313748]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.066276645031248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.066276645031248 | validation: 4.689931345103874]
	TIME [epoch: 0.69 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.114382832169606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.114382832169606 | validation: 3.641162590444074]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.656043364209015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.656043364209015 | validation: 3.5824620093904374]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.446990460678046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.446990460678046 | validation: 3.813161729718512]
	TIME [epoch: 0.691 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.73510282704626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.73510282704626 | validation: 4.615704739608046]
	TIME [epoch: 0.688 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.971504833594941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.971504833594941 | validation: 3.5599387759237047]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.299803433162174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.299803433162174 | validation: 3.6653917022985363]
	TIME [epoch: 0.69 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.691731798859005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.691731798859005 | validation: 2.7258107251565065]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.998741968535471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.998741968535471 | validation: 2.826736956325142]
	TIME [epoch: 0.69 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.968175857805076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.968175857805076 | validation: 2.773311259727349]
	TIME [epoch: 0.689 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.028705857618907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.028705857618907 | validation: 3.3854102642025268]
	TIME [epoch: 0.689 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.304754850787888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.304754850787888 | validation: 2.3917863455224904]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.779899977187312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.779899977187312 | validation: 3.1274176640876687]
	TIME [epoch: 0.691 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.20806694773327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.20806694773327 | validation: 3.227356964272454]
	TIME [epoch: 0.692 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.134682167233791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.134682167233791 | validation: 2.693719357740191]
	TIME [epoch: 0.69 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.87543835588531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.87543835588531 | validation: 2.893015523842342]
	TIME [epoch: 0.691 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.138132947199575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.138132947199575 | validation: 2.5559494821753237]
	TIME [epoch: 0.69 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8047839305400033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8047839305400033 | validation: 2.183469325569983]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.637059105191936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.637059105191936 | validation: 2.230135317671158]
	TIME [epoch: 0.693 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6937234722145957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6937234722145957 | validation: 2.6424292470226995]
	TIME [epoch: 0.693 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.855345361003762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.855345361003762 | validation: 2.2188105821085586]
	TIME [epoch: 0.691 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6542584915004532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6542584915004532 | validation: 2.2587479348654305]
	TIME [epoch: 0.697 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.63607301339497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.63607301339497 | validation: 2.242534122347064]
	TIME [epoch: 0.69 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6589458877484233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6589458877484233 | validation: 2.61079760426845]
	TIME [epoch: 0.692 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7954971683261265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7954971683261265 | validation: 2.1863595193247893]
	TIME [epoch: 0.691 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5846870414012733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5846870414012733 | validation: 2.1410890807648837]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5575936561498587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5575936561498587 | validation: 2.1531732720201595]
	TIME [epoch: 0.694 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5632266507515022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5632266507515022 | validation: 2.4929203294217532]
	TIME [epoch: 0.693 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.712096485584224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.712096485584224 | validation: 2.3849470764313843]
	TIME [epoch: 0.691 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.664338796305855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.664338796305855 | validation: 2.551640396279573]
	TIME [epoch: 0.691 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7162885156670784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7162885156670784 | validation: 2.108805541264766]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.49897559519649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.49897559519649 | validation: 2.1050863612539508]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4815815841839246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4815815841839246 | validation: 2.0764211368509944]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.486156908499121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.486156908499121 | validation: 2.282569832775316]
	TIME [epoch: 0.693 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.552406932664178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.552406932664178 | validation: 2.608678600257374]
	TIME [epoch: 0.691 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.727365728473277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.727365728473277 | validation: 2.1696053818967727]
	TIME [epoch: 0.691 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4744341064554547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4744341064554547 | validation: 2.0933339536508315]
	TIME [epoch: 0.69 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4317219082356383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4317219082356383 | validation: 2.0647909901719337]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.407733694401916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.407733694401916 | validation: 2.1074954581531937]
	TIME [epoch: 0.693 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.407618044139556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.407618044139556 | validation: 2.0810034882024953]
	TIME [epoch: 0.692 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4327124126860036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4327124126860036 | validation: 2.5591736900438193]
	TIME [epoch: 0.69 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.580649004752452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.580649004752452 | validation: 2.182403179039692]
	TIME [epoch: 0.69 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5108338145357734		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.5108338145357734 | validation: 2.320974245558268]
	TIME [epoch: 0.69 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.508557481048001		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 3.508557481048001 | validation: 2.334711239005365]
	TIME [epoch: 0.691 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5114321029720292		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 3.5114321029720292 | validation: 2.464677857027946]
	TIME [epoch: 0.69 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5268148568081097		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 3.5268148568081097 | validation: 2.1109143340234446]
	TIME [epoch: 0.692 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3564045506831435		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 3.3564045506831435 | validation: 2.088926514713551]
	TIME [epoch: 0.693 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.336209997597823		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 3.336209997597823 | validation: 2.2531946734449577]
	TIME [epoch: 0.692 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3819700082017334		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 3.3819700082017334 | validation: 2.2834495626126685]
	TIME [epoch: 0.689 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3788585125336112		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 3.3788585125336112 | validation: 2.3564308631804542]
	TIME [epoch: 0.689 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4059075830074574		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 3.4059075830074574 | validation: 2.4364505487366817]
	TIME [epoch: 0.689 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3936269911469346		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 3.3936269911469346 | validation: 2.2188836376935477]
	TIME [epoch: 0.689 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3395313889387217		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 3.3395313889387217 | validation: 2.128226113555834]
	TIME [epoch: 0.691 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2416483934677602		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 3.2416483934677602 | validation: 2.2257248058231944]
	TIME [epoch: 0.693 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.264491385282724		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 3.264491385282724 | validation: 2.138858918610218]
	TIME [epoch: 0.691 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2140827955750324		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 3.2140827955750324 | validation: 2.1269866566722526]
	TIME [epoch: 0.69 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1917092589391203		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 3.1917092589391203 | validation: 2.200952014163133]
	TIME [epoch: 0.69 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1838723627387466		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 3.1838723627387466 | validation: 2.167276680125356]
	TIME [epoch: 0.69 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.152525547198393		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 3.152525547198393 | validation: 2.1978522605508153]
	TIME [epoch: 0.689 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.132242612941377		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 3.132242612941377 | validation: 2.173186894954437]
	TIME [epoch: 0.689 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0785205480101525		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 3.0785205480101525 | validation: 2.2151369817872513]
	TIME [epoch: 0.696 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0682257649302445		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 3.0682257649302445 | validation: 2.1405818823069893]
	TIME [epoch: 0.689 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.048265269642871		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 3.048265269642871 | validation: 2.30963707751507]
	TIME [epoch: 0.694 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0762091925795483		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 3.0762091925795483 | validation: 2.133716340595405]
	TIME [epoch: 0.692 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1045042544067374		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 3.1045042544067374 | validation: 2.044883450472024]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.81921989820931		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 2.81921989820931 | validation: 2.5418710981869905]
	TIME [epoch: 0.694 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.119667791988856		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 3.119667791988856 | validation: 2.05265962353609]
	TIME [epoch: 0.692 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.728168772509201		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 2.728168772509201 | validation: 2.436462787501682]
	TIME [epoch: 0.69 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5577138136801225		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 2.5577138136801225 | validation: 2.012092785148791]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5145491040012464		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 2.5145491040012464 | validation: 1.574520642334564]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.893580254578237		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.893580254578237 | validation: 2.522671368149563]
	TIME [epoch: 0.691 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6236307104778342		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 2.6236307104778342 | validation: 1.6366778075232973]
	TIME [epoch: 0.688 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7364319044690526		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 2.7364319044690526 | validation: 1.90953197951269]
	TIME [epoch: 0.687 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7947129896418892		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.7947129896418892 | validation: 2.2422507196120107]
	TIME [epoch: 0.687 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9330530107742294		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.9330530107742294 | validation: 1.627601062027852]
	TIME [epoch: 0.686 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.66352273732337		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.66352273732337 | validation: 1.526242594239079]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5715666913278958		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.5715666913278958 | validation: 1.476796756079128]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3354943164069324		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.3354943164069324 | validation: 1.4614470468770135]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.348066604533148		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.348066604533148 | validation: 1.4542217360809917]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3399453850882546		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.3399453850882546 | validation: 1.38774707825477]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4446458980960504		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.4446458980960504 | validation: 1.6744368091084334]
	TIME [epoch: 0.694 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5628108415062751		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.5628108415062751 | validation: 1.4986181208030478]
	TIME [epoch: 0.695 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6012931123278182		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.6012931123278182 | validation: 1.4385449307425224]
	TIME [epoch: 0.693 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3250998654976525		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.3250998654976525 | validation: 1.350643793304527]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2921440895686072		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.2921440895686072 | validation: 1.3967362636551757]
	TIME [epoch: 0.691 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3244471650668357		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.3244471650668357 | validation: 1.351636091771383]
	TIME [epoch: 0.688 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4077219848787694		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.4077219848787694 | validation: 1.5843506612772895]
	TIME [epoch: 0.687 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4968740260426734		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.4968740260426734 | validation: 1.4709972064643497]
	TIME [epoch: 0.687 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5762484620635158		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.5762484620635158 | validation: 1.4101526897806527]
	TIME [epoch: 0.685 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3094801247671028		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.3094801247671028 | validation: 1.2538736931044794]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.274876572488836		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.274876572488836 | validation: 1.4118314072433567]
	TIME [epoch: 0.693 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3159540724645724		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.3159540724645724 | validation: 1.350568382838467]
	TIME [epoch: 0.69 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4452071835773217		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.4452071835773217 | validation: 1.6502021647101115]
	TIME [epoch: 0.687 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4577215486639588		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.4577215486639588 | validation: 1.315923371736664]
	TIME [epoch: 0.687 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3948128479915087		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.3948128479915087 | validation: 1.3125041487965312]
	TIME [epoch: 0.693 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.213064463641208		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.213064463641208 | validation: 1.3930837991984237]
	TIME [epoch: 0.687 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2633251596195865		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.2633251596195865 | validation: 1.2384531682956874]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1800018298424761		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.1800018298424761 | validation: 1.448849530667263]
	TIME [epoch: 0.691 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2923605208482167		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.2923605208482167 | validation: 1.2715469632044696]
	TIME [epoch: 0.687 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1639065908076223		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.1639065908076223 | validation: 1.3451711798135086]
	TIME [epoch: 0.686 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4854251858788847		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.4854251858788847 | validation: 1.8380342463995463]
	TIME [epoch: 0.686 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.543192893930953		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.543192893930953 | validation: 1.6273427535253466]
	TIME [epoch: 0.687 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4198071735651439		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.4198071735651439 | validation: 1.3209755435744985]
	TIME [epoch: 0.685 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3145946274442348		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.3145946274442348 | validation: 1.2460961995873023]
	TIME [epoch: 0.689 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.315374169098602		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.315374169098602 | validation: 1.3840399562463308]
	TIME [epoch: 0.689 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3977962991028396		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.3977962991028396 | validation: 1.3616952988070137]
	TIME [epoch: 0.691 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4654612305999382		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.4654612305999382 | validation: 1.5140711929331578]
	TIME [epoch: 0.692 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3955297320594218		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.3955297320594218 | validation: 1.2421049831386957]
	TIME [epoch: 0.69 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4155317125763347		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.4155317125763347 | validation: 1.314842319030592]
	TIME [epoch: 0.689 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2212965714942978		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.2212965714942978 | validation: 1.1682102571629076]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2337778977810674		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.2337778977810674 | validation: 1.2092588428372524]
	TIME [epoch: 0.69 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2626155441011138		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.2626155441011138 | validation: 1.2647105964139198]
	TIME [epoch: 0.688 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.276767573492269		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.276767573492269 | validation: 1.2715900442754675]
	TIME [epoch: 0.688 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.235523957438634		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.235523957438634 | validation: 1.1777569981129246]
	TIME [epoch: 0.686 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3392492326978132		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.3392492326978132 | validation: 1.544399305649359]
	TIME [epoch: 0.687 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3441192549373946		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.3441192549373946 | validation: 1.0880168580749328]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1558427109772695		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.1558427109772695 | validation: 1.198801879852196]
	TIME [epoch: 0.692 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1342481178450268		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.1342481178450268 | validation: 1.3449950372502695]
	TIME [epoch: 0.689 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1821430314323065		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.1821430314323065 | validation: 1.0917382990937843]
	TIME [epoch: 0.69 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1176468530487784		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.1176468530487784 | validation: 1.5678199068456904]
	TIME [epoch: 0.69 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2946315034757923		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.2946315034757923 | validation: 1.0264203369722513]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3202597844214838		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.3202597844214838 | validation: 1.179354692958804]
	TIME [epoch: 0.692 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1300318916568242		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.1300318916568242 | validation: 1.1786045697426968]
	TIME [epoch: 0.69 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1369585862833669		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.1369585862833669 | validation: 1.2881477487130877]
	TIME [epoch: 0.688 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2182467912627692		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.2182467912627692 | validation: 1.1214446481992262]
	TIME [epoch: 0.686 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1235808902160105		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.1235808902160105 | validation: 1.1786534723823308]
	TIME [epoch: 0.688 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.118566036772294		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.118566036772294 | validation: 1.04232570406913]
	TIME [epoch: 0.687 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.142481255323606		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.142481255323606 | validation: 1.3509373249764278]
	TIME [epoch: 0.686 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2001855442275566		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.2001855442275566 | validation: 0.9467240630307107]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1464942760947796		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.1464942760947796 | validation: 1.128166193385459]
	TIME [epoch: 0.691 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.048441774780017		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.048441774780017 | validation: 1.0187351332601968]
	TIME [epoch: 0.688 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0941212899525457		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.0941212899525457 | validation: 1.392828875879097]
	TIME [epoch: 0.687 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3015747716978192		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.3015747716978192 | validation: 1.128783403904322]
	TIME [epoch: 0.688 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.23050494443766		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.23050494443766 | validation: 1.112198394384551]
	TIME [epoch: 0.7 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0311791103681354		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.0311791103681354 | validation: 0.9110849596419008]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_144.pth
	Model improved!!!
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0288386486888599		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.0288386486888599 | validation: 1.1293502108104236]
	TIME [epoch: 0.691 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0384510198622554		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.0384510198622554 | validation: 0.8972393081128183]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1439084209281416		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.1439084209281416 | validation: 1.3459810801505305]
	TIME [epoch: 0.69 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2051879513908588		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.2051879513908588 | validation: 1.0983403313799216]
	TIME [epoch: 0.689 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2339484978046389		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.2339484978046389 | validation: 1.0214317062407752]
	TIME [epoch: 0.689 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0427610426298344		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.0427610426298344 | validation: 1.0553001810762659]
	TIME [epoch: 0.687 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0219248604814264		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.0219248604814264 | validation: 0.9562484120087785]
	TIME [epoch: 0.687 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0756426017826806		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.0756426017826806 | validation: 1.123752847977807]
	TIME [epoch: 0.687 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.098028747460695		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.098028747460695 | validation: 0.9907918810522003]
	TIME [epoch: 0.687 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0573534230799546		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.0573534230799546 | validation: 0.9519061384513279]
	TIME [epoch: 0.686 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.087370591313621		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.087370591313621 | validation: 1.2007361681291298]
	TIME [epoch: 0.686 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1158765186081752		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.1158765186081752 | validation: 0.9523364542225473]
	TIME [epoch: 0.686 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1551844599770429		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.1551844599770429 | validation: 1.2679502738714046]
	TIME [epoch: 0.687 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1115576292387317		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.1115576292387317 | validation: 1.2463581933476486]
	TIME [epoch: 0.686 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.110298805694147		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.110298805694147 | validation: 0.9847721620414167]
	TIME [epoch: 0.687 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1060424505549693		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.1060424505549693 | validation: 1.1558691295965262]
	TIME [epoch: 0.687 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0372050561139976		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.0372050561139976 | validation: 1.0472139968196836]
	TIME [epoch: 0.687 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.991521288888781		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.991521288888781 | validation: 0.9748662050412303]
	TIME [epoch: 0.686 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9973844340578968		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.9973844340578968 | validation: 1.1460433175037856]
	TIME [epoch: 0.686 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1390299724374418		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.1390299724374418 | validation: 1.0987534446249494]
	TIME [epoch: 0.686 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1413213065389585		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.1413213065389585 | validation: 0.9735721167994964]
	TIME [epoch: 0.687 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0901553959905752		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.0901553959905752 | validation: 1.2507671644063396]
	TIME [epoch: 0.686 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0921621074974837		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.0921621074974837 | validation: 0.9545471188504673]
	TIME [epoch: 0.686 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0731354382913991		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.0731354382913991 | validation: 1.1040161099823984]
	TIME [epoch: 0.688 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.03675433624609		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.03675433624609 | validation: 0.933929158743014]
	TIME [epoch: 0.69 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0101188754450694		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.0101188754450694 | validation: 1.0185886672586577]
	TIME [epoch: 0.691 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9993949047786924		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.9993949047786924 | validation: 0.9820828088471267]
	TIME [epoch: 0.69 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.005622420196463		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.005622420196463 | validation: 1.0724343909411236]
	TIME [epoch: 0.692 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0181067296926893		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.0181067296926893 | validation: 1.0034512018986423]
	TIME [epoch: 0.693 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0526377841075791		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.0526377841075791 | validation: 1.1250706187341426]
	TIME [epoch: 0.69 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0490410463414006		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.0490410463414006 | validation: 1.0824725424878483]
	TIME [epoch: 0.689 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0192404467447498		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.0192404467447498 | validation: 0.893588277045517]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0369125809762434		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.0369125809762434 | validation: 1.0832676397051981]
	TIME [epoch: 0.696 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9696631406987544		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.9696631406987544 | validation: 0.9011000021827968]
	TIME [epoch: 0.693 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9715961726678919		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.9715961726678919 | validation: 1.0665417333100446]
	TIME [epoch: 0.693 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9546604933747798		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.9546604933747798 | validation: 0.9219584140476638]
	TIME [epoch: 0.689 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9384253793601955		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.9384253793601955 | validation: 1.0296295960798567]
	TIME [epoch: 0.688 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9404028239253934		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.9404028239253934 | validation: 0.9958397308798853]
	TIME [epoch: 0.688 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9299636007325256		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.9299636007325256 | validation: 0.8780286939190962]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_183.pth
	Model improved!!!
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.971752505375665		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.971752505375665 | validation: 1.2120012624901577]
	TIME [epoch: 0.694 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1435331102873811		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.1435331102873811 | validation: 1.0878232388122613]
	TIME [epoch: 0.692 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2182041157541768		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.2182041157541768 | validation: 1.2371080005036883]
	TIME [epoch: 0.692 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0898434601737903		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.0898434601737903 | validation: 0.9887444287086065]
	TIME [epoch: 0.691 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0333062319610167		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.0333062319610167 | validation: 1.062863367516354]
	TIME [epoch: 0.69 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9994747745254162		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.9994747745254162 | validation: 0.8139828738734471]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0464394890434936		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.0464394890434936 | validation: 1.0333828609656013]
	TIME [epoch: 0.693 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9246136576779322		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.9246136576779322 | validation: 0.9911841160392904]
	TIME [epoch: 0.689 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9230217208529499		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.9230217208529499 | validation: 1.0334695073875642]
	TIME [epoch: 0.689 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9368829967317172		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.9368829967317172 | validation: 0.8213743022249429]
	TIME [epoch: 0.687 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9662431517174106		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.9662431517174106 | validation: 1.2250826835657325]
	TIME [epoch: 0.689 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1031095100794408		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.1031095100794408 | validation: 0.9640310028548978]
	TIME [epoch: 0.687 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0504752654674507		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.0504752654674507 | validation: 0.9558313617364732]
	TIME [epoch: 0.686 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9258723124068985		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.9258723124068985 | validation: 0.9580827534194548]
	TIME [epoch: 0.686 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9168641979712832		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.9168641979712832 | validation: 0.8548576099702093]
	TIME [epoch: 0.689 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9680079462025709		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.9680079462025709 | validation: 1.030635160831426]
	TIME [epoch: 0.691 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9934332114432108		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.9934332114432108 | validation: 0.9630550795890347]
	TIME [epoch: 0.692 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7178468986404007		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.7178468986404007 | validation: 2.0011295784365184]
	TIME [epoch: 167 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4654089462322082		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.4654089462322082 | validation: 1.6233932983768256]
	TIME [epoch: 1.38 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.237029692113255		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.237029692113255 | validation: 1.1563112505951494]
	TIME [epoch: 1.35 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0649130583023827		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.0649130583023827 | validation: 0.9974414461182326]
	TIME [epoch: 1.35 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9761374802930814		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.9761374802930814 | validation: 0.9650180108302028]
	TIME [epoch: 1.35 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9374377637242732		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.9374377637242732 | validation: 0.9866918076203078]
	TIME [epoch: 1.35 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9519360908085653		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.9519360908085653 | validation: 0.9914513604687403]
	TIME [epoch: 1.35 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9735029208404785		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.9735029208404785 | validation: 1.123110188732634]
	TIME [epoch: 1.35 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1007904694615527		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.1007904694615527 | validation: 1.0764032002921406]
	TIME [epoch: 1.35 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1663155607342355		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.1663155607342355 | validation: 1.0879393261087191]
	TIME [epoch: 1.35 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9879052104274615		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.9879052104274615 | validation: 0.9120064945068016]
	TIME [epoch: 1.35 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9300593548915452		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.9300593548915452 | validation: 0.9013674714077862]
	TIME [epoch: 1.35 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9143859588203427		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.9143859588203427 | validation: 0.9391532627174165]
	TIME [epoch: 1.35 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9263742265391486		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.9263742265391486 | validation: 0.9348217912764604]
	TIME [epoch: 1.34 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.972095822761385		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.972095822761385 | validation: 1.0268890206763779]
	TIME [epoch: 1.35 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0864620536107297		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.0864620536107297 | validation: 1.131279062100608]
	TIME [epoch: 1.35 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0405259712512944		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.0405259712512944 | validation: 0.924760219101133]
	TIME [epoch: 1.35 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0678930264378006		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.0678930264378006 | validation: 1.035837959887163]
	TIME [epoch: 1.35 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.95832706375368		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.95832706375368 | validation: 0.9296129320512911]
	TIME [epoch: 1.35 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9389924942002199		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.9389924942002199 | validation: 0.9201862221162364]
	TIME [epoch: 1.35 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9413863481216348		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.9413863481216348 | validation: 0.9560256399567971]
	TIME [epoch: 1.35 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9806541780596209		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.9806541780596209 | validation: 0.935156501823245]
	TIME [epoch: 1.35 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9515225117240756		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.9515225117240756 | validation: 0.9237040387386956]
	TIME [epoch: 1.36 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9614371398183863		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.9614371398183863 | validation: 1.0058958279159234]
	TIME [epoch: 1.35 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9579960665779308		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.9579960665779308 | validation: 0.8746827814648717]
	TIME [epoch: 1.35 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9602023777683789		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.9602023777683789 | validation: 1.0007345413865472]
	TIME [epoch: 1.35 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9555163028108433		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.9555163028108433 | validation: 0.9079323849318475]
	TIME [epoch: 1.35 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9425278778596508		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.9425278778596508 | validation: 0.9186071907533077]
	TIME [epoch: 1.35 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9228043567139079		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.9228043567139079 | validation: 0.938963795301217]
	TIME [epoch: 1.35 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9273977182328393		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.9273977182328393 | validation: 0.8910701231756664]
	TIME [epoch: 1.35 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9218141118561363		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.9218141118561363 | validation: 0.8905118405741614]
	TIME [epoch: 1.35 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9065507751059012		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.9065507751059012 | validation: 1.0033977461559538]
	TIME [epoch: 1.35 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.940062173859898		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.940062173859898 | validation: 0.87265699627017]
	TIME [epoch: 1.35 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0214051552412304		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.0214051552412304 | validation: 1.0861018793661437]
	TIME [epoch: 1.35 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9670058254085766		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.9670058254085766 | validation: 0.8743882286465884]
	TIME [epoch: 1.35 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8938696121859855		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.8938696121859855 | validation: 0.8204050999941886]
	TIME [epoch: 1.35 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8983285513970068		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.8983285513970068 | validation: 0.9401876139154361]
	TIME [epoch: 1.35 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8823508517597656		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.8823508517597656 | validation: 0.8599159271496088]
	TIME [epoch: 1.35 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8649383216795329		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.8649383216795329 | validation: 0.8381299584159283]
	TIME [epoch: 1.35 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8848452478619487		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.8848452478619487 | validation: 1.01574179751407]
	TIME [epoch: 1.35 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9322224495235409		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.9322224495235409 | validation: 0.8985523950177852]
	TIME [epoch: 1.35 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9802605100318352		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.9802605100318352 | validation: 0.9527135828146394]
	TIME [epoch: 1.35 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8878669039090452		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.8878669039090452 | validation: 0.844398815293523]
	TIME [epoch: 1.35 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8672376187318112		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.8672376187318112 | validation: 0.8568008418755156]
	TIME [epoch: 1.35 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8542414598835548		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.8542414598835548 | validation: 0.8264785902086618]
	TIME [epoch: 1.35 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8746656979694908		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.8746656979694908 | validation: 0.9559681865938132]
	TIME [epoch: 1.35 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9028880103440186		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.9028880103440186 | validation: 0.8763136495563594]
	TIME [epoch: 1.35 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.895929311405268		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.895929311405268 | validation: 0.9016198387851251]
	TIME [epoch: 1.35 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8582644423085232		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.8582644423085232 | validation: 0.8310237451736013]
	TIME [epoch: 1.35 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8576254542229262		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.8576254542229262 | validation: 0.8366286905982723]
	TIME [epoch: 1.35 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8425331024048578		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.8425331024048578 | validation: 0.9021026112549546]
	TIME [epoch: 1.35 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8578039401070955		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.8578039401070955 | validation: 0.7719914614374095]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_252.pth
	Model improved!!!
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8487256691227407		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.8487256691227407 | validation: 0.8123292201858384]
	TIME [epoch: 1.35 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8257438635728056		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.8257438635728056 | validation: 0.8717769085565437]
	TIME [epoch: 1.35 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8253643349162081		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.8253643349162081 | validation: 0.7966634076360566]
	TIME [epoch: 1.35 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8947300580944978		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.8947300580944978 | validation: 1.0757511559094917]
	TIME [epoch: 1.35 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9566995574416159		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.9566995574416159 | validation: 0.8264384155968885]
	TIME [epoch: 1.35 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9231751693287614		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.9231751693287614 | validation: 0.8189910628349072]
	TIME [epoch: 1.35 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7992532076260892		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.7992532076260892 | validation: 0.8160120582831955]
	TIME [epoch: 1.35 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.794531261492098		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.794531261492098 | validation: 0.7808178698347608]
	TIME [epoch: 1.35 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7882559729735887		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.7882559729735887 | validation: 0.8335825512273853]
	TIME [epoch: 1.35 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.818520562283872		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.818520562283872 | validation: 0.9370841133550889]
	TIME [epoch: 1.35 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8853187873919376		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.8853187873919376 | validation: 0.854232848472903]
	TIME [epoch: 1.35 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9009384167615582		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.9009384167615582 | validation: 0.8454027377052538]
	TIME [epoch: 1.35 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.80585555677993		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.80585555677993 | validation: 0.8013379320048877]
	TIME [epoch: 1.35 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7812502953138308		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.7812502953138308 | validation: 0.721379093308801]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7951393295806721		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.7951393295806721 | validation: 0.8315690552524253]
	TIME [epoch: 1.36 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7921125072461286		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.7921125072461286 | validation: 0.7391192527518845]
	TIME [epoch: 1.36 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8076808003986173		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.8076808003986173 | validation: 0.8124121014756739]
	TIME [epoch: 1.36 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7885967620707744		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.7885967620707744 | validation: 0.8435451414134992]
	TIME [epoch: 1.35 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8015103317478481		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.8015103317478481 | validation: 0.8030856643141995]
	TIME [epoch: 1.35 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9092801881840032		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.9092801881840032 | validation: 0.9909733716888767]
	TIME [epoch: 1.35 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8731686712175425		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.8731686712175425 | validation: 0.8118641293772134]
	TIME [epoch: 1.35 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8149584583541516		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.8149584583541516 | validation: 0.7660304328635036]
	TIME [epoch: 1.35 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7591537832353342		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.7591537832353342 | validation: 0.7547746375683337]
	TIME [epoch: 1.35 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7491940007668686		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.7491940007668686 | validation: 0.7397749434125945]
	TIME [epoch: 1.35 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7427558557647416		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.7427558557647416 | validation: 0.773384864483076]
	TIME [epoch: 1.35 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7753581097247733		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.7753581097247733 | validation: 0.8376835398210561]
	TIME [epoch: 1.35 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8028705129367557		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.8028705129367557 | validation: 0.8202310293365855]
	TIME [epoch: 1.35 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8426291917581341		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.8426291917581341 | validation: 0.860768162651085]
	TIME [epoch: 1.35 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8063997950607404		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.8063997950607404 | validation: 0.7557335150204925]
	TIME [epoch: 1.35 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7920557175177318		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.7920557175177318 | validation: 0.8167423098212343]
	TIME [epoch: 1.35 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7596962891170097		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.7596962891170097 | validation: 0.7357249794231807]
	TIME [epoch: 1.35 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7575215877531443		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.7575215877531443 | validation: 0.7720446481698646]
	TIME [epoch: 1.35 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7695922274287454		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.7695922274287454 | validation: 0.7694109189977457]
	TIME [epoch: 1.35 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7769824347168617		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.7769824347168617 | validation: 0.8026706609368732]
	TIME [epoch: 1.36 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7662897443807457		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.7662897443807457 | validation: 0.7650636501429068]
	TIME [epoch: 1.35 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7522204151173654		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.7522204151173654 | validation: 0.776358702742463]
	TIME [epoch: 1.35 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7405380671785936		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.7405380671785936 | validation: 0.7121907459133676]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_289.pth
	Model improved!!!
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7505820556865807		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.7505820556865807 | validation: 0.8048027501693454]
	TIME [epoch: 1.35 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7536796113089637		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.7536796113089637 | validation: 0.7480108272365821]
	TIME [epoch: 1.35 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7893619522115072		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.7893619522115072 | validation: 0.7965353323649902]
	TIME [epoch: 1.35 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.743189392807075		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.743189392807075 | validation: 0.7388812071968723]
	TIME [epoch: 1.35 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7289586401461123		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.7289586401461123 | validation: 0.7382582758330342]
	TIME [epoch: 1.35 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.726411555185096		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.726411555185096 | validation: 0.762968704471623]
	TIME [epoch: 1.35 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7385542900831693		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.7385542900831693 | validation: 0.7351517806567447]
	TIME [epoch: 1.35 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7350103490478597		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.7350103490478597 | validation: 0.7500500065018048]
	TIME [epoch: 1.35 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7372211058132001		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.7372211058132001 | validation: 0.7593603492278199]
	TIME [epoch: 1.35 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7301564845951464		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.7301564845951464 | validation: 0.6921867095811126]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7357456759250951		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.7357456759250951 | validation: 0.7944148282740576]
	TIME [epoch: 1.36 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7332773010001233		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.7332773010001233 | validation: 0.7084362593656617]
	TIME [epoch: 1.36 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7372883864736482		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.7372883864736482 | validation: 0.7704560902671933]
	TIME [epoch: 1.36 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7138492049553813		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.7138492049553813 | validation: 0.6858356989435671]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_303.pth
	Model improved!!!
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7072099353502961		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.7072099353502961 | validation: 0.723487995327456]
	TIME [epoch: 1.35 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6987299622628186		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.6987299622628186 | validation: 0.7051887039546938]
	TIME [epoch: 1.35 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.710966470030882		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.710966470030882 | validation: 0.7189038757742077]
	TIME [epoch: 1.35 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7100223861519086		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.7100223861519086 | validation: 0.7063369442181598]
	TIME [epoch: 1.35 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7121954164906706		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.7121954164906706 | validation: 0.7667504932488273]
	TIME [epoch: 1.35 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7070918389714648		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.7070918389714648 | validation: 0.6887201054894115]
	TIME [epoch: 1.36 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7140256545438814		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.7140256545438814 | validation: 0.7501893203151848]
	TIME [epoch: 1.35 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7030623174895254		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.7030623174895254 | validation: 0.6966785763409975]
	TIME [epoch: 1.35 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6912003121101421		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.6912003121101421 | validation: 0.6761227092124742]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6750029400174011		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.6750029400174011 | validation: 0.7062131742162114]
	TIME [epoch: 1.35 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6737809453318727		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.6737809453318727 | validation: 0.6712056312605021]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_314.pth
	Model improved!!!
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6848122905163118		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.6848122905163118 | validation: 0.6979475837690057]
	TIME [epoch: 1.35 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6789589824773785		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.6789589824773785 | validation: 0.7456771029693798]
	TIME [epoch: 1.35 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6821092430876731		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.6821092430876731 | validation: 0.6994937586870622]
	TIME [epoch: 1.35 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7177169092421266		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.7177169092421266 | validation: 0.7284292150467446]
	TIME [epoch: 1.35 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6893100562251293		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.6893100562251293 | validation: 0.6921519565013996]
	TIME [epoch: 1.35 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6716962430422959		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.6716962430422959 | validation: 0.6301400156695622]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_320.pth
	Model improved!!!
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6609140078021178		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.6609140078021178 | validation: 0.6638816850097419]
	TIME [epoch: 1.36 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6553710606488337		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.6553710606488337 | validation: 0.6505404541506824]
	TIME [epoch: 1.36 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6416157833751278		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.6416157833751278 | validation: 0.6538329961634213]
	TIME [epoch: 1.36 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6602782053050199		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.6602782053050199 | validation: 0.7526240948983335]
	TIME [epoch: 1.36 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6980767633610102		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.6980767633610102 | validation: 0.6821700005273654]
	TIME [epoch: 1.36 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7135509464361749		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.7135509464361749 | validation: 0.6920309172481327]
	TIME [epoch: 1.35 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6512822155567553		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.6512822155567553 | validation: 0.6360144141892974]
	TIME [epoch: 1.35 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6344433569397174		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.6344433569397174 | validation: 0.6345549370548611]
	TIME [epoch: 1.35 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6290884071329389		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.6290884071329389 | validation: 0.6816970449318694]
	TIME [epoch: 1.35 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6482038717189782		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.6482038717189782 | validation: 0.6477922916324442]
	TIME [epoch: 1.36 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6483921545905039		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.6483921545905039 | validation: 0.6452221914136603]
	TIME [epoch: 1.36 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6515302320929445		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.6515302320929445 | validation: 0.6963822290018307]
	TIME [epoch: 1.36 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6499468465179902		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.6499468465179902 | validation: 0.62549411168566]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_333.pth
	Model improved!!!
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6574849133527048		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.6574849133527048 | validation: 0.6856394347478558]
	TIME [epoch: 1.36 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6370137699670164		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.6370137699670164 | validation: 0.6438931948451811]
	TIME [epoch: 1.35 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6363448761998773		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.6363448761998773 | validation: 0.6543906302123159]
	TIME [epoch: 1.35 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.63317794220794		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.63317794220794 | validation: 0.6513461064403834]
	TIME [epoch: 1.35 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.624746304727196		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.624746304727196 | validation: 0.6536096822584547]
	TIME [epoch: 1.35 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.623601009264257		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.623601009264257 | validation: 0.6163574992155567]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_339.pth
	Model improved!!!
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6375567357730546		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.6375567357730546 | validation: 0.6727633431784519]
	TIME [epoch: 1.36 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6258723620596169		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.6258723620596169 | validation: 0.6326662966443901]
	TIME [epoch: 1.36 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6189868848804793		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.6189868848804793 | validation: 0.6228368593116906]
	TIME [epoch: 1.35 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6118411329485977		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.6118411329485977 | validation: 0.6267216627002231]
	TIME [epoch: 1.35 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.619339604544862		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.619339604544862 | validation: 0.6190871314497541]
	TIME [epoch: 1.36 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6065108108488645		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.6065108108488645 | validation: 0.62028555313311]
	TIME [epoch: 1.36 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6090119435344585		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.6090119435344585 | validation: 0.6651981023081828]
	TIME [epoch: 1.36 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6112075631918114		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.6112075631918114 | validation: 0.6155172139989094]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_347.pth
	Model improved!!!
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6215712947595126		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.6215712947595126 | validation: 0.6587516898079576]
	TIME [epoch: 1.36 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6023228909612177		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.6023228909612177 | validation: 0.6020557790916327]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_349.pth
	Model improved!!!
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5965506775265516		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.5965506775265516 | validation: 0.6305577389567018]
	TIME [epoch: 1.36 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5936885810761602		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.5936885810761602 | validation: 0.6191767069913565]
	TIME [epoch: 1.36 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5946217649768214		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.5946217649768214 | validation: 0.6171849285099319]
	TIME [epoch: 1.35 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5970593036725521		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.5970593036725521 | validation: 0.6333682571797734]
	TIME [epoch: 1.35 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6076792495468946		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.6076792495468946 | validation: 0.6790691221530857]
	TIME [epoch: 1.35 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6067575558113114		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.6067575558113114 | validation: 0.615757192887582]
	TIME [epoch: 1.35 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5990001550876695		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.5990001550876695 | validation: 0.6197690705160901]
	TIME [epoch: 1.35 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5797955530440217		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.5797955530440217 | validation: 0.6014711483386556]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_357.pth
	Model improved!!!
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5783619186573032		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.5783619186573032 | validation: 0.6125623689054733]
	TIME [epoch: 1.35 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.578652593966234		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.578652593966234 | validation: 0.6495688211354407]
	TIME [epoch: 1.36 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6082155874979399		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.6082155874979399 | validation: 0.6080974018226646]
	TIME [epoch: 1.36 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5940544287909012		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.5940544287909012 | validation: 0.609646972643978]
	TIME [epoch: 1.36 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5727862569452711		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.5727862569452711 | validation: 0.6271553548578429]
	TIME [epoch: 1.36 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.562406982724704		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.562406982724704 | validation: 0.5814023264100399]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_363.pth
	Model improved!!!
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.57955674597488		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.57955674597488 | validation: 0.6506067629090807]
	TIME [epoch: 1.36 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5755485748835635		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.5755485748835635 | validation: 0.5937134987139973]
	TIME [epoch: 1.36 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5822467689595449		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.5822467689595449 | validation: 0.6140215948823021]
	TIME [epoch: 1.35 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5709576552076344		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.5709576552076344 | validation: 0.6083425379827414]
	TIME [epoch: 1.35 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5778839228932278		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.5778839228932278 | validation: 0.589623687015245]
	TIME [epoch: 1.35 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5729991660738162		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.5729991660738162 | validation: 0.5886276969172466]
	TIME [epoch: 1.35 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5662786029866987		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.5662786029866987 | validation: 0.6046482451163324]
	TIME [epoch: 1.35 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5571666704919221		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.5571666704919221 | validation: 0.5792953435691438]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_371.pth
	Model improved!!!
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5625367071223114		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.5625367071223114 | validation: 0.6119637823353989]
	TIME [epoch: 1.36 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5614618613546946		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.5614618613546946 | validation: 0.5971759398374253]
	TIME [epoch: 1.35 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5711350710694886		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.5711350710694886 | validation: 0.6147699254047604]
	TIME [epoch: 1.36 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5727619394147626		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.5727619394147626 | validation: 0.6030972692411777]
	TIME [epoch: 1.35 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.56692844772969		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.56692844772969 | validation: 0.5955201584498498]
	TIME [epoch: 1.36 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5495072455764958		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.5495072455764958 | validation: 0.5846057075451476]
	TIME [epoch: 1.35 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5519130861407663		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.5519130861407663 | validation: 0.5955249479058383]
	TIME [epoch: 1.35 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5421027580749443		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.5421027580749443 | validation: 0.5790838213290572]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_379.pth
	Model improved!!!
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5574555493479525		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.5574555493479525 | validation: 0.6091765392428833]
	TIME [epoch: 1.36 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5521776347787677		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.5521776347787677 | validation: 0.595518133426001]
	TIME [epoch: 1.35 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5564155400962146		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.5564155400962146 | validation: 0.5739271635424756]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_382.pth
	Model improved!!!
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5437154551056097		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.5437154551056097 | validation: 0.5604588829900102]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_383.pth
	Model improved!!!
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5388242592521446		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.5388242592521446 | validation: 0.5727552907609267]
	TIME [epoch: 1.36 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5294350943355374		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.5294350943355374 | validation: 0.5685388301226395]
	TIME [epoch: 1.35 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5492792021460217		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.5492792021460217 | validation: 0.6337128728777052]
	TIME [epoch: 1.35 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5568520895081154		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.5568520895081154 | validation: 0.5960425807832908]
	TIME [epoch: 1.35 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5538866872723198		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.5538866872723198 | validation: 0.5784670048851899]
	TIME [epoch: 1.35 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5354362691596107		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.5354362691596107 | validation: 0.5624076206445281]
	TIME [epoch: 1.35 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5332121822296988		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.5332121822296988 | validation: 0.5766245560208002]
	TIME [epoch: 1.35 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5301030031256534		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.5301030031256534 | validation: 0.5421300528078364]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_391.pth
	Model improved!!!
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5198975307899781		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.5198975307899781 | validation: 0.5609618625191195]
	TIME [epoch: 1.36 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5263385712214117		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.5263385712214117 | validation: 0.5725818871441126]
	TIME [epoch: 1.35 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5423032501086525		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.5423032501086525 | validation: 0.6498904633803524]
	TIME [epoch: 1.35 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5491896312404773		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.5491896312404773 | validation: 0.5798771898698121]
	TIME [epoch: 1.35 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.548199047589411		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.548199047589411 | validation: 0.5783030614408698]
	TIME [epoch: 1.35 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.522665759110779		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.522665759110779 | validation: 0.5462247427164594]
	TIME [epoch: 1.35 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.516163614555949		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.516163614555949 | validation: 0.5543322913500462]
	TIME [epoch: 1.35 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5123938053225633		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.5123938053225633 | validation: 0.5397031730430947]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_399.pth
	Model improved!!!
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5165188695436472		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.5165188695436472 | validation: 0.5664354686075453]
	TIME [epoch: 1.36 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5311648415616019		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.5311648415616019 | validation: 0.5618973958874064]
	TIME [epoch: 1.36 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5340401866178832		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.5340401866178832 | validation: 0.5863903419396118]
	TIME [epoch: 1.35 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5376495571337732		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.5376495571337732 | validation: 0.5650704079628477]
	TIME [epoch: 1.35 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.533168780988234		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.533168780988234 | validation: 0.596613734886661]
	TIME [epoch: 1.35 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5248743975372965		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.5248743975372965 | validation: 0.5453106186296085]
	TIME [epoch: 1.35 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5157526314660286		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.5157526314660286 | validation: 0.5595961105552409]
	TIME [epoch: 1.35 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5136959333408436		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.5136959333408436 | validation: 0.5542820305053356]
	TIME [epoch: 1.35 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5091824483277612		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.5091824483277612 | validation: 0.5449924122342893]
	TIME [epoch: 1.35 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5202104854856006		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.5202104854856006 | validation: 0.5677444694018139]
	TIME [epoch: 1.35 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5296041843055511		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.5296041843055511 | validation: 0.5701238396972912]
	TIME [epoch: 1.35 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5258485428629062		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.5258485428629062 | validation: 0.5361873441279088]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_411.pth
	Model improved!!!
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5200189185934202		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.5200189185934202 | validation: 0.5692853207412453]
	TIME [epoch: 1.37 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5146827720548941		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.5146827720548941 | validation: 0.5212704509951597]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_413.pth
	Model improved!!!
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5059037560693435		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.5059037560693435 | validation: 0.5734479580404525]
	TIME [epoch: 1.36 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5131204101056837		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.5131204101056837 | validation: 0.5900443936404438]
	TIME [epoch: 1.36 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.533486525623934		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.533486525623934 | validation: 0.5804695024945725]
	TIME [epoch: 1.96 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5217444668568367		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.5217444668568367 | validation: 0.5663301671798026]
	TIME [epoch: 1.35 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5169709949848577		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.5169709949848577 | validation: 0.5686867216243241]
	TIME [epoch: 1.35 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5121685889904944		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.5121685889904944 | validation: 0.5410611710608499]
	TIME [epoch: 1.36 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5035022165214224		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.5035022165214224 | validation: 0.5272893555667214]
	TIME [epoch: 1.36 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4956093446881283		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.4956093446881283 | validation: 0.5360663338884426]
	TIME [epoch: 1.35 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49322238398391777		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.49322238398391777 | validation: 0.5545946864933169]
	TIME [epoch: 1.35 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.501609082157917		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.501609082157917 | validation: 0.5686928732462143]
	TIME [epoch: 1.35 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5204637455270287		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.5204637455270287 | validation: 0.5848238899035625]
	TIME [epoch: 1.35 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5228181549513898		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.5228181549513898 | validation: 0.5415948058242405]
	TIME [epoch: 1.35 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.517179071243128		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.517179071243128 | validation: 0.5505651569695944]
	TIME [epoch: 1.36 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4995332060261744		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.4995332060261744 | validation: 0.5112468561765097]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_427.pth
	Model improved!!!
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49214020893088456		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.49214020893088456 | validation: 0.535030422610163]
	TIME [epoch: 1.36 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4902551216163842		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.4902551216163842 | validation: 0.5338292105235842]
	TIME [epoch: 1.36 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4883303350536632		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.4883303350536632 | validation: 0.5190859473697743]
	TIME [epoch: 1.36 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49297141558213614		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.49297141558213614 | validation: 0.5311119693762282]
	TIME [epoch: 1.35 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5089529441328456		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.5089529441328456 | validation: 0.5889144707414007]
	TIME [epoch: 1.35 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5174181851102988		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.5174181851102988 | validation: 0.552369068798558]
	TIME [epoch: 1.36 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.532392922477789		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.532392922477789 | validation: 0.5733064037959886]
	TIME [epoch: 1.36 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.503825821454162		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.503825821454162 | validation: 0.5211071589803911]
	TIME [epoch: 1.36 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4864969531894613		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.4864969531894613 | validation: 0.5035250920949135]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_436.pth
	Model improved!!!
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4776194367911878		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.4776194367911878 | validation: 0.5071440844241384]
	TIME [epoch: 1.36 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47219488550560024		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.47219488550560024 | validation: 0.5108515371567476]
	TIME [epoch: 1.36 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47742894954707554		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.47742894954707554 | validation: 0.5137853475910034]
	TIME [epoch: 1.36 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4904626582252453		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.4904626582252453 | validation: 0.5663226145130953]
	TIME [epoch: 1.35 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5214427757322515		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.5214427757322515 | validation: 0.542074139577899]
	TIME [epoch: 1.36 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5171109437688646		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.5171109437688646 | validation: 0.5139223019214403]
	TIME [epoch: 1.35 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48623919459817544		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.48623919459817544 | validation: 0.5225335127846549]
	TIME [epoch: 1.35 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47730005035164114		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.47730005035164114 | validation: 0.5218722223062958]
	TIME [epoch: 1.35 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48753445217698327		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.48753445217698327 | validation: 0.5242058302532687]
	TIME [epoch: 1.35 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4871446186331248		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.4871446186331248 | validation: 0.5646763753226977]
	TIME [epoch: 1.35 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49514551615741836		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.49514551615741836 | validation: 0.5199884209504726]
	TIME [epoch: 1.36 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49494459763203963		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.49494459763203963 | validation: 0.5413934030600294]
	TIME [epoch: 1.36 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5005244777430515		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.5005244777430515 | validation: 0.5021993456923343]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_449.pth
	Model improved!!!
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49522031305441444		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.49522031305441444 | validation: 0.5303654177967297]
	TIME [epoch: 1.36 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4880421500087295		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.4880421500087295 | validation: 0.4943939698574529]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_451.pth
	Model improved!!!
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4803440937140334		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.4803440937140334 | validation: 0.5119482837593614]
	TIME [epoch: 1.36 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4728176647874656		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.4728176647874656 | validation: 0.49842847563220344]
	TIME [epoch: 1.36 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4754695409907012		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.4754695409907012 | validation: 0.5194459546909505]
	TIME [epoch: 1.36 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47694422236383816		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.47694422236383816 | validation: 0.5072849522235425]
	TIME [epoch: 1.36 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48135129451439745		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.48135129451439745 | validation: 0.5178582706886433]
	TIME [epoch: 1.35 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4838912035822284		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.4838912035822284 | validation: 0.49840600089632575]
	TIME [epoch: 1.35 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48140173557336696		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.48140173557336696 | validation: 0.5115233302865321]
	TIME [epoch: 1.35 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4779998083506871		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.4779998083506871 | validation: 0.4962434259362496]
	TIME [epoch: 1.36 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46783415056472794		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.46783415056472794 | validation: 0.5027778585132745]
	TIME [epoch: 1.36 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4597641927515318		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.4597641927515318 | validation: 0.4784259065871108]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_461.pth
	Model improved!!!
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4706155523307867		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.4706155523307867 | validation: 0.6299529113189545]
	TIME [epoch: 1.36 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.527862325085463		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.527862325085463 | validation: 0.5688439228679515]
	TIME [epoch: 1.35 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5323772567655475		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.5323772567655475 | validation: 0.5136834733206769]
	TIME [epoch: 1.35 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4830034206343264		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.4830034206343264 | validation: 0.49538471887819996]
	TIME [epoch: 1.36 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4577521279198342		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.4577521279198342 | validation: 0.5079577222367165]
	TIME [epoch: 1.36 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4534718985699692		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.4534718985699692 | validation: 0.47507600091314006]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4509210154823126		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.4509210154823126 | validation: 0.48787344157980145]
	TIME [epoch: 1.36 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4548165658348844		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.4548165658348844 | validation: 0.5038692484184194]
	TIME [epoch: 1.35 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4536193862156964		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.4536193862156964 | validation: 0.5261621781240196]
	TIME [epoch: 1.35 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4710372607240381		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.4710372607240381 | validation: 0.5166216706216161]
	TIME [epoch: 1.35 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5069591297055642		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.5069591297055642 | validation: 0.5698022781210332]
	TIME [epoch: 1.35 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5215158150864936		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.5215158150864936 | validation: 0.5313893841198094]
	TIME [epoch: 1.35 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48446203148724065		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.48446203148724065 | validation: 0.4794661067325988]
	TIME [epoch: 1.35 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4534726800005564		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.4534726800005564 | validation: 0.4845287954426116]
	TIME [epoch: 1.36 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4449405248543728		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.4449405248543728 | validation: 0.4691051718414972]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_476.pth
	Model improved!!!
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44002224674578105		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.44002224674578105 | validation: 0.46461637519586696]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_477.pth
	Model improved!!!
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4472962197849047		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.4472962197849047 | validation: 0.49920216125815103]
	TIME [epoch: 1.35 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4513217976870836		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.4513217976870836 | validation: 0.5001490699236802]
	TIME [epoch: 1.36 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45856432420463467		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.45856432420463467 | validation: 0.4829763172740581]
	TIME [epoch: 1.35 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4594119476334481		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.4594119476334481 | validation: 0.48181761829814046]
	TIME [epoch: 1.35 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46060879230097923		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.46060879230097923 | validation: 0.49881692737894473]
	TIME [epoch: 1.35 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46713425556787713		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.46713425556787713 | validation: 0.506723593129755]
	TIME [epoch: 1.35 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48900594799066277		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.48900594799066277 | validation: 0.5964045790261401]
	TIME [epoch: 1.35 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5219405430331302		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.5219405430331302 | validation: 0.5300072063896332]
	TIME [epoch: 1.35 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5069391943495934		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.5069391943495934 | validation: 0.5096002283022572]
	TIME [epoch: 1.35 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4607582031187296		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.4607582031187296 | validation: 0.4619761368102667]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_487.pth
	Model improved!!!
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43644507321120385		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.43644507321120385 | validation: 0.4685011413879904]
	TIME [epoch: 1.35 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4319656439688057		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.4319656439688057 | validation: 0.4592059535536375]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_489.pth
	Model improved!!!
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43448121257254485		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.43448121257254485 | validation: 0.4600723361995462]
	TIME [epoch: 1.35 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42992557758725314		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.42992557758725314 | validation: 0.4663608267771676]
	TIME [epoch: 1.35 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42682408570907593		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.42682408570907593 | validation: 0.46582662441539746]
	TIME [epoch: 1.35 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42749739763895267		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.42749739763895267 | validation: 0.4652432565533799]
	TIME [epoch: 1.35 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4224568471559957		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.4224568471559957 | validation: 0.46353135019790664]
	TIME [epoch: 1.35 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44064672359171014		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.44064672359171014 | validation: 0.5535984260484906]
	TIME [epoch: 1.36 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49461832348955		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.49461832348955 | validation: 0.5002469638131848]
	TIME [epoch: 1.35 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4979887454033095		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.4979887454033095 | validation: 0.45723046938563494]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_497.pth
	Model improved!!!
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4380173429835006		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.4380173429835006 | validation: 0.45350501741969484]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_498.pth
	Model improved!!!
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41418870515293515		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.41418870515293515 | validation: 0.45353482580930804]
	TIME [epoch: 1.35 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41588233835927424		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.41588233835927424 | validation: 0.4273240972344291]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_500.pth
	Model improved!!!
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4971404563506745		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.4971404563506745 | validation: 0.5909134980175189]
	TIME [epoch: 169 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5139813663850299		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.5139813663850299 | validation: 0.4946453075968239]
	TIME [epoch: 2.7 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46861607026420066		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.46861607026420066 | validation: 0.4589659117691429]
	TIME [epoch: 2.68 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4376589711006663		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.4376589711006663 | validation: 0.48913548121814215]
	TIME [epoch: 2.68 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4353498593338916		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.4353498593338916 | validation: 0.48419797018107247]
	TIME [epoch: 2.68 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4365260237570609		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.4365260237570609 | validation: 0.4816075165568192]
	TIME [epoch: 2.68 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43412439873932146		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.43412439873932146 | validation: 0.4582602915971807]
	TIME [epoch: 2.68 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4274506016291224		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.4274506016291224 | validation: 0.45966519031293296]
	TIME [epoch: 2.68 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43904614458491686		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.43904614458491686 | validation: 0.47721918609109437]
	TIME [epoch: 2.68 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45675857913418433		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.45675857913418433 | validation: 0.4969881800256142]
	TIME [epoch: 2.68 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47091664455928195		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.47091664455928195 | validation: 0.4934029610594497]
	TIME [epoch: 2.68 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4807207590471431		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.4807207590471431 | validation: 0.4852680939188311]
	TIME [epoch: 2.68 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44653567540271366		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.44653567540271366 | validation: 0.45970318371227575]
	TIME [epoch: 2.69 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4332472380655702		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.4332472380655702 | validation: 0.494730483440508]
	TIME [epoch: 2.68 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45113915862945503		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.45113915862945503 | validation: 0.5053154781379526]
	TIME [epoch: 2.68 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45465132939077935		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.45465132939077935 | validation: 0.4786937257602344]
	TIME [epoch: 2.68 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4322333375755806		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.4322333375755806 | validation: 0.4614633218389063]
	TIME [epoch: 2.68 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4241080164748095		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.4241080164748095 | validation: 0.4324473566901829]
	TIME [epoch: 2.68 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4200506369847415		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.4200506369847415 | validation: 0.4489651324208782]
	TIME [epoch: 2.68 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4180332731959716		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.4180332731959716 | validation: 0.4505500467026419]
	TIME [epoch: 2.68 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4230118167652971		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.4230118167652971 | validation: 0.44993692247860206]
	TIME [epoch: 2.68 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4273922818409828		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.4273922818409828 | validation: 0.48371511414474516]
	TIME [epoch: 2.68 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45086800067559085		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.45086800067559085 | validation: 0.577055851969362]
	TIME [epoch: 2.68 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5301486038234123		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.5301486038234123 | validation: 0.5226760978198305]
	TIME [epoch: 2.69 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5071353960253502		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.5071353960253502 | validation: 0.46373401175962115]
	TIME [epoch: 2.68 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43412275506766607		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.43412275506766607 | validation: 0.4398195084916312]
	TIME [epoch: 2.68 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4172097068626782		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.4172097068626782 | validation: 0.42654184967456993]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_527.pth
	Model improved!!!
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42790940142502565		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.42790940142502565 | validation: 0.46462466222082394]
	TIME [epoch: 2.68 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4302817733885415		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.4302817733885415 | validation: 0.4390491930183238]
	TIME [epoch: 2.68 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43634066782142117		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.43634066782142117 | validation: 0.4651861717616723]
	TIME [epoch: 2.68 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4366264175618079		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.4366264175618079 | validation: 0.46581012536842015]
	TIME [epoch: 2.68 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43808275633625815		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.43808275633625815 | validation: 0.48324382074307537]
	TIME [epoch: 2.68 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4479630343790678		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.4479630343790678 | validation: 0.485734594466196]
	TIME [epoch: 2.68 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4479789157179132		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.4479789157179132 | validation: 0.50215101173421]
	TIME [epoch: 2.68 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4524681721456934		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.4524681721456934 | validation: 0.4495623932117795]
	TIME [epoch: 2.68 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4248307475881255		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.4248307475881255 | validation: 0.4506402154191532]
	TIME [epoch: 2.69 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4153536054460571		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.4153536054460571 | validation: 0.4341325066965873]
	TIME [epoch: 2.68 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42355268742054547		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.42355268742054547 | validation: 0.46531260226235105]
	TIME [epoch: 2.68 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43686348864063745		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.43686348864063745 | validation: 0.4595229617408938]
	TIME [epoch: 2.68 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4498155761622402		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.4498155761622402 | validation: 0.48303355897823774]
	TIME [epoch: 2.68 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4548782000428803		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.4548782000428803 | validation: 0.46244226965803076]
	TIME [epoch: 2.68 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43836943036680004		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.43836943036680004 | validation: 0.44756470744843285]
	TIME [epoch: 2.68 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4266436711697689		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.4266436711697689 | validation: 0.4760648604141956]
	TIME [epoch: 2.68 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44388038998276336		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.44388038998276336 | validation: 0.5045251130561639]
	TIME [epoch: 2.68 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4477352120252959		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.4477352120252959 | validation: 0.454479471282321]
	TIME [epoch: 2.68 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42420288539653894		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.42420288539653894 | validation: 0.4325723647343514]
	TIME [epoch: 2.69 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.425634805285087		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.425634805285087 | validation: 0.4376780407024461]
	TIME [epoch: 2.68 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4310854713711085		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.4310854713711085 | validation: 0.43230675176615696]
	TIME [epoch: 2.68 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4292786210198996		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.4292786210198996 | validation: 0.43596633027507165]
	TIME [epoch: 2.68 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42869709262306915		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.42869709262306915 | validation: 0.46006874501628225]
	TIME [epoch: 2.68 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42683283895825264		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.42683283895825264 | validation: 0.4651682767674535]
	TIME [epoch: 2.68 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4340223487622139		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.4340223487622139 | validation: 0.47811161915091543]
	TIME [epoch: 2.68 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44350299538047855		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.44350299538047855 | validation: 0.4512178017390331]
	TIME [epoch: 2.68 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4363278561761797		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.4363278561761797 | validation: 0.44502286071999003]
	TIME [epoch: 2.68 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42431202092197795		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.42431202092197795 | validation: 0.45938054397834893]
	TIME [epoch: 2.68 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4215184155578062		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.4215184155578062 | validation: 0.4526330429683243]
	TIME [epoch: 2.68 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41682906631099975		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.41682906631099975 | validation: 0.4252096493788951]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_557.pth
	Model improved!!!
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4184537802057423		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.4184537802057423 | validation: 0.4631447776978001]
	TIME [epoch: 2.68 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42405384568577176		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.42405384568577176 | validation: 0.4266222171452725]
	TIME [epoch: 2.68 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4278223853320061		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.4278223853320061 | validation: 0.4640090192987219]
	TIME [epoch: 2.68 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4330727692278212		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.4330727692278212 | validation: 0.4427017807179466]
	TIME [epoch: 2.69 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4271224722825339		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.4271224722825339 | validation: 0.4670551446976702]
	TIME [epoch: 2.68 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43095768920080174		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.43095768920080174 | validation: 0.4504577335944845]
	TIME [epoch: 2.68 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4401208386113215		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.4401208386113215 | validation: 0.4813882373370277]
	TIME [epoch: 2.69 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4437302966942734		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.4437302966942734 | validation: 0.48540395743139175]
	TIME [epoch: 2.68 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42922410471517425		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.42922410471517425 | validation: 0.41660325682953403]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_566.pth
	Model improved!!!
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41352305892215996		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.41352305892215996 | validation: 0.42346795935198733]
	TIME [epoch: 2.68 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4126196903056071		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.4126196903056071 | validation: 0.4458438249418876]
	TIME [epoch: 2.69 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40934329363682054		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.40934329363682054 | validation: 0.41078078576160515]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_569.pth
	Model improved!!!
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40819506928546473		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.40819506928546473 | validation: 0.42192215440731734]
	TIME [epoch: 2.68 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40911314182069336		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.40911314182069336 | validation: 0.4482290955732898]
	TIME [epoch: 2.68 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4153579683810816		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.4153579683810816 | validation: 0.469684298759262]
	TIME [epoch: 2.68 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45769325028175994		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.45769325028175994 | validation: 0.5030004958298395]
	TIME [epoch: 2.68 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49130948474206526		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.49130948474206526 | validation: 0.4659717896660497]
	TIME [epoch: 2.68 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4405887565850459		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.4405887565850459 | validation: 0.41611739055650654]
	TIME [epoch: 2.68 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4124433907119469		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.4124433907119469 | validation: 0.4385434369986076]
	TIME [epoch: 2.68 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.401843081241449		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.401843081241449 | validation: 0.4258554178454288]
	TIME [epoch: 2.68 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4170965689116818		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.4170965689116818 | validation: 0.4353177983134881]
	TIME [epoch: 2.68 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4194042665170001		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.4194042665170001 | validation: 0.4505879236024368]
	TIME [epoch: 2.69 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42092708406133755		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.42092708406133755 | validation: 0.413808011584192]
	TIME [epoch: 2.68 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40974358523068377		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.40974358523068377 | validation: 0.41274330221776545]
	TIME [epoch: 2.68 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4112849828031888		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.4112849828031888 | validation: 0.418023649353211]
	TIME [epoch: 2.68 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40611906739393283		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.40611906739393283 | validation: 0.43278637543320375]
	TIME [epoch: 2.68 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4174470857172586		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.4174470857172586 | validation: 0.4780846929492228]
	TIME [epoch: 2.68 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42500666557176003		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.42500666557176003 | validation: 0.49686272383445385]
	TIME [epoch: 2.68 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4557451870468755		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.4557451870468755 | validation: 0.4475698683927243]
	TIME [epoch: 2.69 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4331459703660069		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.4331459703660069 | validation: 0.43006130259102004]
	TIME [epoch: 2.68 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41852382838718016		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.41852382838718016 | validation: 0.43249790983064323]
	TIME [epoch: 2.68 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41432743848262155		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.41432743848262155 | validation: 0.44189759944270535]
	TIME [epoch: 2.68 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41037465717570587		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.41037465717570587 | validation: 0.41194739148994053]
	TIME [epoch: 2.69 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.411266654732907		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.411266654732907 | validation: 0.41674754095595956]
	TIME [epoch: 2.68 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4039620261902323		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.4039620261902323 | validation: 0.4372437747636589]
	TIME [epoch: 2.68 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40466464108001793		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.40466464108001793 | validation: 0.41293110410793743]
	TIME [epoch: 2.68 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.402371531936398		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.402371531936398 | validation: 0.41549910950792185]
	TIME [epoch: 2.68 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4036525993455458		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.4036525993455458 | validation: 0.44919980763431344]
	TIME [epoch: 2.68 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4134538529782523		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.4134538529782523 | validation: 0.48028296016114835]
	TIME [epoch: 2.68 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4348017387894168		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.4348017387894168 | validation: 0.4678048238848358]
	TIME [epoch: 2.68 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4483697269193426		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.4483697269193426 | validation: 0.4515594708138424]
	TIME [epoch: 2.68 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4386130641017201		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.4386130641017201 | validation: 0.4379456447262623]
	TIME [epoch: 2.68 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4161801160914474		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.4161801160914474 | validation: 0.42644265554177496]
	TIME [epoch: 2.68 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40122126600362473		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.40122126600362473 | validation: 0.39347377297267416]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_601.pth
	Model improved!!!
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40340381332714603		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.40340381332714603 | validation: 0.43067034393750236]
	TIME [epoch: 2.68 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40161386795745146		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.40161386795745146 | validation: 0.422879661524095]
	TIME [epoch: 2.68 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4097421383076009		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.4097421383076009 | validation: 0.43945432338901774]
	TIME [epoch: 2.68 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4126679040314967		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.4126679040314967 | validation: 0.4391566763011196]
	TIME [epoch: 2.68 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4181059862334124		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.4181059862334124 | validation: 0.41057504037618975]
	TIME [epoch: 2.68 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40840724229671055		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.40840724229671055 | validation: 0.4001502027424051]
	TIME [epoch: 2.68 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40013990400632915		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.40013990400632915 | validation: 0.43133197026361]
	TIME [epoch: 2.68 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4039611126629311		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.4039611126629311 | validation: 0.4351868930045408]
	TIME [epoch: 2.68 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4104875877604521		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.4104875877604521 | validation: 0.4484625121324509]
	TIME [epoch: 2.68 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4324097106157615		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.4324097106157615 | validation: 0.4502415120876361]
	TIME [epoch: 2.69 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43170542889803953		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.43170542889803953 | validation: 0.4113156322799098]
	TIME [epoch: 2.68 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41194894867259135		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.41194894867259135 | validation: 0.4252806758432427]
	TIME [epoch: 2.68 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4039223481686811		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.4039223481686811 | validation: 0.45109447882429166]
	TIME [epoch: 2.68 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4163332878035835		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.4163332878035835 | validation: 0.44110248655058726]
	TIME [epoch: 2.68 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42186270577261764		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.42186270577261764 | validation: 0.4403206907714739]
	TIME [epoch: 2.68 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4055305915207339		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.4055305915207339 | validation: 0.43480215612561485]
	TIME [epoch: 2.68 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4015122753462174		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.4015122753462174 | validation: 0.4156243383946001]
	TIME [epoch: 2.68 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40004760895169783		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.40004760895169783 | validation: 0.4363729851690707]
	TIME [epoch: 2.68 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4024072515301232		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.4024072515301232 | validation: 0.41786809015214993]
	TIME [epoch: 2.69 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40736648303253076		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.40736648303253076 | validation: 0.43438469944290264]
	TIME [epoch: 2.68 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40556230679108796		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.40556230679108796 | validation: 0.41420953031481517]
	TIME [epoch: 2.69 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40278108540290836		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.40278108540290836 | validation: 0.43017568416222596]
	TIME [epoch: 2.69 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.413947451642113		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.413947451642113 | validation: 0.4378395478124758]
	TIME [epoch: 2.68 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.414401176774265		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.414401176774265 | validation: 0.427646925593203]
	TIME [epoch: 2.68 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41788880552385893		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.41788880552385893 | validation: 0.44175511472608353]
	TIME [epoch: 2.68 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40547501192332575		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.40547501192332575 | validation: 0.4390401795424021]
	TIME [epoch: 2.68 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40676542478010985		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.40676542478010985 | validation: 0.4205502674405889]
	TIME [epoch: 2.68 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4030172262201307		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.4030172262201307 | validation: 0.41777076951036407]
	TIME [epoch: 2.68 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40100520322501937		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.40100520322501937 | validation: 0.4015574161858133]
	TIME [epoch: 2.68 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3970262271386541		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.3970262271386541 | validation: 0.42455995591244655]
	TIME [epoch: 2.68 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4059089969660947		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.4059089969660947 | validation: 0.4321803577974224]
	TIME [epoch: 2.68 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4079374078084657		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.4079374078084657 | validation: 0.41222788246044983]
	TIME [epoch: 2.68 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4129909101696409		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.4129909101696409 | validation: 0.42758182212272344]
	TIME [epoch: 2.69 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4088898457036585		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.4088898457036585 | validation: 0.41662017490508135]
	TIME [epoch: 2.68 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40579910765756083		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.40579910765756083 | validation: 0.37766532309798323]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_636.pth
	Model improved!!!
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4019675584620345		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.4019675584620345 | validation: 0.42005220385215036]
	TIME [epoch: 2.68 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4000217424738314		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.4000217424738314 | validation: 0.4322661998430796]
	TIME [epoch: 2.68 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39840973889660164		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.39840973889660164 | validation: 0.4073299714361211]
	TIME [epoch: 2.68 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4008395571035204		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.4008395571035204 | validation: 0.421129375095433]
	TIME [epoch: 2.68 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40460030859875434		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.40460030859875434 | validation: 0.4175569726124586]
	TIME [epoch: 2.68 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41049286962020143		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.41049286962020143 | validation: 0.4310169298990382]
	TIME [epoch: 2.68 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40640254938300585		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.40640254938300585 | validation: 0.4337763317505058]
	TIME [epoch: 2.68 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40764532956686006		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.40764532956686006 | validation: 0.4308921815613566]
	TIME [epoch: 2.69 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40754658148069667		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.40754658148069667 | validation: 0.4261495898567869]
	TIME [epoch: 2.68 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40153842995804734		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.40153842995804734 | validation: 0.395875824524459]
	TIME [epoch: 2.68 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39955148880936475		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.39955148880936475 | validation: 0.41685435258277614]
	TIME [epoch: 2.68 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39001449114896986		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.39001449114896986 | validation: 0.40409865721402183]
	TIME [epoch: 2.68 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3947748653873304		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.3947748653873304 | validation: 0.42701723872580155]
	TIME [epoch: 2.68 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4026222760862035		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.4026222760862035 | validation: 0.4105519947348928]
	TIME [epoch: 2.68 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3961042297252187		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.3961042297252187 | validation: 0.42315036676647805]
	TIME [epoch: 2.67 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39209925055244116		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.39209925055244116 | validation: 0.408023466657151]
	TIME [epoch: 2.67 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3893687526180073		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.3893687526180073 | validation: 0.4014751863555106]
	TIME [epoch: 2.67 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39193588059690526		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.39193588059690526 | validation: 0.4102562523599811]
	TIME [epoch: 2.67 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4008967218616728		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.4008967218616728 | validation: 0.4482203456305877]
	TIME [epoch: 2.68 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4156311165225957		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.4156311165225957 | validation: 0.45311670238768476]
	TIME [epoch: 2.67 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42529755639559386		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.42529755639559386 | validation: 0.44242332266517065]
	TIME [epoch: 2.67 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4151720457784952		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.4151720457784952 | validation: 0.4340584057246837]
	TIME [epoch: 2.67 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3987307263916115		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.3987307263916115 | validation: 0.40707174741824503]
	TIME [epoch: 2.67 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38861866358971514		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.38861866358971514 | validation: 0.39569045752324583]
	TIME [epoch: 2.68 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3870093339308054		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.3870093339308054 | validation: 0.40321362603472066]
	TIME [epoch: 2.68 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39048620341910145		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.39048620341910145 | validation: 0.3938046945145186]
	TIME [epoch: 2.67 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3941331740486637		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.3941331740486637 | validation: 0.4082064160350942]
	TIME [epoch: 2.68 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3901217201686943		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.3901217201686943 | validation: 0.40574457792205476]
	TIME [epoch: 2.68 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3894073867636086		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.3894073867636086 | validation: 0.3883662443733579]
	TIME [epoch: 2.68 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38935610626940775		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.38935610626940775 | validation: 0.42123884707064985]
	TIME [epoch: 2.68 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40432873016551785		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.40432873016551785 | validation: 0.4399306033258868]
	TIME [epoch: 2.68 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4118444368281618		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.4118444368281618 | validation: 0.4433296473419307]
	TIME [epoch: 2.68 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4115870231356579		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.4115870231356579 | validation: 0.45287701621713233]
	TIME [epoch: 2.68 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4066866327403843		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.4066866327403843 | validation: 0.4180540235740258]
	TIME [epoch: 2.67 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3988252070628597		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.3988252070628597 | validation: 0.4002955570889638]
	TIME [epoch: 2.67 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38425047599767925		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.38425047599767925 | validation: 0.4038193281757835]
	TIME [epoch: 2.68 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37944782703705715		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.37944782703705715 | validation: 0.4062393093131418]
	TIME [epoch: 2.67 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39199723498902417		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.39199723498902417 | validation: 0.4215351600032395]
	TIME [epoch: 2.68 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38784542426369717		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.38784542426369717 | validation: 0.39375928206430966]
	TIME [epoch: 2.67 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3958017864989442		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.3958017864989442 | validation: 0.41181337570509835]
	TIME [epoch: 2.68 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39526522039804735		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.39526522039804735 | validation: 0.4066763324524616]
	TIME [epoch: 2.68 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3974932260087146		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.3974932260087146 | validation: 0.3942478590014963]
	TIME [epoch: 2.68 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39951229144824413		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.39951229144824413 | validation: 0.434881753723132]
	TIME [epoch: 2.67 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3990890447694448		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.3990890447694448 | validation: 0.41773461284607016]
	TIME [epoch: 2.67 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39100967231580913		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.39100967231580913 | validation: 0.3748900668344979]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_681.pth
	Model improved!!!
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39112535521769465		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.39112535521769465 | validation: 0.4219435988529927]
	TIME [epoch: 2.67 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3922976829999465		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.3922976829999465 | validation: 0.38619467755596193]
	TIME [epoch: 2.66 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38214541139875546		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.38214541139875546 | validation: 0.394721478402269]
	TIME [epoch: 2.66 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37797982386899653		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.37797982386899653 | validation: 0.41483701310740373]
	TIME [epoch: 2.66 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.388401033902053		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.388401033902053 | validation: 0.42839983871502113]
	TIME [epoch: 2.67 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3971622641879705		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.3971622641879705 | validation: 0.42735490885661975]
	TIME [epoch: 2.67 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39563569727450687		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.39563569727450687 | validation: 0.4197289530167623]
	TIME [epoch: 2.68 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3891501695848676		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.3891501695848676 | validation: 0.40497540616546995]
	TIME [epoch: 2.68 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37968960999293433		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.37968960999293433 | validation: 0.3904826947204029]
	TIME [epoch: 2.68 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3898378149980307		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.3898378149980307 | validation: 0.42121539072283537]
	TIME [epoch: 2.67 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4032233715741134		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.4032233715741134 | validation: 0.45407090755270607]
	TIME [epoch: 2.68 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4130558635123251		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.4130558635123251 | validation: 0.4086138637688723]
	TIME [epoch: 2.67 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3878099563398698		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.3878099563398698 | validation: 0.38440522275776046]
	TIME [epoch: 2.68 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.389063423180198		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.389063423180198 | validation: 0.39665836955790273]
	TIME [epoch: 2.67 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3870435496756058		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.3870435496756058 | validation: 0.3944740604953114]
	TIME [epoch: 2.67 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37904377365691133		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.37904377365691133 | validation: 0.4010797181738779]
	TIME [epoch: 2.68 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37914495565686834		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.37914495565686834 | validation: 0.40484485741647563]
	TIME [epoch: 2.68 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37994395667335684		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.37994395667335684 | validation: 0.41516875484036914]
	TIME [epoch: 2.67 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39495862484424654		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.39495862484424654 | validation: 0.45329336386422425]
	TIME [epoch: 2.68 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4017295865799143		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.4017295865799143 | validation: 0.4218069398825245]
	TIME [epoch: 2.67 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3913750579759707		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.3913750579759707 | validation: 0.3755243262574614]
	TIME [epoch: 2.67 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38909244952337363		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.38909244952337363 | validation: 0.4094916216326727]
	TIME [epoch: 2.68 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39023772582528166		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.39023772582528166 | validation: 0.4044235796042845]
	TIME [epoch: 2.68 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39104838247437146		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.39104838247437146 | validation: 0.39878398973675155]
	TIME [epoch: 2.67 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38671231099297293		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.38671231099297293 | validation: 0.40348336520410727]
	TIME [epoch: 2.67 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3802784953871168		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.3802784953871168 | validation: 0.40611814480763314]
	TIME [epoch: 2.68 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3864811694983935		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.3864811694983935 | validation: 0.40873681476906293]
	TIME [epoch: 2.68 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37766508368359325		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.37766508368359325 | validation: 0.4041878238916143]
	TIME [epoch: 2.68 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38406266323459437		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.38406266323459437 | validation: 0.4078048897704452]
	TIME [epoch: 2.67 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38191522093591607		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.38191522093591607 | validation: 0.39631712390359364]
	TIME [epoch: 2.68 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38567032385158084		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.38567032385158084 | validation: 0.3889836896795876]
	TIME [epoch: 2.67 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38275411894718814		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.38275411894718814 | validation: 0.4008581070727461]
	TIME [epoch: 2.67 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3814961762858628		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.3814961762858628 | validation: 0.41244168537780124]
	TIME [epoch: 2.67 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3805654872913066		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.3805654872913066 | validation: 0.40636090276734554]
	TIME [epoch: 2.67 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3800813820299029		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.3800813820299029 | validation: 0.3958859450057223]
	TIME [epoch: 2.67 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38228551769208563		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.38228551769208563 | validation: 0.3859372346149386]
	TIME [epoch: 2.68 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3828966402664109		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.3828966402664109 | validation: 0.41136752578089314]
	TIME [epoch: 2.68 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38347150373332545		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.38347150373332545 | validation: 0.44362786989194414]
	TIME [epoch: 2.68 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40310226824466744		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.40310226824466744 | validation: 0.4374053736168835]
	TIME [epoch: 2.67 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4184415263883223		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.4184415263883223 | validation: 0.39250368986013356]
	TIME [epoch: 2.68 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37796589530378555		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.37796589530378555 | validation: 0.4022531868048727]
	TIME [epoch: 2.68 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37712948834696436		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.37712948834696436 | validation: 0.38771553984983903]
	TIME [epoch: 2.68 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38021667520084335		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.38021667520084335 | validation: 0.394695146487633]
	TIME [epoch: 2.67 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3802530359173888		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.3802530359173888 | validation: 0.39578978480637]
	TIME [epoch: 2.67 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3727686295076144		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.3727686295076144 | validation: 0.3910940741164571]
	TIME [epoch: 2.67 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37269613561016435		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.37269613561016435 | validation: 0.4079994849395019]
	TIME [epoch: 2.68 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37470097274656883		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.37470097274656883 | validation: 0.4007672836689244]
	TIME [epoch: 2.67 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3739205485140872		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.3739205485140872 | validation: 0.38848446396733616]
	TIME [epoch: 2.68 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3863729266528743		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.3863729266528743 | validation: 0.4050753178126166]
	TIME [epoch: 2.68 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39867277767767234		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.39867277767767234 | validation: 0.41549204843761933]
	TIME [epoch: 2.67 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3964109953413079		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.3964109953413079 | validation: 0.4197247188359917]
	TIME [epoch: 2.67 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3833266689712316		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.3833266689712316 | validation: 0.387114183201988]
	TIME [epoch: 2.69 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3776254864034394		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.3776254864034394 | validation: 0.40372131075716294]
	TIME [epoch: 2.67 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3786451804032048		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.3786451804032048 | validation: 0.38890422453140916]
	TIME [epoch: 2.68 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3789429607188985		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.3789429607188985 | validation: 0.3816231127681061]
	TIME [epoch: 2.67 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3750791893807292		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.3750791893807292 | validation: 0.38060664882687006]
	TIME [epoch: 2.67 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3735014257199154		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.3735014257199154 | validation: 0.3905109705087891]
	TIME [epoch: 2.67 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37170056435566184		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.37170056435566184 | validation: 0.3832099485680341]
	TIME [epoch: 2.67 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3731052800643444		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.3731052800643444 | validation: 0.38135969292362604]
	TIME [epoch: 2.68 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37114275745100755		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.37114275745100755 | validation: 0.3919864627718257]
	TIME [epoch: 2.68 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3710175230423553		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.3710175230423553 | validation: 0.3929558264405904]
	TIME [epoch: 2.68 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3722712256606168		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.3722712256606168 | validation: 0.41385810327404854]
	TIME [epoch: 2.67 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38907795144632806		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.38907795144632806 | validation: 0.4471068127506936]
	TIME [epoch: 2.68 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41283618517621135		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.41283618517621135 | validation: 0.40812215678459796]
	TIME [epoch: 2.67 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39706340887854596		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.39706340887854596 | validation: 0.40371934810218824]
	TIME [epoch: 2.68 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37674525656379915		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.37674525656379915 | validation: 0.391327951837075]
	TIME [epoch: 2.67 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37117702969623123		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.37117702969623123 | validation: 0.38707028368605]
	TIME [epoch: 2.67 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3777938043033905		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.3777938043033905 | validation: 0.3921473770083279]
	TIME [epoch: 2.67 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36972561844830365		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.36972561844830365 | validation: 0.39770470543839587]
	TIME [epoch: 2.67 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36902468614173906		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.36902468614173906 | validation: 0.38970482812097434]
	TIME [epoch: 2.67 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37333975673169817		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.37333975673169817 | validation: 0.4115551997039674]
	TIME [epoch: 2.67 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37720230958878587		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.37720230958878587 | validation: 0.3983976547147323]
	TIME [epoch: 2.68 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37237026113004057		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.37237026113004057 | validation: 0.4008553727709062]
	TIME [epoch: 2.67 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3810720373726943		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.3810720373726943 | validation: 0.40376272404192326]
	TIME [epoch: 2.68 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3771712429367366		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.3771712429367366 | validation: 0.3994163471005954]
	TIME [epoch: 2.67 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3813391641496316		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.3813391641496316 | validation: 0.39143142251799345]
	TIME [epoch: 2.67 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37099719166233064		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.37099719166233064 | validation: 0.3960689194342455]
	TIME [epoch: 2.67 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36783841574959963		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.36783841574959963 | validation: 0.3889426875213917]
	TIME [epoch: 2.68 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3730865516842725		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.3730865516842725 | validation: 0.4061468073937272]
	TIME [epoch: 2.68 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3766053436095503		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.3766053436095503 | validation: 0.3732356887627729]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_761.pth
	Model improved!!!
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37739167435114923		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.37739167435114923 | validation: 0.38727919866169014]
	TIME [epoch: 2.66 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37801662785352014		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.37801662785352014 | validation: 0.42744341443279243]
	TIME [epoch: 2.66 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3822614080289515		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.3822614080289515 | validation: 0.4065146081438366]
	TIME [epoch: 2.66 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37970866474125586		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.37970866474125586 | validation: 0.37504153152811015]
	TIME [epoch: 2.66 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3717601370962684		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.3717601370962684 | validation: 0.4038328486735136]
	TIME [epoch: 2.67 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3706303267937796		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.3706303267937796 | validation: 0.39175119659549573]
	TIME [epoch: 2.66 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3660179691850858		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.3660179691850858 | validation: 0.3817625642349012]
	TIME [epoch: 2.67 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3661593998753114		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.3661593998753114 | validation: 0.3704132182423669]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_769.pth
	Model improved!!!
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36818046582986264		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.36818046582986264 | validation: 0.4102558928904796]
	TIME [epoch: 2.66 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36433428868883666		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.36433428868883666 | validation: 0.376403653973705]
	TIME [epoch: 2.66 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36498028416663075		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.36498028416663075 | validation: 0.38778143169666013]
	TIME [epoch: 2.66 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3681188519838496		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.3681188519838496 | validation: 0.4017674070739252]
	TIME [epoch: 2.66 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3766982527913662		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.3766982527913662 | validation: 0.42237839102409525]
	TIME [epoch: 2.66 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3907148321823216		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.3907148321823216 | validation: 0.4051116457662993]
	TIME [epoch: 2.66 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3756081917441284		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.3756081917441284 | validation: 0.3846024994070694]
	TIME [epoch: 2.67 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3710990889084287		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.3710990889084287 | validation: 0.38434583893202323]
	TIME [epoch: 2.67 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3668378313005764		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.3668378313005764 | validation: 0.39867119726331696]
	TIME [epoch: 2.66 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3655231236347252		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.3655231236347252 | validation: 0.3821043864697772]
	TIME [epoch: 2.67 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36573936975496124		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.36573936975496124 | validation: 0.3770607077661001]
	TIME [epoch: 2.67 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3740496133672741		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.3740496133672741 | validation: 0.4285541605309597]
	TIME [epoch: 2.66 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37605930382144065		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.37605930382144065 | validation: 0.3936041368513299]
	TIME [epoch: 2.66 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3822280854038569		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.3822280854038569 | validation: 0.3882996025387539]
	TIME [epoch: 2.66 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36684233398482025		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.36684233398482025 | validation: 0.37540995552458706]
	TIME [epoch: 2.67 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36186293728567465		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.36186293728567465 | validation: 0.3991225171634894]
	TIME [epoch: 2.66 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36391324269690545		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.36391324269690545 | validation: 0.3769754368489894]
	TIME [epoch: 2.67 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.365582586939386		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.365582586939386 | validation: 0.3785567303956787]
	TIME [epoch: 2.66 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36515074109789397		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.36515074109789397 | validation: 0.3811337156161395]
	TIME [epoch: 2.67 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3624214359937046		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.3624214359937046 | validation: 0.4040763482927246]
	TIME [epoch: 2.67 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36039878971953415		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.36039878971953415 | validation: 0.377465552975753]
	TIME [epoch: 2.67 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36494858122286367		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.36494858122286367 | validation: 0.383760942319463]
	TIME [epoch: 2.67 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36320547374663364		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.36320547374663364 | validation: 0.39877825785296334]
	TIME [epoch: 2.68 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36994187088349123		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.36994187088349123 | validation: 0.4242107452748181]
	TIME [epoch: 2.68 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39442256318874563		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.39442256318874563 | validation: 0.4227211797723744]
	TIME [epoch: 2.67 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3843986342931922		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.3843986342931922 | validation: 0.390985837381632]
	TIME [epoch: 2.68 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36846710853797193		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.36846710853797193 | validation: 0.39834597556291734]
	TIME [epoch: 2.68 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.361247014024118		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.361247014024118 | validation: 0.40026995114982694]
	TIME [epoch: 2.68 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3642633996525382		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.3642633996525382 | validation: 0.37479434888848806]
	TIME [epoch: 2.68 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3611260449988775		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.3611260449988775 | validation: 0.3946978499676663]
	TIME [epoch: 2.69 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36251581987342346		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.36251581987342346 | validation: 0.4017794988976336]
	TIME [epoch: 2.68 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3637398501777759		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.3637398501777759 | validation: 0.3741105050224237]
	TIME [epoch: 2.67 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36307852808407787		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.36307852808407787 | validation: 0.360069630614992]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_802.pth
	Model improved!!!
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3582737600345701		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.3582737600345701 | validation: 0.40089005789121723]
	TIME [epoch: 2.67 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36200273306447417		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.36200273306447417 | validation: 0.38669576350774976]
	TIME [epoch: 2.68 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3658337432025897		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.3658337432025897 | validation: 0.40492492856514417]
	TIME [epoch: 2.67 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3685588984100683		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.3685588984100683 | validation: 0.3958495971811731]
	TIME [epoch: 2.67 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37092959927541463		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.37092959927541463 | validation: 0.41795695510734093]
	TIME [epoch: 2.67 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3809667128491546		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.3809667128491546 | validation: 0.38776221605708616]
	TIME [epoch: 2.68 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3675673505842711		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.3675673505842711 | validation: 0.41302309191073283]
	TIME [epoch: 2.66 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35797273386566003		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.35797273386566003 | validation: 0.3836860274843187]
	TIME [epoch: 2.67 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36080707736990675		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.36080707736990675 | validation: 0.37467532583867114]
	TIME [epoch: 2.66 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3621582304465447		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.3621582304465447 | validation: 0.4190480410177146]
	TIME [epoch: 2.66 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36022420064588306		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.36022420064588306 | validation: 0.36687679017243746]
	TIME [epoch: 2.66 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35700051468229704		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.35700051468229704 | validation: 0.39041412915233065]
	TIME [epoch: 2.66 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36278065974436485		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.36278065974436485 | validation: 0.3885468936808884]
	TIME [epoch: 2.66 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36051864772388725		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.36051864772388725 | validation: 0.38108539805022495]
	TIME [epoch: 2.66 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.361272679371866		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.361272679371866 | validation: 0.376075744246559]
	TIME [epoch: 2.66 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35733039650547016		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.35733039650547016 | validation: 0.3897097901150255]
	TIME [epoch: 2.66 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36310635222532767		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.36310635222532767 | validation: 0.3543379636811501]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_819.pth
	Model improved!!!
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36378241611623663		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.36378241611623663 | validation: 0.4041990009879706]
	TIME [epoch: 2.68 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36620626789117156		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.36620626789117156 | validation: 0.39060604758496464]
	TIME [epoch: 2.69 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3673743310943124		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.3673743310943124 | validation: 0.4017882911433785]
	TIME [epoch: 2.68 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3622164010881889		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.3622164010881889 | validation: 0.38530185578791104]
	TIME [epoch: 2.68 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3577194331381731		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.3577194331381731 | validation: 0.36308821569374106]
	TIME [epoch: 2.68 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35397801726853373		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.35397801726853373 | validation: 0.4002616317797314]
	TIME [epoch: 2.68 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3603242222672981		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.3603242222672981 | validation: 0.392463111337418]
	TIME [epoch: 2.67 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3618419293059093		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.3618419293059093 | validation: 0.39218595602340134]
	TIME [epoch: 2.66 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3632364605441502		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.3632364605441502 | validation: 0.41271667386890587]
	TIME [epoch: 2.67 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3606564629567203		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.3606564629567203 | validation: 0.3910477719256269]
	TIME [epoch: 2.66 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3555102866070346		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.3555102866070346 | validation: 0.390088900154241]
	TIME [epoch: 2.66 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3601506860282413		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.3601506860282413 | validation: 0.39161793120307453]
	TIME [epoch: 2.66 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3592187633192123		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.3592187633192123 | validation: 0.3932427727655603]
	TIME [epoch: 2.67 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36077242486234035		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.36077242486234035 | validation: 0.384889476213828]
	TIME [epoch: 2.66 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3604130650194843		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.3604130650194843 | validation: 0.38089375781681767]
	TIME [epoch: 2.66 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3440905558850721		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.3440905558850721 | validation: 0.3832318495822973]
	TIME [epoch: 2.66 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35713505455386746		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.35713505455386746 | validation: 0.3736378533457338]
	TIME [epoch: 2.66 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36054865051816576		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.36054865051816576 | validation: 0.4004058576933156]
	TIME [epoch: 2.66 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3586975160886879		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.3586975160886879 | validation: 0.37955408882894265]
	TIME [epoch: 2.66 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35155780469037523		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.35155780469037523 | validation: 0.38593170632517243]
	TIME [epoch: 2.66 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3534126113977974		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.3534126113977974 | validation: 0.40035203028838745]
	TIME [epoch: 2.67 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36189088259424734		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.36189088259424734 | validation: 0.38370050864984695]
	TIME [epoch: 2.67 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3569015269627392		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.3569015269627392 | validation: 0.3835108390088222]
	TIME [epoch: 2.66 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3609580263229695		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.3609580263229695 | validation: 0.4037904116251143]
	TIME [epoch: 2.67 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3568021397509948		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.3568021397509948 | validation: 0.38121343132670243]
	TIME [epoch: 2.68 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3564534391664161		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.3564534391664161 | validation: 0.37565402716009627]
	TIME [epoch: 2.68 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35254771385070127		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.35254771385070127 | validation: 0.3965341059201596]
	TIME [epoch: 2.68 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3560719411255609		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.3560719411255609 | validation: 0.41823966178444005]
	TIME [epoch: 2.67 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35264799777721506		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.35264799777721506 | validation: 0.387944537112648]
	TIME [epoch: 2.68 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3602637653895809		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.3602637653895809 | validation: 0.3876948348768326]
	TIME [epoch: 2.68 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35711377528664257		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.35711377528664257 | validation: 0.37049405318494366]
	TIME [epoch: 2.68 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35418292980721505		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.35418292980721505 | validation: 0.3871009375369041]
	TIME [epoch: 2.68 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3485965699962993		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.3485965699962993 | validation: 0.3650167100064242]
	TIME [epoch: 2.68 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3519296367189743		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.3519296367189743 | validation: 0.3796526238547112]
	TIME [epoch: 2.67 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3450656408804467		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.3450656408804467 | validation: 0.4073888334657527]
	TIME [epoch: 2.69 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35639250896719177		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.35639250896719177 | validation: 0.40607304981183434]
	TIME [epoch: 2.67 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34775014651876446		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.34775014651876446 | validation: 0.36915789199916954]
	TIME [epoch: 2.68 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3486824869729446		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.3486824869729446 | validation: 0.4013639988042161]
	TIME [epoch: 2.68 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34874754571029287		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.34874754571029287 | validation: 0.3760082891606511]
	TIME [epoch: 2.68 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34863877161196255		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.34863877161196255 | validation: 0.3879464702062932]
	TIME [epoch: 2.68 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3477612248333948		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.3477612248333948 | validation: 0.39041424479896614]
	TIME [epoch: 2.68 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34851362210782216		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.34851362210782216 | validation: 0.407561450619622]
	TIME [epoch: 2.68 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36095337906819674		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.36095337906819674 | validation: 0.3918847690005136]
	TIME [epoch: 2.68 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35688419709698593		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.35688419709698593 | validation: 0.3850240777028273]
	TIME [epoch: 2.68 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34858708548395084		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.34858708548395084 | validation: 0.3953198646831202]
	TIME [epoch: 2.68 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3477404708910995		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.3477404708910995 | validation: 0.36819574251529225]
	TIME [epoch: 2.69 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35469260346703474		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.35469260346703474 | validation: 0.4037175283131859]
	TIME [epoch: 2.68 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34549026634654445		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.34549026634654445 | validation: 0.40765252972008625]
	TIME [epoch: 2.68 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3556652259060492		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.3556652259060492 | validation: 0.38160610153029034]
	TIME [epoch: 2.68 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3500510560502771		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.3500510560502771 | validation: 0.38124250040570734]
	TIME [epoch: 2.68 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34311208743844984		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.34311208743844984 | validation: 0.3832064985267163]
	TIME [epoch: 2.68 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3456015673318105		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.3456015673318105 | validation: 0.3812884297025349]
	TIME [epoch: 2.68 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3487113788205873		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.3487113788205873 | validation: 0.3751190236406703]
	TIME [epoch: 2.68 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34326345327626484		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.34326345327626484 | validation: 0.3931861708837394]
	TIME [epoch: 2.68 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3469537037579963		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.3469537037579963 | validation: 0.3733572592124553]
	TIME [epoch: 2.68 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34662904814121476		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.34662904814121476 | validation: 0.39489151047079774]
	TIME [epoch: 2.68 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3412217653925588		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.3412217653925588 | validation: 0.37929253940766483]
	TIME [epoch: 2.68 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35116863020584616		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.35116863020584616 | validation: 0.42346423420560647]
	TIME [epoch: 2.68 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36331143619392026		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.36331143619392026 | validation: 0.39475924584303296]
	TIME [epoch: 2.67 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35168982408889315		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.35168982408889315 | validation: 0.3810922361332055]
	TIME [epoch: 2.68 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3463224569031197		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.3463224569031197 | validation: 0.3923113536260654]
	TIME [epoch: 2.69 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34506649545756524		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.34506649545756524 | validation: 0.3841140670606308]
	TIME [epoch: 2.68 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34603125895958287		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.34603125895958287 | validation: 0.37500994812097144]
	TIME [epoch: 2.68 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34385073113934944		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.34385073113934944 | validation: 0.3932285392223822]
	TIME [epoch: 2.68 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34406190461784947		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.34406190461784947 | validation: 0.3707313678458338]
	TIME [epoch: 2.68 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3499509290231969		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.3499509290231969 | validation: 0.3909651959828944]
	TIME [epoch: 2.68 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34520353981967605		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.34520353981967605 | validation: 0.3944773192100655]
	TIME [epoch: 2.68 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3402398376781146		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.3402398376781146 | validation: 0.3792732784122015]
	TIME [epoch: 2.69 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34034283664467135		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.34034283664467135 | validation: 0.3853819498330123]
	TIME [epoch: 2.68 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3392221103637341		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.3392221103637341 | validation: 0.3625880932974792]
	TIME [epoch: 2.68 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3437827399501758		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.3437827399501758 | validation: 0.39182135767137494]
	TIME [epoch: 2.68 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34404425105836595		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.34404425105836595 | validation: 0.3776237695690594]
	TIME [epoch: 2.68 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34003999883149805		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.34003999883149805 | validation: 0.36926661020100204]
	TIME [epoch: 2.68 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33774447706732896		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.33774447706732896 | validation: 0.39665116361031855]
	TIME [epoch: 2.68 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3410501342674572		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.3410501342674572 | validation: 0.3735480221289207]
	TIME [epoch: 2.67 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34284764776867027		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.34284764776867027 | validation: 0.40476904353145193]
	TIME [epoch: 2.68 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33951148503711354		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.33951148503711354 | validation: 0.3854148359598687]
	TIME [epoch: 2.67 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33917818596520877		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.33917818596520877 | validation: 0.38164244947846426]
	TIME [epoch: 2.67 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34687192961911884		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.34687192961911884 | validation: 0.4025709827975358]
	TIME [epoch: 2.68 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.344102229091662		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.344102229091662 | validation: 0.37363910704724407]
	TIME [epoch: 2.68 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3419054184921713		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.3419054184921713 | validation: 0.3888814041348083]
	TIME [epoch: 2.68 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3386401328977736		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.3386401328977736 | validation: 0.3765685817855091]
	TIME [epoch: 2.66 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3372405846778531		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.3372405846778531 | validation: 0.38450223730131516]
	TIME [epoch: 2.67 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34258017529857343		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.34258017529857343 | validation: 0.39169313462702837]
	TIME [epoch: 2.66 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3429683788205543		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.3429683788205543 | validation: 0.3748643789135643]
	TIME [epoch: 2.66 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3395227951634411		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.3395227951634411 | validation: 0.4080662365047423]
	TIME [epoch: 2.67 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33899970572406624		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.33899970572406624 | validation: 0.3764960559033004]
	TIME [epoch: 2.66 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33218509010113095		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.33218509010113095 | validation: 0.41129417648647926]
	TIME [epoch: 2.66 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33525247022972304		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.33525247022972304 | validation: 0.38263591011768106]
	TIME [epoch: 2.66 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3430828061490021		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.3430828061490021 | validation: 0.38501722495250057]
	TIME [epoch: 2.67 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34553849483278515		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.34553849483278515 | validation: 0.3777471547817909]
	TIME [epoch: 2.66 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33609617561080696		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.33609617561080696 | validation: 0.3840548138545867]
	TIME [epoch: 2.66 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33780825278645055		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.33780825278645055 | validation: 0.3864303604026704]
	TIME [epoch: 2.66 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33269505507414565		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.33269505507414565 | validation: 0.38748108689309124]
	TIME [epoch: 2.66 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3344184604306378		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.3344184604306378 | validation: 0.44614904691339685]
	TIME [epoch: 2.66 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3410481782698671		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.3410481782698671 | validation: 0.3873402926499929]
	TIME [epoch: 2.66 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33966563170301384		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.33966563170301384 | validation: 0.3938269430315729]
	TIME [epoch: 2.66 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3332501620559323		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.3332501620559323 | validation: 0.386088433044733]
	TIME [epoch: 2.66 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3350524396944427		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.3350524396944427 | validation: 0.37406206766519134]
	TIME [epoch: 2.67 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33506695392710983		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.33506695392710983 | validation: 0.4283536978946158]
	TIME [epoch: 2.66 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3367585345255457		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.3367585345255457 | validation: 0.37471414303445905]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd4_20250516_142619/states/model_phi1_4a_distortion_v1_2_v_mmd4_920.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 2229.915 seconds.
