Args:
Namespace(name='model_phi1_4a_distortion_v1_7_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_7/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_7/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.053352892, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3244955274

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.682838517704636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.682838517704636 | validation: 7.429977959537678]
	TIME [epoch: 166 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.499649864120943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.499649864120943 | validation: 7.339843845216241]
	TIME [epoch: 1.08 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.636456616356688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.636456616356688 | validation: 7.219243718472237]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.041099297760227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.041099297760227 | validation: 7.384877743556672]
	TIME [epoch: 0.714 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.807085409952179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.807085409952179 | validation: 7.79595218291785]
	TIME [epoch: 0.715 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.934159482683488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.934159482683488 | validation: 7.306179857661326]
	TIME [epoch: 0.715 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.147219740514417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.147219740514417 | validation: 7.290265725849483]
	TIME [epoch: 0.712 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.691831988401376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.691831988401376 | validation: 7.064473713434369]
	TIME [epoch: 0.717 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.469827629088886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.469827629088886 | validation: 6.890061055622072]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.169120938451358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.169120938451358 | validation: 6.897518420137158]
	TIME [epoch: 0.711 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.986396319994644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.986396319994644 | validation: 6.858074166579072]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.826043060192924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.826043060192924 | validation: 6.741050423905765]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.6502027411835325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.6502027411835325 | validation: 6.524535289724125]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.48528578344924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.48528578344924 | validation: 6.338199636017907]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.211713595067721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.211713595067721 | validation: 6.098748303743393]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.818683370640441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.818683370640441 | validation: 6.382845236040421]
	TIME [epoch: 0.71 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.629117901526964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.629117901526964 | validation: 6.357233242456059]
	TIME [epoch: 0.707 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.239057483766473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.239057483766473 | validation: 6.348414208099529]
	TIME [epoch: 0.708 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.135590072823943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.135590072823943 | validation: 6.313593587184067]
	TIME [epoch: 0.713 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.650367954160884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.650367954160884 | validation: 6.172076917174298]
	TIME [epoch: 0.709 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.126984044800229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.126984044800229 | validation: 5.7608904906643055]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.500405824344105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.500405824344105 | validation: 5.773183044208391]
	TIME [epoch: 0.716 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.847296712844024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.847296712844024 | validation: 5.985227265251913]
	TIME [epoch: 0.713 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.413856767892751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.413856767892751 | validation: 5.688901359654641]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.196617315076175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.196617315076175 | validation: 5.806254986435512]
	TIME [epoch: 0.713 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.574425147641079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.574425147641079 | validation: 5.82376954848167]
	TIME [epoch: 0.709 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.138826042431031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.138826042431031 | validation: 5.363402487654305]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8167193030690956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8167193030690956 | validation: 5.690644851021265]
	TIME [epoch: 0.719 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.832769932216385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.832769932216385 | validation: 5.739879846067644]
	TIME [epoch: 0.708 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.382323737933258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.382323737933258 | validation: 5.454406854819755]
	TIME [epoch: 0.707 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8210392744133386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8210392744133386 | validation: 5.3359030419205]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6927247176025197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6927247176025197 | validation: 5.635742493855132]
	TIME [epoch: 0.711 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.882891736669984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.882891736669984 | validation: 5.440479696447003]
	TIME [epoch: 0.709 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8128379404080013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8128379404080013 | validation: 5.415998161423911]
	TIME [epoch: 0.707 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.890024241051632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.890024241051632 | validation: 5.22333996620685]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.586446477031627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.586446477031627 | validation: 5.306616420847995]
	TIME [epoch: 0.714 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.531576731880107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.531576731880107 | validation: 5.419782352281053]
	TIME [epoch: 0.712 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6053020200693493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6053020200693493 | validation: 5.297426941212469]
	TIME [epoch: 0.712 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7697744656686814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7697744656686814 | validation: 5.218464992517589]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.783014387007961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.783014387007961 | validation: 5.10646778844001]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.435532014938931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.435532014938931 | validation: 5.300881380630258]
	TIME [epoch: 0.713 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5112398661562407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5112398661562407 | validation: 5.28710842163547]
	TIME [epoch: 0.712 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.771958285606824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.771958285606824 | validation: 5.004365994367893]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.392108249255218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.392108249255218 | validation: 5.058335299160243]
	TIME [epoch: 0.712 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3137756019152294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3137756019152294 | validation: 5.155453419117003]
	TIME [epoch: 0.708 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4215213709653285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4215213709653285 | validation: 5.093003115227808]
	TIME [epoch: 0.709 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3679420357668026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3679420357668026 | validation: 5.040527067331162]
	TIME [epoch: 0.707 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4493849821728646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4493849821728646 | validation: 5.021282688301998]
	TIME [epoch: 0.709 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.299970815322871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.299970815322871 | validation: 4.940763653704343]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2333055662246015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2333055662246015 | validation: 4.953851515261427]
	TIME [epoch: 0.711 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.209525485256539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.209525485256539 | validation: 4.929405152198738]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.21878459230166		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.21878459230166 | validation: 4.882855139541632]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.181133753797534		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 3.181133753797534 | validation: 4.866385507390462]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1780634113024426		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 3.1780634113024426 | validation: 4.857277313524114]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.132263162825262		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 3.132263162825262 | validation: 4.838478347575311]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.117063156039591		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 3.117063156039591 | validation: 4.829916423894884]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0363041664380717		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 3.0363041664380717 | validation: 4.773096654358862]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0712721909022607		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 3.0712721909022607 | validation: 4.93132391132592]
	TIME [epoch: 0.713 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.443686165034036		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 3.443686165034036 | validation: 4.693150113630918]
	TIME [epoch: 0.722 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0063538542835104		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 3.0063538542835104 | validation: 4.747552740726973]
	TIME [epoch: 0.709 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.114474995838325		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 3.114474995838325 | validation: 4.63181730844915]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.93400946644639		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 2.93400946644639 | validation: 4.6696838373071685]
	TIME [epoch: 0.711 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0167099917776		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 3.0167099917776 | validation: 4.632999915121569]
	TIME [epoch: 0.708 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.960586980853518		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 2.960586980853518 | validation: 4.5602807681849145]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9325892325982585		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 2.9325892325982585 | validation: 4.5922324115325726]
	TIME [epoch: 0.712 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.939321790424284		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 2.939321790424284 | validation: 4.532877892568337]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8336810076223014		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 2.8336810076223014 | validation: 4.473100298426644]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8292068998718403		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 2.8292068998718403 | validation: 4.530583644943771]
	TIME [epoch: 0.713 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9667043832555544		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 2.9667043832555544 | validation: 4.487246571813222]
	TIME [epoch: 0.709 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.97245184293786		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 2.97245184293786 | validation: 4.471890930656506]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8170699338980194		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 2.8170699338980194 | validation: 4.429610488311398]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7795315014792084		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 2.7795315014792084 | validation: 4.4035412276965165]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.837438077628437		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 2.837438077628437 | validation: 4.451551015976614]
	TIME [epoch: 0.714 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8177411956849805		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 2.8177411956849805 | validation: 4.335022553817873]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7639402982905574		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 2.7639402982905574 | validation: 4.326726253604359]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7348154544501084		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 2.7348154544501084 | validation: 4.345315364361387]
	TIME [epoch: 0.713 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7274697155117207		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 2.7274697155117207 | validation: 4.3103007295003035]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.719279760038805		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 2.719279760038805 | validation: 4.316935986585734]
	TIME [epoch: 0.713 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.717569856310797		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 2.717569856310797 | validation: 4.318656241585518]
	TIME [epoch: 0.712 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.766987327138208		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 2.766987327138208 | validation: 4.397769289613715]
	TIME [epoch: 0.712 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8789742200454707		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 2.8789742200454707 | validation: 4.300426361611332]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.806443396029079		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 2.806443396029079 | validation: 4.272204559332066]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7178784074263977		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 2.7178784074263977 | validation: 4.236460022260795]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.699364427134143		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 2.699364427134143 | validation: 4.225806988266816]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6855790956676424		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 2.6855790956676424 | validation: 4.220021890450663]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6883945260564635		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 2.6883945260564635 | validation: 4.203136078436398]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.686902805962512		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 2.686902805962512 | validation: 4.224920438456135]
	TIME [epoch: 0.715 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.686211646326125		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 2.686211646326125 | validation: 4.1963201887284125]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.702306333550839		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 2.702306333550839 | validation: 4.229116717377828]
	TIME [epoch: 0.712 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.761479000767774		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 2.761479000767774 | validation: 4.202453789749401]
	TIME [epoch: 0.715 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7631806274020447		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 2.7631806274020447 | validation: 4.120011558895557]
	TIME [epoch: 0.716 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.712564450919875		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 2.712564450919875 | validation: 4.152168375332152]
	TIME [epoch: 0.713 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.664306626156424		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 2.664306626156424 | validation: 4.0818545165537]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6661296212503793		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 2.6661296212503793 | validation: 4.1281007213894485]
	TIME [epoch: 0.713 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.659299219525802		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 2.659299219525802 | validation: 4.086211796277673]
	TIME [epoch: 0.711 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6777403010896523		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 2.6777403010896523 | validation: 4.114224182597148]
	TIME [epoch: 0.71 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6906867362967435		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 2.6906867362967435 | validation: 4.055128983871713]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.694069409722809		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 2.694069409722809 | validation: 4.050298771939793]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.642743620114862		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 2.642743620114862 | validation: 4.044161826740747]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6487138859424704		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 2.6487138859424704 | validation: 3.9856744571405294]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6278396237168575		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 2.6278396237168575 | validation: 4.00076149420183]
	TIME [epoch: 0.712 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6293205618131243		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 2.6293205618131243 | validation: 3.9843109155874856]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6001338144435797		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 2.6001338144435797 | validation: 3.954606538909092]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6074841056793843		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 2.6074841056793843 | validation: 3.9991566829523535]
	TIME [epoch: 0.713 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6375293420194788		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 2.6375293420194788 | validation: 3.9007624135451495]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.652546835340433		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 2.652546835340433 | validation: 3.9392791544231986]
	TIME [epoch: 0.71 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.605656843998578		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 2.605656843998578 | validation: 3.904225023446722]
	TIME [epoch: 1.11 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.600925936124069		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 2.600925936124069 | validation: 3.9042003861431045]
	TIME [epoch: 0.712 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.662287644195225		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 2.662287644195225 | validation: 3.887939123592915]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.590968959660236		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 2.590968959660236 | validation: 3.81862814167841]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.56282045432387		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 2.56282045432387 | validation: 3.7909670251501297]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5514290796435		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 2.5514290796435 | validation: 3.802463191043678]
	TIME [epoch: 0.711 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5573266085894852		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 2.5573266085894852 | validation: 3.769312235539065]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5566080175728367		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 2.5566080175728367 | validation: 3.7896150381874834]
	TIME [epoch: 0.71 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.570541048978057		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 2.570541048978057 | validation: 3.7640757138685674]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.559013595914322		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 2.559013595914322 | validation: 3.706046490361621]
	TIME [epoch: 0.716 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5233420521304244		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 2.5233420521304244 | validation: 3.736062963079213]
	TIME [epoch: 0.711 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5222470026188875		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 2.5222470026188875 | validation: 3.634051782109575]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.537033661730096		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 2.537033661730096 | validation: 3.696435682224933]
	TIME [epoch: 0.714 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.506312626494373		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 2.506312626494373 | validation: 3.6094912810978146]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5058343393098226		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 2.5058343393098226 | validation: 3.637767984470886]
	TIME [epoch: 0.714 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5046840875737852		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 2.5046840875737852 | validation: 3.563679672886632]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4444346101684182		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 2.4444346101684182 | validation: 3.540061062968579]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.431367158762805		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 2.431367158762805 | validation: 3.4994041174467188]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4151350008448595		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 2.4151350008448595 | validation: 3.442686758174027]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4161381868607927		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 2.4161381868607927 | validation: 3.4263222001566813]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4660427671152756		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 2.4660427671152756 | validation: 3.5607334077499533]
	TIME [epoch: 0.715 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5020283991243315		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 2.5020283991243315 | validation: 3.3291070921752155]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.435708795928246		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 2.435708795928246 | validation: 3.303497786652175]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.339976173524992		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 2.339976173524992 | validation: 3.252418215335876]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.343656989769241		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 2.343656989769241 | validation: 3.1485132560045734]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3755870402720873		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 2.3755870402720873 | validation: 3.200474433934149]
	TIME [epoch: 0.715 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2841063640600687		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 2.2841063640600687 | validation: 2.963240674114539]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2422405131012684		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 2.2422405131012684 | validation: 2.897735209633989]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.167076512508756		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 2.167076512508756 | validation: 2.6507373939633534]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0875697153924673		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 2.0875697153924673 | validation: 2.3555679907390648]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9406060311787994		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.9406060311787994 | validation: 1.7842425558374835]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8910770436978084		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.8910770436978084 | validation: 3.210577651142588]
	TIME [epoch: 0.713 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.420220133155614		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 2.420220133155614 | validation: 1.5369818685247976]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_139.pth
	Model improved!!!
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.914092626642034		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.914092626642034 | validation: 2.484030338036147]
	TIME [epoch: 0.712 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5384582244050193		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 2.5384582244050193 | validation: 1.2207482461843986]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.484662507071746		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.484662507071746 | validation: 2.237897835156985]
	TIME [epoch: 0.713 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8290716693654867		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.8290716693654867 | validation: 1.495313104102093]
	TIME [epoch: 0.711 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4508643913853314		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.4508643913853314 | validation: 1.1841928158543549]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_144.pth
	Model improved!!!
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3829433316986044		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.3829433316986044 | validation: 1.1329384648172787]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2853074602787131		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.2853074602787131 | validation: 1.0827738673159022]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2347858601561146		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.2347858601561146 | validation: 1.0690515070811917]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2000503173460098		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.2000503173460098 | validation: 1.045202808344176]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1687413032701284		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.1687413032701284 | validation: 1.054310996399764]
	TIME [epoch: 0.714 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1152197153716596		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.1152197153716596 | validation: 0.9710791022605726]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_150.pth
	Model improved!!!
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0908951212893527		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.0908951212893527 | validation: 1.0374501516479757]
	TIME [epoch: 0.712 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.071521476669167		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.071521476669167 | validation: 0.9944749450316422]
	TIME [epoch: 0.71 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0651042761842813		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.0651042761842813 | validation: 1.042152709830102]
	TIME [epoch: 0.709 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1075977638191337		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.1075977638191337 | validation: 1.1967998157360074]
	TIME [epoch: 0.709 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1976510441897026		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.1976510441897026 | validation: 0.9999362286014009]
	TIME [epoch: 0.708 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1251155605072973		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.1251155605072973 | validation: 0.832789947349694]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.063918943989087		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.063918943989087 | validation: 1.3120821083916612]
	TIME [epoch: 0.716 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2051773758557915		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.2051773758557915 | validation: 0.8562304439344738]
	TIME [epoch: 0.715 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1616516143792988		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.1616516143792988 | validation: 1.0236356114367602]
	TIME [epoch: 0.714 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.039999804387605		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.039999804387605 | validation: 0.8600417345728343]
	TIME [epoch: 0.713 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9575086124127834		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.9575086124127834 | validation: 0.9302222394018926]
	TIME [epoch: 0.715 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9707026834194895		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.9707026834194895 | validation: 1.3858973449157004]
	TIME [epoch: 0.715 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2492802589179843		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.2492802589179843 | validation: 1.1109381621201404]
	TIME [epoch: 0.712 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1679497856821		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.1679497856821 | validation: 0.914502342850955]
	TIME [epoch: 0.713 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0056568782386284		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.0056568782386284 | validation: 0.9490614015811899]
	TIME [epoch: 0.715 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0750722616855841		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.0750722616855841 | validation: 0.9503961746965223]
	TIME [epoch: 0.715 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9756269652041923		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.9756269652041923 | validation: 0.8138488615067058]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_167.pth
	Model improved!!!
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9712667822323856		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.9712667822323856 | validation: 0.9882870851431174]
	TIME [epoch: 0.715 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9556793329123278		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.9556793329123278 | validation: 0.8564089812843009]
	TIME [epoch: 0.715 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.95725300277082		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.95725300277082 | validation: 1.0130276196215786]
	TIME [epoch: 0.715 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9315468303097808		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.9315468303097808 | validation: 2.7221502440928704]
	TIME [epoch: 0.717 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.212450791907808		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 2.212450791907808 | validation: 1.9097918171166108]
	TIME [epoch: 0.715 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8142688796559725		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.8142688796559725 | validation: 1.9595585809975824]
	TIME [epoch: 0.714 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8625483819189037		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.8625483819189037 | validation: 1.8205869369848604]
	TIME [epoch: 0.714 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.744772739446943		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.744772739446943 | validation: 1.8687669269924325]
	TIME [epoch: 0.714 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7651095488237594		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.7651095488237594 | validation: 1.786574348009888]
	TIME [epoch: 0.724 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7532427686160852		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.7532427686160852 | validation: 1.801071255009131]
	TIME [epoch: 0.714 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7393039961438224		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.7393039961438224 | validation: 1.792560915020793]
	TIME [epoch: 0.714 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7378287790759037		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.7378287790759037 | validation: 1.764667839650473]
	TIME [epoch: 0.715 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.74026563040641		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.74026563040641 | validation: 1.8080323779962022]
	TIME [epoch: 0.716 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.736261746768483		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.736261746768483 | validation: 1.7534953633793475]
	TIME [epoch: 0.715 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7348351774114843		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.7348351774114843 | validation: 1.7715809912873255]
	TIME [epoch: 0.716 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7388436731628858		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.7388436731628858 | validation: 1.753048075237948]
	TIME [epoch: 0.716 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7311634252154016		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.7311634252154016 | validation: 1.754305669885345]
	TIME [epoch: 0.715 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7296497562403539		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.7296497562403539 | validation: 1.7595459705450711]
	TIME [epoch: 0.714 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7285518207478276		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.7285518207478276 | validation: 1.7850339967036604]
	TIME [epoch: 0.715 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7274826553569005		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.7274826553569005 | validation: 1.7870277741997151]
	TIME [epoch: 0.717 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7925817343283807		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.7925817343283807 | validation: 2.124721534757434]
	TIME [epoch: 0.716 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9266262495385584		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.9266262495385584 | validation: 1.8119160299766617]
	TIME [epoch: 0.715 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8138482150502901		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.8138482150502901 | validation: 1.774001092536394]
	TIME [epoch: 0.716 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7327779244453214		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.7327779244453214 | validation: 1.7129147079627964]
	TIME [epoch: 0.715 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7200890599130874		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.7200890599130874 | validation: 1.7337215976349087]
	TIME [epoch: 0.715 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7126109421140654		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.7126109421140654 | validation: 1.7151911247891611]
	TIME [epoch: 0.714 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7042592918049553		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.7042592918049553 | validation: 1.7150488522048333]
	TIME [epoch: 0.718 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7082663480000322		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.7082663480000322 | validation: 1.708166301622162]
	TIME [epoch: 0.714 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7225118934030064		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.7225118934030064 | validation: 1.8215140153503395]
	TIME [epoch: 0.714 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7501661579708339		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.7501661579708339 | validation: 1.8834170169803024]
	TIME [epoch: 0.714 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.828228583326166		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.828228583326166 | validation: 2.035216273058094]
	TIME [epoch: 0.715 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8644720031605593		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.8644720031605593 | validation: 1.7412051944681046]
	TIME [epoch: 0.714 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.767193281903028		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.767193281903028 | validation: 1.7462412104652707]
	TIME [epoch: 0.714 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7137554280383358		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.7137554280383358 | validation: 1.6772120956000192]
	TIME [epoch: 173 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6910795058998036		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.6910795058998036 | validation: 1.668132929628592]
	TIME [epoch: 1.39 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6863898359136593		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.6863898359136593 | validation: 1.674374885177905]
	TIME [epoch: 1.39 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6834515209458436		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.6834515209458436 | validation: 1.6821692973937918]
	TIME [epoch: 1.39 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6842163799384369		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.6842163799384369 | validation: 1.6799504485848638]
	TIME [epoch: 1.39 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6973198384023982		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.6973198384023982 | validation: 1.677285968753755]
	TIME [epoch: 1.39 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7233449304360036		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.7233449304360036 | validation: 1.7990090964600656]
	TIME [epoch: 1.39 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7607875947864167		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.7607875947864167 | validation: 1.8977953385934292]
	TIME [epoch: 1.39 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8438190769814078		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.8438190769814078 | validation: 1.926448620802313]
	TIME [epoch: 1.39 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7936952979250964		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.7936952979250964 | validation: 1.687032805675404]
	TIME [epoch: 1.39 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7011808445477403		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.7011808445477403 | validation: 1.6758999932664072]
	TIME [epoch: 1.39 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6737910253888737		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.6737910253888737 | validation: 1.6681726308560052]
	TIME [epoch: 1.39 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.673107875047953		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 1.673107875047953 | validation: 1.679353736251776]
	TIME [epoch: 1.39 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6706243234261815		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.6706243234261815 | validation: 1.6778256411043373]
	TIME [epoch: 1.39 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6853711710422759		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 1.6853711710422759 | validation: 1.7970578381495494]
	TIME [epoch: 1.4 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7292813549320607		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.7292813549320607 | validation: 1.7338718848205432]
	TIME [epoch: 1.4 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7310097445717443		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.7310097445717443 | validation: 1.804597476619617]
	TIME [epoch: 1.4 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7392959751446255		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.7392959751446255 | validation: 1.7094617387746394]
	TIME [epoch: 1.4 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7421126153011026		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.7421126153011026 | validation: 1.7922248050583716]
	TIME [epoch: 1.39 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7342052093339198		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.7342052093339198 | validation: 1.7011417383672425]
	TIME [epoch: 1.39 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7046603065046213		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 1.7046603065046213 | validation: 1.6910820708085046]
	TIME [epoch: 1.39 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6516669012562266		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.6516669012562266 | validation: 1.6445389073575876]
	TIME [epoch: 1.39 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6468034778550198		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 1.6468034778550198 | validation: 1.6779239475213164]
	TIME [epoch: 1.39 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6449251122952058		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.6449251122952058 | validation: 1.6411502686200063]
	TIME [epoch: 1.4 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.661865663629257		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 1.661865663629257 | validation: 1.7327629620117315]
	TIME [epoch: 1.39 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.681470447270453		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 1.681470447270453 | validation: 1.73526521411803]
	TIME [epoch: 1.39 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.727047966139469		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.727047966139469 | validation: 1.778337538577551]
	TIME [epoch: 1.39 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.715779229070799		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.715779229070799 | validation: 1.6638108287966582]
	TIME [epoch: 1.39 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6946248119454326		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 1.6946248119454326 | validation: 1.6595547404204294]
	TIME [epoch: 1.39 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6594829103602553		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.6594829103602553 | validation: 1.635487292441954]
	TIME [epoch: 1.39 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6545931400509728		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 1.6545931400509728 | validation: 1.6222042998811947]
	TIME [epoch: 1.39 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6646624828499947		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.6646624828499947 | validation: 1.7080491971555234]
	TIME [epoch: 1.39 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.710014584593584		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 1.710014584593584 | validation: 1.6969460941868588]
	TIME [epoch: 1.39 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7170014424998443		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.7170014424998443 | validation: 1.855196840969351]
	TIME [epoch: 1.39 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7394588870847525		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 1.7394588870847525 | validation: 1.6865574741223874]
	TIME [epoch: 1.39 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6897603036201492		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.6897603036201492 | validation: 1.6749041498740418]
	TIME [epoch: 1.4 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6516094643481165		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 1.6516094643481165 | validation: 1.6266654412763666]
	TIME [epoch: 1.39 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6496715354498672		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 1.6496715354498672 | validation: 1.6834246103141544]
	TIME [epoch: 1.4 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.661892326480471		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 1.661892326480471 | validation: 1.6892977178435875]
	TIME [epoch: 1.4 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6760885852861231		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 1.6760885852861231 | validation: 1.6503932010230191]
	TIME [epoch: 1.39 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6490901607950235		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 1.6490901607950235 | validation: 1.6116676638296146]
	TIME [epoch: 1.39 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6386860922260753		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 1.6386860922260753 | validation: 1.621760558420939]
	TIME [epoch: 1.39 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6573678567706975		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 1.6573678567706975 | validation: 1.635677874803294]
	TIME [epoch: 1.39 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.655982355000483		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 1.655982355000483 | validation: 1.6749437144026509]
	TIME [epoch: 1.39 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6753625939121617		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 1.6753625939121617 | validation: 1.8807382131109642]
	TIME [epoch: 1.39 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7399396301104242		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.7399396301104242 | validation: 1.6990204220475573]
	TIME [epoch: 1.39 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6943561947112051		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 1.6943561947112051 | validation: 1.707239757203263]
	TIME [epoch: 1.39 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6866194853522873		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 1.6866194853522873 | validation: 1.6278274264014632]
	TIME [epoch: 1.39 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.667483516227583		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 1.667483516227583 | validation: 1.6848532617697929]
	TIME [epoch: 1.39 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6579887104548758		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 1.6579887104548758 | validation: 1.6405284614575029]
	TIME [epoch: 1.39 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6519887094203156		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 1.6519887094203156 | validation: 1.648965858590512]
	TIME [epoch: 1.39 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6409780177016746		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 1.6409780177016746 | validation: 1.6186068586864917]
	TIME [epoch: 1.39 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6430816588115014		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 1.6430816588115014 | validation: 1.6859000637440378]
	TIME [epoch: 1.39 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.646222241604393		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 1.646222241604393 | validation: 1.6509891380135675]
	TIME [epoch: 1.39 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6573400598990626		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 1.6573400598990626 | validation: 1.7225060451915635]
	TIME [epoch: 1.39 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.674209956904106		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 1.674209956904106 | validation: 1.6542349274223005]
	TIME [epoch: 1.39 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6734788466655182		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 1.6734788466655182 | validation: 1.7154305207762883]
	TIME [epoch: 1.4 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.684804359644524		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 1.684804359644524 | validation: 1.603028315841376]
	TIME [epoch: 1.39 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6471241339472786		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 1.6471241339472786 | validation: 1.6334067491116653]
	TIME [epoch: 1.39 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6262316329892594		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 1.6262316329892594 | validation: 1.5982444323065697]
	TIME [epoch: 1.39 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6254093642203626		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 1.6254093642203626 | validation: 1.6604846122598873]
	TIME [epoch: 1.39 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.628887210082317		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 1.628887210082317 | validation: 1.6590540054603005]
	TIME [epoch: 1.39 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6645448568614347		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 1.6645448568614347 | validation: 1.740342038981507]
	TIME [epoch: 1.39 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6776025263311407		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 1.6776025263311407 | validation: 1.698423575676246]
	TIME [epoch: 1.39 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6785882994933599		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 1.6785882994933599 | validation: 1.6639431354929106]
	TIME [epoch: 1.39 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.652889508906664		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 1.652889508906664 | validation: 1.6349063033656723]
	TIME [epoch: 1.39 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.659438369101307		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 1.659438369101307 | validation: 1.6338679106481615]
	TIME [epoch: 1.39 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.684576167178119		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 1.684576167178119 | validation: 1.6090803273481022]
	TIME [epoch: 1.39 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.629234840530584		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 1.629234840530584 | validation: 1.5873894046606865]
	TIME [epoch: 1.39 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6059698183307831		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 1.6059698183307831 | validation: 1.6481078905823292]
	TIME [epoch: 1.39 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6226725872119079		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 1.6226725872119079 | validation: 1.6786800532831254]
	TIME [epoch: 1.39 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.664074435678778		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 1.664074435678778 | validation: 1.808587222011417]
	TIME [epoch: 1.39 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7092671391027605		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 1.7092671391027605 | validation: 1.6349025136400863]
	TIME [epoch: 1.39 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6541393331197214		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 1.6541393331197214 | validation: 1.642100266047508]
	TIME [epoch: 1.39 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6260310465211947		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 1.6260310465211947 | validation: 1.6038324051571788]
	TIME [epoch: 1.39 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6231245368149043		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 1.6231245368149043 | validation: 1.6487392965770082]
	TIME [epoch: 1.39 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.62625953523153		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 1.62625953523153 | validation: 1.6393868624485763]
	TIME [epoch: 1.39 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6441853139981784		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 1.6441853139981784 | validation: 1.703142761506979]
	TIME [epoch: 1.39 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6518196939612795		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 1.6518196939612795 | validation: 1.6704727833080666]
	TIME [epoch: 1.4 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6665198775547827		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 1.6665198775547827 | validation: 1.6774565770320384]
	TIME [epoch: 1.39 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6611216673135112		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 1.6611216673135112 | validation: 1.6340691745512945]
	TIME [epoch: 1.39 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.661514259849932		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 1.661514259849932 | validation: 1.695473737572697]
	TIME [epoch: 1.39 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6712285197409233		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 1.6712285197409233 | validation: 1.6603236995285233]
	TIME [epoch: 1.39 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6510044997380724		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 1.6510044997380724 | validation: 1.6957498283401131]
	TIME [epoch: 1.39 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6436759190435806		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 1.6436759190435806 | validation: 1.6347078229094223]
	TIME [epoch: 1.39 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6330947360205037		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 1.6330947360205037 | validation: 1.6579128876093188]
	TIME [epoch: 1.39 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6305077653824491		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 1.6305077653824491 | validation: 1.6329954366957342]
	TIME [epoch: 1.39 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6276546510562877		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 1.6276546510562877 | validation: 1.6300358632220988]
	TIME [epoch: 1.39 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6288977027207174		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 1.6288977027207174 | validation: 1.6276676928748939]
	TIME [epoch: 1.39 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6378274867670768		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 1.6378274867670768 | validation: 1.641446700367]
	TIME [epoch: 1.39 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.632395224443273		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 1.632395224443273 | validation: 1.6182207453237583]
	TIME [epoch: 1.39 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.627244311720633		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 1.627244311720633 | validation: 1.6350376593535545]
	TIME [epoch: 1.39 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6229403467006944		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 1.6229403467006944 | validation: 1.681178616248788]
	TIME [epoch: 1.39 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6439290869127954		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 1.6439290869127954 | validation: 1.7214949745114152]
	TIME [epoch: 1.39 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6571279416693765		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 1.6571279416693765 | validation: 1.613180176105241]
	TIME [epoch: 1.39 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.626791750567986		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 1.626791750567986 | validation: 1.591978376598898]
	TIME [epoch: 1.39 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6112309077009102		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 1.6112309077009102 | validation: 1.606633471593681]
	TIME [epoch: 1.39 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6311313883216425		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 1.6311313883216425 | validation: 1.6192579153317077]
	TIME [epoch: 1.39 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6517802996118593		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 1.6517802996118593 | validation: 1.7520846452401817]
	TIME [epoch: 1.39 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6998338834673115		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 1.6998338834673115 | validation: 1.7300719801881852]
	TIME [epoch: 1.4 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6915559121986274		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 1.6915559121986274 | validation: 1.6236745096467815]
	TIME [epoch: 1.39 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.61643536271007		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 1.61643536271007 | validation: 1.5939734189860837]
	TIME [epoch: 1.39 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.591186104296678		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 1.591186104296678 | validation: 1.5209599864085226]
	TIME [epoch: 1.4 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4806208898644257		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 1.4806208898644257 | validation: 2.0904267459887245]
	TIME [epoch: 1.4 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3293723303844796		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 2.3293723303844796 | validation: 1.6778912120214509]
	TIME [epoch: 1.39 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.344663125288118		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 1.344663125288118 | validation: 1.392562605966916]
	TIME [epoch: 1.39 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.171299127266244		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 1.171299127266244 | validation: 1.433917506387118]
	TIME [epoch: 1.39 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3831530139028752		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 1.3831530139028752 | validation: 1.3772519304285975]
	TIME [epoch: 1.39 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.20950548299895		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 1.20950548299895 | validation: 1.3076595197950702]
	TIME [epoch: 1.39 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0787425521346625		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 1.0787425521346625 | validation: 1.377084824357403]
	TIME [epoch: 1.39 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.483698847669409		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 1.483698847669409 | validation: 1.2228158167463403]
	TIME [epoch: 1.39 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9941444079088387		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.9941444079088387 | validation: 1.0599572336214713]
	TIME [epoch: 1.39 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1806984798568119		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 1.1806984798568119 | validation: 0.7522428425938146]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_313.pth
	Model improved!!!
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5167447098681077		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 1.5167447098681077 | validation: 0.493852522532917]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_314.pth
	Model improved!!!
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.698361521125275		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.698361521125275 | validation: 1.8829717678184605]
	TIME [epoch: 1.39 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.026451513159476		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 2.026451513159476 | validation: 1.8612666278488952]
	TIME [epoch: 1.39 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.875548129077373		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 1.875548129077373 | validation: 1.6741987431552723]
	TIME [epoch: 1.39 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6238598681436298		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 1.6238598681436298 | validation: 1.6695819171845638]
	TIME [epoch: 1.39 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.637132967604371		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 1.637132967604371 | validation: 1.6786541038209681]
	TIME [epoch: 1.39 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.641679113567385		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 1.641679113567385 | validation: 1.6850846655665643]
	TIME [epoch: 1.4 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6153144081002628		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 1.6153144081002628 | validation: 2.92457483673132]
	TIME [epoch: 1.39 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2427698516972137		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 2.2427698516972137 | validation: 1.6784714995698025]
	TIME [epoch: 1.39 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.642359692638386		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 1.642359692638386 | validation: 1.6583665610664504]
	TIME [epoch: 1.39 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6482993529496304		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 1.6482993529496304 | validation: 1.5440849660457614]
	TIME [epoch: 1.39 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5853302450632338		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 1.5853302450632338 | validation: 1.5080128411020721]
	TIME [epoch: 1.39 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5585087883462985		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 1.5585087883462985 | validation: 1.5031706558738078]
	TIME [epoch: 1.39 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5508413779524615		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 1.5508413779524615 | validation: 1.4806634305713917]
	TIME [epoch: 1.39 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5230025638465226		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 1.5230025638465226 | validation: 1.3857445433520252]
	TIME [epoch: 1.39 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4359876850762896		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 1.4359876850762896 | validation: 1.3708102704673086]
	TIME [epoch: 1.39 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2667368234556127		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 1.2667368234556127 | validation: 1.3741203133399449]
	TIME [epoch: 1.39 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1611581890809597		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 1.1611581890809597 | validation: 1.353493136510811]
	TIME [epoch: 1.39 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1639744074567913		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 1.1639744074567913 | validation: 1.3241702244600866]
	TIME [epoch: 1.39 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1664967323742597		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 1.1664967323742597 | validation: 1.2537775240308422]
	TIME [epoch: 1.39 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0135543242618668		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 1.0135543242618668 | validation: 1.1487911500595198]
	TIME [epoch: 1.4 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9573444900963496		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.9573444900963496 | validation: 0.5542926368582286]
	TIME [epoch: 1.4 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6529457640095601		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.6529457640095601 | validation: 0.4503050347825287]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_336.pth
	Model improved!!!
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6715375381560281		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.6715375381560281 | validation: 1.2881793731117535]
	TIME [epoch: 1.4 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.337606002986093		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 1.337606002986093 | validation: 1.2794270591248689]
	TIME [epoch: 1.39 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.102281573826972		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 1.102281573826972 | validation: 1.3353301482724143]
	TIME [epoch: 1.39 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2677098152768114		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 1.2677098152768114 | validation: 1.0672724470325885]
	TIME [epoch: 1.39 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8797773845347487		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.8797773845347487 | validation: 0.5181540134996023]
	TIME [epoch: 1.4 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.575345659273537		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.575345659273537 | validation: 0.42074130421004013]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_342.pth
	Model improved!!!
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6778609475970996		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.6778609475970996 | validation: 1.2073123792949514]
	TIME [epoch: 1.4 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1319790972652481		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 1.1319790972652481 | validation: 1.126161544098344]
	TIME [epoch: 1.4 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9360373074947795		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.9360373074947795 | validation: 0.6276648314953454]
	TIME [epoch: 1.39 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.089518994634108		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 1.089518994634108 | validation: 0.4733291556828132]
	TIME [epoch: 1.39 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5612224287986712		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.5612224287986712 | validation: 0.925496421842799]
	TIME [epoch: 1.39 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9323988273893145		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.9323988273893145 | validation: 0.47002945256260364]
	TIME [epoch: 1.39 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5253259884134984		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.5253259884134984 | validation: 0.4943485976720525]
	TIME [epoch: 1.39 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48622426332448315		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.48622426332448315 | validation: 0.6714367566331787]
	TIME [epoch: 1.39 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6454661380518809		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.6454661380518809 | validation: 0.4358289468324672]
	TIME [epoch: 1.39 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6557996351332838		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.6557996351332838 | validation: 0.5694196906165823]
	TIME [epoch: 1.39 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5053076617999025		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.5053076617999025 | validation: 0.45851546376810703]
	TIME [epoch: 1.39 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4550909541035968		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.4550909541035968 | validation: 0.42220757147289484]
	TIME [epoch: 1.4 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46644280558034723		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.46644280558034723 | validation: 0.49663517086680575]
	TIME [epoch: 1.39 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4585418536425766		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.4585418536425766 | validation: 0.3998768434333301]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.535175423515283		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.535175423515283 | validation: 0.7271195841268588]
	TIME [epoch: 1.39 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6683411831519005		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.6683411831519005 | validation: 0.41946940002936817]
	TIME [epoch: 1.39 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6308998649490313		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.6308998649490313 | validation: 0.6068674182221278]
	TIME [epoch: 1.39 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.536251815276499		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.536251815276499 | validation: 0.45057652661421105]
	TIME [epoch: 1.39 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4536318423295516		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.4536318423295516 | validation: 0.4353537068210135]
	TIME [epoch: 1.4 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4237292771887959		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.4237292771887959 | validation: 0.4549396210895311]
	TIME [epoch: 1.39 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41556788087078195		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.41556788087078195 | validation: 0.40153993598375703]
	TIME [epoch: 1.39 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40554434040384024		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.40554434040384024 | validation: 1.0861193401865044]
	TIME [epoch: 1.39 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2542062747575538		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 1.2542062747575538 | validation: 0.7798272140528395]
	TIME [epoch: 1.39 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8634272364051409		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.8634272364051409 | validation: 0.49421537124578907]
	TIME [epoch: 1.39 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.959065608178779		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.959065608178779 | validation: 0.40280904170071435]
	TIME [epoch: 1.39 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5486202649001308		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.5486202649001308 | validation: 0.7694326475176463]
	TIME [epoch: 1.39 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6862337416882399		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.6862337416882399 | validation: 0.41569771654416815]
	TIME [epoch: 1.39 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4588109098019266		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.4588109098019266 | validation: 0.39112714640199375]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_370.pth
	Model improved!!!
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43998269589696504		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.43998269589696504 | validation: 0.4811037856619962]
	TIME [epoch: 1.39 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4512776421886909		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.4512776421886909 | validation: 0.4153819277930508]
	TIME [epoch: 1.4 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5110252999893142		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.5110252999893142 | validation: 0.5281145277280749]
	TIME [epoch: 1.39 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4754455744015874		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.4754455744015874 | validation: 0.4028576259208581]
	TIME [epoch: 1.39 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.450757073172508		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.450757073172508 | validation: 0.6998327879427491]
	TIME [epoch: 1.39 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7293840267581149		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.7293840267581149 | validation: 0.36261451382327387]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_376.pth
	Model improved!!!
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.522205259607525		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.522205259607525 | validation: 0.4357482932164537]
	TIME [epoch: 1.39 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.399930374113724		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.399930374113724 | validation: 0.4136473835417072]
	TIME [epoch: 1.39 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38417327697463666		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.38417327697463666 | validation: 0.3633168102167288]
	TIME [epoch: 1.39 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41622847010610886		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.41622847010610886 | validation: 0.42559990048377094]
	TIME [epoch: 1.39 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38744714304545097		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.38744714304545097 | validation: 0.38576942633076466]
	TIME [epoch: 1.39 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36680774793577414		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.36680774793577414 | validation: 0.3870870173342329]
	TIME [epoch: 1.39 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36565997772958775		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.36565997772958775 | validation: 0.4212598938106893]
	TIME [epoch: 1.39 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3732073599388876		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.3732073599388876 | validation: 0.34376197458998037]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_384.pth
	Model improved!!!
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42297247438628377		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.42297247438628377 | validation: 0.6077154263402477]
	TIME [epoch: 1.39 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5315359293980266		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.5315359293980266 | validation: 0.40096632482222994]
	TIME [epoch: 1.39 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7027674231021341		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.7027674231021341 | validation: 0.38885194050563054]
	TIME [epoch: 1.39 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3753432117632815		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.3753432117632815 | validation: 0.5530863542166133]
	TIME [epoch: 1.38 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47572279690315283		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.47572279690315283 | validation: 0.3642128178822563]
	TIME [epoch: 1.38 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5964253257343051		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.5964253257343051 | validation: 0.46227985172535907]
	TIME [epoch: 1.38 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4133501086595547		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.4133501086595547 | validation: 0.42150031225768547]
	TIME [epoch: 1.39 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3757599381070578		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.3757599381070578 | validation: 0.37229228469896253]
	TIME [epoch: 1.38 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3551097644101881		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.3551097644101881 | validation: 0.3432506631170154]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_393.pth
	Model improved!!!
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.353008674484247		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.353008674484247 | validation: 0.43077430153131796]
	TIME [epoch: 1.39 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3651498447895137		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.3651498447895137 | validation: 0.31677155117125855]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_395.pth
	Model improved!!!
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4359944747371368		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.4359944747371368 | validation: 0.5249893487609801]
	TIME [epoch: 1.39 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4935246867014684		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.4935246867014684 | validation: 0.4160292866072977]
	TIME [epoch: 1.39 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6778811763160824		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.6778811763160824 | validation: 0.38925029298982067]
	TIME [epoch: 1.39 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35620611613395015		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.35620611613395015 | validation: 0.3894412544582999]
	TIME [epoch: 1.39 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35317113600385613		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.35317113600385613 | validation: 0.303199669766665]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_400.pth
	Model improved!!!
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4203246284375365		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.4203246284375365 | validation: 0.46226941527011994]
	TIME [epoch: 1.39 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39059770742251304		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.39059770742251304 | validation: 0.335372357605061]
	TIME [epoch: 1.4 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3659662763762793		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.3659662763762793 | validation: 0.4334922539894442]
	TIME [epoch: 1.39 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38810513413068315		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.38810513413068315 | validation: 0.34017029852458897]
	TIME [epoch: 1.39 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44960421451250737		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.44960421451250737 | validation: 0.46188688137301814]
	TIME [epoch: 1.39 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38052244096432825		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.38052244096432825 | validation: 0.3091879570017222]
	TIME [epoch: 1.39 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3448668869768356		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.3448668869768356 | validation: 0.3685392164051073]
	TIME [epoch: 1.39 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3173926917667004		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.3173926917667004 | validation: 0.3091312338972692]
	TIME [epoch: 1.39 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32923415715730797		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.32923415715730797 | validation: 0.4145865692804418]
	TIME [epoch: 1.39 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.350954550439405		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.350954550439405 | validation: 0.3078879764921018]
	TIME [epoch: 1.39 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3998487391547455		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.3998487391547455 | validation: 0.4895356352683143]
	TIME [epoch: 1.39 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45318482576218133		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.45318482576218133 | validation: 0.31921567581027815]
	TIME [epoch: 1.39 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48160225749484775		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.48160225749484775 | validation: 0.5447963792647331]
	TIME [epoch: 1.39 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48527612847916424		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.48527612847916424 | validation: 0.3227887191799621]
	TIME [epoch: 1.39 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3353209941612859		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.3353209941612859 | validation: 0.3432051464833037]
	TIME [epoch: 1.39 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30467277265981735		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.30467277265981735 | validation: 0.42737705232792583]
	TIME [epoch: 1.39 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34501197686729		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.34501197686729 | validation: 0.29591960957433505]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_417.pth
	Model improved!!!
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37743982724422		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.37743982724422 | validation: 0.41366745984527165]
	TIME [epoch: 1.39 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3430262986641662		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.3430262986641662 | validation: 0.32236181318124746]
	TIME [epoch: 1.39 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31515071610572504		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.31515071610572504 | validation: 0.3138938580881961]
	TIME [epoch: 1.39 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2970196169218415		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.2970196169218415 | validation: 0.38403539837722445]
	TIME [epoch: 1.39 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30410329288880983		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.30410329288880983 | validation: 0.2858171223232738]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_422.pth
	Model improved!!!
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3057689280329715		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.3057689280329715 | validation: 0.35823337297909874]
	TIME [epoch: 1.39 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2968317776718894		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.2968317776718894 | validation: 0.29079483806390144]
	TIME [epoch: 1.39 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3401483797352971		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.3401483797352971 | validation: 0.45138661378545986]
	TIME [epoch: 1.39 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39500211657948425		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.39500211657948425 | validation: 0.3381525221241635]
	TIME [epoch: 1.39 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5335060069600381		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.5335060069600381 | validation: 0.3724204138203269]
	TIME [epoch: 1.39 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31667896308837135		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.31667896308837135 | validation: 0.31196994243228016]
	TIME [epoch: 1.39 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3055940618467984		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.3055940618467984 | validation: 0.3427311552098446]
	TIME [epoch: 1.39 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.285568821896218		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.285568821896218 | validation: 0.31951057715000836]
	TIME [epoch: 1.39 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28128996641817594		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.28128996641817594 | validation: 0.31945105984459027]
	TIME [epoch: 1.39 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2752406322532475		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.2752406322532475 | validation: 0.3242426349978403]
	TIME [epoch: 1.39 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27103457952523047		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.27103457952523047 | validation: 0.2843075652760589]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_433.pth
	Model improved!!!
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26139961115836097		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.26139961115836097 | validation: 0.3461331207748457]
	TIME [epoch: 1.39 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2825443791818306		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.2825443791818306 | validation: 0.2975133081149524]
	TIME [epoch: 1.39 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49776872575863856		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.49776872575863856 | validation: 0.5822378628868512]
	TIME [epoch: 1.39 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5033230862964215		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.5033230862964215 | validation: 0.2785289876215614]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_437.pth
	Model improved!!!
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34476132413127347		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.34476132413127347 | validation: 0.29885962910678465]
	TIME [epoch: 1.39 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2681248236620976		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.2681248236620976 | validation: 0.3282455776663659]
	TIME [epoch: 1.39 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2678943185089973		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.2678943185089973 | validation: 0.29661415219148873]
	TIME [epoch: 1.39 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2591895404356542		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.2591895404356542 | validation: 0.29658989591053614]
	TIME [epoch: 1.39 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2537461365324041		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.2537461365324041 | validation: 0.30881675529582764]
	TIME [epoch: 1.39 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25697653338898946		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.25697653338898946 | validation: 0.27402352869253316]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_443.pth
	Model improved!!!
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32927542500606904		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.32927542500606904 | validation: 0.5506421515894767]
	TIME [epoch: 1.4 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5107522781699732		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.5107522781699732 | validation: 0.28716400749763665]
	TIME [epoch: 1.39 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4491469113837877		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.4491469113837877 | validation: 0.34705157129440917]
	TIME [epoch: 1.39 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3014481714556609		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.3014481714556609 | validation: 0.3335667268906095]
	TIME [epoch: 1.39 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29738197671181277		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.29738197671181277 | validation: 0.2526539035114473]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_448.pth
	Model improved!!!
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32142882295618264		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.32142882295618264 | validation: 0.5369450022626854]
	TIME [epoch: 1.39 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43404291099295805		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.43404291099295805 | validation: 0.2528212405923217]
	TIME [epoch: 1.39 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2893794989377789		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.2893794989377789 | validation: 0.29026438485966083]
	TIME [epoch: 1.39 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2519240338550604		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.2519240338550604 | validation: 0.3112093075884657]
	TIME [epoch: 1.39 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2490893313776026		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.2490893313776026 | validation: 0.28375863709577803]
	TIME [epoch: 1.39 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24871077381443207		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.24871077381443207 | validation: 0.29745719723091635]
	TIME [epoch: 1.39 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25525273897550926		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.25525273897550926 | validation: 0.30396145770513794]
	TIME [epoch: 1.39 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24381647106489085		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.24381647106489085 | validation: 0.2573264382564069]
	TIME [epoch: 1.39 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24243958504776986		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.24243958504776986 | validation: 0.30772273810667305]
	TIME [epoch: 1.39 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25021514167255327		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.25021514167255327 | validation: 0.2465438278625324]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_458.pth
	Model improved!!!
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3301989930396276		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.3301989930396276 | validation: 0.7339354930537807]
	TIME [epoch: 1.39 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7823912252937996		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.7823912252937996 | validation: 0.3128887420710744]
	TIME [epoch: 1.39 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2969043790310162		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.2969043790310162 | validation: 0.3011210961329154]
	TIME [epoch: 1.39 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46032915814713		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.46032915814713 | validation: 0.3759685543011353]
	TIME [epoch: 1.39 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33528906594940167		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.33528906594940167 | validation: 0.24163420225393098]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_463.pth
	Model improved!!!
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25247647118906247		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.25247647118906247 | validation: 0.25502067993441885]
	TIME [epoch: 1.39 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2518527146801112		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.2518527146801112 | validation: 0.27948747550750985]
	TIME [epoch: 1.39 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2231216250541839		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.2231216250541839 | validation: 0.24382332072581958]
	TIME [epoch: 1.39 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23706059533733745		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.23706059533733745 | validation: 0.34026479132053783]
	TIME [epoch: 1.39 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2645786930436433		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.2645786930436433 | validation: 0.24509847303247334]
	TIME [epoch: 1.39 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2641003835494259		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.2641003835494259 | validation: 0.3114446871857189]
	TIME [epoch: 1.39 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24879627550863034		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.24879627550863034 | validation: 0.227526560135103]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_470.pth
	Model improved!!!
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25397091051394166		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.25397091051394166 | validation: 0.3774759379780402]
	TIME [epoch: 1.39 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3411070590710857		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.3411070590710857 | validation: 0.2523277854888353]
	TIME [epoch: 1.39 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3803433317534497		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.3803433317534497 | validation: 0.342653753397818]
	TIME [epoch: 1.39 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27954610050819645		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.27954610050819645 | validation: 0.3280468178898834]
	TIME [epoch: 1.39 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2604826938800495		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.2604826938800495 | validation: 0.2752697577621554]
	TIME [epoch: 1.39 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29687837015311835		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.29687837015311835 | validation: 0.3988396275720976]
	TIME [epoch: 1.39 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2962003234572782		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.2962003234572782 | validation: 0.23685585490191408]
	TIME [epoch: 1.39 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23374684791335992		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.23374684791335992 | validation: 0.28828847882780084]
	TIME [epoch: 1.39 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22676180709231944		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.22676180709231944 | validation: 0.27704676962668773]
	TIME [epoch: 1.39 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21927666437403165		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.21927666437403165 | validation: 0.2899016750744495]
	TIME [epoch: 1.39 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2443719612767282		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.2443719612767282 | validation: 0.23726457485351343]
	TIME [epoch: 1.39 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3022540924604733		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.3022540924604733 | validation: 0.39011223135615014]
	TIME [epoch: 1.39 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30146964085934386		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.30146964085934386 | validation: 0.2575184930762723]
	TIME [epoch: 1.4 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22606538382264815		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.22606538382264815 | validation: 0.24532273841342378]
	TIME [epoch: 1.39 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20432712265074654		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.20432712265074654 | validation: 0.26135512012369966]
	TIME [epoch: 1.39 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.205534709276992		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.205534709276992 | validation: 0.20848797195735758]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_486.pth
	Model improved!!!
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19782049419242392		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.19782049419242392 | validation: 0.27086100982821176]
	TIME [epoch: 1.39 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2042623254646832		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.2042623254646832 | validation: 0.20772412947054622]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_488.pth
	Model improved!!!
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24715421094982964		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.24715421094982964 | validation: 0.4345857932625057]
	TIME [epoch: 1.39 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37753725039797753		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.37753725039797753 | validation: 0.2697451282710818]
	TIME [epoch: 1.39 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4000794602648935		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.4000794602648935 | validation: 0.3553640262789799]
	TIME [epoch: 1.39 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2748857663377314		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.2748857663377314 | validation: 0.34612681873150425]
	TIME [epoch: 1.39 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2757357621944425		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.2757357621944425 | validation: 0.25362207596027375]
	TIME [epoch: 1.39 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21310324960444627		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.21310324960444627 | validation: 0.3023703104874365]
	TIME [epoch: 1.39 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21686444267702645		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.21686444267702645 | validation: 0.23681630362941952]
	TIME [epoch: 1.39 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19843264577333122		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.19843264577333122 | validation: 0.27501733958784785]
	TIME [epoch: 1.39 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2160651374823233		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.2160651374823233 | validation: 0.2258517804081982]
	TIME [epoch: 1.39 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21169004632303573		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.21169004632303573 | validation: 0.35201024602970143]
	TIME [epoch: 1.39 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2915383597409545		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.2915383597409545 | validation: 0.24462915361905335]
	TIME [epoch: 1.39 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3185753761051123		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.3185753761051123 | validation: 0.30418571396364763]
	TIME [epoch: 1.39 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2284144596434239		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.2284144596434239 | validation: 0.25253492361247754]
	TIME [epoch: 177 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20314988779976204		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.20314988779976204 | validation: 0.23389233625016426]
	TIME [epoch: 2.75 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1916275289590376		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.1916275289590376 | validation: 0.30622750133856746]
	TIME [epoch: 2.73 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2255963119768856		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.2255963119768856 | validation: 0.22559387178348278]
	TIME [epoch: 2.75 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2689989645982357		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.2689989645982357 | validation: 0.3520367539702869]
	TIME [epoch: 2.74 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28947094473762397		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.28947094473762397 | validation: 0.25538424791910014]
	TIME [epoch: 2.74 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.286109264309467		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.286109264309467 | validation: 0.2929650532943364]
	TIME [epoch: 2.73 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2316462969221172		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.2316462969221172 | validation: 0.22997589215783912]
	TIME [epoch: 2.74 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1839454645158277		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.1839454645158277 | validation: 0.2138699503879222]
	TIME [epoch: 2.74 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1863463277594482		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.1863463277594482 | validation: 0.2736214301035757]
	TIME [epoch: 2.74 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20114591913723645		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.20114591913723645 | validation: 0.21240041647301589]
	TIME [epoch: 2.74 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23324038251997958		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.23324038251997958 | validation: 0.32461868337733957]
	TIME [epoch: 2.74 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26070321920250167		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.26070321920250167 | validation: 0.2131028885181359]
	TIME [epoch: 2.73 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24449010500948745		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.24449010500948745 | validation: 0.2759416536923533]
	TIME [epoch: 2.73 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20734582456358508		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.20734582456358508 | validation: 0.2062070095088008]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_515.pth
	Model improved!!!
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18231046777419502		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.18231046777419502 | validation: 0.24071341078617536]
	TIME [epoch: 2.74 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17540141178843155		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.17540141178843155 | validation: 0.20108300573218083]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_517.pth
	Model improved!!!
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17937737849958427		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.17937737849958427 | validation: 0.25133193506271256]
	TIME [epoch: 2.75 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.194342723658982		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.194342723658982 | validation: 0.209307033950636]
	TIME [epoch: 2.76 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2589258676298431		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.2589258676298431 | validation: 0.35458393709673963]
	TIME [epoch: 2.73 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27899524732283143		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.27899524732283143 | validation: 0.22695295674272475]
	TIME [epoch: 2.74 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28365904512988843		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.28365904512988843 | validation: 0.4353823227560642]
	TIME [epoch: 2.73 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2974471570124572		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.2974471570124572 | validation: 0.23431247985471684]
	TIME [epoch: 2.74 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18149167666815808		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.18149167666815808 | validation: 0.21092824452249595]
	TIME [epoch: 2.74 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17907703630295788		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.17907703630295788 | validation: 0.2913896916305535]
	TIME [epoch: 2.74 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2062419157064315		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.2062419157064315 | validation: 0.2124649762229523]
	TIME [epoch: 2.74 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23936705603841554		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.23936705603841554 | validation: 0.2761758417299453]
	TIME [epoch: 2.74 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2088326813552919		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.2088326813552919 | validation: 0.18498233790030558]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_528.pth
	Model improved!!!
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1804531960928804		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.1804531960928804 | validation: 0.2543064838511286]
	TIME [epoch: 2.75 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18513588044669566		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.18513588044669566 | validation: 0.19898612659407655]
	TIME [epoch: 2.75 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17622591932250609		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.17622591932250609 | validation: 0.24581614610695565]
	TIME [epoch: 2.75 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1792311244490157		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.1792311244490157 | validation: 0.1986512708140128]
	TIME [epoch: 2.75 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2215805822558959		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.2215805822558959 | validation: 0.35833116267229337]
	TIME [epoch: 2.75 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2537111380941532		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.2537111380941532 | validation: 0.20176127734362936]
	TIME [epoch: 2.75 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19457585136899005		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.19457585136899005 | validation: 0.24249600581242686]
	TIME [epoch: 2.75 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1824889685963543		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.1824889685963543 | validation: 0.22194493559525186]
	TIME [epoch: 2.76 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20158451399359606		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.20158451399359606 | validation: 0.3468904456834992]
	TIME [epoch: 2.75 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2522537754416504		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.2522537754416504 | validation: 0.19720884565693847]
	TIME [epoch: 2.74 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24134611249420734		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.24134611249420734 | validation: 0.3131713497648729]
	TIME [epoch: 2.75 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.206237144606661		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.206237144606661 | validation: 0.20877271422316715]
	TIME [epoch: 2.75 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16094541255811373		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.16094541255811373 | validation: 0.20382196652798182]
	TIME [epoch: 2.75 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15286677975818655		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.15286677975818655 | validation: 0.21582880578817953]
	TIME [epoch: 2.75 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15543644551644392		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.15543644551644392 | validation: 0.19786905476482083]
	TIME [epoch: 2.75 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15499832032714822		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.15499832032714822 | validation: 0.23065389377250078]
	TIME [epoch: 2.76 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16361420208373748		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.16361420208373748 | validation: 0.18827551418212413]
	TIME [epoch: 2.75 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2252724582921585		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.2252724582921585 | validation: 0.3155227491840049]
	TIME [epoch: 2.75 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24132400278197216		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.24132400278197216 | validation: 0.19545553531491966]
	TIME [epoch: 2.76 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2483338245401501		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.2483338245401501 | validation: 0.3357742454722937]
	TIME [epoch: 2.76 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22228944310005788		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.22228944310005788 | validation: 0.21612512651847493]
	TIME [epoch: 2.76 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18183685289628387		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.18183685289628387 | validation: 0.22826014908016382]
	TIME [epoch: 2.76 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18067531349427798		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.18067531349427798 | validation: 0.1904932933278293]
	TIME [epoch: 2.75 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1805572494307076		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.1805572494307076 | validation: 0.26415090276113995]
	TIME [epoch: 2.76 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1918605364100906		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.1918605364100906 | validation: 0.1689817380055156]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_553.pth
	Model improved!!!
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19453497023503816		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.19453497023503816 | validation: 0.23046542366289524]
	TIME [epoch: 2.74 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16647592306944298		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.16647592306944298 | validation: 0.19437682653543525]
	TIME [epoch: 2.74 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16017806815300745		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.16017806815300745 | validation: 0.26570025130574837]
	TIME [epoch: 2.74 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.181258623987519		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.181258623987519 | validation: 0.17956869177855073]
	TIME [epoch: 2.75 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18980731872309287		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.18980731872309287 | validation: 0.29917498519714886]
	TIME [epoch: 2.74 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22306575138713985		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.22306575138713985 | validation: 0.17308235036767117]
	TIME [epoch: 2.74 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21382036826411122		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.21382036826411122 | validation: 0.2652438502000406]
	TIME [epoch: 2.74 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18577233399845272		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.18577233399845272 | validation: 0.1648595436490764]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_561.pth
	Model improved!!!
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14967918815541895		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.14967918815541895 | validation: 0.1904343914535159]
	TIME [epoch: 2.74 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14368167722792013		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.14368167722792013 | validation: 0.16520780551378422]
	TIME [epoch: 2.74 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14586214231107042		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.14586214231107042 | validation: 0.20922333126223364]
	TIME [epoch: 2.74 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15170822878079465		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.15170822878079465 | validation: 0.16575468294354223]
	TIME [epoch: 2.75 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17645884287629748		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.17645884287629748 | validation: 0.28107438573084126]
	TIME [epoch: 2.74 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2109499836421893		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.2109499836421893 | validation: 0.18617735291722504]
	TIME [epoch: 2.74 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25772582966846463		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.25772582966846463 | validation: 0.368613840112106]
	TIME [epoch: 2.75 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22481107701851755		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.22481107701851755 | validation: 0.24595400547081883]
	TIME [epoch: 2.74 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18254432545171942		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.18254432545171942 | validation: 0.2124030382015707]
	TIME [epoch: 2.74 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15735662505505796		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.15735662505505796 | validation: 0.19630946233833832]
	TIME [epoch: 2.74 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1446806738574851		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.1446806738574851 | validation: 0.18484319091441367]
	TIME [epoch: 2.74 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14733818782360525		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.14733818782360525 | validation: 0.19263936986334684]
	TIME [epoch: 2.74 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14783476059899997		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.14783476059899997 | validation: 0.17729666471550087]
	TIME [epoch: 2.74 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16313763889359162		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.16313763889359162 | validation: 0.26452585712975524]
	TIME [epoch: 2.74 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20511425963089372		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.20511425963089372 | validation: 0.18762018898419724]
	TIME [epoch: 2.73 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23843996851033278		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.23843996851033278 | validation: 0.2835942649609951]
	TIME [epoch: 2.74 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19230306203411782		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.19230306203411782 | validation: 0.17757410485347927]
	TIME [epoch: 2.74 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13992153469428187		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.13992153469428187 | validation: 0.17615638545628204]
	TIME [epoch: 2.75 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1362061253826445		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.1362061253826445 | validation: 0.19091580097019129]
	TIME [epoch: 2.74 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1376566650975223		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.1376566650975223 | validation: 0.17935209010904526]
	TIME [epoch: 2.74 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1841773230842291		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.1841773230842291 | validation: 0.33824368746755334]
	TIME [epoch: 2.73 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.250984237755647		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.250984237755647 | validation: 0.17442564301306493]
	TIME [epoch: 2.74 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21710089824847173		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.21710089824847173 | validation: 0.1718677147646227]
	TIME [epoch: 2.73 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13488138494943144		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.13488138494943144 | validation: 0.1818902584519679]
	TIME [epoch: 2.74 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1409415456731031		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.1409415456731031 | validation: 0.16672187024162716]
	TIME [epoch: 2.74 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17046763277628		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.17046763277628 | validation: 0.2949691473000436]
	TIME [epoch: 2.74 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20077970836301698		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.20077970836301698 | validation: 0.1561103867468303]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_588.pth
	Model improved!!!
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1885919679506071		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.1885919679506071 | validation: 0.26978527965263854]
	TIME [epoch: 2.76 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18786627804553116		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.18786627804553116 | validation: 0.14577432232654788]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_590.pth
	Model improved!!!
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1442135341743792		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.1442135341743792 | validation: 0.18195717664765776]
	TIME [epoch: 2.74 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13259003369837374		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.13259003369837374 | validation: 0.197728989845973]
	TIME [epoch: 2.74 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13742800035707275		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.13742800035707275 | validation: 0.1904823681417342]
	TIME [epoch: 2.74 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15857665213041283		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.15857665213041283 | validation: 0.255711413348169]
	TIME [epoch: 2.74 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19070450973279682		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.19070450973279682 | validation: 0.16540123861376974]
	TIME [epoch: 2.73 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19031890243728383		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.19031890243728383 | validation: 0.22203088005777616]
	TIME [epoch: 2.74 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15519861742118293		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.15519861742118293 | validation: 0.16425495059398396]
	TIME [epoch: 2.74 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14070719835529327		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.14070719835529327 | validation: 0.2002986640624286]
	TIME [epoch: 2.73 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1410979427279665		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.1410979427279665 | validation: 0.15061913731329413]
	TIME [epoch: 2.73 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15528165650852388		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.15528165650852388 | validation: 0.22154603042837018]
	TIME [epoch: 2.74 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18456780810994872		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.18456780810994872 | validation: 0.14754661108844022]
	TIME [epoch: 2.76 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16996995833129894		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.16996995833129894 | validation: 0.21180480016377623]
	TIME [epoch: 2.75 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14051217292112475		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.14051217292112475 | validation: 0.17293731705111245]
	TIME [epoch: 2.75 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17129691199783426		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.17129691199783426 | validation: 0.2829524617920992]
	TIME [epoch: 2.75 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18442055320757422		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.18442055320757422 | validation: 0.1504238175613975]
	TIME [epoch: 2.75 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15882480091529128		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.15882480091529128 | validation: 0.17687635752705683]
	TIME [epoch: 2.75 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13988776497326671		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.13988776497326671 | validation: 0.14197221367024762]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_607.pth
	Model improved!!!
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13976133373606683		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.13976133373606683 | validation: 0.20402818164371014]
	TIME [epoch: 2.75 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1458576271119575		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.1458576271119575 | validation: 0.14087316024954372]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_609.pth
	Model improved!!!
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14974890578596042		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.14974890578596042 | validation: 0.22012655185285823]
	TIME [epoch: 2.76 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16339158612573218		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.16339158612573218 | validation: 0.15163230887532433]
	TIME [epoch: 2.75 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17207661564743007		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.17207661564743007 | validation: 0.22386432785018676]
	TIME [epoch: 2.75 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16008486433371735		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.16008486433371735 | validation: 0.13890180965256618]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_613.pth
	Model improved!!!
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13939597241842566		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.13939597241842566 | validation: 0.21873632043352337]
	TIME [epoch: 2.74 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1459395060546571		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.1459395060546571 | validation: 0.1745080112192755]
	TIME [epoch: 2.75 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18368422034723178		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.18368422034723178 | validation: 0.37716628161812377]
	TIME [epoch: 2.75 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23870471959067976		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.23870471959067976 | validation: 0.16233169129606242]
	TIME [epoch: 2.75 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13566736505571086		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.13566736505571086 | validation: 0.19063869301609612]
	TIME [epoch: 2.75 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1617048628684696		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.1617048628684696 | validation: 0.24246362484764253]
	TIME [epoch: 2.75 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17169737187041917		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.17169737187041917 | validation: 0.16760772718011344]
	TIME [epoch: 2.75 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18153596295950367		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.18153596295950367 | validation: 0.19249448585107415]
	TIME [epoch: 2.76 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13404159749608013		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.13404159749608013 | validation: 0.17251435963371908]
	TIME [epoch: 2.75 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12573175199941153		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.12573175199941153 | validation: 0.15124704798487873]
	TIME [epoch: 2.75 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11746455747167676		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.11746455747167676 | validation: 0.1852317711521153]
	TIME [epoch: 2.75 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13436585689108382		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.13436585689108382 | validation: 0.15148107084895854]
	TIME [epoch: 2.75 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16143189899144858		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.16143189899144858 | validation: 0.2671629895447415]
	TIME [epoch: 2.75 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1841542342989961		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.1841542342989961 | validation: 0.15021413828971522]
	TIME [epoch: 2.75 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.142482061891799		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.142482061891799 | validation: 0.15025245875499998]
	TIME [epoch: 2.75 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1224203284082287		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.1224203284082287 | validation: 0.17648301880839723]
	TIME [epoch: 2.75 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1272468651718958		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.1272468651718958 | validation: 0.14617368430604968]
	TIME [epoch: 2.75 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13665556413980895		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.13665556413980895 | validation: 0.20117525470826123]
	TIME [epoch: 2.76 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1600679309476379		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.1600679309476379 | validation: 0.15243948890326162]
	TIME [epoch: 2.75 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17323843575176198		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.17323843575176198 | validation: 0.24476435142284989]
	TIME [epoch: 2.75 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15916945628654258		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.15916945628654258 | validation: 0.1728059251551237]
	TIME [epoch: 2.75 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.176997849454444		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.176997849454444 | validation: 0.198460128744057]
	TIME [epoch: 2.75 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15196632314578243		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.15196632314578243 | validation: 0.160181586638541]
	TIME [epoch: 2.75 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13489700483099637		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.13489700483099637 | validation: 0.15090652032708746]
	TIME [epoch: 2.75 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11697721924946601		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.11697721924946601 | validation: 0.17018010884997115]
	TIME [epoch: 2.75 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12161668560873447		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.12161668560873447 | validation: 0.1502197135055635]
	TIME [epoch: 2.75 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12438159567060661		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.12438159567060661 | validation: 0.16143264766279847]
	TIME [epoch: 2.75 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12209057006394602		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.12209057006394602 | validation: 0.1486509440301255]
	TIME [epoch: 2.75 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13569005691994676		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.13569005691994676 | validation: 0.2235644085123056]
	TIME [epoch: 2.75 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17758516768407528		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.17758516768407528 | validation: 0.14579932159554962]
	TIME [epoch: 2.75 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17895468482035248		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.17895468482035248 | validation: 0.1836012465308856]
	TIME [epoch: 2.75 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13307724949214036		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.13307724949214036 | validation: 0.16081933506733437]
	TIME [epoch: 2.75 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13812148275367436		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.13812148275367436 | validation: 0.21943154311514113]
	TIME [epoch: 2.75 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1467896200680558		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.1467896200680558 | validation: 0.14804717120052124]
	TIME [epoch: 2.75 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12574390373581198		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.12574390373581198 | validation: 0.16187804916932164]
	TIME [epoch: 2.75 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12332368175750336		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.12332368175750336 | validation: 0.13430307676470518]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_649.pth
	Model improved!!!
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12282722553448892		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.12282722553448892 | validation: 0.19612175363223083]
	TIME [epoch: 2.73 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14528410455029556		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.14528410455029556 | validation: 0.1585980485978194]
	TIME [epoch: 2.74 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20314847262056987		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.20314847262056987 | validation: 0.2914332063976412]
	TIME [epoch: 2.74 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1872721041495587		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.1872721041495587 | validation: 0.15731277598135307]
	TIME [epoch: 2.74 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11702549189029647		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.11702549189029647 | validation: 0.14992607400397837]
	TIME [epoch: 2.73 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1259057925403704		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.1259057925403704 | validation: 0.1701857440859431]
	TIME [epoch: 2.74 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12309946337871358		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.12309946337871358 | validation: 0.1420305149714321]
	TIME [epoch: 2.73 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1297126161994965		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.1297126161994965 | validation: 0.15392391256077884]
	TIME [epoch: 2.74 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12112369009035052		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.12112369009035052 | validation: 0.13447521986928251]
	TIME [epoch: 2.73 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1364545155920784		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.1364545155920784 | validation: 0.25599965006400716]
	TIME [epoch: 2.74 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17341853500935187		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.17341853500935187 | validation: 0.1472681521370864]
	TIME [epoch: 2.73 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14918524860767615		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.14918524860767615 | validation: 0.15893643701183727]
	TIME [epoch: 2.73 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12446962542768512		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.12446962542768512 | validation: 0.13765975023121932]
	TIME [epoch: 2.73 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11744098705584372		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.11744098705584372 | validation: 0.16842926345727088]
	TIME [epoch: 2.74 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11701917073576208		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.11701917073576208 | validation: 0.13248907926308837]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_664.pth
	Model improved!!!
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11356313153914455		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.11356313153914455 | validation: 0.15330962008621163]
	TIME [epoch: 2.74 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11423251986420649		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.11423251986420649 | validation: 0.14374168006051333]
	TIME [epoch: 2.74 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13121775212147935		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.13121775212147935 | validation: 0.2474239265452215]
	TIME [epoch: 2.74 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18990555908546508		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.18990555908546508 | validation: 0.15006183075244459]
	TIME [epoch: 2.74 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19097974616784782		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.19097974616784782 | validation: 0.22256172223012044]
	TIME [epoch: 2.74 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14697006125658113		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.14697006125658113 | validation: 0.13275492323366012]
	TIME [epoch: 2.74 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10738387007215144		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.10738387007215144 | validation: 0.12490142461003458]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_671.pth
	Model improved!!!
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11981345767571414		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.11981345767571414 | validation: 0.17263085501829556]
	TIME [epoch: 2.74 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13146459513157882		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.13146459513157882 | validation: 0.14017337765579058]
	TIME [epoch: 2.74 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13510349290773324		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.13510349290773324 | validation: 0.1510598000008565]
	TIME [epoch: 2.74 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11066358159440594		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.11066358159440594 | validation: 0.13987135060606182]
	TIME [epoch: 2.74 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10837634878938616		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.10837634878938616 | validation: 0.14306940323103995]
	TIME [epoch: 2.74 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11060581600165675		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.11060581600165675 | validation: 0.14604294600971085]
	TIME [epoch: 2.74 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11067536348178297		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.11067536348178297 | validation: 0.13070360408587842]
	TIME [epoch: 2.74 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11302500448778494		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.11302500448778494 | validation: 0.17490743820061586]
	TIME [epoch: 2.74 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1354685963726045		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.1354685963726045 | validation: 0.1530318097063056]
	TIME [epoch: 2.74 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18495232972479553		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.18495232972479553 | validation: 0.2650643334289276]
	TIME [epoch: 2.74 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1778737777689306		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.1778737777689306 | validation: 0.13335281440936372]
	TIME [epoch: 2.74 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12171579429704689		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.12171579429704689 | validation: 0.15449375600628032]
	TIME [epoch: 2.74 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11045832763756147		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.11045832763756147 | validation: 0.14600798231680082]
	TIME [epoch: 2.74 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10679449717020381		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.10679449717020381 | validation: 0.15439595880763804]
	TIME [epoch: 2.74 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1078409463996297		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.1078409463996297 | validation: 0.13111411681074234]
	TIME [epoch: 2.74 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11523050987749134		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.11523050987749134 | validation: 0.200096644348843]
	TIME [epoch: 2.74 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1439025457733121		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.1439025457733121 | validation: 0.13852327748410473]
	TIME [epoch: 2.74 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13402052160150962		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.13402052160150962 | validation: 0.1445435417408505]
	TIME [epoch: 2.74 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11363069487669238		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.11363069487669238 | validation: 0.14057024159361836]
	TIME [epoch: 2.74 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10790608224989592		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.10790608224989592 | validation: 0.1389926489945722]
	TIME [epoch: 2.74 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10820596539311254		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.10820596539311254 | validation: 0.16195315620310619]
	TIME [epoch: 2.73 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11753134993529366		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.11753134993529366 | validation: 0.1443186736853209]
	TIME [epoch: 2.73 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14233788101010672		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.14233788101010672 | validation: 0.2734776854854807]
	TIME [epoch: 2.74 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19626377071740359		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.19626377071740359 | validation: 0.12966773395360373]
	TIME [epoch: 2.74 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1338649266715799		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.1338649266715799 | validation: 0.14108525090923948]
	TIME [epoch: 2.74 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10692667373572395		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.10692667373572395 | validation: 0.14707047350843563]
	TIME [epoch: 2.74 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10976816764536079		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.10976816764536079 | validation: 0.13875208771980452]
	TIME [epoch: 2.74 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1199217260349451		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.1199217260349451 | validation: 0.17384439294875126]
	TIME [epoch: 2.74 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12696778815901094		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.12696778815901094 | validation: 0.13627059927041488]
	TIME [epoch: 2.73 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14036852698287963		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.14036852698287963 | validation: 0.19194688642549643]
	TIME [epoch: 2.74 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14104266554479472		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.14104266554479472 | validation: 0.12990551170113285]
	TIME [epoch: 2.74 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11963710211870074		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.11963710211870074 | validation: 0.14223813436187843]
	TIME [epoch: 2.74 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10835594459004554		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.10835594459004554 | validation: 0.13550749141973015]
	TIME [epoch: 2.74 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10560831385960812		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.10560831385960812 | validation: 0.13206200154433487]
	TIME [epoch: 2.74 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1035698754538534		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.1035698754538534 | validation: 0.13236600013336705]
	TIME [epoch: 2.74 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10280359498247527		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.10280359498247527 | validation: 0.13652286056567894]
	TIME [epoch: 2.74 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10498697343449172		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.10498697343449172 | validation: 0.14503444880140334]
	TIME [epoch: 2.74 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10630425399651122		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.10630425399651122 | validation: 0.1333236827310762]
	TIME [epoch: 2.74 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10761503977289189		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.10761503977289189 | validation: 0.18076337401954062]
	TIME [epoch: 2.74 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1362752017646565		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.1362752017646565 | validation: 0.14486183506438582]
	TIME [epoch: 2.74 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1848691468075999		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.1848691468075999 | validation: 0.2177517912214861]
	TIME [epoch: 2.74 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15662780148681027		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.15662780148681027 | validation: 0.12706982092666777]
	TIME [epoch: 2.73 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10989643248782803		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.10989643248782803 | validation: 0.13334922187211395]
	TIME [epoch: 2.74 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10840770452553768		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.10840770452553768 | validation: 0.14143871150109907]
	TIME [epoch: 2.74 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11323467850601698		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.11323467850601698 | validation: 0.1328593125266249]
	TIME [epoch: 2.74 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11877317653931609		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.11877317653931609 | validation: 0.13878313671532638]
	TIME [epoch: 2.75 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.109940469917484		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.109940469917484 | validation: 0.12671228908979998]
	TIME [epoch: 2.74 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10493984981569793		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.10493984981569793 | validation: 0.1289000471378585]
	TIME [epoch: 2.74 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10266968516358446		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.10266968516358446 | validation: 0.13396916000698963]
	TIME [epoch: 2.74 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10108435416585784		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.10108435416585784 | validation: 0.1579989370803471]
	TIME [epoch: 2.74 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11025711287834333		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.11025711287834333 | validation: 0.12398353733343621]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_722.pth
	Model improved!!!
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12384469487016722		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.12384469487016722 | validation: 0.1739642611009186]
	TIME [epoch: 2.74 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1349961618891171		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.1349961618891171 | validation: 0.13136897665512293]
	TIME [epoch: 2.74 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1292493492624787		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.1292493492624787 | validation: 0.1655036908762453]
	TIME [epoch: 2.74 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12005383412363617		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.12005383412363617 | validation: 0.12402551014091322]
	TIME [epoch: 2.74 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10044728471193903		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.10044728471193903 | validation: 0.12730261153223307]
	TIME [epoch: 2.75 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10209625881642918		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.10209625881642918 | validation: 0.11361144755979923]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_728.pth
	Model improved!!!
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1005445050997101		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.1005445050997101 | validation: 0.14010511968040018]
	TIME [epoch: 2.74 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10090742246374174		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.10090742246374174 | validation: 0.13338139835026971]
	TIME [epoch: 2.74 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11452570958923634		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.11452570958923634 | validation: 0.1743396215579211]
	TIME [epoch: 2.74 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13602673278886898		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.13602673278886898 | validation: 0.12488484848081338]
	TIME [epoch: 2.74 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1362589606557919		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.1362589606557919 | validation: 0.15619621882691948]
	TIME [epoch: 2.74 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12039127374589086		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.12039127374589086 | validation: 0.12531820496711785]
	TIME [epoch: 2.74 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11075189595957036		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.11075189595957036 | validation: 0.1545865355854096]
	TIME [epoch: 2.74 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11636221191425464		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.11636221191425464 | validation: 0.11810049587419442]
	TIME [epoch: 2.74 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11041861809665829		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.11041861809665829 | validation: 0.15408518891946923]
	TIME [epoch: 2.74 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1109945351062102		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.1109945351062102 | validation: 0.12050850843455124]
	TIME [epoch: 2.74 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11329825136835442		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.11329825136835442 | validation: 0.15206213380891775]
	TIME [epoch: 2.74 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11426431724096546		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.11426431724096546 | validation: 0.1249758067083969]
	TIME [epoch: 2.74 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10768733129563848		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.10768733129563848 | validation: 0.1345145578695446]
	TIME [epoch: 2.74 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09909870134618132		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.09909870134618132 | validation: 0.12418892159057743]
	TIME [epoch: 2.74 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09744945380684009		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.09744945380684009 | validation: 0.1361795068799403]
	TIME [epoch: 2.74 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10103724138555274		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.10103724138555274 | validation: 0.12175572149007884]
	TIME [epoch: 2.74 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11059446077542068		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.11059446077542068 | validation: 0.1556034466259243]
	TIME [epoch: 2.74 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11741109647573662		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.11741109647573662 | validation: 0.12417130050146026]
	TIME [epoch: 2.75 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11350024949194172		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.11350024949194172 | validation: 0.1492607071133497]
	TIME [epoch: 2.75 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1146739152475		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.1146739152475 | validation: 0.13270975444242136]
	TIME [epoch: 2.74 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12061380737975193		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.12061380737975193 | validation: 0.18352761246318208]
	TIME [epoch: 2.75 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13232270089783257		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.13232270089783257 | validation: 0.12578885344575053]
	TIME [epoch: 2.74 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10704922412381479		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.10704922412381479 | validation: 0.12286842061514057]
	TIME [epoch: 2.74 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09773049105438823		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.09773049105438823 | validation: 0.1310504969940222]
	TIME [epoch: 2.74 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09555555660804571		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.09555555660804571 | validation: 0.12806164862583613]
	TIME [epoch: 2.74 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09750368030672966		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.09750368030672966 | validation: 0.12089456098188428]
	TIME [epoch: 2.75 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09478621483933772		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.09478621483933772 | validation: 0.11202825880095896]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_755.pth
	Model improved!!!
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10507302701120401		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.10507302701120401 | validation: 0.1553188262459161]
	TIME [epoch: 2.76 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11688906085714657		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.11688906085714657 | validation: 0.12769296092675267]
	TIME [epoch: 2.76 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13371704159246176		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.13371704159246176 | validation: 0.16427407831597932]
	TIME [epoch: 2.76 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12625117159506777		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.12625117159506777 | validation: 0.13819113244339262]
	TIME [epoch: 2.77 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12314280975827402		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.12314280975827402 | validation: 0.15065279054948383]
	TIME [epoch: 2.76 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11574661323103679		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.11574661323103679 | validation: 0.12064427418416274]
	TIME [epoch: 2.76 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10055873695967397		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.10055873695967397 | validation: 0.11891219246860313]
	TIME [epoch: 2.76 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09216692841943785		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.09216692841943785 | validation: 0.12801015910707708]
	TIME [epoch: 2.75 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09594259517941119		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.09594259517941119 | validation: 0.11608637363205464]
	TIME [epoch: 2.74 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10175146289317706		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.10175146289317706 | validation: 0.14373780103429576]
	TIME [epoch: 2.75 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12239057570244256		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.12239057570244256 | validation: 0.12382128874170895]
	TIME [epoch: 2.75 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10918002737273533		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.10918002737273533 | validation: 0.1273281511693583]
	TIME [epoch: 2.74 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10158096832281814		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.10158096832281814 | validation: 0.11537810792371567]
	TIME [epoch: 2.74 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09438259000697449		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.09438259000697449 | validation: 0.12110138453303497]
	TIME [epoch: 2.74 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09270172053608593		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.09270172053608593 | validation: 0.11939021733979294]
	TIME [epoch: 2.75 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09149909720888502		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.09149909720888502 | validation: 0.12428918058403374]
	TIME [epoch: 2.74 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09068647277097629		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.09068647277097629 | validation: 0.11792000272495691]
	TIME [epoch: 2.75 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09662133205765391		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.09662133205765391 | validation: 0.1415577854583507]
	TIME [epoch: 2.75 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10162614527916301		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.10162614527916301 | validation: 0.13857191630486318]
	TIME [epoch: 2.74 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14019049042164372		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.14019049042164372 | validation: 0.1800062639395307]
	TIME [epoch: 2.74 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14366112139925827		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.14366112139925827 | validation: 0.11489527401100293]
	TIME [epoch: 2.74 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11046013947119576		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.11046013947119576 | validation: 0.13288844408926068]
	TIME [epoch: 2.74 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0956038101962475		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.0956038101962475 | validation: 0.12625050586777853]
	TIME [epoch: 2.74 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09218546319256431		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.09218546319256431 | validation: 0.11310721700554055]
	TIME [epoch: 2.75 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0958874684048364		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.0958874684048364 | validation: 0.13076743792860965]
	TIME [epoch: 2.74 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09967212197893316		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.09967212197893316 | validation: 0.1167194662654822]
	TIME [epoch: 2.75 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09599263957739711		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.09599263957739711 | validation: 0.13106075274005327]
	TIME [epoch: 2.74 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09191107588269137		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.09191107588269137 | validation: 0.11154955271930773]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_783.pth
	Model improved!!!
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09082531629927282		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.09082531629927282 | validation: 0.11000242948862585]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_784.pth
	Model improved!!!
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08998292365859566		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.08998292365859566 | validation: 0.11219732174188862]
	TIME [epoch: 2.75 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0920506846721879		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.0920506846721879 | validation: 0.13555555853952525]
	TIME [epoch: 2.75 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09967314266572848		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.09967314266572848 | validation: 0.13041354162156296]
	TIME [epoch: 2.75 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11349032635967186		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.11349032635967186 | validation: 0.1585929709918512]
	TIME [epoch: 2.75 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11860586461176968		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.11860586461176968 | validation: 0.12231900967869905]
	TIME [epoch: 2.75 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11672033345709676		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.11672033345709676 | validation: 0.13795402027858858]
	TIME [epoch: 2.75 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1033339828779766		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.1033339828779766 | validation: 0.11856305687519447]
	TIME [epoch: 2.76 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09288266971456079		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.09288266971456079 | validation: 0.12503098842457308]
	TIME [epoch: 2.75 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08631572097253119		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.08631572097253119 | validation: 0.11640183756392025]
	TIME [epoch: 2.75 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08792522781930684		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.08792522781930684 | validation: 0.11130114067775515]
	TIME [epoch: 2.75 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0890628354856003		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.0890628354856003 | validation: 0.12216336439581671]
	TIME [epoch: 2.75 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0896854281793299		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.0896854281793299 | validation: 0.13107757511141124]
	TIME [epoch: 2.75 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0969492313130666		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.0969492313130666 | validation: 0.11815046261867246]
	TIME [epoch: 2.75 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10712714621428912		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.10712714621428912 | validation: 0.1484884495940734]
	TIME [epoch: 2.75 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11730923038554135		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.11730923038554135 | validation: 0.10576167357707411]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_799.pth
	Model improved!!!
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11101430120599176		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.11101430120599176 | validation: 0.12791027150091414]
	TIME [epoch: 2.75 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09690631910035638		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.09690631910035638 | validation: 0.11143980512247807]
	TIME [epoch: 2.74 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09698553543339732		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.09698553543339732 | validation: 0.13887918116943257]
	TIME [epoch: 2.75 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10315377777845437		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.10315377777845437 | validation: 0.11699415455872247]
	TIME [epoch: 2.74 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09604803275612371		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.09604803275612371 | validation: 0.12749783915494026]
	TIME [epoch: 2.74 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09189933048919845		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.09189933048919845 | validation: 0.10862669101757608]
	TIME [epoch: 2.74 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09242026926484648		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.09242026926484648 | validation: 0.12159349001524707]
	TIME [epoch: 2.74 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09665166950448273		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.09665166950448273 | validation: 0.10927187631221265]
	TIME [epoch: 2.74 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0960336824271726		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.0960336824271726 | validation: 0.11951001462812272]
	TIME [epoch: 2.74 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09235802637774686		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.09235802637774686 | validation: 0.11317608182817668]
	TIME [epoch: 2.74 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09155713365963035		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.09155713365963035 | validation: 0.1264971738645671]
	TIME [epoch: 2.74 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09466280385574594		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.09466280385574594 | validation: 0.11206693135517565]
	TIME [epoch: 2.75 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10021156907900836		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.10021156907900836 | validation: 0.15081048352670334]
	TIME [epoch: 2.74 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10997311998655114		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.10997311998655114 | validation: 0.10930625915418253]
	TIME [epoch: 2.75 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10509387091160932		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.10509387091160932 | validation: 0.11231546126214971]
	TIME [epoch: 2.74 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08945076295289553		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.08945076295289553 | validation: 0.10871919376082224]
	TIME [epoch: 2.75 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08542713158117617		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.08542713158117617 | validation: 0.11347731946791453]
	TIME [epoch: 2.74 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08586277733300676		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.08586277733300676 | validation: 0.11243378603197925]
	TIME [epoch: 2.74 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08225002442343052		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.08225002442343052 | validation: 0.11122958117294274]
	TIME [epoch: 2.74 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08586389842825154		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.08586389842825154 | validation: 0.12877971083823866]
	TIME [epoch: 2.74 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09225331113707927		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.09225331113707927 | validation: 0.12101458239624048]
	TIME [epoch: 2.74 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1173722813526849		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.1173722813526849 | validation: 0.1602698514478094]
	TIME [epoch: 2.74 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12871414592293381		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.12871414592293381 | validation: 0.10906941385639075]
	TIME [epoch: 2.74 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1010733968152383		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.1010733968152383 | validation: 0.10853690277539257]
	TIME [epoch: 2.75 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08615790231392283		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.08615790231392283 | validation: 0.10521320000504908]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_824.pth
	Model improved!!!
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08805057863244717		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.08805057863244717 | validation: 0.11071022409447347]
	TIME [epoch: 2.75 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08892365726684343		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.08892365726684343 | validation: 0.1237828364656616]
	TIME [epoch: 2.74 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09623146741490966		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.09623146741490966 | validation: 0.11302023130923473]
	TIME [epoch: 2.75 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09805162220108983		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.09805162220108983 | validation: 0.13922097102639816]
	TIME [epoch: 2.75 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10018476490565423		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.10018476490565423 | validation: 0.11494974111171517]
	TIME [epoch: 2.74 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09370443946023013		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.09370443946023013 | validation: 0.11665920915631207]
	TIME [epoch: 2.74 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09246520898410462		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.09246520898410462 | validation: 0.10482606540867488]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_831.pth
	Model improved!!!
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08714614347162954		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.08714614347162954 | validation: 0.11489294433668912]
	TIME [epoch: 2.74 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0869493157456581		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.0869493157456581 | validation: 0.10700625235618394]
	TIME [epoch: 2.74 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08697782582721043		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.08697782582721043 | validation: 0.11645522549798258]
	TIME [epoch: 2.75 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08624279744430542		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.08624279744430542 | validation: 0.10590860364782176]
	TIME [epoch: 2.75 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08850264658512448		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.08850264658512448 | validation: 0.122585310044752]
	TIME [epoch: 2.75 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0884895123017682		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.0884895123017682 | validation: 0.10658222604062327]
	TIME [epoch: 2.74 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09489299439589685		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.09489299439589685 | validation: 0.13471122064202037]
	TIME [epoch: 2.75 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11134740820579672		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.11134740820579672 | validation: 0.10897002012366618]
	TIME [epoch: 2.74 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11333981939006034		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.11333981939006034 | validation: 0.15121132455156125]
	TIME [epoch: 2.74 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10028347054148132		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.10028347054148132 | validation: 0.10657256312506563]
	TIME [epoch: 2.74 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08313506782817816		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.08313506782817816 | validation: 0.10080548862294583]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_842.pth
	Model improved!!!
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08455526678290784		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.08455526678290784 | validation: 0.11544748267855698]
	TIME [epoch: 2.74 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08678583538745142		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.08678583538745142 | validation: 0.10900233050267305]
	TIME [epoch: 2.75 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08837202613667351		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.08837202613667351 | validation: 0.11284659221401139]
	TIME [epoch: 2.74 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08537530493264664		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.08537530493264664 | validation: 0.10520173711629144]
	TIME [epoch: 2.74 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08725769012588758		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.08725769012588758 | validation: 0.11885666297220704]
	TIME [epoch: 2.74 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09142085437809656		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.09142085437809656 | validation: 0.1054460447766345]
	TIME [epoch: 2.74 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09976082587789244		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.09976082587789244 | validation: 0.14537327883859097]
	TIME [epoch: 2.74 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10431713342042626		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.10431713342042626 | validation: 0.10619637206309336]
	TIME [epoch: 2.74 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08743941915305864		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.08743941915305864 | validation: 0.10794083160361351]
	TIME [epoch: 2.74 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0829031291376535		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.0829031291376535 | validation: 0.09892427608460669]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_852.pth
	Model improved!!!
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.083263541737979		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.083263541737979 | validation: 0.12230561701660624]
	TIME [epoch: 2.75 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08447388942351115		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.08447388942351115 | validation: 0.11473794720417728]
	TIME [epoch: 2.75 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08344740370793943		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.08344740370793943 | validation: 0.10930591976202023]
	TIME [epoch: 2.76 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08009092578731793		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.08009092578731793 | validation: 0.10962984530710362]
	TIME [epoch: 2.75 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08115012865515635		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.08115012865515635 | validation: 0.1065472773340354]
	TIME [epoch: 2.75 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08186694122289889		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.08186694122289889 | validation: 0.10791047335902318]
	TIME [epoch: 2.75 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08697926746843156		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.08697926746843156 | validation: 0.13841690451405514]
	TIME [epoch: 2.74 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11155880328125146		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.11155880328125146 | validation: 0.11343290511546722]
	TIME [epoch: 2.76 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12443021166504917		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.12443021166504917 | validation: 0.1384180802192835]
	TIME [epoch: 2.74 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09410300932415734		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.09410300932415734 | validation: 0.10469242062221094]
	TIME [epoch: 2.75 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08241656398898295		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.08241656398898295 | validation: 0.1015629004464159]
	TIME [epoch: 2.74 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0836104836796949		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.0836104836796949 | validation: 0.10838086243086312]
	TIME [epoch: 2.75 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08121965060051375		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.08121965060051375 | validation: 0.1052380165707707]
	TIME [epoch: 2.74 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08216402349570266		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.08216402349570266 | validation: 0.10317896468152135]
	TIME [epoch: 2.76 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07998799727227177		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.07998799727227177 | validation: 0.10829609769651748]
	TIME [epoch: 2.75 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07859525328617903		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.07859525328617903 | validation: 0.10135620698713572]
	TIME [epoch: 2.75 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08240126532981902		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.08240126532981902 | validation: 0.1094953198016515]
	TIME [epoch: 2.75 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08466650168161652		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.08466650168161652 | validation: 0.10075102394664928]
	TIME [epoch: 2.75 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09418711955781911		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.09418711955781911 | validation: 0.13048662690893884]
	TIME [epoch: 2.75 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10534728160275535		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.10534728160275535 | validation: 0.10128604452038958]
	TIME [epoch: 2.75 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10187760562667343		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.10187760562667343 | validation: 0.11230363912934849]
	TIME [epoch: 2.74 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08344807307011838		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.08344807307011838 | validation: 0.10086656540423632]
	TIME [epoch: 2.75 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07934552280693745		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.07934552280693745 | validation: 0.10314932868980549]
	TIME [epoch: 2.75 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07964556092945416		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.07964556092945416 | validation: 0.10306152343925588]
	TIME [epoch: 2.75 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07764017395591177		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.07764017395591177 | validation: 0.10749349136452942]
	TIME [epoch: 2.75 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07948427290667526		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.07948427290667526 | validation: 0.10238306156454652]
	TIME [epoch: 2.73 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08088291484377715		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.08088291484377715 | validation: 0.10629668548649818]
	TIME [epoch: 2.73 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08004456496917378		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.08004456496917378 | validation: 0.10107576656393934]
	TIME [epoch: 2.73 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08142591417764532		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.08142591417764532 | validation: 0.11398665907216535]
	TIME [epoch: 2.74 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08092609217272316		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.08092609217272316 | validation: 0.10525769654278881]
	TIME [epoch: 2.73 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08566209750325683		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.08566209750325683 | validation: 0.12340602380349613]
	TIME [epoch: 2.74 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10043590418512072		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.10043590418512072 | validation: 0.11349867809716581]
	TIME [epoch: 2.74 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11670938676939657		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.11670938676939657 | validation: 0.13441863909252336]
	TIME [epoch: 2.74 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09416940238710683		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.09416940238710683 | validation: 0.10329674717476639]
	TIME [epoch: 2.73 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07958130718896177		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.07958130718896177 | validation: 0.10040354205083175]
	TIME [epoch: 2.75 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08617061955105297		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.08617061955105297 | validation: 0.11152449496798433]
	TIME [epoch: 2.73 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08629756388171365		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.08629756388171365 | validation: 0.09948028625617374]
	TIME [epoch: 2.74 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08246873205429261		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.08246873205429261 | validation: 0.1058152396268791]
	TIME [epoch: 2.73 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07642378125798029		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.07642378125798029 | validation: 0.10234322915290395]
	TIME [epoch: 2.73 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08257862166979425		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.08257862166979425 | validation: 0.1031286246398969]
	TIME [epoch: 2.73 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08555439785641657		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.08555439785641657 | validation: 0.12880243803009514]
	TIME [epoch: 2.73 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08839000575264638		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.08839000575264638 | validation: 0.10454673038379543]
	TIME [epoch: 2.73 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08220373683261883		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.08220373683261883 | validation: 0.10557075634358713]
	TIME [epoch: 2.73 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07942825900462945		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.07942825900462945 | validation: 0.10579917798583038]
	TIME [epoch: 2.74 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07710894011779416		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.07710894011779416 | validation: 0.0944576894906215]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_897.pth
	Model improved!!!
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08175034555198699		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.08175034555198699 | validation: 0.11333823305978055]
	TIME [epoch: 2.75 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08155859855302353		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.08155859855302353 | validation: 0.10412754460253314]
	TIME [epoch: 2.75 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08236858458465193		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.08236858458465193 | validation: 0.10231866745219627]
	TIME [epoch: 2.75 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08470252673896597		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.08470252673896597 | validation: 0.09572551387060795]
	TIME [epoch: 2.74 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08105608328390325		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.08105608328390325 | validation: 0.11554049815503341]
	TIME [epoch: 2.75 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08622900637835289		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.08622900637835289 | validation: 0.10779367949702219]
	TIME [epoch: 2.75 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09039467176043026		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.09039467176043026 | validation: 0.12840033065907383]
	TIME [epoch: 2.75 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0950824139224962		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.0950824139224962 | validation: 0.10334286909939466]
	TIME [epoch: 2.74 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08165078594226038		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.08165078594226038 | validation: 0.09993519743472082]
	TIME [epoch: 2.74 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07772172511784256		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.07772172511784256 | validation: 0.09359766264480185]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_907.pth
	Model improved!!!
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07634125148472733		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.07634125148472733 | validation: 0.1009516353762545]
	TIME [epoch: 2.75 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07573465737543864		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.07573465737543864 | validation: 0.09707865022838655]
	TIME [epoch: 2.74 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07742310700080465		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.07742310700080465 | validation: 0.10027922348029024]
	TIME [epoch: 2.75 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08610482784866749		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.08610482784866749 | validation: 0.12684028291427982]
	TIME [epoch: 2.74 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09788552563822701		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.09788552563822701 | validation: 0.09909531445228542]
	TIME [epoch: 2.75 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08586248172547822		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.08586248172547822 | validation: 0.10664228863521204]
	TIME [epoch: 2.74 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07854803157249027		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.07854803157249027 | validation: 0.0986732154346318]
	TIME [epoch: 2.75 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07919168881409042		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.07919168881409042 | validation: 0.10351387960654512]
	TIME [epoch: 2.75 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07603240612843902		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.07603240612843902 | validation: 0.0982290265508135]
	TIME [epoch: 2.74 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07760602404993003		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.07760602404993003 | validation: 0.09832611028778167]
	TIME [epoch: 2.75 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07399494047892906		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.07399494047892906 | validation: 0.10057743833560653]
	TIME [epoch: 2.75 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07720568696607726		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.07720568696607726 | validation: 0.09127712477338545]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_919.pth
	Model improved!!!
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07599755556237955		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.07599755556237955 | validation: 0.10811380512413736]
	TIME [epoch: 2.74 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07947021154178932		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.07947021154178932 | validation: 0.10344115616548384]
	TIME [epoch: 2.74 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08409182195643015		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.08409182195643015 | validation: 0.12137341704457315]
	TIME [epoch: 2.74 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09508059778535514		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.09508059778535514 | validation: 0.10144411094665884]
	TIME [epoch: 2.74 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09839206992612085		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.09839206992612085 | validation: 0.09973901106951788]
	TIME [epoch: 2.74 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08397125216245539		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.08397125216245539 | validation: 0.09755314745132908]
	TIME [epoch: 2.74 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07552283471265735		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.07552283471265735 | validation: 0.09015545260654463]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_926.pth
	Model improved!!!
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0743142660197679		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.0743142660197679 | validation: 0.09467975184757954]
	TIME [epoch: 2.74 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07561489534381247		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.07561489534381247 | validation: 0.09863106289008822]
	TIME [epoch: 2.73 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07609437098968921		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.07609437098968921 | validation: 0.10205077993517518]
	TIME [epoch: 2.75 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07591414040494823		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.07591414040494823 | validation: 0.09512848248316506]
	TIME [epoch: 2.74 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07719049781983096		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.07719049781983096 | validation: 0.10259469769394523]
	TIME [epoch: 2.74 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07571512108330963		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.07571512108330963 | validation: 0.09383024809070503]
	TIME [epoch: 2.74 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0767419054864153		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.0767419054864153 | validation: 0.1198223508497549]
	TIME [epoch: 2.74 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08867883864026578		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.08867883864026578 | validation: 0.09842886501457448]
	TIME [epoch: 2.74 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09435292826248631		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.09435292826248631 | validation: 0.11930372457948091]
	TIME [epoch: 2.74 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08035816553985668		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.08035816553985668 | validation: 0.09012430318436769]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_936.pth
	Model improved!!!
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07307271085542204		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.07307271085542204 | validation: 0.10101643297309844]
	TIME [epoch: 2.74 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07234582118621159		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.07234582118621159 | validation: 0.10026407235972115]
	TIME [epoch: 2.74 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07420404486707537		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.07420404486707537 | validation: 0.09344308789976227]
	TIME [epoch: 2.74 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0754559950179813		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.0754559950179813 | validation: 0.09536679751676608]
	TIME [epoch: 2.74 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07603471703642095		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.07603471703642095 | validation: 0.09613235626360342]
	TIME [epoch: 2.74 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07643712474136515		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.07643712474136515 | validation: 0.1105259495841171]
	TIME [epoch: 2.74 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07958647345387		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.07958647345387 | validation: 0.08891363735351565]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_943.pth
	Model improved!!!
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07821390524280754		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.07821390524280754 | validation: 0.10213121984420384]
	TIME [epoch: 2.73 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07998833055792241		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.07998833055792241 | validation: 0.08957077567650001]
	TIME [epoch: 2.74 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08003704561367478		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.08003704561367478 | validation: 0.1025601961129215]
	TIME [epoch: 2.74 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08232902301950297		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.08232902301950297 | validation: 0.090399320874744]
	TIME [epoch: 2.74 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0807500864472065		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.0807500864472065 | validation: 0.10121751090997747]
	TIME [epoch: 2.74 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07390402629903405		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.07390402629903405 | validation: 0.09879485495120803]
	TIME [epoch: 2.74 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07477421430912629		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.07477421430912629 | validation: 0.09495155947639394]
	TIME [epoch: 2.74 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07340721076347893		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.07340721076347893 | validation: 0.09567986390763389]
	TIME [epoch: 2.73 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07460355130150165		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.07460355130150165 | validation: 0.10391359094311002]
	TIME [epoch: 2.74 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0744564218462547		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.0744564218462547 | validation: 0.09004906666280295]
	TIME [epoch: 2.74 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07489869525815769		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.07489869525815769 | validation: 0.10583159686393742]
	TIME [epoch: 2.74 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07881319984670747		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.07881319984670747 | validation: 0.09036934490081878]
	TIME [epoch: 2.73 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08954450219397124		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.08954450219397124 | validation: 0.11602528585604324]
	TIME [epoch: 2.73 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08301043828207001		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.08301043828207001 | validation: 0.09253453740993207]
	TIME [epoch: 2.74 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07481509138403268		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.07481509138403268 | validation: 0.09376860938825088]
	TIME [epoch: 2.74 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07361068560770709		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.07361068560770709 | validation: 0.0926752978026662]
	TIME [epoch: 2.74 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07423111810247274		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.07423111810247274 | validation: 0.09190205151011155]
	TIME [epoch: 2.74 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07770494350350658		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.07770494350350658 | validation: 0.10187701327657467]
	TIME [epoch: 2.75 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07628263340432577		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.07628263340432577 | validation: 0.09215046130018488]
	TIME [epoch: 2.74 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07565905699144138		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.07565905699144138 | validation: 0.10122412686477095]
	TIME [epoch: 2.73 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07491522418223183		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.07491522418223183 | validation: 0.10227169265156842]
	TIME [epoch: 2.74 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07180383157669558		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.07180383157669558 | validation: 0.09471310795902148]
	TIME [epoch: 2.74 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07195054337499122		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.07195054337499122 | validation: 0.09096360998911654]
	TIME [epoch: 2.74 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07113085146770383		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.07113085146770383 | validation: 0.09288113948502451]
	TIME [epoch: 2.74 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07533161902082666		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.07533161902082666 | validation: 0.09618804313732804]
	TIME [epoch: 2.74 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07044305708040868		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.07044305708040868 | validation: 0.09294079297876495]
	TIME [epoch: 2.74 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07304721989932574		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.07304721989932574 | validation: 0.11125556099914805]
	TIME [epoch: 2.74 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08197364385245143		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.08197364385245143 | validation: 0.09587808537000454]
	TIME [epoch: 2.74 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10240732030550255		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.10240732030550255 | validation: 0.10285360047857953]
	TIME [epoch: 2.74 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07574069190768401		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.07574069190768401 | validation: 0.09636550780128775]
	TIME [epoch: 2.74 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07276627044999255		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.07276627044999255 | validation: 0.0869599850380703]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_974.pth
	Model improved!!!
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0787861053102964		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.0787861053102964 | validation: 0.10074002856601952]
	TIME [epoch: 2.74 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07768237240971117		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.07768237240971117 | validation: 0.09518344659778853]
	TIME [epoch: 2.74 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07519076560340969		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.07519076560340969 | validation: 0.09372232388588232]
	TIME [epoch: 2.75 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07209654929873134		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.07209654929873134 | validation: 0.09872371150996212]
	TIME [epoch: 2.74 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07095249929528878		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.07095249929528878 | validation: 0.09333086894578185]
	TIME [epoch: 2.74 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07262150982735398		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.07262150982735398 | validation: 0.08584305185602877]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_980.pth
	Model improved!!!
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0709543786976227		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.0709543786976227 | validation: 0.09683420763304539]
	TIME [epoch: 2.74 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07209721118115985		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.07209721118115985 | validation: 0.08880522059537854]
	TIME [epoch: 2.75 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07113450953599237		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.07113450953599237 | validation: 0.09668594689167762]
	TIME [epoch: 2.74 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07815554830289069		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.07815554830289069 | validation: 0.08333614894165522]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_984.pth
	Model improved!!!
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08047537854692131		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.08047537854692131 | validation: 0.1076247130467474]
	TIME [epoch: 2.74 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07705278246071609		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.07705278246071609 | validation: 0.09208382452140859]
	TIME [epoch: 2.74 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0728611843917116		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.0728611843917116 | validation: 0.08600589148579296]
	TIME [epoch: 2.74 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07171435886996474		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.07171435886996474 | validation: 0.09001556617031631]
	TIME [epoch: 2.74 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07110682617502324		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.07110682617502324 | validation: 0.08911051614987175]
	TIME [epoch: 2.74 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06984337886556066		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.06984337886556066 | validation: 0.09105244853455768]
	TIME [epoch: 2.74 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06970418923937192		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.06970418923937192 | validation: 0.09909880699977232]
	TIME [epoch: 2.74 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07018492023472436		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.07018492023472436 | validation: 0.09044590130780451]
	TIME [epoch: 2.74 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07118998161816371		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.07118998161816371 | validation: 0.09598046678689284]
	TIME [epoch: 2.75 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07555104723681515		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.07555104723681515 | validation: 0.09054415487585993]
	TIME [epoch: 2.74 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0771941883842165		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.0771941883842165 | validation: 0.0952558879220709]
	TIME [epoch: 2.74 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07346722997751977		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.07346722997751977 | validation: 0.09445788417930816]
	TIME [epoch: 2.74 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07224852158370496		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.07224852158370496 | validation: 0.09522357777530223]
	TIME [epoch: 2.74 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07457777764893085		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.07457777764893085 | validation: 0.08524005834967747]
	TIME [epoch: 2.74 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07263064065965068		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.07263064065965068 | validation: 0.08419469969382498]
	TIME [epoch: 2.74 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06997378401256936		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.06997378401256936 | validation: 0.09222742864620964]
	TIME [epoch: 2.74 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06792979487810363		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.06792979487810363 | validation: 0.09043683500082512]
	TIME [epoch: 180 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06926779522378064		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.06926779522378064 | validation: 0.08956238766685043]
	TIME [epoch: 5.9 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06990994022608506		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.06990994022608506 | validation: 0.09179345749760107]
	TIME [epoch: 5.89 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07003863099759552		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.07003863099759552 | validation: 0.09165323517811597]
	TIME [epoch: 5.88 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07173180122063742		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.07173180122063742 | validation: 0.09504677581642258]
	TIME [epoch: 5.9 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07645963793152619		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.07645963793152619 | validation: 0.09018722483652244]
	TIME [epoch: 5.89 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0791363477758229		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.0791363477758229 | validation: 0.09223441736060473]
	TIME [epoch: 5.89 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07124796767656918		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.07124796767656918 | validation: 0.08100897485093983]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1008.pth
	Model improved!!!
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06926263615306372		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.06926263615306372 | validation: 0.0852589593535659]
	TIME [epoch: 5.89 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06939552234695415		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.06939552234695415 | validation: 0.08880849518324303]
	TIME [epoch: 5.88 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06774075388064796		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.06774075388064796 | validation: 0.089983409788195]
	TIME [epoch: 5.88 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07043910121351245		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.07043910121351245 | validation: 0.09690748358069745]
	TIME [epoch: 5.89 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07582688754952065		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.07582688754952065 | validation: 0.09005577376200688]
	TIME [epoch: 5.89 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08161181024189046		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.08161181024189046 | validation: 0.10055209641044914]
	TIME [epoch: 5.89 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07466084200224717		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.07466084200224717 | validation: 0.09046538283892218]
	TIME [epoch: 5.89 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06870896593068918		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.06870896593068918 | validation: 0.0809142689634505]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1016.pth
	Model improved!!!
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06874574516269638		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.06874574516269638 | validation: 0.08528292858801306]
	TIME [epoch: 5.88 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07015028019540706		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.07015028019540706 | validation: 0.0838218991015596]
	TIME [epoch: 5.89 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06908382581009315		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.06908382581009315 | validation: 0.09518726940293101]
	TIME [epoch: 5.9 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06942537442926092		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.06942537442926092 | validation: 0.093300238427194]
	TIME [epoch: 5.88 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0691694123958516		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.0691694123958516 | validation: 0.09838394215844708]
	TIME [epoch: 5.89 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0671678812991455		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.0671678812991455 | validation: 0.08472237682213055]
	TIME [epoch: 5.88 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06560200695859165		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.06560200695859165 | validation: 0.08343077268242544]
	TIME [epoch: 5.9 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0704311317786936		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.0704311317786936 | validation: 0.09558964604254243]
	TIME [epoch: 5.89 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07569851353038345		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.07569851353038345 | validation: 0.09128631781202677]
	TIME [epoch: 5.9 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07579000769941616		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.07579000769941616 | validation: 0.09267456188094]
	TIME [epoch: 5.88 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07242249699967951		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.07242249699967951 | validation: 0.08678874373884736]
	TIME [epoch: 5.88 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06809421923965914		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.06809421923965914 | validation: 0.08497909974557945]
	TIME [epoch: 5.89 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.069437785419392		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.069437785419392 | validation: 0.09454954608398616]
	TIME [epoch: 5.88 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07479560269353336		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.07479560269353336 | validation: 0.08315101614781696]
	TIME [epoch: 5.89 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07414874221605905		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.07414874221605905 | validation: 0.08588321347095247]
	TIME [epoch: 5.88 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0668553236381095		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.0668553236381095 | validation: 0.08159363883873186]
	TIME [epoch: 5.89 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06913085617292838		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.06913085617292838 | validation: 0.08902400686315938]
	TIME [epoch: 5.89 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06866661435266767		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.06866661435266767 | validation: 0.0923294762375077]
	TIME [epoch: 5.89 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06882925377577563		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.06882925377577563 | validation: 0.08802021969514458]
	TIME [epoch: 5.89 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06812003274481936		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.06812003274481936 | validation: 0.0905958275220625]
	TIME [epoch: 5.88 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06755197775936511		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.06755197775936511 | validation: 0.08448078945819372]
	TIME [epoch: 5.89 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06539918828063726		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.06539918828063726 | validation: 0.0826118464035514]
	TIME [epoch: 5.89 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06916555713457213		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.06916555713457213 | validation: 0.09296709585281604]
	TIME [epoch: 5.89 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06906566899062073		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.06906566899062073 | validation: 0.08816130906930378]
	TIME [epoch: 5.89 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06601672721707773		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.06601672721707773 | validation: 0.08521219546022557]
	TIME [epoch: 5.89 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06630583356900348		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.06630583356900348 | validation: 0.08372626778516139]
	TIME [epoch: 5.89 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06649806861057819		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.06649806861057819 | validation: 0.0949015518894881]
	TIME [epoch: 5.9 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07341595550336111		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.07341595550336111 | validation: 0.08903855993718135]
	TIME [epoch: 5.89 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08876666567040943		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.08876666567040943 | validation: 0.0969406409034067]
	TIME [epoch: 5.88 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07437856862018852		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.07437856862018852 | validation: 0.07799598584083411]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1046.pth
	Model improved!!!
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06741778133748973		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.06741778133748973 | validation: 0.08304368543740377]
	TIME [epoch: 5.88 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0687308259280355		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.0687308259280355 | validation: 0.09156804422988864]
	TIME [epoch: 5.89 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0682308723316347		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.0682308723316347 | validation: 0.08465172280479585]
	TIME [epoch: 5.89 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06739591382782373		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.06739591382782373 | validation: 0.08546568097803803]
	TIME [epoch: 5.89 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06649938675220363		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.06649938675220363 | validation: 0.08654516544913286]
	TIME [epoch: 5.89 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0647486100402195		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.0647486100402195 | validation: 0.08297603127266355]
	TIME [epoch: 5.89 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06554049432290804		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.06554049432290804 | validation: 0.08017766068289217]
	TIME [epoch: 5.9 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0653213416574442		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.0653213416574442 | validation: 0.08256363944058046]
	TIME [epoch: 5.89 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06566823140800661		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.06566823140800661 | validation: 0.08220069553009259]
	TIME [epoch: 5.89 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07100022493126154		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.07100022493126154 | validation: 0.09482270960434619]
	TIME [epoch: 5.88 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06687771906658815		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.06687771906658815 | validation: 0.08244033683754064]
	TIME [epoch: 5.89 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07141125835330747		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.07141125835330747 | validation: 0.09785137374137715]
	TIME [epoch: 5.9 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06943352471837817		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.06943352471837817 | validation: 0.07840317496512243]
	TIME [epoch: 5.89 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06990432305346587		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.06990432305346587 | validation: 0.07811283907318284]
	TIME [epoch: 5.89 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06473004062799032		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.06473004062799032 | validation: 0.08782284480576631]
	TIME [epoch: 5.89 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06433700266511892		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.06433700266511892 | validation: 0.08684609002536406]
	TIME [epoch: 5.89 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06345431638562299		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.06345431638562299 | validation: 0.08653729710403851]
	TIME [epoch: 5.9 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06675347570049188		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.06675347570049188 | validation: 0.08559835443493528]
	TIME [epoch: 5.88 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06529472253764673		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.06529472253764673 | validation: 0.08719821666920556]
	TIME [epoch: 5.88 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06605558051623789		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.06605558051623789 | validation: 0.099365261535932]
	TIME [epoch: 5.89 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07168284812616327		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.07168284812616327 | validation: 0.08590467877095105]
	TIME [epoch: 5.89 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07235766102859856		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.07235766102859856 | validation: 0.08998335900692508]
	TIME [epoch: 5.9 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06862009122646918		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.06862009122646918 | validation: 0.08087478410036233]
	TIME [epoch: 5.89 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06703587300637766		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.06703587300637766 | validation: 0.08285594922225556]
	TIME [epoch: 5.89 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06620405488358634		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.06620405488358634 | validation: 0.08672433608568704]
	TIME [epoch: 5.89 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06614547613542379		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.06614547613542379 | validation: 0.07909947336781148]
	TIME [epoch: 5.89 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06441190583700884		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.06441190583700884 | validation: 0.08424038593363914]
	TIME [epoch: 5.9 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06570777098950832		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.06570777098950832 | validation: 0.08263669478209829]
	TIME [epoch: 5.89 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06438421042646274		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.06438421042646274 | validation: 0.07782249686759901]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1075.pth
	Model improved!!!
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06576557536127306		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.06576557536127306 | validation: 0.0874095453101014]
	TIME [epoch: 5.89 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06529984178073749		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.06529984178073749 | validation: 0.07728500491896452]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1077.pth
	Model improved!!!
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0718439854668906		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.0718439854668906 | validation: 0.09616260549504432]
	TIME [epoch: 5.9 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07332174354412967		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.07332174354412967 | validation: 0.08861010132028076]
	TIME [epoch: 5.89 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07785758666610944		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.07785758666610944 | validation: 0.08188077970554503]
	TIME [epoch: 5.89 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06701842909175751		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.06701842909175751 | validation: 0.08478198009810084]
	TIME [epoch: 5.89 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06720239714072795		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.06720239714072795 | validation: 0.07861426281226996]
	TIME [epoch: 5.89 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06873104542732475		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.06873104542732475 | validation: 0.08465900826105144]
	TIME [epoch: 5.89 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06651105803433943		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.06651105803433943 | validation: 0.07668064760003701]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1084.pth
	Model improved!!!
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06415011182338376		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.06415011182338376 | validation: 0.08503489850868767]
	TIME [epoch: 5.89 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061692023233420004		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.061692023233420004 | validation: 0.0825495810133712]
	TIME [epoch: 5.89 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06576673523382455		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.06576673523382455 | validation: 0.08577892129352288]
	TIME [epoch: 5.89 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06511023511267477		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.06511023511267477 | validation: 0.08383280904694933]
	TIME [epoch: 5.89 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06533395338280537		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.06533395338280537 | validation: 0.07832507600806081]
	TIME [epoch: 5.89 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06291330581978671		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.06291330581978671 | validation: 0.09022414691283288]
	TIME [epoch: 5.89 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06175182887512607		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.06175182887512607 | validation: 0.07931004965506598]
	TIME [epoch: 5.88 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06405340166097194		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.06405340166097194 | validation: 0.07876479429631088]
	TIME [epoch: 5.89 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06600864978229609		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.06600864978229609 | validation: 0.08078177991951023]
	TIME [epoch: 5.89 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07069947559549895		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.07069947559549895 | validation: 0.09246965840838076]
	TIME [epoch: 5.89 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07291918371475807		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.07291918371475807 | validation: 0.08643001549226756]
	TIME [epoch: 5.89 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06871778589763722		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.06871778589763722 | validation: 0.0843745897437292]
	TIME [epoch: 5.89 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06402917786303669		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.06402917786303669 | validation: 0.08612336377238106]
	TIME [epoch: 5.89 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06423659460997522		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.06423659460997522 | validation: 0.07908235108341509]
	TIME [epoch: 5.89 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06526811567201961		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.06526811567201961 | validation: 0.08037703705808709]
	TIME [epoch: 5.89 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06332371534121531		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.06332371534121531 | validation: 0.08235687561348258]
	TIME [epoch: 5.89 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062462476642757496		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.062462476642757496 | validation: 0.0845108762702193]
	TIME [epoch: 5.89 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06433868353577717		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.06433868353577717 | validation: 0.0778020470003451]
	TIME [epoch: 5.89 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06382368494585867		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.06382368494585867 | validation: 0.07800533919229305]
	TIME [epoch: 5.89 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06282910540029839		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.06282910540029839 | validation: 0.0804244925179427]
	TIME [epoch: 5.89 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061711419812473474		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.061711419812473474 | validation: 0.08494251928739965]
	TIME [epoch: 5.89 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06436150914795505		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.06436150914795505 | validation: 0.0897751528744322]
	TIME [epoch: 5.89 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06782981738506771		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.06782981738506771 | validation: 0.07859906402923639]
	TIME [epoch: 5.88 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06890558676655012		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.06890558676655012 | validation: 0.08672938965098403]
	TIME [epoch: 5.89 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06309826262069824		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.06309826262069824 | validation: 0.07953053246415666]
	TIME [epoch: 5.88 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06396686422870357		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.06396686422870357 | validation: 0.0833079085159754]
	TIME [epoch: 5.89 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06543293982862677		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.06543293982862677 | validation: 0.07877956960943977]
	TIME [epoch: 5.88 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06223074767451022		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.06223074767451022 | validation: 0.08031538574620678]
	TIME [epoch: 5.89 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06277514853011953		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.06277514853011953 | validation: 0.07853226703743789]
	TIME [epoch: 5.89 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06519710089657732		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.06519710089657732 | validation: 0.08524410745294747]
	TIME [epoch: 5.89 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06831021105245205		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.06831021105245205 | validation: 0.0791227286421625]
	TIME [epoch: 5.88 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07028244926654044		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.07028244926654044 | validation: 0.0792944145235668]
	TIME [epoch: 5.89 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06079360772505687		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.06079360772505687 | validation: 0.07451102270544648]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1117.pth
	Model improved!!!
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06235185096646702		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.06235185096646702 | validation: 0.08107218108639046]
	TIME [epoch: 5.89 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06172269427554085		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.06172269427554085 | validation: 0.08893843174502046]
	TIME [epoch: 5.88 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06449703273442443		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.06449703273442443 | validation: 0.08592998956961945]
	TIME [epoch: 5.89 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06286516904503169		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.06286516904503169 | validation: 0.07518952448324848]
	TIME [epoch: 5.88 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059937821758203585		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.059937821758203585 | validation: 0.07553849104601554]
	TIME [epoch: 5.89 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06171847707446374		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.06171847707446374 | validation: 0.07830607524493385]
	TIME [epoch: 5.89 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0627269211330625		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.0627269211330625 | validation: 0.07957402749555076]
	TIME [epoch: 5.89 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06348216025980652		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.06348216025980652 | validation: 0.07755832982126343]
	TIME [epoch: 5.88 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061451955874170565		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.061451955874170565 | validation: 0.08056748729105337]
	TIME [epoch: 5.88 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06290935314414055		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.06290935314414055 | validation: 0.08253612234209573]
	TIME [epoch: 5.89 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06872210286380846		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.06872210286380846 | validation: 0.0886259853699746]
	TIME [epoch: 5.89 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06975045465577147		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.06975045465577147 | validation: 0.0805087351155629]
	TIME [epoch: 5.88 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06650669952299383		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.06650669952299383 | validation: 0.08098963031993076]
	TIME [epoch: 5.88 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06009240502168698		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.06009240502168698 | validation: 0.07767114606734928]
	TIME [epoch: 5.88 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06250170719973577		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.06250170719973577 | validation: 0.07581215936949345]
	TIME [epoch: 5.89 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06290632675227835		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.06290632675227835 | validation: 0.08208943708365411]
	TIME [epoch: 5.9 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06564309244225301		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.06564309244225301 | validation: 0.07829681201915759]
	TIME [epoch: 5.89 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06514962595212842		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.06514962595212842 | validation: 0.07699594016618859]
	TIME [epoch: 5.88 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06214607033463923		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.06214607033463923 | validation: 0.08251967239278545]
	TIME [epoch: 5.89 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06389656384613919		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.06389656384613919 | validation: 0.07735151904538057]
	TIME [epoch: 5.88 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06318892708760875		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.06318892708760875 | validation: 0.0796888412011908]
	TIME [epoch: 5.89 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06131523438412878		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.06131523438412878 | validation: 0.07972088663237106]
	TIME [epoch: 5.88 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061470869481251605		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.061470869481251605 | validation: 0.07981536520445724]
	TIME [epoch: 5.9 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060574417728159226		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.060574417728159226 | validation: 0.07997863552594933]
	TIME [epoch: 5.89 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061745531504034265		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.061745531504034265 | validation: 0.08128433419200681]
	TIME [epoch: 5.88 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06279591379374881		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.06279591379374881 | validation: 0.07984430432346085]
	TIME [epoch: 5.88 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06535564424384253		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.06535564424384253 | validation: 0.08290846405763691]
	TIME [epoch: 5.89 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060302162495837135		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.060302162495837135 | validation: 0.0747648067340024]
	TIME [epoch: 5.89 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0627157356977442		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.0627157356977442 | validation: 0.0752479209612839]
	TIME [epoch: 5.88 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05845059937227247		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.05845059937227247 | validation: 0.07922781738418595]
	TIME [epoch: 5.88 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06162108794448727		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.06162108794448727 | validation: 0.0776903615934722]
	TIME [epoch: 5.88 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06181734100997363		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.06181734100997363 | validation: 0.07357568164455107]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1149.pth
	Model improved!!!
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06208247046896746		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.06208247046896746 | validation: 0.08468870692121333]
	TIME [epoch: 5.89 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07001107466700603		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.07001107466700603 | validation: 0.0763302639857916]
	TIME [epoch: 5.88 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06748868953250135		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.06748868953250135 | validation: 0.08489555373569553]
	TIME [epoch: 5.88 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06059134495597327		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.06059134495597327 | validation: 0.07783472876200762]
	TIME [epoch: 5.88 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06383944042548767		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.06383944042548767 | validation: 0.07798606911458038]
	TIME [epoch: 5.9 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06042369168582662		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.06042369168582662 | validation: 0.07101536297157703]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1155.pth
	Model improved!!!
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06011960896473162		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.06011960896473162 | validation: 0.07483131776145112]
	TIME [epoch: 5.9 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060092699396533576		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.060092699396533576 | validation: 0.07616305675838012]
	TIME [epoch: 5.89 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06140465146800422		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.06140465146800422 | validation: 0.08171949817052689]
	TIME [epoch: 5.88 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05975165326980665		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.05975165326980665 | validation: 0.0780798945397415]
	TIME [epoch: 5.89 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06137112345636535		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.06137112345636535 | validation: 0.07393360978261125]
	TIME [epoch: 5.89 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06060848154140332		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.06060848154140332 | validation: 0.07912124155370875]
	TIME [epoch: 5.88 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061827059994017546		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.061827059994017546 | validation: 0.0716621741704376]
	TIME [epoch: 5.88 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060795361980746566		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.060795361980746566 | validation: 0.07641689802291811]
	TIME [epoch: 5.88 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061470281091062076		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.061470281091062076 | validation: 0.07398505884509415]
	TIME [epoch: 5.89 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061420836718106595		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.061420836718106595 | validation: 0.08543016283681415]
	TIME [epoch: 5.88 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06431354760815007		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.06431354760815007 | validation: 0.07291659679245875]
	TIME [epoch: 5.89 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06161638276777359		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.06161638276777359 | validation: 0.07433378218332189]
	TIME [epoch: 5.88 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061299067940790784		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.061299067940790784 | validation: 0.07615138003302273]
	TIME [epoch: 5.88 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05858182099289641		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.05858182099289641 | validation: 0.07887037934532981]
	TIME [epoch: 5.89 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06202779002140066		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.06202779002140066 | validation: 0.07374619558160704]
	TIME [epoch: 5.89 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0643772324858013		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.0643772324858013 | validation: 0.07657899049633182]
	TIME [epoch: 5.88 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06147397360140094		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.06147397360140094 | validation: 0.07459126285849671]
	TIME [epoch: 5.88 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06209087058008015		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.06209087058008015 | validation: 0.07293114749071643]
	TIME [epoch: 5.89 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059125289196741024		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.059125289196741024 | validation: 0.06867794163802696]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1174.pth
	Model improved!!!
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061441965808348174		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.061441965808348174 | validation: 0.07579308064682717]
	TIME [epoch: 5.89 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05819713319359391		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.05819713319359391 | validation: 0.07864967127481211]
	TIME [epoch: 5.88 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05966227047288153		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.05966227047288153 | validation: 0.07249508368455991]
	TIME [epoch: 5.89 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06113305500411121		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.06113305500411121 | validation: 0.0749383960687217]
	TIME [epoch: 5.88 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059083177687370966		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.059083177687370966 | validation: 0.07633795499178722]
	TIME [epoch: 5.89 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05926716572213692		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.05926716572213692 | validation: 0.07023735294039492]
	TIME [epoch: 5.88 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05935231521750993		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.05935231521750993 | validation: 0.07425074071526304]
	TIME [epoch: 5.89 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06114775063719577		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.06114775063719577 | validation: 0.07546892139301035]
	TIME [epoch: 5.89 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059926689240927214		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.059926689240927214 | validation: 0.07742684756898616]
	TIME [epoch: 5.88 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06142300593793891		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.06142300593793891 | validation: 0.07578477232929878]
	TIME [epoch: 5.89 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061346778798597325		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.061346778798597325 | validation: 0.0739513488225763]
	TIME [epoch: 5.89 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06068025121386359		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.06068025121386359 | validation: 0.08193781845681629]
	TIME [epoch: 5.89 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06209093644218955		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.06209093644218955 | validation: 0.07625205407573939]
	TIME [epoch: 5.88 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0659408898345151		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.0659408898345151 | validation: 0.08123650108297954]
	TIME [epoch: 5.88 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06505325019529182		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.06505325019529182 | validation: 0.07535744736685467]
	TIME [epoch: 5.88 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059569130981878236		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.059569130981878236 | validation: 0.07549877708347935]
	TIME [epoch: 5.88 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06177136693919045		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.06177136693919045 | validation: 0.07643603123216806]
	TIME [epoch: 5.88 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05879448745045993		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.05879448745045993 | validation: 0.0768861351011191]
	TIME [epoch: 5.88 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05805225915934544		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.05805225915934544 | validation: 0.07256103480090005]
	TIME [epoch: 5.88 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058189820849717674		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.058189820849717674 | validation: 0.07201219200208704]
	TIME [epoch: 5.89 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06190712327414954		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.06190712327414954 | validation: 0.07378357061133003]
	TIME [epoch: 5.89 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05981664283632687		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.05981664283632687 | validation: 0.07577832708679959]
	TIME [epoch: 5.89 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05937247486898192		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.05937247486898192 | validation: 0.08055212317975212]
	TIME [epoch: 5.89 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057800825611237894		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.057800825611237894 | validation: 0.07079124471280199]
	TIME [epoch: 5.88 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05755555379308214		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.05755555379308214 | validation: 0.07766609467327411]
	TIME [epoch: 5.9 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061265088187283094		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.061265088187283094 | validation: 0.07196265873728479]
	TIME [epoch: 5.88 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059537006122502394		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.059537006122502394 | validation: 0.07603597938323935]
	TIME [epoch: 5.88 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0585413588942485		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.0585413588942485 | validation: 0.06759409367051639]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1202.pth
	Model improved!!!
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057644363891090615		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.057644363891090615 | validation: 0.0733949399434684]
	TIME [epoch: 5.89 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05980332609569717		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.05980332609569717 | validation: 0.07137975544253189]
	TIME [epoch: 5.89 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059332549504256475		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.059332549504256475 | validation: 0.07843254952338441]
	TIME [epoch: 5.89 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058754338085256035		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.058754338085256035 | validation: 0.06854058784449636]
	TIME [epoch: 5.89 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059071119132593976		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.059071119132593976 | validation: 0.07099737181658301]
	TIME [epoch: 5.89 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06000481864421312		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.06000481864421312 | validation: 0.07606058259279808]
	TIME [epoch: 5.88 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06153209444501236		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.06153209444501236 | validation: 0.06743622050569863]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1209.pth
	Model improved!!!
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0631081696315192		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.0631081696315192 | validation: 0.07405035442705907]
	TIME [epoch: 5.89 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05813145080773418		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.05813145080773418 | validation: 0.07484157717323346]
	TIME [epoch: 5.89 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0580676695522532		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.0580676695522532 | validation: 0.07025688139241239]
	TIME [epoch: 5.88 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058035209250285345		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.058035209250285345 | validation: 0.0692286829719053]
	TIME [epoch: 5.88 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05834912096575108		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.05834912096575108 | validation: 0.07156804638020726]
	TIME [epoch: 5.88 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06023005193798468		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.06023005193798468 | validation: 0.07954159757661544]
	TIME [epoch: 5.88 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06013653141672329		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.06013653141672329 | validation: 0.07582265536883454]
	TIME [epoch: 5.88 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05618063312760185		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.05618063312760185 | validation: 0.070539282694828]
	TIME [epoch: 5.88 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059426883825181624		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.059426883825181624 | validation: 0.07367994245843683]
	TIME [epoch: 5.88 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05833394508755145		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.05833394508755145 | validation: 0.07059536651244214]
	TIME [epoch: 5.89 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05971142252799675		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.05971142252799675 | validation: 0.07482027360325152]
	TIME [epoch: 5.88 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060573916637412635		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.060573916637412635 | validation: 0.07429996304678421]
	TIME [epoch: 5.88 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06423671680196671		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.06423671680196671 | validation: 0.0870362209524623]
	TIME [epoch: 5.88 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06917098309024114		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.06917098309024114 | validation: 0.07651546285823295]
	TIME [epoch: 5.88 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05965089348122396		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.05965089348122396 | validation: 0.06965326581019403]
	TIME [epoch: 5.88 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060091339116557646		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.060091339116557646 | validation: 0.06871637456112198]
	TIME [epoch: 5.88 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0592239637927278		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.0592239637927278 | validation: 0.07650097268526593]
	TIME [epoch: 5.88 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05821414394020726		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.05821414394020726 | validation: 0.07306773961922423]
	TIME [epoch: 5.88 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0560927961354062		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.0560927961354062 | validation: 0.07125128119821411]
	TIME [epoch: 5.88 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05587784589584512		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.05587784589584512 | validation: 0.07415684251446639]
	TIME [epoch: 5.89 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0563566910627219		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.0563566910627219 | validation: 0.07639364577546803]
	TIME [epoch: 5.89 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05899733802192946		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.05899733802192946 | validation: 0.06788681294518889]
	TIME [epoch: 5.89 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05884622070458805		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.05884622070458805 | validation: 0.07129973168358662]
	TIME [epoch: 5.88 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056928810866553374		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.056928810866553374 | validation: 0.07357272602135047]
	TIME [epoch: 5.88 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05903627828646817		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.05903627828646817 | validation: 0.0703375588395244]
	TIME [epoch: 5.89 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05694764902506362		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.05694764902506362 | validation: 0.06648192369886845]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1235.pth
	Model improved!!!
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05719764799533439		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.05719764799533439 | validation: 0.07075387143288]
	TIME [epoch: 5.89 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05741253468504569		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.05741253468504569 | validation: 0.0732756222814414]
	TIME [epoch: 5.89 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060252569167019224		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.060252569167019224 | validation: 0.06987061941342287]
	TIME [epoch: 5.88 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05884043840059897		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.05884043840059897 | validation: 0.07257319877518915]
	TIME [epoch: 5.89 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0598620306821607		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.0598620306821607 | validation: 0.06842110901048284]
	TIME [epoch: 5.88 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054505673249114324		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.054505673249114324 | validation: 0.06392433560798426]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1241.pth
	Model improved!!!
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05708045819617337		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.05708045819617337 | validation: 0.06444737891588208]
	TIME [epoch: 5.88 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05772271165847873		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.05772271165847873 | validation: 0.07521056360615636]
	TIME [epoch: 5.88 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05658451078812281		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.05658451078812281 | validation: 0.07053464450968304]
	TIME [epoch: 5.89 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057590351091561125		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.057590351091561125 | validation: 0.0779603733922251]
	TIME [epoch: 5.88 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05830797145775676		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.05830797145775676 | validation: 0.08157739728811828]
	TIME [epoch: 5.89 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06298299516571865		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.06298299516571865 | validation: 0.06572846439630231]
	TIME [epoch: 5.88 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05758752928072061		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.05758752928072061 | validation: 0.07227333276263362]
	TIME [epoch: 5.89 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059767374895447596		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.059767374895447596 | validation: 0.0738223535025087]
	TIME [epoch: 5.89 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057551489849015064		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.057551489849015064 | validation: 0.07471814796316871]
	TIME [epoch: 5.89 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05935247822940228		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.05935247822940228 | validation: 0.06947547182756236]
	TIME [epoch: 5.88 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055325468351521065		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.055325468351521065 | validation: 0.07456495689236198]
	TIME [epoch: 5.88 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06014524128581456		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.06014524128581456 | validation: 0.06399366103516622]
	TIME [epoch: 5.88 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05945842809594691		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.05945842809594691 | validation: 0.0741166730574989]
	TIME [epoch: 5.89 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0588550905896405		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.0588550905896405 | validation: 0.06255310005102743]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1255.pth
	Model improved!!!
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05788689022903577		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.05788689022903577 | validation: 0.07032895032322846]
	TIME [epoch: 5.85 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057693837584964144		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.057693837584964144 | validation: 0.07085557788384246]
	TIME [epoch: 5.85 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05575305646194416		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.05575305646194416 | validation: 0.06633233691969738]
	TIME [epoch: 5.86 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05801656935416889		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.05801656935416889 | validation: 0.07425427545002775]
	TIME [epoch: 5.85 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05639400555010691		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.05639400555010691 | validation: 0.07106473188031369]
	TIME [epoch: 5.86 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058912979562458306		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.058912979562458306 | validation: 0.06929973937202855]
	TIME [epoch: 5.88 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057736647327004786		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.057736647327004786 | validation: 0.06672929788217677]
	TIME [epoch: 5.88 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05572583639694388		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.05572583639694388 | validation: 0.06975379966125672]
	TIME [epoch: 5.87 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05531846024461138		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.05531846024461138 | validation: 0.06482040876821164]
	TIME [epoch: 5.89 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058825362838856456		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.058825362838856456 | validation: 0.07409378374347962]
	TIME [epoch: 5.88 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06010516928970587		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.06010516928970587 | validation: 0.07202374907799873]
	TIME [epoch: 5.89 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05668518693799511		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.05668518693799511 | validation: 0.06355179466271683]
	TIME [epoch: 5.87 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05606529136781981		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.05606529136781981 | validation: 0.06765797034141507]
	TIME [epoch: 5.88 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05619194716816555		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.05619194716816555 | validation: 0.06574778411849938]
	TIME [epoch: 5.88 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05641448351436462		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.05641448351436462 | validation: 0.06430743230493179]
	TIME [epoch: 5.89 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05761809391701558		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.05761809391701558 | validation: 0.07003466568542988]
	TIME [epoch: 5.88 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055942568721536516		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.055942568721536516 | validation: 0.07145417890905069]
	TIME [epoch: 5.88 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056893991515928555		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.056893991515928555 | validation: 0.07218737029378158]
	TIME [epoch: 5.88 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05829442958854316		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.05829442958854316 | validation: 0.06723506288605632]
	TIME [epoch: 5.89 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057765032056088426		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.057765032056088426 | validation: 0.07341992505360316]
	TIME [epoch: 5.88 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05693085033459612		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.05693085033459612 | validation: 0.07639634250578599]
	TIME [epoch: 5.86 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05628882160172666		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.05628882160172666 | validation: 0.07192386205619417]
	TIME [epoch: 5.85 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05497086432873725		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.05497086432873725 | validation: 0.06690402630165206]
	TIME [epoch: 5.85 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05511958147531816		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.05511958147531816 | validation: 0.06965713273386451]
	TIME [epoch: 5.86 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0559736051746504		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.0559736051746504 | validation: 0.06379573741561047]
	TIME [epoch: 5.86 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0560654650033714		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.0560654650033714 | validation: 0.07241024839890535]
	TIME [epoch: 5.86 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055322519368781964		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.055322519368781964 | validation: 0.06868297602070655]
	TIME [epoch: 5.85 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055635915636640214		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.055635915636640214 | validation: 0.06905075929271974]
	TIME [epoch: 5.85 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05669796037607517		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.05669796037607517 | validation: 0.06748898471817942]
	TIME [epoch: 5.86 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055605297819608324		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.055605297819608324 | validation: 0.06865177226471722]
	TIME [epoch: 5.86 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05527388326108751		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.05527388326108751 | validation: 0.06386914313499036]
	TIME [epoch: 5.85 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05599236947926791		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.05599236947926791 | validation: 0.06884350994147069]
	TIME [epoch: 5.85 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05574452055765392		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.05574452055765392 | validation: 0.06752228543524678]
	TIME [epoch: 5.85 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05560373259079542		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.05560373259079542 | validation: 0.07012993207620978]
	TIME [epoch: 5.86 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05735443968105647		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.05735443968105647 | validation: 0.06910003923171239]
	TIME [epoch: 5.86 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05611979259309446		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.05611979259309446 | validation: 0.06946806797582392]
	TIME [epoch: 5.86 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054696473273929465		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.054696473273929465 | validation: 0.06872057149345824]
	TIME [epoch: 5.85 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058078039790116644		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.058078039790116644 | validation: 0.07301662798794888]
	TIME [epoch: 5.85 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05535613870234177		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.05535613870234177 | validation: 0.06672759543172516]
	TIME [epoch: 5.85 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055449893950130724		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.055449893950130724 | validation: 0.06463295930879313]
	TIME [epoch: 5.86 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056592739084135514		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.056592739084135514 | validation: 0.06761241563178905]
	TIME [epoch: 5.86 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055192842947206736		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.055192842947206736 | validation: 0.06995532977796429]
	TIME [epoch: 5.86 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05463920119615171		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.05463920119615171 | validation: 0.06686912033513376]
	TIME [epoch: 5.85 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055096116232318915		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.055096116232318915 | validation: 0.0681277097349917]
	TIME [epoch: 5.87 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05610342911422292		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.05610342911422292 | validation: 0.06451000552417799]
	TIME [epoch: 5.85 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05602240094485616		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.05602240094485616 | validation: 0.06988081761620024]
	TIME [epoch: 5.88 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056351447551566125		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.056351447551566125 | validation: 0.06378649962061925]
	TIME [epoch: 5.88 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05657026673531794		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.05657026673531794 | validation: 0.06808543190016839]
	TIME [epoch: 5.87 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05649398786505036		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.05649398786505036 | validation: 0.0661007875255436]
	TIME [epoch: 5.88 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05581426418692754		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.05581426418692754 | validation: 0.06418265329978004]
	TIME [epoch: 5.87 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05329923647627711		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.05329923647627711 | validation: 0.06393113678061596]
	TIME [epoch: 5.88 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05618946954903116		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.05618946954903116 | validation: 0.06593392785025554]
	TIME [epoch: 5.88 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05589192073461599		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.05589192073461599 | validation: 0.06932940681041316]
	TIME [epoch: 5.88 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05538169937406149		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.05538169937406149 | validation: 0.06466217452597642]
	TIME [epoch: 5.87 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05503569275613182		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.05503569275613182 | validation: 0.06140229985924496]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1310.pth
	Model improved!!!
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05492983685805747		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.05492983685805747 | validation: 0.0703861946894192]
	TIME [epoch: 5.86 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05608981511642521		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.05608981511642521 | validation: 0.06355968161292817]
	TIME [epoch: 5.86 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05591884388655294		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.05591884388655294 | validation: 0.06650535041443956]
	TIME [epoch: 5.86 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05807761847497552		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.05807761847497552 | validation: 0.06692688286339497]
	TIME [epoch: 5.87 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05549179082112977		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.05549179082112977 | validation: 0.06967006244907074]
	TIME [epoch: 5.88 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05373254186429231		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.05373254186429231 | validation: 0.06562865015586847]
	TIME [epoch: 5.88 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05563866894765454		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.05563866894765454 | validation: 0.06453688726119261]
	TIME [epoch: 5.88 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05445978191137854		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.05445978191137854 | validation: 0.06845533634021626]
	TIME [epoch: 5.87 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05536433769140051		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.05536433769140051 | validation: 0.06327505428084228]
	TIME [epoch: 5.87 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05468328058043918		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.05468328058043918 | validation: 0.06457307312612229]
	TIME [epoch: 5.88 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05466198580987598		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.05466198580987598 | validation: 0.06151589720707547]
	TIME [epoch: 5.87 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05546210468372663		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.05546210468372663 | validation: 0.06185818471981113]
	TIME [epoch: 5.89 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0559137589156554		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.0559137589156554 | validation: 0.07012523738751812]
	TIME [epoch: 5.87 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05441188405662259		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.05441188405662259 | validation: 0.06332250829501607]
	TIME [epoch: 5.89 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05554491265609046		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.05554491265609046 | validation: 0.07061738111851806]
	TIME [epoch: 5.88 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05642158067311532		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.05642158067311532 | validation: 0.07053150016776655]
	TIME [epoch: 5.88 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05665373874265711		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.05665373874265711 | validation: 0.07209305644080186]
	TIME [epoch: 5.87 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05577427052042844		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.05577427052042844 | validation: 0.0664006546819552]
	TIME [epoch: 5.89 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05370872448262795		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.05370872448262795 | validation: 0.0678010320517189]
	TIME [epoch: 5.87 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05357375648760344		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.05357375648760344 | validation: 0.06422689105437444]
	TIME [epoch: 5.88 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05357229083033019		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.05357229083033019 | validation: 0.06179329933422724]
	TIME [epoch: 5.87 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05504543618041002		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.05504543618041002 | validation: 0.0634251395332125]
	TIME [epoch: 5.85 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054283840717914644		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.054283840717914644 | validation: 0.06336651177101049]
	TIME [epoch: 5.84 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05510093343324145		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.05510093343324145 | validation: 0.06408467006081682]
	TIME [epoch: 5.85 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05526214059918086		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.05526214059918086 | validation: 0.06585115000808334]
	TIME [epoch: 5.85 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05609389976893416		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.05609389976893416 | validation: 0.06640527856249391]
	TIME [epoch: 5.85 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05413280864490051		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.05413280864490051 | validation: 0.06248733574424025]
	TIME [epoch: 5.84 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05582572066299127		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.05582572066299127 | validation: 0.06489673257963118]
	TIME [epoch: 5.85 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05659116711653417		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.05659116711653417 | validation: 0.05970206061286204]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1339.pth
	Model improved!!!
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05375024438939044		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.05375024438939044 | validation: 0.0637880022275259]
	TIME [epoch: 5.9 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05394202717753812		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.05394202717753812 | validation: 0.06415705454010588]
	TIME [epoch: 5.88 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05325960667364435		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.05325960667364435 | validation: 0.06262956948807673]
	TIME [epoch: 5.88 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05587359078051689		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.05587359078051689 | validation: 0.06563204289476231]
	TIME [epoch: 5.84 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0542735571930852		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.0542735571930852 | validation: 0.06284671695640008]
	TIME [epoch: 5.84 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05527085676567664		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.05527085676567664 | validation: 0.06396891072637588]
	TIME [epoch: 5.85 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05570047556533405		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.05570047556533405 | validation: 0.06929818578005557]
	TIME [epoch: 5.86 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05513746069435256		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.05513746069435256 | validation: 0.06581975119091059]
	TIME [epoch: 5.85 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05446098882340424		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.05446098882340424 | validation: 0.06751980740446065]
	TIME [epoch: 5.85 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05270964256142384		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.05270964256142384 | validation: 0.06499356766314529]
	TIME [epoch: 5.84 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05417926665871708		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.05417926665871708 | validation: 0.06559051064684274]
	TIME [epoch: 5.86 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05183503741517112		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.05183503741517112 | validation: 0.06995473803041956]
	TIME [epoch: 5.85 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05290516824164735		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.05290516824164735 | validation: 0.06450806715081225]
	TIME [epoch: 5.84 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054347987594867926		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.054347987594867926 | validation: 0.06264125311382217]
	TIME [epoch: 5.87 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05675987329279252		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.05675987329279252 | validation: 0.06018465769702927]
	TIME [epoch: 5.88 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053654078861088		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.053654078861088 | validation: 0.06053923723950815]
	TIME [epoch: 5.88 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05238041562256071		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.05238041562256071 | validation: 0.06817561934420881]
	TIME [epoch: 5.88 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05266838381082957		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.05266838381082957 | validation: 0.06630686423816058]
	TIME [epoch: 5.87 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05439467311769297		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.05439467311769297 | validation: 0.06272038591442573]
	TIME [epoch: 5.88 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055058200210547366		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.055058200210547366 | validation: 0.06374437437397333]
	TIME [epoch: 5.87 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05255283279248902		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.05255283279248902 | validation: 0.0629003861053562]
	TIME [epoch: 5.88 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05375870784430406		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.05375870784430406 | validation: 0.05839527390194515]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1361.pth
	Model improved!!!
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05380582871147611		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.05380582871147611 | validation: 0.06645887496606645]
	TIME [epoch: 5.86 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0542577622017691		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.0542577622017691 | validation: 0.062278566791699345]
	TIME [epoch: 5.86 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055769389467522815		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.055769389467522815 | validation: 0.06661066110562903]
	TIME [epoch: 5.86 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05316635622280854		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.05316635622280854 | validation: 0.06918389734867052]
	TIME [epoch: 5.86 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05103662480406818		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.05103662480406818 | validation: 0.06620628357272638]
	TIME [epoch: 5.87 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05417366074003569		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.05417366074003569 | validation: 0.06266715477790784]
	TIME [epoch: 5.88 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05238629310345233		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.05238629310345233 | validation: 0.06412865164698582]
	TIME [epoch: 5.89 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051137493046876537		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.051137493046876537 | validation: 0.0614783629338031]
	TIME [epoch: 5.88 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051279283332136646		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.051279283332136646 | validation: 0.06341844867774721]
	TIME [epoch: 5.9 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05255363044655134		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.05255363044655134 | validation: 0.06080110843822048]
	TIME [epoch: 5.89 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0543120698959269		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.0543120698959269 | validation: 0.0660901087709659]
	TIME [epoch: 5.89 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05418420415526276		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.05418420415526276 | validation: 0.06832602429976281]
	TIME [epoch: 5.88 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0526401098812654		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.0526401098812654 | validation: 0.06481611313159966]
	TIME [epoch: 5.88 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05188614194658658		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.05188614194658658 | validation: 0.06742573935022741]
	TIME [epoch: 5.88 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05239748500842461		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.05239748500842461 | validation: 0.06450957992398101]
	TIME [epoch: 5.89 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05283795558785118		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.05283795558785118 | validation: 0.0619041389822084]
	TIME [epoch: 5.88 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054026789714862994		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.054026789714862994 | validation: 0.06363117481926363]
	TIME [epoch: 5.89 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052283018398582154		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.052283018398582154 | validation: 0.06158086288606837]
	TIME [epoch: 5.87 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052435477445322974		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.052435477445322974 | validation: 0.060908167368481286]
	TIME [epoch: 5.88 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05509386618107337		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.05509386618107337 | validation: 0.06438813068939674]
	TIME [epoch: 5.88 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05262057176986806		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.05262057176986806 | validation: 0.070406468092617]
	TIME [epoch: 5.88 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05171250477712011		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.05171250477712011 | validation: 0.06750738012787143]
	TIME [epoch: 5.87 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05352078263217545		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.05352078263217545 | validation: 0.05727200470983948]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1384.pth
	Model improved!!!
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05437978522135285		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.05437978522135285 | validation: 0.06169743557149106]
	TIME [epoch: 5.87 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05434346328733578		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.05434346328733578 | validation: 0.06151994799963696]
	TIME [epoch: 5.86 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05251961925935861		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.05251961925935861 | validation: 0.06355895332105926]
	TIME [epoch: 5.87 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052169609299724674		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.052169609299724674 | validation: 0.060781532926186016]
	TIME [epoch: 5.86 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05219371981828624		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.05219371981828624 | validation: 0.061896477370243445]
	TIME [epoch: 5.87 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05513164539408489		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.05513164539408489 | validation: 0.05894370661321209]
	TIME [epoch: 5.87 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052827093539509866		[learning rate: 8.6788e-05]
	Learning Rate: 8.67884e-05
	LOSS [training: 0.052827093539509866 | validation: 0.06323077884633797]
	TIME [epoch: 5.88 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05206188776045267		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.05206188776045267 | validation: 0.06839926228109988]
	TIME [epoch: 5.87 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05177356071492637		[learning rate: 8.6176e-05]
	Learning Rate: 8.61757e-05
	LOSS [training: 0.05177356071492637 | validation: 0.06251295814199266]
	TIME [epoch: 5.87 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05227436158492297		[learning rate: 8.5871e-05]
	Learning Rate: 8.5871e-05
	LOSS [training: 0.05227436158492297 | validation: 0.06167753215875517]
	TIME [epoch: 5.87 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05210222174394127		[learning rate: 8.5567e-05]
	Learning Rate: 8.55673e-05
	LOSS [training: 0.05210222174394127 | validation: 0.06293604794487853]
	TIME [epoch: 5.86 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05267156124660332		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.05267156124660332 | validation: 0.0719706669697645]
	TIME [epoch: 5.85 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05162200288141179		[learning rate: 8.4963e-05]
	Learning Rate: 8.49632e-05
	LOSS [training: 0.05162200288141179 | validation: 0.056484007051798915]
	TIME [epoch: 5.85 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1397.pth
	Model improved!!!
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05306652061134777		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.05306652061134777 | validation: 0.06269820899609173]
	TIME [epoch: 5.89 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05487517362776531		[learning rate: 8.4363e-05]
	Learning Rate: 8.43634e-05
	LOSS [training: 0.05487517362776531 | validation: 0.0651085691844698]
	TIME [epoch: 5.88 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053638879142477586		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.053638879142477586 | validation: 0.06634085037762635]
	TIME [epoch: 5.88 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052515259080911036		[learning rate: 8.3768e-05]
	Learning Rate: 8.37678e-05
	LOSS [training: 0.052515259080911036 | validation: 0.06034097406937399]
	TIME [epoch: 5.88 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051775112838338626		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.051775112838338626 | validation: 0.06798158457644043]
	TIME [epoch: 5.88 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0546678316542084		[learning rate: 8.3176e-05]
	Learning Rate: 8.31764e-05
	LOSS [training: 0.0546678316542084 | validation: 0.062112214488890954]
	TIME [epoch: 5.89 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05159609844133749		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.05159609844133749 | validation: 0.06208458198923046]
	TIME [epoch: 5.9 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05190925110958681		[learning rate: 8.2589e-05]
	Learning Rate: 8.25892e-05
	LOSS [training: 0.05190925110958681 | validation: 0.06306492514313272]
	TIME [epoch: 5.9 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05151486614470488		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.05151486614470488 | validation: 0.06333022005554899]
	TIME [epoch: 5.9 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053426834717208604		[learning rate: 8.2006e-05]
	Learning Rate: 8.20061e-05
	LOSS [training: 0.053426834717208604 | validation: 0.06351126612424522]
	TIME [epoch: 5.9 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054164350537497		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.054164350537497 | validation: 0.059722373244865345]
	TIME [epoch: 5.89 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05263995848562693		[learning rate: 8.1427e-05]
	Learning Rate: 8.14272e-05
	LOSS [training: 0.05263995848562693 | validation: 0.056272708853021616]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1409.pth
	Model improved!!!
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05468302766327051		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.05468302766327051 | validation: 0.06292040428520843]
	TIME [epoch: 5.86 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052596940041515675		[learning rate: 8.0852e-05]
	Learning Rate: 8.08523e-05
	LOSS [training: 0.052596940041515675 | validation: 0.06302930163187236]
	TIME [epoch: 5.86 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05240633368422817		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.05240633368422817 | validation: 0.05922822806494719]
	TIME [epoch: 5.85 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05103484636201287		[learning rate: 8.0281e-05]
	Learning Rate: 8.02815e-05
	LOSS [training: 0.05103484636201287 | validation: 0.06377409439230991]
	TIME [epoch: 5.86 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05090040988771533		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.05090040988771533 | validation: 0.06313435446720254]
	TIME [epoch: 5.85 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0547086756237859		[learning rate: 7.9715e-05]
	Learning Rate: 7.97147e-05
	LOSS [training: 0.0547086756237859 | validation: 0.06746781804515617]
	TIME [epoch: 5.86 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052492907101100744		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.052492907101100744 | validation: 0.06317449789388255]
	TIME [epoch: 5.86 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05289771760096065		[learning rate: 7.9152e-05]
	Learning Rate: 7.9152e-05
	LOSS [training: 0.05289771760096065 | validation: 0.05842610607769777]
	TIME [epoch: 5.88 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05093379526410141		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.05093379526410141 | validation: 0.058144336617041825]
	TIME [epoch: 5.88 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0525110323803674		[learning rate: 7.8593e-05]
	Learning Rate: 7.85931e-05
	LOSS [training: 0.0525110323803674 | validation: 0.057647183788793424]
	TIME [epoch: 5.88 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05306315563565895		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.05306315563565895 | validation: 0.0595429177476184]
	TIME [epoch: 5.9 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052097072870298806		[learning rate: 7.8038e-05]
	Learning Rate: 7.80383e-05
	LOSS [training: 0.052097072870298806 | validation: 0.057730678351006995]
	TIME [epoch: 5.89 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05233844870330722		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.05233844870330722 | validation: 0.054504589382313944]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1422.pth
	Model improved!!!
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053318915462277856		[learning rate: 7.7487e-05]
	Learning Rate: 7.74873e-05
	LOSS [training: 0.053318915462277856 | validation: 0.06322762333589224]
	TIME [epoch: 5.91 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05224869029563865		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.05224869029563865 | validation: 0.06024478828220784]
	TIME [epoch: 5.85 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05339122445371127		[learning rate: 7.694e-05]
	Learning Rate: 7.69403e-05
	LOSS [training: 0.05339122445371127 | validation: 0.05710874916073055]
	TIME [epoch: 5.86 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05139168976342745		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.05139168976342745 | validation: 0.061152069829586556]
	TIME [epoch: 5.85 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05208358447806791		[learning rate: 7.6397e-05]
	Learning Rate: 7.63971e-05
	LOSS [training: 0.05208358447806791 | validation: 0.06042965611473005]
	TIME [epoch: 5.85 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05123338997799534		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.05123338997799534 | validation: 0.06089684506656631]
	TIME [epoch: 5.85 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05062395985047585		[learning rate: 7.5858e-05]
	Learning Rate: 7.58578e-05
	LOSS [training: 0.05062395985047585 | validation: 0.06168725541371367]
	TIME [epoch: 5.85 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05111554449524517		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.05111554449524517 | validation: 0.062401751287626456]
	TIME [epoch: 5.86 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0523002048775528		[learning rate: 7.5322e-05]
	Learning Rate: 7.53222e-05
	LOSS [training: 0.0523002048775528 | validation: 0.06113710621076092]
	TIME [epoch: 5.85 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053550447503485706		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.053550447503485706 | validation: 0.05786855426093346]
	TIME [epoch: 5.85 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05126765375645127		[learning rate: 7.479e-05]
	Learning Rate: 7.47905e-05
	LOSS [training: 0.05126765375645127 | validation: 0.06351704449337549]
	TIME [epoch: 5.85 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051463208748382894		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.051463208748382894 | validation: 0.05975147440219221]
	TIME [epoch: 5.84 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0527448873211024		[learning rate: 7.4262e-05]
	Learning Rate: 7.42625e-05
	LOSS [training: 0.0527448873211024 | validation: 0.0602273469948959]
	TIME [epoch: 5.86 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05387667747971758		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.05387667747971758 | validation: 0.05921278992569681]
	TIME [epoch: 5.84 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05224119161524956		[learning rate: 7.3738e-05]
	Learning Rate: 7.37382e-05
	LOSS [training: 0.05224119161524956 | validation: 0.06231630027710764]
	TIME [epoch: 5.85 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05133170650002552		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.05133170650002552 | validation: 0.05862906111725822]
	TIME [epoch: 5.84 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05213007388901243		[learning rate: 7.3218e-05]
	Learning Rate: 7.32176e-05
	LOSS [training: 0.05213007388901243 | validation: 0.0623699857558877]
	TIME [epoch: 5.84 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050657863224370506		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.050657863224370506 | validation: 0.06757370139545885]
	TIME [epoch: 5.85 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04971642449849926		[learning rate: 7.2701e-05]
	Learning Rate: 7.27007e-05
	LOSS [training: 0.04971642449849926 | validation: 0.05594618134238364]
	TIME [epoch: 5.86 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052343698065319695		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.052343698065319695 | validation: 0.06230274064299497]
	TIME [epoch: 5.85 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05096668845662136		[learning rate: 7.2187e-05]
	Learning Rate: 7.21874e-05
	LOSS [training: 0.05096668845662136 | validation: 0.05754267500450355]
	TIME [epoch: 5.85 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05235986758436607		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.05235986758436607 | validation: 0.05660697849175035]
	TIME [epoch: 5.84 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052679341499808054		[learning rate: 7.1678e-05]
	Learning Rate: 7.16778e-05
	LOSS [training: 0.052679341499808054 | validation: 0.05854437409753546]
	TIME [epoch: 5.87 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050721700353256675		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.050721700353256675 | validation: 0.06141158162397584]
	TIME [epoch: 5.85 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050686839331558285		[learning rate: 7.1172e-05]
	Learning Rate: 7.11718e-05
	LOSS [training: 0.050686839331558285 | validation: 0.056279279153216046]
	TIME [epoch: 5.86 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05094693927604263		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.05094693927604263 | validation: 0.05856108874613021]
	TIME [epoch: 5.85 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050934594609056597		[learning rate: 7.0669e-05]
	Learning Rate: 7.06693e-05
	LOSS [training: 0.050934594609056597 | validation: 0.059852078225854366]
	TIME [epoch: 5.85 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05120036227424467		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.05120036227424467 | validation: 0.060997718157213526]
	TIME [epoch: 5.85 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05118976692686245		[learning rate: 7.017e-05]
	Learning Rate: 7.01704e-05
	LOSS [training: 0.05118976692686245 | validation: 0.06634038791790652]
	TIME [epoch: 5.85 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05213492885300333		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.05213492885300333 | validation: 0.058790102617784536]
	TIME [epoch: 5.85 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05068009384065826		[learning rate: 6.9675e-05]
	Learning Rate: 6.9675e-05
	LOSS [training: 0.05068009384065826 | validation: 0.05878014086404685]
	TIME [epoch: 5.85 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05099592364995411		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.05099592364995411 | validation: 0.06405657593244321]
	TIME [epoch: 5.84 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052296979050570215		[learning rate: 6.9183e-05]
	Learning Rate: 6.91831e-05
	LOSS [training: 0.052296979050570215 | validation: 0.057922780569591964]
	TIME [epoch: 5.86 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04887645678490898		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.04887645678490898 | validation: 0.058011582023996935]
	TIME [epoch: 5.84 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05087057354075165		[learning rate: 6.8695e-05]
	Learning Rate: 6.86947e-05
	LOSS [training: 0.05087057354075165 | validation: 0.060415385205556185]
	TIME [epoch: 5.84 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052798452818110936		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.052798452818110936 | validation: 0.06304084846508831]
	TIME [epoch: 5.85 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05122134779227071		[learning rate: 6.821e-05]
	Learning Rate: 6.82097e-05
	LOSS [training: 0.05122134779227071 | validation: 0.06028693469926985]
	TIME [epoch: 5.85 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05128782174741284		[learning rate: 6.7969e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.05128782174741284 | validation: 0.05983422812501939]
	TIME [epoch: 5.86 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05037835720643205		[learning rate: 6.7728e-05]
	Learning Rate: 6.77282e-05
	LOSS [training: 0.05037835720643205 | validation: 0.06274178881024665]
	TIME [epoch: 5.85 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050729924810561704		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.050729924810561704 | validation: 0.06075756755583633]
	TIME [epoch: 5.85 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04952907958793949		[learning rate: 6.725e-05]
	Learning Rate: 6.725e-05
	LOSS [training: 0.04952907958793949 | validation: 0.055083272600611626]
	TIME [epoch: 5.85 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050816934939015235		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.050816934939015235 | validation: 0.05571941131784717]
	TIME [epoch: 5.85 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04901110556823209		[learning rate: 6.6775e-05]
	Learning Rate: 6.67752e-05
	LOSS [training: 0.04901110556823209 | validation: 0.060913648329346495]
	TIME [epoch: 5.85 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051604988460125595		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.051604988460125595 | validation: 0.06305252322845366]
	TIME [epoch: 5.84 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04932679402763066		[learning rate: 6.6304e-05]
	Learning Rate: 6.63038e-05
	LOSS [training: 0.04932679402763066 | validation: 0.06385999366483157]
	TIME [epoch: 5.85 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05299276255971335		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.05299276255971335 | validation: 0.05786381054553559]
	TIME [epoch: 5.85 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05175303878713569		[learning rate: 6.5836e-05]
	Learning Rate: 6.58357e-05
	LOSS [training: 0.05175303878713569 | validation: 0.06215708403752405]
	TIME [epoch: 5.85 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04993996824393765		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.04993996824393765 | validation: 0.061048941458192854]
	TIME [epoch: 5.86 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050738310286798115		[learning rate: 6.5371e-05]
	Learning Rate: 6.53709e-05
	LOSS [training: 0.050738310286798115 | validation: 0.06444223257278513]
	TIME [epoch: 5.85 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05025266346220454		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.05025266346220454 | validation: 0.05924305018348436]
	TIME [epoch: 5.86 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05050641705171194		[learning rate: 6.4909e-05]
	Learning Rate: 6.49094e-05
	LOSS [training: 0.05050641705171194 | validation: 0.06320230707650652]
	TIME [epoch: 5.85 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05154678464115992		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.05154678464115992 | validation: 0.060240271107312594]
	TIME [epoch: 5.85 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05067384779984334		[learning rate: 6.4451e-05]
	Learning Rate: 6.44512e-05
	LOSS [training: 0.05067384779984334 | validation: 0.05678323929551754]
	TIME [epoch: 5.85 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049593210448791426		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.049593210448791426 | validation: 0.06261783955521273]
	TIME [epoch: 5.86 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04917607128612764		[learning rate: 6.3996e-05]
	Learning Rate: 6.39962e-05
	LOSS [training: 0.04917607128612764 | validation: 0.06008496353795737]
	TIME [epoch: 5.86 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04930449412748723		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.04930449412748723 | validation: 0.06077779456459548]
	TIME [epoch: 5.87 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05037622545089203		[learning rate: 6.3544e-05]
	Learning Rate: 6.35444e-05
	LOSS [training: 0.05037622545089203 | validation: 0.06111108332887344]
	TIME [epoch: 5.86 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048521526286599885		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.048521526286599885 | validation: 0.05609655543946804]
	TIME [epoch: 5.85 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05043282249843219		[learning rate: 6.3096e-05]
	Learning Rate: 6.30958e-05
	LOSS [training: 0.05043282249843219 | validation: 0.05652904885119703]
	TIME [epoch: 5.86 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051211743958082555		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.051211743958082555 | validation: 0.0665660673085077]
	TIME [epoch: 5.86 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048729341639695854		[learning rate: 6.265e-05]
	Learning Rate: 6.26503e-05
	LOSS [training: 0.048729341639695854 | validation: 0.05923134809087297]
	TIME [epoch: 5.85 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05213022461716002		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.05213022461716002 | validation: 0.06253642992049929]
	TIME [epoch: 5.86 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04880402004027283		[learning rate: 6.2208e-05]
	Learning Rate: 6.2208e-05
	LOSS [training: 0.04880402004027283 | validation: 0.06444854635146378]
	TIME [epoch: 5.86 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04813501307409096		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.04813501307409096 | validation: 0.06030417490447472]
	TIME [epoch: 5.86 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05100817959385081		[learning rate: 6.1769e-05]
	Learning Rate: 6.17688e-05
	LOSS [training: 0.05100817959385081 | validation: 0.05963769915151808]
	TIME [epoch: 5.85 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04850037125291067		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.04850037125291067 | validation: 0.06384275985696626]
	TIME [epoch: 5.85 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05135528227033296		[learning rate: 6.1333e-05]
	Learning Rate: 6.13327e-05
	LOSS [training: 0.05135528227033296 | validation: 0.0600148607887818]
	TIME [epoch: 5.85 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05089371582423281		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.05089371582423281 | validation: 0.06325635248463744]
	TIME [epoch: 5.85 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05057760950012919		[learning rate: 6.09e-05]
	Learning Rate: 6.08998e-05
	LOSS [training: 0.05057760950012919 | validation: 0.05536471956650373]
	TIME [epoch: 5.86 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04933158946704661		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.04933158946704661 | validation: 0.06343625576412322]
	TIME [epoch: 5.85 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05064118929678363		[learning rate: 6.047e-05]
	Learning Rate: 6.04698e-05
	LOSS [training: 0.05064118929678363 | validation: 0.06166940455377075]
	TIME [epoch: 5.85 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05112349649347536		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.05112349649347536 | validation: 0.05880862536775187]
	TIME [epoch: 5.86 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05086068945031084		[learning rate: 6.0043e-05]
	Learning Rate: 6.00429e-05
	LOSS [training: 0.05086068945031084 | validation: 0.06093516836858303]
	TIME [epoch: 5.86 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05088545454812454		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.05088545454812454 | validation: 0.05985463531353098]
	TIME [epoch: 5.85 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049894251039145666		[learning rate: 5.9619e-05]
	Learning Rate: 5.9619e-05
	LOSS [training: 0.049894251039145666 | validation: 0.06024813210609851]
	TIME [epoch: 5.86 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0502021731982877		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.0502021731982877 | validation: 0.05775850497819556]
	TIME [epoch: 5.85 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0496187796229657		[learning rate: 5.9198e-05]
	Learning Rate: 5.91981e-05
	LOSS [training: 0.0496187796229657 | validation: 0.06277413064858096]
	TIME [epoch: 5.86 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05270236967805543		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.05270236967805543 | validation: 0.05097795551079978]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1500.pth
	Model improved!!!
EPOCH 1501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0497856817971125		[learning rate: 5.878e-05]
	Learning Rate: 5.87802e-05
	LOSS [training: 0.0497856817971125 | validation: 0.05698527611140184]
	TIME [epoch: 5.85 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049522730611696264		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.049522730611696264 | validation: 0.057646196043183696]
	TIME [epoch: 5.85 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04922483693164278		[learning rate: 5.8365e-05]
	Learning Rate: 5.83652e-05
	LOSS [training: 0.04922483693164278 | validation: 0.05681195571635052]
	TIME [epoch: 5.86 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050522193846827025		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.050522193846827025 | validation: 0.05966858093717771]
	TIME [epoch: 5.85 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04887466201009163		[learning rate: 5.7953e-05]
	Learning Rate: 5.79531e-05
	LOSS [training: 0.04887466201009163 | validation: 0.05762752286996253]
	TIME [epoch: 5.85 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05008787601649968		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.05008787601649968 | validation: 0.0651573879115001]
	TIME [epoch: 5.85 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050487476917822446		[learning rate: 5.7544e-05]
	Learning Rate: 5.7544e-05
	LOSS [training: 0.050487476917822446 | validation: 0.06188218465032134]
	TIME [epoch: 5.85 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050431873848795446		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.050431873848795446 | validation: 0.05914238890231547]
	TIME [epoch: 5.84 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04914870552475762		[learning rate: 5.7138e-05]
	Learning Rate: 5.71378e-05
	LOSS [training: 0.04914870552475762 | validation: 0.058208408906446754]
	TIME [epoch: 5.85 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05042297462614811		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.05042297462614811 | validation: 0.058130701009555655]
	TIME [epoch: 5.85 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049321303825973536		[learning rate: 5.6734e-05]
	Learning Rate: 5.67344e-05
	LOSS [training: 0.049321303825973536 | validation: 0.057363075287330034]
	TIME [epoch: 5.86 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050011712491036864		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.050011712491036864 | validation: 0.056386350139269736]
	TIME [epoch: 5.85 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05186480487499365		[learning rate: 5.6334e-05]
	Learning Rate: 5.63338e-05
	LOSS [training: 0.05186480487499365 | validation: 0.06317171412903529]
	TIME [epoch: 5.85 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0490926906905874		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.0490926906905874 | validation: 0.0569934210342771]
	TIME [epoch: 5.85 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05295091318327311		[learning rate: 5.5936e-05]
	Learning Rate: 5.59361e-05
	LOSS [training: 0.05295091318327311 | validation: 0.05528243049804532]
	TIME [epoch: 5.85 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04978763257701102		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.04978763257701102 | validation: 0.06086846067400597]
	TIME [epoch: 5.85 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048784252607566136		[learning rate: 5.5541e-05]
	Learning Rate: 5.55412e-05
	LOSS [training: 0.048784252607566136 | validation: 0.05496454616679221]
	TIME [epoch: 5.85 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04878862060711047		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.04878862060711047 | validation: 0.06387923590874137]
	TIME [epoch: 5.85 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04996817876513421		[learning rate: 5.5149e-05]
	Learning Rate: 5.51491e-05
	LOSS [training: 0.04996817876513421 | validation: 0.05957348894838554]
	TIME [epoch: 5.85 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05058829491820277		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.05058829491820277 | validation: 0.056534637845146875]
	TIME [epoch: 5.85 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04938125933970505		[learning rate: 5.476e-05]
	Learning Rate: 5.47598e-05
	LOSS [training: 0.04938125933970505 | validation: 0.05735195641550711]
	TIME [epoch: 5.86 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0497001069058268		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.0497001069058268 | validation: 0.06187196421050063]
	TIME [epoch: 5.85 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04984940845302205		[learning rate: 5.4373e-05]
	Learning Rate: 5.43732e-05
	LOSS [training: 0.04984940845302205 | validation: 0.055772441026447156]
	TIME [epoch: 5.86 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05034053018660478		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.05034053018660478 | validation: 0.05583418631012922]
	TIME [epoch: 5.85 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0482857000949512		[learning rate: 5.3989e-05]
	Learning Rate: 5.39893e-05
	LOSS [training: 0.0482857000949512 | validation: 0.060866147949680184]
	TIME [epoch: 5.88 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050687663381400905		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.050687663381400905 | validation: 0.06313360524907102]
	TIME [epoch: 5.85 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05028727903466946		[learning rate: 5.3608e-05]
	Learning Rate: 5.36082e-05
	LOSS [training: 0.05028727903466946 | validation: 0.056714748727461975]
	TIME [epoch: 5.85 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04896507602531597		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.04896507602531597 | validation: 0.05844810385313355]
	TIME [epoch: 5.85 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04911199159149836		[learning rate: 5.323e-05]
	Learning Rate: 5.32297e-05
	LOSS [training: 0.04911199159149836 | validation: 0.05638561206776717]
	TIME [epoch: 5.86 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04929812954827868		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.04929812954827868 | validation: 0.05765973655499407]
	TIME [epoch: 5.85 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0524825811196357		[learning rate: 5.2854e-05]
	Learning Rate: 5.28539e-05
	LOSS [training: 0.0524825811196357 | validation: 0.059005797622687074]
	TIME [epoch: 5.86 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04780163077782612		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.04780163077782612 | validation: 0.06877513689754759]
	TIME [epoch: 5.85 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04832328408338287		[learning rate: 5.2481e-05]
	Learning Rate: 5.24807e-05
	LOSS [training: 0.04832328408338287 | validation: 0.05866930924465537]
	TIME [epoch: 5.86 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04699840165300888		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.04699840165300888 | validation: 0.05778769277173044]
	TIME [epoch: 5.84 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04937527577484626		[learning rate: 5.211e-05]
	Learning Rate: 5.21103e-05
	LOSS [training: 0.04937527577484626 | validation: 0.056656098122735844]
	TIME [epoch: 5.85 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049777307621076516		[learning rate: 5.1926e-05]
	Learning Rate: 5.1926e-05
	LOSS [training: 0.049777307621076516 | validation: 0.05537462641515128]
	TIME [epoch: 5.85 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05228186052674657		[learning rate: 5.1742e-05]
	Learning Rate: 5.17424e-05
	LOSS [training: 0.05228186052674657 | validation: 0.059708237092531716]
	TIME [epoch: 5.85 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05029698890991459		[learning rate: 5.1559e-05]
	Learning Rate: 5.15594e-05
	LOSS [training: 0.05029698890991459 | validation: 0.056596402227954735]
	TIME [epoch: 5.84 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048951208189619184		[learning rate: 5.1377e-05]
	Learning Rate: 5.13771e-05
	LOSS [training: 0.048951208189619184 | validation: 0.05814984071806983]
	TIME [epoch: 5.85 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049653876415602004		[learning rate: 5.1195e-05]
	Learning Rate: 5.11954e-05
	LOSS [training: 0.049653876415602004 | validation: 0.060307052075310744]
	TIME [epoch: 5.85 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049842064694780704		[learning rate: 5.1014e-05]
	Learning Rate: 5.10144e-05
	LOSS [training: 0.049842064694780704 | validation: 0.05891449094938276]
	TIME [epoch: 5.86 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049062088579678934		[learning rate: 5.0834e-05]
	Learning Rate: 5.0834e-05
	LOSS [training: 0.049062088579678934 | validation: 0.05419179615740471]
	TIME [epoch: 5.85 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04978526655190587		[learning rate: 5.0654e-05]
	Learning Rate: 5.06542e-05
	LOSS [training: 0.04978526655190587 | validation: 0.06192165219215136]
	TIME [epoch: 5.86 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049642208434801986		[learning rate: 5.0475e-05]
	Learning Rate: 5.04751e-05
	LOSS [training: 0.049642208434801986 | validation: 0.05525698217297634]
	TIME [epoch: 5.85 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04947725163003055		[learning rate: 5.0297e-05]
	Learning Rate: 5.02966e-05
	LOSS [training: 0.04947725163003055 | validation: 0.05503709514940907]
	TIME [epoch: 5.85 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05019101736728459		[learning rate: 5.0119e-05]
	Learning Rate: 5.01187e-05
	LOSS [training: 0.05019101736728459 | validation: 0.058889932197002094]
	TIME [epoch: 5.85 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04879420575634473		[learning rate: 4.9942e-05]
	Learning Rate: 4.99415e-05
	LOSS [training: 0.04879420575634473 | validation: 0.054421176642858754]
	TIME [epoch: 5.85 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04988583871826979		[learning rate: 4.9765e-05]
	Learning Rate: 4.97649e-05
	LOSS [training: 0.04988583871826979 | validation: 0.0617291465767377]
	TIME [epoch: 5.84 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04930224241897809		[learning rate: 4.9589e-05]
	Learning Rate: 4.95889e-05
	LOSS [training: 0.04930224241897809 | validation: 0.056608536116563996]
	TIME [epoch: 5.87 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050142882307781224		[learning rate: 4.9414e-05]
	Learning Rate: 4.94136e-05
	LOSS [training: 0.050142882307781224 | validation: 0.05978255778181486]
	TIME [epoch: 5.84 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049588355583298		[learning rate: 4.9239e-05]
	Learning Rate: 4.92388e-05
	LOSS [training: 0.049588355583298 | validation: 0.05551836671534327]
	TIME [epoch: 5.85 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04932797825704624		[learning rate: 4.9065e-05]
	Learning Rate: 4.90647e-05
	LOSS [training: 0.04932797825704624 | validation: 0.05591978826906501]
	TIME [epoch: 5.84 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0479793061343291		[learning rate: 4.8891e-05]
	Learning Rate: 4.88912e-05
	LOSS [training: 0.0479793061343291 | validation: 0.06026422941168017]
	TIME [epoch: 5.85 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04805454322172369		[learning rate: 4.8718e-05]
	Learning Rate: 4.87183e-05
	LOSS [training: 0.04805454322172369 | validation: 0.06113321875686725]
	TIME [epoch: 5.85 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0472313139411431		[learning rate: 4.8546e-05]
	Learning Rate: 4.85461e-05
	LOSS [training: 0.0472313139411431 | validation: 0.059194018488508485]
	TIME [epoch: 5.85 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047911835738013456		[learning rate: 4.8374e-05]
	Learning Rate: 4.83744e-05
	LOSS [training: 0.047911835738013456 | validation: 0.05870464699828566]
	TIME [epoch: 5.85 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04857361397535077		[learning rate: 4.8203e-05]
	Learning Rate: 4.82033e-05
	LOSS [training: 0.04857361397535077 | validation: 0.05563069934367684]
	TIME [epoch: 5.86 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048807661507567365		[learning rate: 4.8033e-05]
	Learning Rate: 4.80329e-05
	LOSS [training: 0.048807661507567365 | validation: 0.057272133307786834]
	TIME [epoch: 5.85 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048952647037924145		[learning rate: 4.7863e-05]
	Learning Rate: 4.7863e-05
	LOSS [training: 0.048952647037924145 | validation: 0.05916239338193834]
	TIME [epoch: 5.86 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04986483830757393		[learning rate: 4.7694e-05]
	Learning Rate: 4.76938e-05
	LOSS [training: 0.04986483830757393 | validation: 0.06409164372081937]
	TIME [epoch: 5.85 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04815950205363057		[learning rate: 4.7525e-05]
	Learning Rate: 4.75251e-05
	LOSS [training: 0.04815950205363057 | validation: 0.058596896371024644]
	TIME [epoch: 5.85 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048632458943527135		[learning rate: 4.7357e-05]
	Learning Rate: 4.73571e-05
	LOSS [training: 0.048632458943527135 | validation: 0.060922144437658754]
	TIME [epoch: 5.85 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048444942103821304		[learning rate: 4.719e-05]
	Learning Rate: 4.71896e-05
	LOSS [training: 0.048444942103821304 | validation: 0.0516173045095612]
	TIME [epoch: 5.85 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04614346986969135		[learning rate: 4.7023e-05]
	Learning Rate: 4.70227e-05
	LOSS [training: 0.04614346986969135 | validation: 0.051904189162736016]
	TIME [epoch: 5.85 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05003479430542684		[learning rate: 4.6856e-05]
	Learning Rate: 4.68564e-05
	LOSS [training: 0.05003479430542684 | validation: 0.0623462630554149]
	TIME [epoch: 5.85 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04749173211435899		[learning rate: 4.6691e-05]
	Learning Rate: 4.66907e-05
	LOSS [training: 0.04749173211435899 | validation: 0.06088331696404928]
	TIME [epoch: 5.85 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048884594879624245		[learning rate: 4.6526e-05]
	Learning Rate: 4.65257e-05
	LOSS [training: 0.048884594879624245 | validation: 0.049698921396167696]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1567.pth
	Model improved!!!
EPOCH 1568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049465744039396124		[learning rate: 4.6361e-05]
	Learning Rate: 4.63611e-05
	LOSS [training: 0.049465744039396124 | validation: 0.05253285275692196]
	TIME [epoch: 5.85 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04718970471229639		[learning rate: 4.6197e-05]
	Learning Rate: 4.61972e-05
	LOSS [training: 0.04718970471229639 | validation: 0.059261747799567555]
	TIME [epoch: 5.86 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051602031010582146		[learning rate: 4.6034e-05]
	Learning Rate: 4.60338e-05
	LOSS [training: 0.051602031010582146 | validation: 0.05752891709742097]
	TIME [epoch: 5.84 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04886433680851437		[learning rate: 4.5871e-05]
	Learning Rate: 4.5871e-05
	LOSS [training: 0.04886433680851437 | validation: 0.05574060448233749]
	TIME [epoch: 5.85 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04866760545957179		[learning rate: 4.5709e-05]
	Learning Rate: 4.57088e-05
	LOSS [training: 0.04866760545957179 | validation: 0.05498195709093863]
	TIME [epoch: 5.85 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047548038117110177		[learning rate: 4.5547e-05]
	Learning Rate: 4.55472e-05
	LOSS [training: 0.047548038117110177 | validation: 0.058689496283853185]
	TIME [epoch: 5.84 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049138833142081284		[learning rate: 4.5386e-05]
	Learning Rate: 4.53861e-05
	LOSS [training: 0.049138833142081284 | validation: 0.055578284317787364]
	TIME [epoch: 5.85 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04827393525043215		[learning rate: 4.5226e-05]
	Learning Rate: 4.52256e-05
	LOSS [training: 0.04827393525043215 | validation: 0.057811120609127103]
	TIME [epoch: 5.84 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04777739797083221		[learning rate: 4.5066e-05]
	Learning Rate: 4.50657e-05
	LOSS [training: 0.04777739797083221 | validation: 0.0609595801080965]
	TIME [epoch: 5.85 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04867312543543353		[learning rate: 4.4906e-05]
	Learning Rate: 4.49064e-05
	LOSS [training: 0.04867312543543353 | validation: 0.0631317832376632]
	TIME [epoch: 5.85 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047896378450838724		[learning rate: 4.4748e-05]
	Learning Rate: 4.47476e-05
	LOSS [training: 0.047896378450838724 | validation: 0.05346244765542742]
	TIME [epoch: 5.85 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049284327542762		[learning rate: 4.4589e-05]
	Learning Rate: 4.45893e-05
	LOSS [training: 0.049284327542762 | validation: 0.05537572204302698]
	TIME [epoch: 5.85 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049781875300932976		[learning rate: 4.4432e-05]
	Learning Rate: 4.44316e-05
	LOSS [training: 0.049781875300932976 | validation: 0.06129653260070803]
	TIME [epoch: 5.85 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04800106498767713		[learning rate: 4.4275e-05]
	Learning Rate: 4.42745e-05
	LOSS [training: 0.04800106498767713 | validation: 0.06187860974085982]
	TIME [epoch: 5.84 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04765541952509494		[learning rate: 4.4118e-05]
	Learning Rate: 4.4118e-05
	LOSS [training: 0.04765541952509494 | validation: 0.05825742817123641]
	TIME [epoch: 5.86 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04805775698085966		[learning rate: 4.3962e-05]
	Learning Rate: 4.3962e-05
	LOSS [training: 0.04805775698085966 | validation: 0.06551255817896674]
	TIME [epoch: 5.86 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048441624035232236		[learning rate: 4.3807e-05]
	Learning Rate: 4.38065e-05
	LOSS [training: 0.048441624035232236 | validation: 0.05974536137077929]
	TIME [epoch: 5.85 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047761328360199154		[learning rate: 4.3652e-05]
	Learning Rate: 4.36516e-05
	LOSS [training: 0.047761328360199154 | validation: 0.05826567042175571]
	TIME [epoch: 5.86 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04952507557970277		[learning rate: 4.3497e-05]
	Learning Rate: 4.34972e-05
	LOSS [training: 0.04952507557970277 | validation: 0.05838785129862596]
	TIME [epoch: 5.85 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048459969454947655		[learning rate: 4.3343e-05]
	Learning Rate: 4.33434e-05
	LOSS [training: 0.048459969454947655 | validation: 0.0580326135248092]
	TIME [epoch: 5.85 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04996931196814203		[learning rate: 4.319e-05]
	Learning Rate: 4.31902e-05
	LOSS [training: 0.04996931196814203 | validation: 0.053479170416184454]
	TIME [epoch: 5.85 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047770402355633		[learning rate: 4.3037e-05]
	Learning Rate: 4.30374e-05
	LOSS [training: 0.047770402355633 | validation: 0.058383175335250496]
	TIME [epoch: 5.85 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04923129859950179		[learning rate: 4.2885e-05]
	Learning Rate: 4.28852e-05
	LOSS [training: 0.04923129859950179 | validation: 0.06071684416965764]
	TIME [epoch: 5.85 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049020421130350975		[learning rate: 4.2734e-05]
	Learning Rate: 4.27336e-05
	LOSS [training: 0.049020421130350975 | validation: 0.054417386853596365]
	TIME [epoch: 5.85 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04835296336947784		[learning rate: 4.2582e-05]
	Learning Rate: 4.25825e-05
	LOSS [training: 0.04835296336947784 | validation: 0.058735031795012375]
	TIME [epoch: 5.85 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04687106062929539		[learning rate: 4.2432e-05]
	Learning Rate: 4.24319e-05
	LOSS [training: 0.04687106062929539 | validation: 0.05686533139889182]
	TIME [epoch: 5.86 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0475566343565027		[learning rate: 4.2282e-05]
	Learning Rate: 4.22819e-05
	LOSS [training: 0.0475566343565027 | validation: 0.05746678642432808]
	TIME [epoch: 5.85 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048175040950048		[learning rate: 4.2132e-05]
	Learning Rate: 4.21323e-05
	LOSS [training: 0.048175040950048 | validation: 0.05576478804706785]
	TIME [epoch: 5.85 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045510637737012194		[learning rate: 4.1983e-05]
	Learning Rate: 4.19833e-05
	LOSS [training: 0.045510637737012194 | validation: 0.060139427750150666]
	TIME [epoch: 5.85 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0477029876346354		[learning rate: 4.1835e-05]
	Learning Rate: 4.18349e-05
	LOSS [training: 0.0477029876346354 | validation: 0.055929250664753016]
	TIME [epoch: 5.86 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049456814395401616		[learning rate: 4.1687e-05]
	Learning Rate: 4.1687e-05
	LOSS [training: 0.049456814395401616 | validation: 0.0542371889362426]
	TIME [epoch: 5.85 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04869421974756028		[learning rate: 4.154e-05]
	Learning Rate: 4.15395e-05
	LOSS [training: 0.04869421974756028 | validation: 0.05421700903274393]
	TIME [epoch: 5.85 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04972196935836769		[learning rate: 4.1393e-05]
	Learning Rate: 4.13926e-05
	LOSS [training: 0.04972196935836769 | validation: 0.05517592425207851]
	TIME [epoch: 5.85 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047567196221453154		[learning rate: 4.1246e-05]
	Learning Rate: 4.12463e-05
	LOSS [training: 0.047567196221453154 | validation: 0.049660432667998106]
	TIME [epoch: 5.85 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1601.pth
	Model improved!!!
EPOCH 1602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046183419117183044		[learning rate: 4.11e-05]
	Learning Rate: 4.11004e-05
	LOSS [training: 0.046183419117183044 | validation: 0.05402254004603773]
	TIME [epoch: 5.86 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04847729927442188		[learning rate: 4.0955e-05]
	Learning Rate: 4.09551e-05
	LOSS [training: 0.04847729927442188 | validation: 0.05966949379097617]
	TIME [epoch: 5.86 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048669753028634286		[learning rate: 4.081e-05]
	Learning Rate: 4.08103e-05
	LOSS [training: 0.048669753028634286 | validation: 0.05229727623901642]
	TIME [epoch: 5.86 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047242120305406914		[learning rate: 4.0666e-05]
	Learning Rate: 4.06659e-05
	LOSS [training: 0.047242120305406914 | validation: 0.05434485741588166]
	TIME [epoch: 5.85 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04999450524434662		[learning rate: 4.0522e-05]
	Learning Rate: 4.05221e-05
	LOSS [training: 0.04999450524434662 | validation: 0.05552825687295752]
	TIME [epoch: 5.84 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04798995318291166		[learning rate: 4.0379e-05]
	Learning Rate: 4.03788e-05
	LOSS [training: 0.04798995318291166 | validation: 0.057039361532059255]
	TIME [epoch: 5.85 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04982981869947125		[learning rate: 4.0236e-05]
	Learning Rate: 4.02361e-05
	LOSS [training: 0.04982981869947125 | validation: 0.05517594883588106]
	TIME [epoch: 5.85 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04863202511945785		[learning rate: 4.0094e-05]
	Learning Rate: 4.00938e-05
	LOSS [training: 0.04863202511945785 | validation: 0.05867592623488374]
	TIME [epoch: 5.86 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047665155366488215		[learning rate: 3.9952e-05]
	Learning Rate: 3.9952e-05
	LOSS [training: 0.047665155366488215 | validation: 0.04997241207911612]
	TIME [epoch: 5.86 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047619890199625876		[learning rate: 3.9811e-05]
	Learning Rate: 3.98107e-05
	LOSS [training: 0.047619890199625876 | validation: 0.05321710135842675]
	TIME [epoch: 5.85 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047021463837524514		[learning rate: 3.967e-05]
	Learning Rate: 3.967e-05
	LOSS [training: 0.047021463837524514 | validation: 0.05716489746584985]
	TIME [epoch: 5.86 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048421981087236006		[learning rate: 3.953e-05]
	Learning Rate: 3.95297e-05
	LOSS [training: 0.048421981087236006 | validation: 0.05675355621174565]
	TIME [epoch: 5.85 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047112066171646386		[learning rate: 3.939e-05]
	Learning Rate: 3.93899e-05
	LOSS [training: 0.047112066171646386 | validation: 0.05426400790298106]
	TIME [epoch: 5.86 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048090420191291346		[learning rate: 3.9251e-05]
	Learning Rate: 3.92506e-05
	LOSS [training: 0.048090420191291346 | validation: 0.05373816081489413]
	TIME [epoch: 5.85 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04798930024269881		[learning rate: 3.9112e-05]
	Learning Rate: 3.91118e-05
	LOSS [training: 0.04798930024269881 | validation: 0.054140499605865144]
	TIME [epoch: 5.85 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04913725911233783		[learning rate: 3.8973e-05]
	Learning Rate: 3.89735e-05
	LOSS [training: 0.04913725911233783 | validation: 0.050200900606362536]
	TIME [epoch: 5.85 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04817218137540852		[learning rate: 3.8836e-05]
	Learning Rate: 3.88357e-05
	LOSS [training: 0.04817218137540852 | validation: 0.06053126453083969]
	TIME [epoch: 5.85 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049714218286215285		[learning rate: 3.8698e-05]
	Learning Rate: 3.86983e-05
	LOSS [training: 0.049714218286215285 | validation: 0.06456202070732293]
	TIME [epoch: 5.85 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049194101556883627		[learning rate: 3.8561e-05]
	Learning Rate: 3.85615e-05
	LOSS [training: 0.049194101556883627 | validation: 0.051052528248872]
	TIME [epoch: 5.85 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04696411338019123		[learning rate: 3.8425e-05]
	Learning Rate: 3.84251e-05
	LOSS [training: 0.04696411338019123 | validation: 0.05229402932165611]
	TIME [epoch: 5.85 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046590118621187976		[learning rate: 3.8289e-05]
	Learning Rate: 3.82893e-05
	LOSS [training: 0.046590118621187976 | validation: 0.06067292370850402]
	TIME [epoch: 5.85 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047326604785484165		[learning rate: 3.8154e-05]
	Learning Rate: 3.81539e-05
	LOSS [training: 0.047326604785484165 | validation: 0.053777888386000054]
	TIME [epoch: 5.84 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04835222488635465		[learning rate: 3.8019e-05]
	Learning Rate: 3.8019e-05
	LOSS [training: 0.04835222488635465 | validation: 0.05515235387937171]
	TIME [epoch: 5.84 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046798386514762404		[learning rate: 3.7885e-05]
	Learning Rate: 3.78845e-05
	LOSS [training: 0.046798386514762404 | validation: 0.05559577396458623]
	TIME [epoch: 5.85 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046867611917810845		[learning rate: 3.7751e-05]
	Learning Rate: 3.77505e-05
	LOSS [training: 0.046867611917810845 | validation: 0.059123570103011516]
	TIME [epoch: 5.85 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04753872378174283		[learning rate: 3.7617e-05]
	Learning Rate: 3.7617e-05
	LOSS [training: 0.04753872378174283 | validation: 0.054754809427993816]
	TIME [epoch: 5.85 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046813047997164715		[learning rate: 3.7484e-05]
	Learning Rate: 3.7484e-05
	LOSS [training: 0.046813047997164715 | validation: 0.05655002739065414]
	TIME [epoch: 5.85 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04952647379870878		[learning rate: 3.7351e-05]
	Learning Rate: 3.73515e-05
	LOSS [training: 0.04952647379870878 | validation: 0.056090602534697956]
	TIME [epoch: 5.85 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04872642889002469		[learning rate: 3.7219e-05]
	Learning Rate: 3.72194e-05
	LOSS [training: 0.04872642889002469 | validation: 0.06030242486268681]
	TIME [epoch: 5.85 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04793833337413922		[learning rate: 3.7088e-05]
	Learning Rate: 3.70878e-05
	LOSS [training: 0.04793833337413922 | validation: 0.053626023848724835]
	TIME [epoch: 5.85 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04647389188317739		[learning rate: 3.6957e-05]
	Learning Rate: 3.69566e-05
	LOSS [training: 0.04647389188317739 | validation: 0.05779388976105328]
	TIME [epoch: 5.85 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04848590469098749		[learning rate: 3.6826e-05]
	Learning Rate: 3.68259e-05
	LOSS [training: 0.04848590469098749 | validation: 0.05672728817279023]
	TIME [epoch: 5.85 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04667952161193385		[learning rate: 3.6696e-05]
	Learning Rate: 3.66957e-05
	LOSS [training: 0.04667952161193385 | validation: 0.05616622889321576]
	TIME [epoch: 5.86 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047267451109203086		[learning rate: 3.6566e-05]
	Learning Rate: 3.6566e-05
	LOSS [training: 0.047267451109203086 | validation: 0.05642574031767072]
	TIME [epoch: 5.85 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04755875201573704		[learning rate: 3.6437e-05]
	Learning Rate: 3.64367e-05
	LOSS [training: 0.04755875201573704 | validation: 0.05897694848160315]
	TIME [epoch: 5.86 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04831813916656592		[learning rate: 3.6308e-05]
	Learning Rate: 3.63078e-05
	LOSS [training: 0.04831813916656592 | validation: 0.053542641982654585]
	TIME [epoch: 5.85 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048787235707592774		[learning rate: 3.6179e-05]
	Learning Rate: 3.61794e-05
	LOSS [training: 0.048787235707592774 | validation: 0.0563721578959452]
	TIME [epoch: 5.85 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046130662099275986		[learning rate: 3.6051e-05]
	Learning Rate: 3.60515e-05
	LOSS [training: 0.046130662099275986 | validation: 0.053817829877375614]
	TIME [epoch: 5.85 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0474787216918914		[learning rate: 3.5924e-05]
	Learning Rate: 3.5924e-05
	LOSS [training: 0.0474787216918914 | validation: 0.05400905338860898]
	TIME [epoch: 5.85 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04781468112512004		[learning rate: 3.5797e-05]
	Learning Rate: 3.5797e-05
	LOSS [training: 0.04781468112512004 | validation: 0.06120596119590121]
	TIME [epoch: 5.86 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046823756233471736		[learning rate: 3.567e-05]
	Learning Rate: 3.56704e-05
	LOSS [training: 0.046823756233471736 | validation: 0.05743414017725852]
	TIME [epoch: 5.85 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04945607465694518		[learning rate: 3.5544e-05]
	Learning Rate: 3.55442e-05
	LOSS [training: 0.04945607465694518 | validation: 0.053783840732895466]
	TIME [epoch: 5.85 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047369406463201445		[learning rate: 3.5419e-05]
	Learning Rate: 3.54186e-05
	LOSS [training: 0.047369406463201445 | validation: 0.049472697883338426]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1644.pth
	Model improved!!!
EPOCH 1645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046447714981013334		[learning rate: 3.5293e-05]
	Learning Rate: 3.52933e-05
	LOSS [training: 0.046447714981013334 | validation: 0.05300813055776174]
	TIME [epoch: 5.86 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04773571365305339		[learning rate: 3.5169e-05]
	Learning Rate: 3.51685e-05
	LOSS [training: 0.04773571365305339 | validation: 0.05548332935011399]
	TIME [epoch: 5.85 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04835195490693914		[learning rate: 3.5044e-05]
	Learning Rate: 3.50441e-05
	LOSS [training: 0.04835195490693914 | validation: 0.05295912667556324]
	TIME [epoch: 5.86 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046393889819385754		[learning rate: 3.492e-05]
	Learning Rate: 3.49202e-05
	LOSS [training: 0.046393889819385754 | validation: 0.051012919384601924]
	TIME [epoch: 5.85 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04564169378446385		[learning rate: 3.4797e-05]
	Learning Rate: 3.47967e-05
	LOSS [training: 0.04564169378446385 | validation: 0.05317673379073687]
	TIME [epoch: 5.85 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049014095251080726		[learning rate: 3.4674e-05]
	Learning Rate: 3.46737e-05
	LOSS [training: 0.049014095251080726 | validation: 0.057453464591461334]
	TIME [epoch: 5.85 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046679086610223186		[learning rate: 3.4551e-05]
	Learning Rate: 3.45511e-05
	LOSS [training: 0.046679086610223186 | validation: 0.055516907347040505]
	TIME [epoch: 5.85 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04800087951177904		[learning rate: 3.4429e-05]
	Learning Rate: 3.44289e-05
	LOSS [training: 0.04800087951177904 | validation: 0.0476155490694443]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1652.pth
	Model improved!!!
EPOCH 1653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047254117922630796		[learning rate: 3.4307e-05]
	Learning Rate: 3.43072e-05
	LOSS [training: 0.047254117922630796 | validation: 0.05528527490470825]
	TIME [epoch: 5.85 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047421694772510285		[learning rate: 3.4186e-05]
	Learning Rate: 3.41858e-05
	LOSS [training: 0.047421694772510285 | validation: 0.05467600755091573]
	TIME [epoch: 5.85 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04813566610677372		[learning rate: 3.4065e-05]
	Learning Rate: 3.4065e-05
	LOSS [training: 0.04813566610677372 | validation: 0.05863661947227117]
	TIME [epoch: 5.86 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04537988542485038		[learning rate: 3.3944e-05]
	Learning Rate: 3.39445e-05
	LOSS [training: 0.04537988542485038 | validation: 0.05200053897920899]
	TIME [epoch: 5.86 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04973828744804941		[learning rate: 3.3824e-05]
	Learning Rate: 3.38245e-05
	LOSS [training: 0.04973828744804941 | validation: 0.05539470969315764]
	TIME [epoch: 5.86 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04840907250875935		[learning rate: 3.3705e-05]
	Learning Rate: 3.37049e-05
	LOSS [training: 0.04840907250875935 | validation: 0.05201482068219291]
	TIME [epoch: 5.85 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047812204536754485		[learning rate: 3.3586e-05]
	Learning Rate: 3.35857e-05
	LOSS [training: 0.047812204536754485 | validation: 0.05793171021062865]
	TIME [epoch: 5.87 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049158907839630415		[learning rate: 3.3467e-05]
	Learning Rate: 3.34669e-05
	LOSS [training: 0.049158907839630415 | validation: 0.05581316049499303]
	TIME [epoch: 5.86 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04691687186407638		[learning rate: 3.3349e-05]
	Learning Rate: 3.33486e-05
	LOSS [training: 0.04691687186407638 | validation: 0.054240759308758116]
	TIME [epoch: 5.86 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04630615222196677		[learning rate: 3.3231e-05]
	Learning Rate: 3.32306e-05
	LOSS [training: 0.04630615222196677 | validation: 0.054843266114251416]
	TIME [epoch: 5.85 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046883252457634854		[learning rate: 3.3113e-05]
	Learning Rate: 3.31131e-05
	LOSS [training: 0.046883252457634854 | validation: 0.052093123116653955]
	TIME [epoch: 5.86 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04700575808310303		[learning rate: 3.2996e-05]
	Learning Rate: 3.2996e-05
	LOSS [training: 0.04700575808310303 | validation: 0.054716615463414824]
	TIME [epoch: 5.85 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04751280901943118		[learning rate: 3.2879e-05]
	Learning Rate: 3.28794e-05
	LOSS [training: 0.04751280901943118 | validation: 0.055408372335613625]
	TIME [epoch: 5.85 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047357910032474124		[learning rate: 3.2763e-05]
	Learning Rate: 3.27631e-05
	LOSS [training: 0.047357910032474124 | validation: 0.05393424712923892]
	TIME [epoch: 5.85 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045874161036173505		[learning rate: 3.2647e-05]
	Learning Rate: 3.26472e-05
	LOSS [training: 0.045874161036173505 | validation: 0.057245841012934166]
	TIME [epoch: 5.85 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04658682798908471		[learning rate: 3.2532e-05]
	Learning Rate: 3.25318e-05
	LOSS [training: 0.04658682798908471 | validation: 0.05878823318171326]
	TIME [epoch: 5.85 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045779877304069246		[learning rate: 3.2417e-05]
	Learning Rate: 3.24167e-05
	LOSS [training: 0.045779877304069246 | validation: 0.054871658336020646]
	TIME [epoch: 5.86 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045269440789566746		[learning rate: 3.2302e-05]
	Learning Rate: 3.23021e-05
	LOSS [training: 0.045269440789566746 | validation: 0.05805854749008629]
	TIME [epoch: 5.86 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0456955331795146		[learning rate: 3.2188e-05]
	Learning Rate: 3.21879e-05
	LOSS [training: 0.0456955331795146 | validation: 0.05563598926586985]
	TIME [epoch: 5.86 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04731938001250909		[learning rate: 3.2074e-05]
	Learning Rate: 3.20741e-05
	LOSS [training: 0.04731938001250909 | validation: 0.05513962842473061]
	TIME [epoch: 5.85 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045845978154964925		[learning rate: 3.1961e-05]
	Learning Rate: 3.19606e-05
	LOSS [training: 0.045845978154964925 | validation: 0.052438323987420966]
	TIME [epoch: 5.86 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047786814648123795		[learning rate: 3.1848e-05]
	Learning Rate: 3.18476e-05
	LOSS [training: 0.047786814648123795 | validation: 0.05831723578660963]
	TIME [epoch: 5.85 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04598356358804417		[learning rate: 3.1735e-05]
	Learning Rate: 3.1735e-05
	LOSS [training: 0.04598356358804417 | validation: 0.05516859615878172]
	TIME [epoch: 5.86 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0481408187292472		[learning rate: 3.1623e-05]
	Learning Rate: 3.16228e-05
	LOSS [training: 0.0481408187292472 | validation: 0.052232456125762605]
	TIME [epoch: 5.85 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04597416296683417		[learning rate: 3.1511e-05]
	Learning Rate: 3.1511e-05
	LOSS [training: 0.04597416296683417 | validation: 0.05751800168321102]
	TIME [epoch: 5.85 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04660817652619939		[learning rate: 3.14e-05]
	Learning Rate: 3.13995e-05
	LOSS [training: 0.04660817652619939 | validation: 0.05646875534250676]
	TIME [epoch: 5.86 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04610176926470397		[learning rate: 3.1288e-05]
	Learning Rate: 3.12885e-05
	LOSS [training: 0.04610176926470397 | validation: 0.05449210269903333]
	TIME [epoch: 5.85 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04733878130126577		[learning rate: 3.1178e-05]
	Learning Rate: 3.11779e-05
	LOSS [training: 0.04733878130126577 | validation: 0.054322374865781446]
	TIME [epoch: 5.86 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045138492659773474		[learning rate: 3.1068e-05]
	Learning Rate: 3.10676e-05
	LOSS [training: 0.045138492659773474 | validation: 0.055101257030702566]
	TIME [epoch: 5.86 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046929017566124685		[learning rate: 3.0958e-05]
	Learning Rate: 3.09577e-05
	LOSS [training: 0.046929017566124685 | validation: 0.056696296570434235]
	TIME [epoch: 5.86 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047013647624880694		[learning rate: 3.0848e-05]
	Learning Rate: 3.08483e-05
	LOSS [training: 0.047013647624880694 | validation: 0.051640379419779864]
	TIME [epoch: 5.86 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04696768081855027		[learning rate: 3.0739e-05]
	Learning Rate: 3.07392e-05
	LOSS [training: 0.04696768081855027 | validation: 0.05263431558749683]
	TIME [epoch: 5.85 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04751146937753174		[learning rate: 3.063e-05]
	Learning Rate: 3.06305e-05
	LOSS [training: 0.04751146937753174 | validation: 0.05056946386138084]
	TIME [epoch: 5.85 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045957651509852704		[learning rate: 3.0522e-05]
	Learning Rate: 3.05222e-05
	LOSS [training: 0.045957651509852704 | validation: 0.055554921160953265]
	TIME [epoch: 5.85 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04541428561602823		[learning rate: 3.0414e-05]
	Learning Rate: 3.04142e-05
	LOSS [training: 0.04541428561602823 | validation: 0.058552928912708635]
	TIME [epoch: 5.85 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045646535832460054		[learning rate: 3.0307e-05]
	Learning Rate: 3.03067e-05
	LOSS [training: 0.045646535832460054 | validation: 0.05978093002732657]
	TIME [epoch: 5.85 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04658878571468173		[learning rate: 3.02e-05]
	Learning Rate: 3.01995e-05
	LOSS [training: 0.04658878571468173 | validation: 0.05672075995963019]
	TIME [epoch: 5.85 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04773420296826457		[learning rate: 3.0093e-05]
	Learning Rate: 3.00927e-05
	LOSS [training: 0.04773420296826457 | validation: 0.05614815224200431]
	TIME [epoch: 5.85 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04472185505761691		[learning rate: 2.9986e-05]
	Learning Rate: 2.99863e-05
	LOSS [training: 0.04472185505761691 | validation: 0.05592219875585827]
	TIME [epoch: 5.85 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04654143893180592		[learning rate: 2.988e-05]
	Learning Rate: 2.98803e-05
	LOSS [training: 0.04654143893180592 | validation: 0.059302869275146324]
	TIME [epoch: 5.85 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04619125344375031		[learning rate: 2.9775e-05]
	Learning Rate: 2.97746e-05
	LOSS [training: 0.04619125344375031 | validation: 0.05991749545429224]
	TIME [epoch: 5.86 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047001781675344856		[learning rate: 2.9669e-05]
	Learning Rate: 2.96693e-05
	LOSS [training: 0.047001781675344856 | validation: 0.05264431660517699]
	TIME [epoch: 5.85 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04445910131751287		[learning rate: 2.9564e-05]
	Learning Rate: 2.95644e-05
	LOSS [training: 0.04445910131751287 | validation: 0.05164778085723475]
	TIME [epoch: 5.85 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048172435583921445		[learning rate: 2.946e-05]
	Learning Rate: 2.94599e-05
	LOSS [training: 0.048172435583921445 | validation: 0.051328991867363054]
	TIME [epoch: 5.85 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04765954600774722		[learning rate: 2.9356e-05]
	Learning Rate: 2.93557e-05
	LOSS [training: 0.04765954600774722 | validation: 0.051521465950813895]
	TIME [epoch: 5.85 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04748447650549476		[learning rate: 2.9252e-05]
	Learning Rate: 2.92519e-05
	LOSS [training: 0.04748447650549476 | validation: 0.05684768267022658]
	TIME [epoch: 5.86 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0475785202514699		[learning rate: 2.9148e-05]
	Learning Rate: 2.91485e-05
	LOSS [training: 0.0475785202514699 | validation: 0.057886242055213324]
	TIME [epoch: 5.85 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047494355481098816		[learning rate: 2.9045e-05]
	Learning Rate: 2.90454e-05
	LOSS [training: 0.047494355481098816 | validation: 0.05267419833033452]
	TIME [epoch: 5.86 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047297026849760365		[learning rate: 2.8943e-05]
	Learning Rate: 2.89427e-05
	LOSS [training: 0.047297026849760365 | validation: 0.04939425781368824]
	TIME [epoch: 5.85 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045887824175155564		[learning rate: 2.884e-05]
	Learning Rate: 2.88403e-05
	LOSS [training: 0.045887824175155564 | validation: 0.05389699935833528]
	TIME [epoch: 5.85 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04653624695972057		[learning rate: 2.8738e-05]
	Learning Rate: 2.87383e-05
	LOSS [training: 0.04653624695972057 | validation: 0.0564955383876987]
	TIME [epoch: 5.85 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04505390977723286		[learning rate: 2.8637e-05]
	Learning Rate: 2.86367e-05
	LOSS [training: 0.04505390977723286 | validation: 0.054152473260554314]
	TIME [epoch: 5.85 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04643351601517916		[learning rate: 2.8535e-05]
	Learning Rate: 2.85355e-05
	LOSS [training: 0.04643351601517916 | validation: 0.05096834244467888]
	TIME [epoch: 5.86 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04815689985790002		[learning rate: 2.8435e-05]
	Learning Rate: 2.84345e-05
	LOSS [training: 0.04815689985790002 | validation: 0.054026920031289095]
	TIME [epoch: 5.86 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0447998780573978		[learning rate: 2.8334e-05]
	Learning Rate: 2.8334e-05
	LOSS [training: 0.0447998780573978 | validation: 0.05648969641394515]
	TIME [epoch: 5.86 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046078431994878634		[learning rate: 2.8234e-05]
	Learning Rate: 2.82338e-05
	LOSS [training: 0.046078431994878634 | validation: 0.055872909236505966]
	TIME [epoch: 5.86 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04782388991743005		[learning rate: 2.8134e-05]
	Learning Rate: 2.8134e-05
	LOSS [training: 0.04782388991743005 | validation: 0.05081120159500995]
	TIME [epoch: 5.85 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048600238804466615		[learning rate: 2.8034e-05]
	Learning Rate: 2.80345e-05
	LOSS [training: 0.048600238804466615 | validation: 0.052831371782867044]
	TIME [epoch: 5.86 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04613221943544469		[learning rate: 2.7935e-05]
	Learning Rate: 2.79353e-05
	LOSS [training: 0.04613221943544469 | validation: 0.05798735360835582]
	TIME [epoch: 5.85 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04606275087331943		[learning rate: 2.7837e-05]
	Learning Rate: 2.78366e-05
	LOSS [training: 0.04606275087331943 | validation: 0.04850926017432255]
	TIME [epoch: 5.86 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04436276154524773		[learning rate: 2.7738e-05]
	Learning Rate: 2.77381e-05
	LOSS [training: 0.04436276154524773 | validation: 0.05518867674481272]
	TIME [epoch: 5.86 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04560242059497756		[learning rate: 2.764e-05]
	Learning Rate: 2.764e-05
	LOSS [training: 0.04560242059497756 | validation: 0.052285656509307867]
	TIME [epoch: 5.86 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044838030648273776		[learning rate: 2.7542e-05]
	Learning Rate: 2.75423e-05
	LOSS [training: 0.044838030648273776 | validation: 0.049882154321911176]
	TIME [epoch: 5.85 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04574732708430945		[learning rate: 2.7445e-05]
	Learning Rate: 2.74449e-05
	LOSS [training: 0.04574732708430945 | validation: 0.05732556099198678]
	TIME [epoch: 5.85 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043970419378976375		[learning rate: 2.7348e-05]
	Learning Rate: 2.73478e-05
	LOSS [training: 0.043970419378976375 | validation: 0.05849735917077036]
	TIME [epoch: 5.85 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046785176313502465		[learning rate: 2.7251e-05]
	Learning Rate: 2.72511e-05
	LOSS [training: 0.046785176313502465 | validation: 0.0537246419168631]
	TIME [epoch: 5.87 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04649137606717954		[learning rate: 2.7155e-05]
	Learning Rate: 2.71548e-05
	LOSS [training: 0.04649137606717954 | validation: 0.05262200049955758]
	TIME [epoch: 5.85 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04750690433054281		[learning rate: 2.7059e-05]
	Learning Rate: 2.70587e-05
	LOSS [training: 0.04750690433054281 | validation: 0.05932344509420454]
	TIME [epoch: 5.87 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04745954046963384		[learning rate: 2.6963e-05]
	Learning Rate: 2.69631e-05
	LOSS [training: 0.04745954046963384 | validation: 0.052691903723177835]
	TIME [epoch: 5.86 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04721510597683267		[learning rate: 2.6868e-05]
	Learning Rate: 2.68677e-05
	LOSS [training: 0.04721510597683267 | validation: 0.05188902965991487]
	TIME [epoch: 5.85 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046839227796777405		[learning rate: 2.6773e-05]
	Learning Rate: 2.67727e-05
	LOSS [training: 0.046839227796777405 | validation: 0.056038996948734425]
	TIME [epoch: 5.86 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0485172513261308		[learning rate: 2.6678e-05]
	Learning Rate: 2.6678e-05
	LOSS [training: 0.0485172513261308 | validation: 0.05013874558800563]
	TIME [epoch: 5.85 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045787440842327644		[learning rate: 2.6584e-05]
	Learning Rate: 2.65837e-05
	LOSS [training: 0.045787440842327644 | validation: 0.052334347056794864]
	TIME [epoch: 5.86 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046073342258068396		[learning rate: 2.649e-05]
	Learning Rate: 2.64897e-05
	LOSS [training: 0.046073342258068396 | validation: 0.04906698938399029]
	TIME [epoch: 5.85 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046922796522817396		[learning rate: 2.6396e-05]
	Learning Rate: 2.6396e-05
	LOSS [training: 0.046922796522817396 | validation: 0.05675692910594415]
	TIME [epoch: 5.86 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04769670434361679		[learning rate: 2.6303e-05]
	Learning Rate: 2.63027e-05
	LOSS [training: 0.04769670434361679 | validation: 0.0557739562548228]
	TIME [epoch: 5.87 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044951346328534036		[learning rate: 2.621e-05]
	Learning Rate: 2.62097e-05
	LOSS [training: 0.044951346328534036 | validation: 0.05817700226611383]
	TIME [epoch: 5.85 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04602080129894641		[learning rate: 2.6117e-05]
	Learning Rate: 2.6117e-05
	LOSS [training: 0.04602080129894641 | validation: 0.05020046070213026]
	TIME [epoch: 5.85 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04763217237478396		[learning rate: 2.6025e-05]
	Learning Rate: 2.60246e-05
	LOSS [training: 0.04763217237478396 | validation: 0.04573186147142949]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1731.pth
	Model improved!!!
EPOCH 1732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04594854604733522		[learning rate: 2.5933e-05]
	Learning Rate: 2.59326e-05
	LOSS [training: 0.04594854604733522 | validation: 0.05306810403529543]
	TIME [epoch: 5.85 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04763662519816736		[learning rate: 2.5841e-05]
	Learning Rate: 2.58409e-05
	LOSS [training: 0.04763662519816736 | validation: 0.05328817272866798]
	TIME [epoch: 5.86 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04621888620476353		[learning rate: 2.575e-05]
	Learning Rate: 2.57495e-05
	LOSS [training: 0.04621888620476353 | validation: 0.049634409756734624]
	TIME [epoch: 5.85 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045382726759084384		[learning rate: 2.5658e-05]
	Learning Rate: 2.56585e-05
	LOSS [training: 0.045382726759084384 | validation: 0.05189475242201104]
	TIME [epoch: 5.85 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04542003082785847		[learning rate: 2.5568e-05]
	Learning Rate: 2.55677e-05
	LOSS [training: 0.04542003082785847 | validation: 0.056013387592139675]
	TIME [epoch: 5.84 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046002867260677024		[learning rate: 2.5477e-05]
	Learning Rate: 2.54773e-05
	LOSS [training: 0.046002867260677024 | validation: 0.05343360750343166]
	TIME [epoch: 5.85 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048543028258153244		[learning rate: 2.5387e-05]
	Learning Rate: 2.53872e-05
	LOSS [training: 0.048543028258153244 | validation: 0.05229616055288897]
	TIME [epoch: 5.85 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048425318617503876		[learning rate: 2.5297e-05]
	Learning Rate: 2.52975e-05
	LOSS [training: 0.048425318617503876 | validation: 0.05627430541153055]
	TIME [epoch: 5.86 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046292777189602605		[learning rate: 2.5208e-05]
	Learning Rate: 2.5208e-05
	LOSS [training: 0.046292777189602605 | validation: 0.054235074136591434]
	TIME [epoch: 5.86 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04816432796079011		[learning rate: 2.5119e-05]
	Learning Rate: 2.51189e-05
	LOSS [training: 0.04816432796079011 | validation: 0.05634240896877566]
	TIME [epoch: 5.85 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04563321855344306		[learning rate: 2.503e-05]
	Learning Rate: 2.503e-05
	LOSS [training: 0.04563321855344306 | validation: 0.051087481746614774]
	TIME [epoch: 5.85 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04518407559646297		[learning rate: 2.4942e-05]
	Learning Rate: 2.49415e-05
	LOSS [training: 0.04518407559646297 | validation: 0.05425551264171783]
	TIME [epoch: 5.85 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04617192405907989		[learning rate: 2.4853e-05]
	Learning Rate: 2.48533e-05
	LOSS [training: 0.04617192405907989 | validation: 0.05292779037485423]
	TIME [epoch: 5.88 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04545188389275858		[learning rate: 2.4765e-05]
	Learning Rate: 2.47655e-05
	LOSS [training: 0.04545188389275858 | validation: 0.05543140116703689]
	TIME [epoch: 5.85 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04696005371693193		[learning rate: 2.4678e-05]
	Learning Rate: 2.46779e-05
	LOSS [training: 0.04696005371693193 | validation: 0.04924698199918862]
	TIME [epoch: 5.85 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04488546158117059		[learning rate: 2.4591e-05]
	Learning Rate: 2.45906e-05
	LOSS [training: 0.04488546158117059 | validation: 0.05387388652635289]
	TIME [epoch: 5.85 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04641976837338846		[learning rate: 2.4504e-05]
	Learning Rate: 2.45037e-05
	LOSS [training: 0.04641976837338846 | validation: 0.0520013053989955]
	TIME [epoch: 5.85 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04479618981987714		[learning rate: 2.4417e-05]
	Learning Rate: 2.4417e-05
	LOSS [training: 0.04479618981987714 | validation: 0.054846982147994106]
	TIME [epoch: 5.85 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04514510706736198		[learning rate: 2.4331e-05]
	Learning Rate: 2.43307e-05
	LOSS [training: 0.04514510706736198 | validation: 0.05408051614104703]
	TIME [epoch: 5.85 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0465172449362566		[learning rate: 2.4245e-05]
	Learning Rate: 2.42446e-05
	LOSS [training: 0.0465172449362566 | validation: 0.05232230028832477]
	TIME [epoch: 5.85 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04676582595199906		[learning rate: 2.4159e-05]
	Learning Rate: 2.41589e-05
	LOSS [training: 0.04676582595199906 | validation: 0.05408560370197089]
	TIME [epoch: 5.85 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04794731324973505		[learning rate: 2.4073e-05]
	Learning Rate: 2.40735e-05
	LOSS [training: 0.04794731324973505 | validation: 0.05361328750985174]
	TIME [epoch: 5.86 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044789879251828084		[learning rate: 2.3988e-05]
	Learning Rate: 2.39883e-05
	LOSS [training: 0.044789879251828084 | validation: 0.05320781581883898]
	TIME [epoch: 5.84 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04575556436544953		[learning rate: 2.3904e-05]
	Learning Rate: 2.39035e-05
	LOSS [training: 0.04575556436544953 | validation: 0.054531354970479055]
	TIME [epoch: 5.85 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04637240083858018		[learning rate: 2.3819e-05]
	Learning Rate: 2.3819e-05
	LOSS [training: 0.04637240083858018 | validation: 0.055061265137584164]
	TIME [epoch: 5.85 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045692431021552904		[learning rate: 2.3735e-05]
	Learning Rate: 2.37347e-05
	LOSS [training: 0.045692431021552904 | validation: 0.05054580473762798]
	TIME [epoch: 5.86 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04714495145397193		[learning rate: 2.3651e-05]
	Learning Rate: 2.36508e-05
	LOSS [training: 0.04714495145397193 | validation: 0.05264309601901554]
	TIME [epoch: 5.85 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045725414578056026		[learning rate: 2.3567e-05]
	Learning Rate: 2.35672e-05
	LOSS [training: 0.045725414578056026 | validation: 0.05178948255427962]
	TIME [epoch: 5.85 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04476133730223103		[learning rate: 2.3484e-05]
	Learning Rate: 2.34838e-05
	LOSS [training: 0.04476133730223103 | validation: 0.05848648640685755]
	TIME [epoch: 5.85 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04447094629131871		[learning rate: 2.3401e-05]
	Learning Rate: 2.34008e-05
	LOSS [training: 0.04447094629131871 | validation: 0.05247530620247775]
	TIME [epoch: 5.85 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0447610678375486		[learning rate: 2.3318e-05]
	Learning Rate: 2.33181e-05
	LOSS [training: 0.0447610678375486 | validation: 0.053741721994569336]
	TIME [epoch: 5.85 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04663003331997744		[learning rate: 2.3236e-05]
	Learning Rate: 2.32356e-05
	LOSS [training: 0.04663003331997744 | validation: 0.0526479651205401]
	TIME [epoch: 5.86 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045415485710359964		[learning rate: 2.3153e-05]
	Learning Rate: 2.31534e-05
	LOSS [training: 0.045415485710359964 | validation: 0.05023743630569401]
	TIME [epoch: 5.85 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04480016203141798		[learning rate: 2.3072e-05]
	Learning Rate: 2.30716e-05
	LOSS [training: 0.04480016203141798 | validation: 0.05309836741486275]
	TIME [epoch: 5.85 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047419522910122726		[learning rate: 2.299e-05]
	Learning Rate: 2.299e-05
	LOSS [training: 0.047419522910122726 | validation: 0.0497254242831148]
	TIME [epoch: 5.85 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04659389076049115		[learning rate: 2.2909e-05]
	Learning Rate: 2.29087e-05
	LOSS [training: 0.04659389076049115 | validation: 0.05411544493983287]
	TIME [epoch: 5.85 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04532993069435392		[learning rate: 2.2828e-05]
	Learning Rate: 2.28277e-05
	LOSS [training: 0.04532993069435392 | validation: 0.049143936104020715]
	TIME [epoch: 5.85 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04732185323290499		[learning rate: 2.2747e-05]
	Learning Rate: 2.27469e-05
	LOSS [training: 0.04732185323290499 | validation: 0.05239162749337509]
	TIME [epoch: 5.85 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04544273156310396		[learning rate: 2.2667e-05]
	Learning Rate: 2.26665e-05
	LOSS [training: 0.04544273156310396 | validation: 0.04706466710836199]
	TIME [epoch: 5.85 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04388512279111192		[learning rate: 2.2586e-05]
	Learning Rate: 2.25864e-05
	LOSS [training: 0.04388512279111192 | validation: 0.05243233780345607]
	TIME [epoch: 5.86 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04565329154118363		[learning rate: 2.2506e-05]
	Learning Rate: 2.25065e-05
	LOSS [training: 0.04565329154118363 | validation: 0.04952443938896969]
	TIME [epoch: 5.84 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046260229213116504		[learning rate: 2.2427e-05]
	Learning Rate: 2.24269e-05
	LOSS [training: 0.046260229213116504 | validation: 0.05179766309563009]
	TIME [epoch: 5.86 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046160462116863546		[learning rate: 2.2348e-05]
	Learning Rate: 2.23476e-05
	LOSS [training: 0.046160462116863546 | validation: 0.05292705695380006]
	TIME [epoch: 5.85 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04514900924801953		[learning rate: 2.2269e-05]
	Learning Rate: 2.22686e-05
	LOSS [training: 0.04514900924801953 | validation: 0.053049526857473175]
	TIME [epoch: 5.85 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04673657392871689		[learning rate: 2.219e-05]
	Learning Rate: 2.21898e-05
	LOSS [training: 0.04673657392871689 | validation: 0.05226249419017397]
	TIME [epoch: 5.85 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045896773120746366		[learning rate: 2.2111e-05]
	Learning Rate: 2.21114e-05
	LOSS [training: 0.045896773120746366 | validation: 0.05188712525289168]
	TIME [epoch: 5.85 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04393846649164538		[learning rate: 2.2033e-05]
	Learning Rate: 2.20332e-05
	LOSS [training: 0.04393846649164538 | validation: 0.049818967577082846]
	TIME [epoch: 5.85 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046317123406940726		[learning rate: 2.1955e-05]
	Learning Rate: 2.19553e-05
	LOSS [training: 0.046317123406940726 | validation: 0.05140361434138246]
	TIME [epoch: 5.87 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04512737892659992		[learning rate: 2.1878e-05]
	Learning Rate: 2.18776e-05
	LOSS [training: 0.04512737892659992 | validation: 0.053982238754941105]
	TIME [epoch: 5.85 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04704368587775483		[learning rate: 2.18e-05]
	Learning Rate: 2.18003e-05
	LOSS [training: 0.04704368587775483 | validation: 0.05165769243859977]
	TIME [epoch: 5.86 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04617443928233816		[learning rate: 2.1723e-05]
	Learning Rate: 2.17232e-05
	LOSS [training: 0.04617443928233816 | validation: 0.04951114562299015]
	TIME [epoch: 5.85 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04471393955793425		[learning rate: 2.1646e-05]
	Learning Rate: 2.16464e-05
	LOSS [training: 0.04471393955793425 | validation: 0.051877422845207334]
	TIME [epoch: 5.86 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044897790480573845		[learning rate: 2.157e-05]
	Learning Rate: 2.15698e-05
	LOSS [training: 0.044897790480573845 | validation: 0.05419937344956928]
	TIME [epoch: 5.87 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04636487345897525		[learning rate: 2.1494e-05]
	Learning Rate: 2.14935e-05
	LOSS [training: 0.04636487345897525 | validation: 0.05452465570888946]
	TIME [epoch: 5.85 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04591951991034563		[learning rate: 2.1418e-05]
	Learning Rate: 2.14175e-05
	LOSS [training: 0.04591951991034563 | validation: 0.055644274071387216]
	TIME [epoch: 5.85 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04339809076850083		[learning rate: 2.1342e-05]
	Learning Rate: 2.13418e-05
	LOSS [training: 0.04339809076850083 | validation: 0.05231395240057546]
	TIME [epoch: 5.86 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04793781719216124		[learning rate: 2.1266e-05]
	Learning Rate: 2.12663e-05
	LOSS [training: 0.04793781719216124 | validation: 0.05424126747295758]
	TIME [epoch: 5.85 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045555469376292734		[learning rate: 2.1191e-05]
	Learning Rate: 2.11911e-05
	LOSS [training: 0.045555469376292734 | validation: 0.05281927807034538]
	TIME [epoch: 5.87 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04556593216363803		[learning rate: 2.1116e-05]
	Learning Rate: 2.11162e-05
	LOSS [training: 0.04556593216363803 | validation: 0.051735824445824696]
	TIME [epoch: 5.85 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04643355750374095		[learning rate: 2.1042e-05]
	Learning Rate: 2.10415e-05
	LOSS [training: 0.04643355750374095 | validation: 0.051538755753076884]
	TIME [epoch: 5.86 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045960706658789936		[learning rate: 2.0967e-05]
	Learning Rate: 2.09671e-05
	LOSS [training: 0.045960706658789936 | validation: 0.055881669365832165]
	TIME [epoch: 5.85 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045246110899785016		[learning rate: 2.0893e-05]
	Learning Rate: 2.0893e-05
	LOSS [training: 0.045246110899785016 | validation: 0.052728359316333774]
	TIME [epoch: 5.85 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04556858275032535		[learning rate: 2.0819e-05]
	Learning Rate: 2.08191e-05
	LOSS [training: 0.04556858275032535 | validation: 0.05227130326505086]
	TIME [epoch: 5.89 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045164682532362777		[learning rate: 2.0745e-05]
	Learning Rate: 2.07455e-05
	LOSS [training: 0.045164682532362777 | validation: 0.05399018952503674]
	TIME [epoch: 5.86 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046094009654962936		[learning rate: 2.0672e-05]
	Learning Rate: 2.06721e-05
	LOSS [training: 0.046094009654962936 | validation: 0.05330760259918374]
	TIME [epoch: 5.85 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045871607310821086		[learning rate: 2.0599e-05]
	Learning Rate: 2.0599e-05
	LOSS [training: 0.045871607310821086 | validation: 0.05451982873740022]
	TIME [epoch: 5.86 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04651738839344386		[learning rate: 2.0526e-05]
	Learning Rate: 2.05262e-05
	LOSS [training: 0.04651738839344386 | validation: 0.054508047917671804]
	TIME [epoch: 5.85 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0442045509119458		[learning rate: 2.0454e-05]
	Learning Rate: 2.04536e-05
	LOSS [training: 0.0442045509119458 | validation: 0.05272671518139426]
	TIME [epoch: 5.87 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04525526671646121		[learning rate: 2.0381e-05]
	Learning Rate: 2.03812e-05
	LOSS [training: 0.04525526671646121 | validation: 0.051504227696338015]
	TIME [epoch: 5.85 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046034584101406914		[learning rate: 2.0309e-05]
	Learning Rate: 2.03092e-05
	LOSS [training: 0.046034584101406914 | validation: 0.053718160686121674]
	TIME [epoch: 5.87 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044770602745570605		[learning rate: 2.0237e-05]
	Learning Rate: 2.02374e-05
	LOSS [training: 0.044770602745570605 | validation: 0.05455742599626721]
	TIME [epoch: 5.86 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04513881213865964		[learning rate: 2.0166e-05]
	Learning Rate: 2.01658e-05
	LOSS [training: 0.04513881213865964 | validation: 0.048954250081792376]
	TIME [epoch: 5.85 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04591818151015731		[learning rate: 2.0094e-05]
	Learning Rate: 2.00945e-05
	LOSS [training: 0.04591818151015731 | validation: 0.054176431796306095]
	TIME [epoch: 5.85 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04529063279121098		[learning rate: 2.0023e-05]
	Learning Rate: 2.00234e-05
	LOSS [training: 0.04529063279121098 | validation: 0.0527010275455225]
	TIME [epoch: 5.85 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04590151634587729		[learning rate: 1.9953e-05]
	Learning Rate: 1.99526e-05
	LOSS [training: 0.04590151634587729 | validation: 0.05434726205416118]
	TIME [epoch: 5.86 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04696778417397019		[learning rate: 1.9882e-05]
	Learning Rate: 1.98821e-05
	LOSS [training: 0.04696778417397019 | validation: 0.05074485865000372]
	TIME [epoch: 5.85 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0453204736231617		[learning rate: 1.9812e-05]
	Learning Rate: 1.98118e-05
	LOSS [training: 0.0453204736231617 | validation: 0.04994782381944229]
	TIME [epoch: 5.85 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04690388958492487		[learning rate: 1.9742e-05]
	Learning Rate: 1.97417e-05
	LOSS [training: 0.04690388958492487 | validation: 0.052599997646371494]
	TIME [epoch: 5.85 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04551918416890665		[learning rate: 1.9672e-05]
	Learning Rate: 1.96719e-05
	LOSS [training: 0.04551918416890665 | validation: 0.05178555209083098]
	TIME [epoch: 5.85 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046334477098723924		[learning rate: 1.9602e-05]
	Learning Rate: 1.96023e-05
	LOSS [training: 0.046334477098723924 | validation: 0.056097459236794926]
	TIME [epoch: 5.86 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04634261050414411		[learning rate: 1.9533e-05]
	Learning Rate: 1.9533e-05
	LOSS [training: 0.04634261050414411 | validation: 0.04957471217753902]
	TIME [epoch: 5.85 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04586599118079294		[learning rate: 1.9464e-05]
	Learning Rate: 1.94639e-05
	LOSS [training: 0.04586599118079294 | validation: 0.05558274314326427]
	TIME [epoch: 5.85 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04542404307575661		[learning rate: 1.9395e-05]
	Learning Rate: 1.93951e-05
	LOSS [training: 0.04542404307575661 | validation: 0.05002934628615301]
	TIME [epoch: 5.85 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046430723443703165		[learning rate: 1.9327e-05]
	Learning Rate: 1.93265e-05
	LOSS [training: 0.046430723443703165 | validation: 0.0530380248272231]
	TIME [epoch: 5.86 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046643042909327956		[learning rate: 1.9258e-05]
	Learning Rate: 1.92582e-05
	LOSS [training: 0.046643042909327956 | validation: 0.05270215632324824]
	TIME [epoch: 5.86 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04506893038113507		[learning rate: 1.919e-05]
	Learning Rate: 1.91901e-05
	LOSS [training: 0.04506893038113507 | validation: 0.05277001114419107]
	TIME [epoch: 5.86 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04477222830312329		[learning rate: 1.9122e-05]
	Learning Rate: 1.91222e-05
	LOSS [training: 0.04477222830312329 | validation: 0.05439490863971052]
	TIME [epoch: 5.85 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04520879447300736		[learning rate: 1.9055e-05]
	Learning Rate: 1.90546e-05
	LOSS [training: 0.04520879447300736 | validation: 0.05045858570761888]
	TIME [epoch: 5.85 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04539815397247051		[learning rate: 1.8987e-05]
	Learning Rate: 1.89872e-05
	LOSS [training: 0.04539815397247051 | validation: 0.053022806558938235]
	TIME [epoch: 5.84 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04420325966961615		[learning rate: 1.892e-05]
	Learning Rate: 1.89201e-05
	LOSS [training: 0.04420325966961615 | validation: 0.05207963105852988]
	TIME [epoch: 5.86 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046806783754964505		[learning rate: 1.8853e-05]
	Learning Rate: 1.88532e-05
	LOSS [training: 0.046806783754964505 | validation: 0.051547295675963695]
	TIME [epoch: 5.85 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045716636210603505		[learning rate: 1.8787e-05]
	Learning Rate: 1.87865e-05
	LOSS [training: 0.045716636210603505 | validation: 0.04850398428811136]
	TIME [epoch: 5.86 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0441961061962963		[learning rate: 1.872e-05]
	Learning Rate: 1.87201e-05
	LOSS [training: 0.0441961061962963 | validation: 0.04859339764824491]
	TIME [epoch: 5.85 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04556444006438183		[learning rate: 1.8654e-05]
	Learning Rate: 1.86539e-05
	LOSS [training: 0.04556444006438183 | validation: 0.051331773125710245]
	TIME [epoch: 5.85 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046258473682111355		[learning rate: 1.8588e-05]
	Learning Rate: 1.85879e-05
	LOSS [training: 0.046258473682111355 | validation: 0.05004564383003024]
	TIME [epoch: 5.86 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04524662236421843		[learning rate: 1.8522e-05]
	Learning Rate: 1.85222e-05
	LOSS [training: 0.04524662236421843 | validation: 0.058064860784122485]
	TIME [epoch: 5.85 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04437677466916323		[learning rate: 1.8457e-05]
	Learning Rate: 1.84567e-05
	LOSS [training: 0.04437677466916323 | validation: 0.049692290141801446]
	TIME [epoch: 5.85 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04619404875715362		[learning rate: 1.8391e-05]
	Learning Rate: 1.83914e-05
	LOSS [training: 0.04619404875715362 | validation: 0.054081024746767285]
	TIME [epoch: 5.85 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04429575106999143		[learning rate: 1.8326e-05]
	Learning Rate: 1.83264e-05
	LOSS [training: 0.04429575106999143 | validation: 0.052621495122359446]
	TIME [epoch: 5.85 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04509728747771112		[learning rate: 1.8262e-05]
	Learning Rate: 1.82616e-05
	LOSS [training: 0.04509728747771112 | validation: 0.05131274021992177]
	TIME [epoch: 5.85 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04397365429234262		[learning rate: 1.8197e-05]
	Learning Rate: 1.8197e-05
	LOSS [training: 0.04397365429234262 | validation: 0.051891138929060755]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_7_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_7_v_mmd4_1832.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 7687.804 seconds.
