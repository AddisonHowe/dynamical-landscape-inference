Args:
Namespace(name='model_phi1_4a_distortion_v2_3_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_3/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_3/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.020969864, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3433203352

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.998789046448945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.998789046448945 | validation: 6.250624065690393]
	TIME [epoch: 168 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.801781454425959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.801781454425959 | validation: 6.163503463848728]
	TIME [epoch: 0.811 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.729966776333809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.729966776333809 | validation: 6.281509701352062]
	TIME [epoch: 0.694 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.498589462609135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.498589462609135 | validation: 6.4768816321637335]
	TIME [epoch: 0.693 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.5323272850879635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.5323272850879635 | validation: 5.9008611344454485]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.187950325317315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.187950325317315 | validation: 5.902647517137256]
	TIME [epoch: 0.7 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.086642280996219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.086642280996219 | validation: 5.807105969547794]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.872650976610398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.872650976610398 | validation: 5.866808842599923]
	TIME [epoch: 0.726 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.722825644897929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.722825644897929 | validation: 5.759452376732792]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.589607746525626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.589607746525626 | validation: 5.659755099183695]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.539038315983489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.539038315983489 | validation: 5.310839070304288]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.39160121105687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.39160121105687 | validation: 5.631918342574077]
	TIME [epoch: 0.693 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.354395223230577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.354395223230577 | validation: 5.347863152614935]
	TIME [epoch: 0.692 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.077807720946128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.077807720946128 | validation: 4.907153634306012]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.111339845047836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.111339845047836 | validation: 5.322391433995948]
	TIME [epoch: 0.697 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9533406464232574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9533406464232574 | validation: 4.97536609642288]
	TIME [epoch: 0.695 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.792512553129634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.792512553129634 | validation: 4.726334707640876]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5148734212447637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5148734212447637 | validation: 4.804343104081053]
	TIME [epoch: 0.697 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4899638263807704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4899638263807704 | validation: 4.148560139013077]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.327985065856331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.327985065856331 | validation: 4.265133274863191]
	TIME [epoch: 0.694 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.222906331583151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.222906331583151 | validation: 3.698480056232159]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0580105771696786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0580105771696786 | validation: 4.236671649272767]
	TIME [epoch: 0.693 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0898006501392117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0898006501392117 | validation: 2.558874948397481]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0972820745398097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0972820745398097 | validation: 3.8074686126521264]
	TIME [epoch: 0.697 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0448599238980547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0448599238980547 | validation: 3.624099383213983]
	TIME [epoch: 0.692 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.701241914358148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.701241914358148 | validation: 2.956369604994352]
	TIME [epoch: 0.693 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5836887473777277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5836887473777277 | validation: 2.6775157498113145]
	TIME [epoch: 0.693 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3381463883244065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3381463883244065 | validation: 2.3271456523204344]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2257671714529685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2257671714529685 | validation: 2.0742852461087034]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2474157567387802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2474157567387802 | validation: 3.4323669974759867]
	TIME [epoch: 0.692 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.805124658063213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.805124658063213 | validation: 2.3694470627719655]
	TIME [epoch: 0.691 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.087033803895541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.087033803895541 | validation: 2.3619926808556344]
	TIME [epoch: 0.693 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.534947331876753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.534947331876753 | validation: 2.383067655416983]
	TIME [epoch: 0.692 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1931573195397154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1931573195397154 | validation: 2.1456563156425488]
	TIME [epoch: 0.692 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.99794017927094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.99794017927094 | validation: 2.045471218311227]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0302717474255636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0302717474255636 | validation: 1.9239219351871644]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.973792632987822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.973792632987822 | validation: 1.8327943125807984]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.947014796213055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.947014796213055 | validation: 1.82534262374518]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9456381002461103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9456381002461103 | validation: 1.810568501402245]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9347117001147829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9347117001147829 | validation: 1.7494872706601095]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9332050718832507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9332050718832507 | validation: 1.8123121321015963]
	TIME [epoch: 0.695 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9314005386894633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9314005386894633 | validation: 1.7877357933495839]
	TIME [epoch: 0.693 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9611911841318577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9611911841318577 | validation: 2.012723259864351]
	TIME [epoch: 0.695 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0293402971881513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0293402971881513 | validation: 1.8643200389755574]
	TIME [epoch: 0.693 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.997971432042455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.997971432042455 | validation: 1.8855764061290827]
	TIME [epoch: 0.693 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9347830907456027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9347830907456027 | validation: 1.6540459745848253]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.88878258438697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.88878258438697 | validation: 1.6184469737137015]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8731091837809788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8731091837809788 | validation: 1.6312003598894558]
	TIME [epoch: 0.7 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8578972607428283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8578972607428283 | validation: 1.6276159473825134]
	TIME [epoch: 0.697 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.858017678393997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.858017678393997 | validation: 1.5865926007357007]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8713293425902402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8713293425902402 | validation: 2.0303598047584366]
	TIME [epoch: 0.693 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0288934808835437		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 2.0288934808835437 | validation: 1.935213538531977]
	TIME [epoch: 0.693 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.06440932771299		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 2.06440932771299 | validation: 1.948949361235836]
	TIME [epoch: 0.691 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9157178466058777		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.9157178466058777 | validation: 1.6474408897868078]
	TIME [epoch: 0.692 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8148313029526324		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.8148313029526324 | validation: 1.5070995326271526]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8364400821505498		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.8364400821505498 | validation: 1.778079905284666]
	TIME [epoch: 0.693 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8654673177527505		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.8654673177527505 | validation: 1.6751922954084908]
	TIME [epoch: 0.691 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.836018174122195		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.836018174122195 | validation: 1.6180504057447505]
	TIME [epoch: 0.692 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.812110556579016		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.812110556579016 | validation: 1.5347824152360894]
	TIME [epoch: 0.696 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8063786678377063		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.8063786678377063 | validation: 1.7002503416196433]
	TIME [epoch: 0.692 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8305425204674195		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.8305425204674195 | validation: 1.5646820429356614]
	TIME [epoch: 0.691 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.845896677834563		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.845896677834563 | validation: 1.8622677024422494]
	TIME [epoch: 0.691 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9017607895614317		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.9017607895614317 | validation: 1.6243671972765874]
	TIME [epoch: 0.692 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.825949148921904		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.825949148921904 | validation: 1.5164386550461406]
	TIME [epoch: 0.691 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7816807267767878		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.7816807267767878 | validation: 1.5133289689416314]
	TIME [epoch: 0.69 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7442097878320721		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.7442097878320721 | validation: 1.4790110741571714]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7284529321357491		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.7284529321357491 | validation: 1.346337579420603]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7644996311878733		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.7644996311878733 | validation: 2.3895182029912294]
	TIME [epoch: 0.694 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.114576813099924		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 2.114576813099924 | validation: 1.3606455778160187]
	TIME [epoch: 0.694 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7353242016057788		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.7353242016057788 | validation: 1.379559871174045]
	TIME [epoch: 0.694 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8359617403366246		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.8359617403366246 | validation: 1.9544800029969336]
	TIME [epoch: 0.691 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9153055349293937		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.9153055349293937 | validation: 1.8282396441967628]
	TIME [epoch: 0.692 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8077261581747102		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.8077261581747102 | validation: 1.3502920994974486]
	TIME [epoch: 0.692 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7331643522611593		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.7331643522611593 | validation: 1.3523995361766765]
	TIME [epoch: 0.693 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7401402107559545		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.7401402107559545 | validation: 1.6459727965220539]
	TIME [epoch: 0.692 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7643834262137381		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.7643834262137381 | validation: 1.4144336926962593]
	TIME [epoch: 0.692 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.719974353932313		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.719974353932313 | validation: 1.3358045488063715]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7228206703100286		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.7228206703100286 | validation: 1.668721163398836]
	TIME [epoch: 0.693 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7714342253866004		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.7714342253866004 | validation: 1.3578664401210403]
	TIME [epoch: 0.692 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7488880947637448		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.7488880947637448 | validation: 1.7153299868262726]
	TIME [epoch: 0.692 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8021413149492487		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.8021413149492487 | validation: 1.440121964098002]
	TIME [epoch: 0.692 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7117220019641304		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.7117220019641304 | validation: 1.3308930576738711]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6861350071157801		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.6861350071157801 | validation: 1.5466588129712573]
	TIME [epoch: 0.693 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7177252003447396		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.7177252003447396 | validation: 1.083007221359372]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.789075336539217		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.789075336539217 | validation: 2.1638846828916316]
	TIME [epoch: 0.697 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.987714914470071		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.987714914470071 | validation: 1.4874678165202055]
	TIME [epoch: 0.694 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6970385384662303		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.6970385384662303 | validation: 1.1397148843007212]
	TIME [epoch: 0.694 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7642945585756018		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.7642945585756018 | validation: 1.433699864041916]
	TIME [epoch: 0.695 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6862293486860696		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.6862293486860696 | validation: 1.485952870227939]
	TIME [epoch: 0.692 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7015992452772626		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.7015992452772626 | validation: 1.3457820077028677]
	TIME [epoch: 0.693 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.666018201475639		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.666018201475639 | validation: 1.2318107178427125]
	TIME [epoch: 0.693 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6613439531048537		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.6613439531048537 | validation: 1.5856005696508726]
	TIME [epoch: 0.694 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7258163604822665		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.7258163604822665 | validation: 1.3729830590162053]
	TIME [epoch: 0.692 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7658237499146308		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.7658237499146308 | validation: 1.7234981710221136]
	TIME [epoch: 0.693 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8028318338078146		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.8028318338078146 | validation: 1.3429152865350555]
	TIME [epoch: 0.693 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6616452407404796		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.6616452407404796 | validation: 1.1993341625972886]
	TIME [epoch: 0.693 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6465612548555608		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.6465612548555608 | validation: 1.5510131115534076]
	TIME [epoch: 0.694 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6951287162572504		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.6951287162572504 | validation: 1.1655547349461752]
	TIME [epoch: 0.694 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6975356597016276		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.6975356597016276 | validation: 1.7896887487336806]
	TIME [epoch: 0.694 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7961338452813647		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.7961338452813647 | validation: 1.2020785077994613]
	TIME [epoch: 0.693 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6457871724395028		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.6457871724395028 | validation: 1.217574953197756]
	TIME [epoch: 0.693 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6432563845951638		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.6432563845951638 | validation: 1.5094086507318516]
	TIME [epoch: 0.696 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6808584325824711		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.6808584325824711 | validation: 1.1348067056548483]
	TIME [epoch: 0.693 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.669164325035248		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.669164325035248 | validation: 1.7127884875137676]
	TIME [epoch: 0.693 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7281904070834335		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.7281904070834335 | validation: 1.1434632258660098]
	TIME [epoch: 0.694 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6304680129823916		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.6304680129823916 | validation: 1.4073090970502138]
	TIME [epoch: 0.694 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6400880305517285		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.6400880305517285 | validation: 1.198648492663393]
	TIME [epoch: 0.693 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.71378350243134		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.71378350243134 | validation: 1.7773241113131206]
	TIME [epoch: 0.694 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8418238005590701		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.8418238005590701 | validation: 1.1757698824719829]
	TIME [epoch: 0.694 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6167246403965292		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.6167246403965292 | validation: 1.4609866149655835]
	TIME [epoch: 0.694 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7375795709785344		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.7375795709785344 | validation: 1.274980341091069]
	TIME [epoch: 0.693 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6768398288213977		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.6768398288213977 | validation: 1.6511221570451942]
	TIME [epoch: 0.692 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7105475154931167		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.7105475154931167 | validation: 1.028857977695832]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.666212468591769		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.666212468591769 | validation: 1.43598403532603]
	TIME [epoch: 0.693 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6286692576110977		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.6286692576110977 | validation: 1.1605552754784025]
	TIME [epoch: 0.693 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5949516130370691		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.5949516130370691 | validation: 1.2522926183989114]
	TIME [epoch: 0.695 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.603790418395302		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.603790418395302 | validation: 1.1444548934032979]
	TIME [epoch: 0.692 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6284068124195301		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.6284068124195301 | validation: 1.840763454045105]
	TIME [epoch: 0.694 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7874467526339681		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.7874467526339681 | validation: 1.005295697113901]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6591578874381097		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.6591578874381097 | validation: 1.5056783199196095]
	TIME [epoch: 0.7 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.638219148835171		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.638219148835171 | validation: 1.1036304670586592]
	TIME [epoch: 0.696 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5911014360703826		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.5911014360703826 | validation: 1.263879714580071]
	TIME [epoch: 0.697 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5781552260039222		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.5781552260039222 | validation: 1.069064640076667]
	TIME [epoch: 0.697 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5836786783608647		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.5836786783608647 | validation: 1.5156495624236574]
	TIME [epoch: 0.698 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6408261707250722		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.6408261707250722 | validation: 1.0279640275848763]
	TIME [epoch: 0.696 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8402470041073375		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.8402470041073375 | validation: 1.9447609748055208]
	TIME [epoch: 0.692 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.823479291688526		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.823479291688526 | validation: 1.5119569037872562]
	TIME [epoch: 0.693 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6547095403401386		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.6547095403401386 | validation: 1.1343492983420898]
	TIME [epoch: 0.695 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7100961877056486		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.7100961877056486 | validation: 1.3099801599429188]
	TIME [epoch: 0.692 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.629322494349187		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.629322494349187 | validation: 1.3745760579227184]
	TIME [epoch: 0.692 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5978334488019688		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.5978334488019688 | validation: 1.0916517400998889]
	TIME [epoch: 0.692 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5888199225154012		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.5888199225154012 | validation: 1.182006799256119]
	TIME [epoch: 0.691 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5644336355991477		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.5644336355991477 | validation: 1.2244557966896132]
	TIME [epoch: 0.692 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5621252177317804		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.5621252177317804 | validation: 1.0830699756549405]
	TIME [epoch: 0.692 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.565381298091017		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.565381298091017 | validation: 1.3583485206603159]
	TIME [epoch: 0.694 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.579004890535649		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.579004890535649 | validation: 0.9829150857727321]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6286225606426135		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.6286225606426135 | validation: 1.6990558005323864]
	TIME [epoch: 0.699 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7375830925553806		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.7375830925553806 | validation: 1.0334315814582495]
	TIME [epoch: 0.697 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5681877718143886		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.5681877718143886 | validation: 1.2274697817664373]
	TIME [epoch: 0.698 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5881347433933255		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.5881347433933255 | validation: 1.2836152840753132]
	TIME [epoch: 0.698 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6129231992690334		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.6129231992690334 | validation: 1.3815010354766324]
	TIME [epoch: 0.698 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5905947830283265		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.5905947830283265 | validation: 1.0035160867632331]
	TIME [epoch: 0.699 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6524163162109833		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.6524163162109833 | validation: 1.8442618997029614]
	TIME [epoch: 0.699 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7562227887236928		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.7562227887236928 | validation: 1.2014445594416252]
	TIME [epoch: 0.699 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.546889907070182		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.546889907070182 | validation: 1.0152957020657678]
	TIME [epoch: 0.699 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6262255214580403		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.6262255214580403 | validation: 1.471734541707782]
	TIME [epoch: 0.699 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.614534838594499		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.614534838594499 | validation: 1.193931860817452]
	TIME [epoch: 0.7 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.524247375324011		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.524247375324011 | validation: 1.0083781055872152]
	TIME [epoch: 0.7 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5561629295142012		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.5561629295142012 | validation: 1.4401494316696222]
	TIME [epoch: 0.7 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5814542595216916		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.5814542595216916 | validation: 1.0245329003584507]
	TIME [epoch: 0.701 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5318510784660704		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.5318510784660704 | validation: 1.232150731902176]
	TIME [epoch: 0.7 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5098835751759319		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.5098835751759319 | validation: 1.0492067001073766]
	TIME [epoch: 0.701 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.513941668515444		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.513941668515444 | validation: 1.522700463614798]
	TIME [epoch: 0.7 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6023339191280581		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.6023339191280581 | validation: 1.01181909915776]
	TIME [epoch: 0.701 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5842621620500559		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.5842621620500559 | validation: 1.5997800059144607]
	TIME [epoch: 0.7 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6235066484420662		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.6235066484420662 | validation: 0.9823472722918452]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5202572654754603		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.5202572654754603 | validation: 1.3840780336736211]
	TIME [epoch: 0.701 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6092327384829674		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.6092327384829674 | validation: 1.125222941504067]
	TIME [epoch: 0.702 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.510772298382834		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.510772298382834 | validation: 1.1224766048862904]
	TIME [epoch: 0.701 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4746354860184971		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.4746354860184971 | validation: 1.125117364180735]
	TIME [epoch: 0.701 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4686892464983408		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.4686892464983408 | validation: 1.0298471178272428]
	TIME [epoch: 0.703 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4638950080734947		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.4638950080734947 | validation: 1.6179062839591198]
	TIME [epoch: 0.703 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6333815187901064		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.6333815187901064 | validation: 0.9825479547384994]
	TIME [epoch: 0.703 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8710167202726724		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.8710167202726724 | validation: 1.7101630754237895]
	TIME [epoch: 0.702 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6629958615738794		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.6629958615738794 | validation: 1.2844894545905658]
	TIME [epoch: 0.709 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5061863777883122		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.5061863777883122 | validation: 0.9314322908599593]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5820911198701477		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.5820911198701477 | validation: 1.3088770601633124]
	TIME [epoch: 0.702 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.521011043104881		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.521011043104881 | validation: 1.1238907150976245]
	TIME [epoch: 0.703 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4784851503005434		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.4784851503005434 | validation: 1.0318673419137976]
	TIME [epoch: 0.703 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.473289402984991		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.473289402984991 | validation: 1.1796467568733369]
	TIME [epoch: 0.702 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.460641591821628		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.460641591821628 | validation: 1.0426077940159661]
	TIME [epoch: 0.702 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4483697808552967		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.4483697808552967 | validation: 1.1764401588295985]
	TIME [epoch: 0.703 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.444014198937989		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.444014198937989 | validation: 0.9525507686573914]
	TIME [epoch: 0.703 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4482938620537393		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.4482938620537393 | validation: 1.5394513545907949]
	TIME [epoch: 0.702 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5900063754366505		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.5900063754366505 | validation: 0.9344307868167867]
	TIME [epoch: 0.702 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6744752121658548		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.6744752121658548 | validation: 1.5967490054585274]
	TIME [epoch: 0.702 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5961347395018706		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.5961347395018706 | validation: 1.148289895695374]
	TIME [epoch: 0.703 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4468384615721943		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.4468384615721943 | validation: 0.9008995174243749]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5320818925590767		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.5320818925590767 | validation: 1.5034831575430052]
	TIME [epoch: 0.702 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.56784394816989		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.56784394816989 | validation: 1.0285274234814683]
	TIME [epoch: 0.705 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4302779964648482		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.4302779964648482 | validation: 0.9843361307885286]
	TIME [epoch: 0.702 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.430434868354305		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.430434868354305 | validation: 1.2557380941451517]
	TIME [epoch: 0.702 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4570458581232808		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.4570458581232808 | validation: 0.9455023097247637]
	TIME [epoch: 0.706 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4698514259085795		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.4698514259085795 | validation: 1.4752364722785438]
	TIME [epoch: 0.701 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.548108588374612		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.548108588374612 | validation: 0.9211125624575569]
	TIME [epoch: 0.701 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.464138911169968		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.464138911169968 | validation: 1.3621470992357656]
	TIME [epoch: 0.701 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.485744407723864		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.485744407723864 | validation: 1.1021620663930445]
	TIME [epoch: 0.702 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.469433060173689		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.469433060173689 | validation: 1.2655341462397123]
	TIME [epoch: 0.699 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4659806143880587		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.4659806143880587 | validation: 0.9624927444612834]
	TIME [epoch: 0.698 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4354213481455742		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.4354213481455742 | validation: 1.4531314834055902]
	TIME [epoch: 0.699 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5093212509145693		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.5093212509145693 | validation: 0.8968498578785996]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4708599784618839		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.4708599784618839 | validation: 1.3634052637958496]
	TIME [epoch: 0.7 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.475124436444525		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.475124436444525 | validation: 0.9097462453665948]
	TIME [epoch: 0.698 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.427485420130984		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.427485420130984 | validation: 1.236307817797307]
	TIME [epoch: 0.698 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4347497961484348		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.4347497961484348 | validation: 0.9419859292770593]
	TIME [epoch: 0.698 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4360659898246302		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.4360659898246302 | validation: 1.4400767895257696]
	TIME [epoch: 0.696 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4916027154080933		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.4916027154080933 | validation: 0.9174132975698686]
	TIME [epoch: 0.695 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4443428395401086		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.4443428395401086 | validation: 1.2855951181746166]
	TIME [epoch: 0.698 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4390121688691648		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.4390121688691648 | validation: 0.9178229238775608]
	TIME [epoch: 0.695 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4204710623365924		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.4204710623365924 | validation: 1.3389531065176428]
	TIME [epoch: 0.696 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4567165479556707		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.4567165479556707 | validation: 0.9103174396766385]
	TIME [epoch: 185 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4461595939950331		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.4461595939950331 | validation: 1.3607253259499201]
	TIME [epoch: 1.39 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.475244450501701		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.475244450501701 | validation: 0.9162807222544797]
	TIME [epoch: 1.36 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4000176208882353		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.4000176208882353 | validation: 1.2505743625641026]
	TIME [epoch: 1.36 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4103072407722996		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.4103072407722996 | validation: 1.0428237397358209]
	TIME [epoch: 1.36 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.458215681956648		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.458215681956648 | validation: 1.5207067690444926]
	TIME [epoch: 1.36 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5250897756687687		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.5250897756687687 | validation: 1.0052953121757733]
	TIME [epoch: 1.36 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.376874164653119		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.376874164653119 | validation: 1.1002743152908623]
	TIME [epoch: 1.36 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.356175227447409		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.356175227447409 | validation: 0.9034266984261795]
	TIME [epoch: 1.36 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3957188210046827		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.3957188210046827 | validation: 1.621832642152478]
	TIME [epoch: 1.36 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5825626036718756		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.5825626036718756 | validation: 0.8784226932975726]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.457394010433888		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.457394010433888 | validation: 1.2541933740651565]
	TIME [epoch: 1.36 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4100625637792366		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 1.4100625637792366 | validation: 1.0001807330616745]
	TIME [epoch: 1.36 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3813335597438898		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.3813335597438898 | validation: 1.2672150052856777]
	TIME [epoch: 1.36 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4005292890966967		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 1.4005292890966967 | validation: 1.0507809922097804]
	TIME [epoch: 1.36 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3893125073162815		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.3893125073162815 | validation: 1.238489501086276]
	TIME [epoch: 1.36 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3892193454510011		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.3892193454510011 | validation: 0.9622685210031858]
	TIME [epoch: 1.36 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.401805557574508		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.401805557574508 | validation: 1.4484567903007566]
	TIME [epoch: 1.36 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4819907892734023		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.4819907892734023 | validation: 0.8858638354374453]
	TIME [epoch: 1.36 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4646732325607774		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.4646732325607774 | validation: 1.3336474157205676]
	TIME [epoch: 1.36 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4379121963059296		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 1.4379121963059296 | validation: 0.9381864737336074]
	TIME [epoch: 1.37 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.34613915625737		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.34613915625737 | validation: 1.1297159060163044]
	TIME [epoch: 1.36 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3554341600709277		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 1.3554341600709277 | validation: 0.9844123494301009]
	TIME [epoch: 1.36 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.389446328885237		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.389446328885237 | validation: 1.4557781081557462]
	TIME [epoch: 1.36 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4582928800372288		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 1.4582928800372288 | validation: 0.9218126915279412]
	TIME [epoch: 1.36 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3720913351357609		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 1.3720913351357609 | validation: 1.2348599613836653]
	TIME [epoch: 1.36 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3841090313683442		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.3841090313683442 | validation: 0.9188421948250121]
	TIME [epoch: 1.36 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.382610734827262		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.382610734827262 | validation: 1.4091004816180686]
	TIME [epoch: 1.36 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4489235704179877		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 1.4489235704179877 | validation: 0.9211072017253304]
	TIME [epoch: 1.36 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3914706070173468		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.3914706070173468 | validation: 1.230604516176654]
	TIME [epoch: 1.38 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3725944402946473		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 1.3725944402946473 | validation: 1.0035515518947469]
	TIME [epoch: 1.36 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3529728909048873		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.3529728909048873 | validation: 1.2621725964218564]
	TIME [epoch: 1.36 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3801152005108765		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 1.3801152005108765 | validation: 1.0013366415590483]
	TIME [epoch: 1.36 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.349037204693092		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.349037204693092 | validation: 1.2762776101774502]
	TIME [epoch: 1.36 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3636564103652626		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 1.3636564103652626 | validation: 0.9233664331045305]
	TIME [epoch: 1.36 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.399057872215039		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.399057872215039 | validation: 1.4871328141835747]
	TIME [epoch: 1.36 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4916706870035064		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 1.4916706870035064 | validation: 0.8742878475799017]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_237.pth
	Model improved!!!
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3957673891947613		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 1.3957673891947613 | validation: 1.144432603016274]
	TIME [epoch: 1.36 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3356495596311972		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 1.3356495596311972 | validation: 0.9473153954176503]
	TIME [epoch: 1.36 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3322391174753283		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 1.3322391174753283 | validation: 1.2557064549817516]
	TIME [epoch: 1.36 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3566240787118213		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 1.3566240787118213 | validation: 1.0012042947004829]
	TIME [epoch: 1.36 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3527402917047786		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 1.3527402917047786 | validation: 1.3057196275595875]
	TIME [epoch: 1.36 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3752077586524063		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 1.3752077586524063 | validation: 0.9485821873294736]
	TIME [epoch: 1.37 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3438811109793642		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 1.3438811109793642 | validation: 1.2984037808616216]
	TIME [epoch: 1.36 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3781107817362237		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 1.3781107817362237 | validation: 0.8908301801729185]
	TIME [epoch: 1.36 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3973779696507622		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.3973779696507622 | validation: 1.3179681936865137]
	TIME [epoch: 1.36 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4108493419170918		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 1.4108493419170918 | validation: 0.9073874351698257]
	TIME [epoch: 1.36 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3392205337389829		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 1.3392205337389829 | validation: 1.1535670561471034]
	TIME [epoch: 1.36 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3183252861377464		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 1.3183252861377464 | validation: 1.018754576478855]
	TIME [epoch: 1.36 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3315422422689498		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 1.3315422422689498 | validation: 1.2483211614007153]
	TIME [epoch: 1.36 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3461214744934216		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 1.3461214744934216 | validation: 0.9672639051156953]
	TIME [epoch: 1.36 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3280184715185113		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 1.3280184715185113 | validation: 1.2052592275763727]
	TIME [epoch: 1.36 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3419894520330433		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 1.3419894520330433 | validation: 0.9191217702345352]
	TIME [epoch: 1.36 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3601440099002162		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 1.3601440099002162 | validation: 1.41096512960186]
	TIME [epoch: 1.36 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4227978050416277		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 1.4227978050416277 | validation: 0.9019462958847253]
	TIME [epoch: 1.36 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.371990134019535		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 1.371990134019535 | validation: 1.2377324818088389]
	TIME [epoch: 1.36 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3387680633897174		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 1.3387680633897174 | validation: 0.9357964759179627]
	TIME [epoch: 1.36 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2960060395398894		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 1.2960060395398894 | validation: 1.1605726030139007]
	TIME [epoch: 1.36 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3169264853519462		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 1.3169264853519462 | validation: 1.000630950211869]
	TIME [epoch: 1.36 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3406405932424803		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 1.3406405932424803 | validation: 1.2226795759547313]
	TIME [epoch: 1.37 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3417007021200973		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 1.3417007021200973 | validation: 0.940094129476774]
	TIME [epoch: 1.36 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3166478978811111		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 1.3166478978811111 | validation: 1.3735884275007366]
	TIME [epoch: 1.36 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3776785914359129		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 1.3776785914359129 | validation: 0.9214627368024346]
	TIME [epoch: 1.36 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3354701314597281		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 1.3354701314597281 | validation: 1.2567689163844045]
	TIME [epoch: 1.36 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3274429287769434		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 1.3274429287769434 | validation: 0.9152231397806297]
	TIME [epoch: 1.37 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3168317574721793		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 1.3168317574721793 | validation: 1.1845676720422738]
	TIME [epoch: 1.36 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3113246308369595		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 1.3113246308369595 | validation: 0.9168082823651931]
	TIME [epoch: 1.36 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3137149556633239		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 1.3137149556633239 | validation: 1.248523853227118]
	TIME [epoch: 1.36 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3249856203192494		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 1.3249856203192494 | validation: 0.9417919706410309]
	TIME [epoch: 1.36 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3226863827615136		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 1.3226863827615136 | validation: 1.2869262271520971]
	TIME [epoch: 1.36 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3305936820114734		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 1.3305936820114734 | validation: 0.9239982611145732]
	TIME [epoch: 1.36 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2957534971047426		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 1.2957534971047426 | validation: 1.213350831936604]
	TIME [epoch: 1.36 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3028760429068131		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 1.3028760429068131 | validation: 0.9198106992771736]
	TIME [epoch: 1.36 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3162933274198276		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 1.3162933274198276 | validation: 1.2242520450453995]
	TIME [epoch: 1.36 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3175983894989987		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 1.3175983894989987 | validation: 0.9242033368845475]
	TIME [epoch: 1.36 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3001158269740498		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 1.3001158269740498 | validation: 1.2087539786708112]
	TIME [epoch: 1.36 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2966223679796562		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 1.2966223679796562 | validation: 0.981789038120727]
	TIME [epoch: 1.36 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2795804504555743		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 1.2795804504555743 | validation: 1.2533053412434867]
	TIME [epoch: 1.36 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3049699489057072		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 1.3049699489057072 | validation: 0.9812675830126664]
	TIME [epoch: 1.36 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2898508771404718		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 1.2898508771404718 | validation: 1.2417752938748947]
	TIME [epoch: 1.36 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3066887580744921		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 1.3066887580744921 | validation: 0.9189129094590641]
	TIME [epoch: 1.36 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3115967421238117		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 1.3115967421238117 | validation: 1.2497921898739315]
	TIME [epoch: 1.36 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3302403207775613		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 1.3302403207775613 | validation: 0.9226943752894304]
	TIME [epoch: 1.36 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2816277260682158		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 1.2816277260682158 | validation: 1.2433916718020375]
	TIME [epoch: 1.36 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3014399205595595		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 1.3014399205595595 | validation: 0.9809291211457207]
	TIME [epoch: 1.36 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2941301069292148		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 1.2941301069292148 | validation: 1.285625386879143]
	TIME [epoch: 1.36 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3145494932035593		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 1.3145494932035593 | validation: 0.958907131299807]
	TIME [epoch: 1.36 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2768517909232882		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 1.2768517909232882 | validation: 1.1907861673529134]
	TIME [epoch: 1.36 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2813418184956515		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 1.2813418184956515 | validation: 0.9146456062838825]
	TIME [epoch: 1.36 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.262289361012285		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 1.262289361012285 | validation: 1.2250590916832256]
	TIME [epoch: 1.36 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2938368263402544		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 1.2938368263402544 | validation: 0.938024125952563]
	TIME [epoch: 1.36 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3157777522710457		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 1.3157777522710457 | validation: 1.341273804433658]
	TIME [epoch: 1.36 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3324066013315263		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 1.3324066013315263 | validation: 0.9430750648425532]
	TIME [epoch: 1.36 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2546812038869777		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 1.2546812038869777 | validation: 1.1165877678881777]
	TIME [epoch: 1.36 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2376161785600752		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 1.2376161785600752 | validation: 0.9725520393862379]
	TIME [epoch: 1.36 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.257485110357325		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 1.257485110357325 | validation: 1.2113951873776005]
	TIME [epoch: 1.36 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.280887000045345		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 1.280887000045345 | validation: 0.9617151538832994]
	TIME [epoch: 1.36 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.275116626784639		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 1.275116626784639 | validation: 1.283255453076481]
	TIME [epoch: 1.36 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2885145165881278		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 1.2885145165881278 | validation: 0.9334302381912256]
	TIME [epoch: 1.36 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2955248433911928		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 1.2955248433911928 | validation: 1.2733608601683357]
	TIME [epoch: 1.36 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3239058336171945		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 1.3239058336171945 | validation: 0.9079672420633598]
	TIME [epoch: 1.36 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2587764052186743		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 1.2587764052186743 | validation: 1.120154114701674]
	TIME [epoch: 1.36 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2425326071920113		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 1.2425326071920113 | validation: 0.9956431135195039]
	TIME [epoch: 1.36 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2391391245104384		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 1.2391391245104384 | validation: 1.1490623151118784]
	TIME [epoch: 1.36 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2318200376584214		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 1.2318200376584214 | validation: 0.9709406284068074]
	TIME [epoch: 1.36 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.253802097923392		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 1.253802097923392 | validation: 1.3151573449453302]
	TIME [epoch: 1.36 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3129136249429407		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 1.3129136249429407 | validation: 0.9440825193075011]
	TIME [epoch: 1.36 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.319925577692594		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 1.319925577692594 | validation: 1.2615127158879849]
	TIME [epoch: 1.36 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2887538042851707		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 1.2887538042851707 | validation: 0.927993483836877]
	TIME [epoch: 1.36 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2412449322595214		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 1.2412449322595214 | validation: 1.1115362475842028]
	TIME [epoch: 1.36 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2352562269151255		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 1.2352562269151255 | validation: 0.984614534149406]
	TIME [epoch: 1.36 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2514771234246866		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 1.2514771234246866 | validation: 1.191122894643318]
	TIME [epoch: 1.36 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2794741715779074		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 1.2794741715779074 | validation: 0.9383490825429579]
	TIME [epoch: 1.36 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2547072989275228		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 1.2547072989275228 | validation: 1.228070013831086]
	TIME [epoch: 1.36 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.265060790463779		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 1.265060790463779 | validation: 0.9588297275352204]
	TIME [epoch: 1.36 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2629777147133496		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 1.2629777147133496 | validation: 1.2485811812255947]
	TIME [epoch: 1.36 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2691120259855941		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 1.2691120259855941 | validation: 0.9750235783322799]
	TIME [epoch: 1.36 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.240359079170316		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 1.240359079170316 | validation: 1.1607890947088684]
	TIME [epoch: 1.36 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2446711127814616		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 1.2446711127814616 | validation: 0.9439192443722084]
	TIME [epoch: 1.36 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2459954863053202		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 1.2459954863053202 | validation: 1.2257523948135118]
	TIME [epoch: 1.36 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2652828561262797		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 1.2652828561262797 | validation: 0.9221333027421779]
	TIME [epoch: 1.36 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2522102688583503		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 1.2522102688583503 | validation: 1.1853768571757397]
	TIME [epoch: 1.36 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.235117256657142		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 1.235117256657142 | validation: 0.9649451819819067]
	TIME [epoch: 1.36 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2401579199727317		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 1.2401579199727317 | validation: 1.1926299934746494]
	TIME [epoch: 1.36 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2407674538470863		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 1.2407674538470863 | validation: 0.9779506682143135]
	TIME [epoch: 1.36 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2368826616974082		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 1.2368826616974082 | validation: 1.1819268470475692]
	TIME [epoch: 1.36 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2410389101749388		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 1.2410389101749388 | validation: 0.9580300272904347]
	TIME [epoch: 1.36 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2417569359758773		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 1.2417569359758773 | validation: 1.2132432613644013]
	TIME [epoch: 1.36 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2621524803716786		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 1.2621524803716786 | validation: 0.9528085760630305]
	TIME [epoch: 1.36 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2343124698945653		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 1.2343124698945653 | validation: 1.1787786678719037]
	TIME [epoch: 1.36 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2372624663580698		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 1.2372624663580698 | validation: 0.9651551160014084]
	TIME [epoch: 1.36 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2219885614952257		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 1.2219885614952257 | validation: 1.1907775262853093]
	TIME [epoch: 1.35 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.232108814741301		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 1.232108814741301 | validation: 0.955524311958005]
	TIME [epoch: 1.35 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2338877774860424		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 1.2338877774860424 | validation: 1.2177654664017243]
	TIME [epoch: 1.35 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2468249786994279		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 1.2468249786994279 | validation: 0.9690252374461906]
	TIME [epoch: 1.35 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2322156455036637		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 1.2322156455036637 | validation: 1.1719989628949488]
	TIME [epoch: 1.35 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2337527948567963		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 1.2337527948567963 | validation: 0.9646303132455043]
	TIME [epoch: 1.35 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2420944337140687		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 1.2420944337140687 | validation: 1.163413520244256]
	TIME [epoch: 1.35 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.242879528515805		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 1.242879528515805 | validation: 0.9672794229722381]
	TIME [epoch: 1.35 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.207566799851085		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 1.207566799851085 | validation: 1.1677757966969124]
	TIME [epoch: 1.35 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2106646887611625		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 1.2106646887611625 | validation: 0.9872381417684015]
	TIME [epoch: 1.36 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2480085191238701		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 1.2480085191238701 | validation: 1.280953534642308]
	TIME [epoch: 1.35 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.244379083073238		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 1.244379083073238 | validation: 0.955688428682856]
	TIME [epoch: 1.36 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2239351048547424		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 1.2239351048547424 | validation: 1.149179808874553]
	TIME [epoch: 1.35 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2153272397660997		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 1.2153272397660997 | validation: 0.9655483127978903]
	TIME [epoch: 1.35 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2180910374519642		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 1.2180910374519642 | validation: 1.1893603500430565]
	TIME [epoch: 1.35 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.238419838523611		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 1.238419838523611 | validation: 0.946556008745929]
	TIME [epoch: 1.35 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2254991966099564		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 1.2254991966099564 | validation: 1.1773408378046586]
	TIME [epoch: 1.35 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2235896448429424		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 1.2235896448429424 | validation: 0.9641858823963352]
	TIME [epoch: 1.35 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2301741718789447		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 1.2301741718789447 | validation: 1.1934925783035457]
	TIME [epoch: 1.35 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2201760828359998		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 1.2201760828359998 | validation: 0.9306263339222385]
	TIME [epoch: 1.35 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2148141134805055		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 1.2148141134805055 | validation: 1.1629588893153284]
	TIME [epoch: 1.36 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.211551049732867		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 1.211551049732867 | validation: 0.9418503920604027]
	TIME [epoch: 1.36 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.216734668927089		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 1.216734668927089 | validation: 1.1887593840162514]
	TIME [epoch: 1.36 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2336301502390934		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 1.2336301502390934 | validation: 0.9592207999946347]
	TIME [epoch: 1.36 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2055691332346217		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 1.2055691332346217 | validation: 1.1860615640709493]
	TIME [epoch: 1.35 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2044418432688067		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 1.2044418432688067 | validation: 0.9817658371528157]
	TIME [epoch: 1.36 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2095200504801178		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 1.2095200504801178 | validation: 1.1778504058851027]
	TIME [epoch: 1.36 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2118844618309634		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 1.2118844618309634 | validation: 0.9662983413546009]
	TIME [epoch: 1.36 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.218740920122607		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 1.218740920122607 | validation: 1.1896411396489979]
	TIME [epoch: 1.36 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.225262091050948		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 1.225262091050948 | validation: 0.9446245896350359]
	TIME [epoch: 1.36 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2118503813244923		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 1.2118503813244923 | validation: 1.157465983202674]
	TIME [epoch: 1.36 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.217694238978286		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 1.217694238978286 | validation: 0.9569807705800489]
	TIME [epoch: 1.36 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1934437692362154		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 1.1934437692362154 | validation: 1.1090386333126394]
	TIME [epoch: 1.36 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1826516203309092		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 1.1826516203309092 | validation: 1.0023186998612077]
	TIME [epoch: 1.36 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1879364258632916		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 1.1879364258632916 | validation: 1.2137621522798414]
	TIME [epoch: 1.36 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.215345483417791		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 1.215345483417791 | validation: 0.9986827450093071]
	TIME [epoch: 1.36 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.221143833809283		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 1.221143833809283 | validation: 1.1988960627294938]
	TIME [epoch: 1.36 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2125666085019975		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 1.2125666085019975 | validation: 0.9479818514097279]
	TIME [epoch: 1.36 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.20649614195554		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 1.20649614195554 | validation: 1.20039192089507]
	TIME [epoch: 1.37 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.227680759749627		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 1.227680759749627 | validation: 0.9430456794512406]
	TIME [epoch: 1.36 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1977142258824303		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 1.1977142258824303 | validation: 1.1266424413322842]
	TIME [epoch: 1.36 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1823341470260718		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 1.1823341470260718 | validation: 0.984006878932934]
	TIME [epoch: 1.36 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1785184962193085		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 1.1785184962193085 | validation: 1.1249266373696207]
	TIME [epoch: 1.36 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.180928519341975		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 1.180928519341975 | validation: 1.0056588918292027]
	TIME [epoch: 1.35 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.180949365702838		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 1.180949365702838 | validation: 1.2029354481012267]
	TIME [epoch: 1.35 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2067580755210263		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 1.2067580755210263 | validation: 0.9290204631445133]
	TIME [epoch: 1.35 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2381087144190983		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 1.2381087144190983 | validation: 1.1991125257549748]
	TIME [epoch: 1.35 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2423842595681571		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 1.2423842595681571 | validation: 0.9632222594199192]
	TIME [epoch: 1.35 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1887509850425213		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 1.1887509850425213 | validation: 1.1139220518808213]
	TIME [epoch: 1.35 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1792173501931662		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 1.1792173501931662 | validation: 1.0011222350333548]
	TIME [epoch: 1.35 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1629294321593664		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 1.1629294321593664 | validation: 1.108449559597105]
	TIME [epoch: 1.35 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1773062788539124		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 1.1773062788539124 | validation: 0.9703829624731175]
	TIME [epoch: 1.35 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.173650758773596		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 1.173650758773596 | validation: 1.1739091745501034]
	TIME [epoch: 1.35 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1963411727842512		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 1.1963411727842512 | validation: 0.9279455471456848]
	TIME [epoch: 1.35 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.216464452473809		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 1.216464452473809 | validation: 1.1987195781709]
	TIME [epoch: 1.35 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2263446277824699		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 1.2263446277824699 | validation: 0.9433527182361708]
	TIME [epoch: 1.35 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1939189937643948		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 1.1939189937643948 | validation: 1.0888734909155748]
	TIME [epoch: 1.36 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.169655964674702		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 1.169655964674702 | validation: 0.9764054435250302]
	TIME [epoch: 1.35 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1671879116944603		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 1.1671879116944603 | validation: 1.1155978670607627]
	TIME [epoch: 1.36 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1684527814212808		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 1.1684527814212808 | validation: 0.9785886685517504]
	TIME [epoch: 1.35 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1816122066133035		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 1.1816122066133035 | validation: 1.181644626051705]
	TIME [epoch: 1.35 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2022434981169747		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 1.2022434981169747 | validation: 0.9408650002690071]
	TIME [epoch: 1.35 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2073823672722201		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 1.2073823672722201 | validation: 1.1824381919885185]
	TIME [epoch: 1.35 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.189115850183688		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 1.189115850183688 | validation: 0.9781493998417128]
	TIME [epoch: 1.35 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.171351560102026		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 1.171351560102026 | validation: 1.0990334633922778]
	TIME [epoch: 1.36 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1713686544782718		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 1.1713686544782718 | validation: 0.9677535850954571]
	TIME [epoch: 1.35 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1584419761099225		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 1.1584419761099225 | validation: 1.1195436893835593]
	TIME [epoch: 1.35 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1812106414042287		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 1.1812106414042287 | validation: 0.9499864198470658]
	TIME [epoch: 1.35 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1966721398303517		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 1.1966721398303517 | validation: 1.1747731970626583]
	TIME [epoch: 1.35 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.190791223590798		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 1.190791223590798 | validation: 0.9630339478755165]
	TIME [epoch: 1.36 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1792954801468825		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 1.1792954801468825 | validation: 1.1527352685549312]
	TIME [epoch: 1.36 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.175030260877193		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 1.175030260877193 | validation: 0.9586089351886108]
	TIME [epoch: 1.36 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1680611024495173		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 1.1680611024495173 | validation: 1.132082656808188]
	TIME [epoch: 1.36 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.162989499585679		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 1.162989499585679 | validation: 0.9697833753700693]
	TIME [epoch: 1.36 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.165136654268501		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 1.165136654268501 | validation: 1.1347000852128448]
	TIME [epoch: 1.36 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1771872187373547		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 1.1771872187373547 | validation: 0.9338497020915331]
	TIME [epoch: 1.36 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1732665402232996		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 1.1732665402232996 | validation: 1.1281197350085237]
	TIME [epoch: 1.36 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1758797986543585		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 1.1758797986543585 | validation: 0.9384504874708514]
	TIME [epoch: 1.36 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1593366688095232		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 1.1593366688095232 | validation: 1.138166880173692]
	TIME [epoch: 1.36 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1692902729785433		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 1.1692902729785433 | validation: 0.9326297884351611]
	TIME [epoch: 1.36 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1585234482434974		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 1.1585234482434974 | validation: 1.1238332537071174]
	TIME [epoch: 1.36 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.169853412485728		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 1.169853412485728 | validation: 0.9631313568720685]
	TIME [epoch: 1.36 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1638353358366917		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 1.1638353358366917 | validation: 1.1156145800385766]
	TIME [epoch: 1.36 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.155528780410874		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 1.155528780410874 | validation: 0.9720474197164699]
	TIME [epoch: 1.36 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1590780660987565		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 1.1590780660987565 | validation: 1.1046319456443277]
	TIME [epoch: 1.36 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.157647269736145		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 1.157647269736145 | validation: 0.9430988416708299]
	TIME [epoch: 1.36 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1636253419592744		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 1.1636253419592744 | validation: 1.125722821748523]
	TIME [epoch: 1.36 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1726158433905511		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 1.1726158433905511 | validation: 0.9530177992744456]
	TIME [epoch: 1.36 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.159533930148947		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 1.159533930148947 | validation: 1.122415482774958]
	TIME [epoch: 1.36 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1615756782431514		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 1.1615756782431514 | validation: 0.9483270951996623]
	TIME [epoch: 1.36 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1590244932172469		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 1.1590244932172469 | validation: 1.1433884670067431]
	TIME [epoch: 1.36 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.16238157173738		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 1.16238157173738 | validation: 0.9364391686737896]
	TIME [epoch: 1.36 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.162927879501078		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 1.162927879501078 | validation: 1.12696895925096]
	TIME [epoch: 1.36 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1663888247495746		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 1.1663888247495746 | validation: 0.9645443633419483]
	TIME [epoch: 1.36 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.13527389987775		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 1.13527389987775 | validation: 1.0732306140249432]
	TIME [epoch: 1.35 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1329256620467751		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 1.1329256620467751 | validation: 0.9908426927291041]
	TIME [epoch: 1.35 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1363796545184774		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 1.1363796545184774 | validation: 1.124425499124174]
	TIME [epoch: 1.35 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1387920168813255		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 1.1387920168813255 | validation: 0.9446715546728076]
	TIME [epoch: 1.35 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1738736507778886		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 1.1738736507778886 | validation: 1.1639252846734818]
	TIME [epoch: 1.35 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.182346810286325		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 1.182346810286325 | validation: 0.9334126074309858]
	TIME [epoch: 1.35 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1598343540929208		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 1.1598343540929208 | validation: 1.0647273314324128]
	TIME [epoch: 1.35 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1298934110621957		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 1.1298934110621957 | validation: 0.9470157538628471]
	TIME [epoch: 1.35 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.121995591273241		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 1.121995591273241 | validation: 1.0463260952585474]
	TIME [epoch: 1.35 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1083755818914935		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 1.1083755818914935 | validation: 0.9584581628718487]
	TIME [epoch: 1.35 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.12571539097026		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 1.12571539097026 | validation: 1.1176518459258287]
	TIME [epoch: 1.35 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1606450859804136		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 1.1606450859804136 | validation: 0.9336789903049738]
	TIME [epoch: 1.35 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1716685438607328		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 1.1716685438607328 | validation: 1.1633779722748163]
	TIME [epoch: 1.35 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1825468038372937		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 1.1825468038372937 | validation: 0.9205409523041909]
	TIME [epoch: 1.36 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1412441248667125		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 1.1412441248667125 | validation: 1.0477209227928228]
	TIME [epoch: 1.35 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.118575985949807		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 1.118575985949807 | validation: 0.9623787856172975]
	TIME [epoch: 1.35 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1145690847060452		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 1.1145690847060452 | validation: 1.0508804768823579]
	TIME [epoch: 1.35 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1131232912563918		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 1.1131232912563918 | validation: 0.9511297011707063]
	TIME [epoch: 1.35 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1246176938497827		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 1.1246176938497827 | validation: 1.1297442889887224]
	TIME [epoch: 1.35 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1532462209509513		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 1.1532462209509513 | validation: 0.8984716394064445]
	TIME [epoch: 1.35 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1744504549936547		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 1.1744504549936547 | validation: 1.1270941455561891]
	TIME [epoch: 1.35 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1591981542596832		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 1.1591981542596832 | validation: 0.9458953140411753]
	TIME [epoch: 1.35 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.124763578444094		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 1.124763578444094 | validation: 1.0436150542343765]
	TIME [epoch: 1.35 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1065092661091713		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 1.1065092661091713 | validation: 0.9573461033493589]
	TIME [epoch: 1.35 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0998747146718468		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 1.0998747146718468 | validation: 1.0088096817639118]
	TIME [epoch: 1.35 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1036256486954668		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 1.1036256486954668 | validation: 0.9580002545660294]
	TIME [epoch: 1.35 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1135707817862481		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 1.1135707817862481 | validation: 1.1058025347870253]
	TIME [epoch: 1.35 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1331843106546922		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 1.1331843106546922 | validation: 0.924327421381226]
	TIME [epoch: 1.35 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1608001476310015		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 1.1608001476310015 | validation: 1.142180662883442]
	TIME [epoch: 1.35 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1850706619695046		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 1.1850706619695046 | validation: 0.9091270582342088]
	TIME [epoch: 1.35 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1508478294478357		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 1.1508478294478357 | validation: 1.0729272425566996]
	TIME [epoch: 1.35 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1103664681025234		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 1.1103664681025234 | validation: 0.9366457762690994]
	TIME [epoch: 1.35 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1079613268791408		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 1.1079613268791408 | validation: 1.0502130752245378]
	TIME [epoch: 1.35 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1060662274507407		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 1.1060662274507407 | validation: 0.9577422565072834]
	TIME [epoch: 1.35 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1068260918707509		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 1.1068260918707509 | validation: 1.0597988090391486]
	TIME [epoch: 1.36 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1129990278336075		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 1.1129990278336075 | validation: 0.91635506056044]
	TIME [epoch: 1.36 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1462684795057558		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 1.1462684795057558 | validation: 1.1258896094900408]
	TIME [epoch: 1.35 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1513617556524909		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 1.1513617556524909 | validation: 0.9235698162452043]
	TIME [epoch: 1.35 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1165509725748803		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 1.1165509725748803 | validation: 1.0526878938621163]
	TIME [epoch: 1.36 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0968625119600954		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 1.0968625119600954 | validation: 0.950793523818817]
	TIME [epoch: 1.35 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0997551957926226		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 1.0997551957926226 | validation: 1.066313871391649]
	TIME [epoch: 1.35 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1086386034885454		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 1.1086386034885454 | validation: 0.922103536764849]
	TIME [epoch: 1.35 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1254100712689836		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 1.1254100712689836 | validation: 1.102988089172074]
	TIME [epoch: 1.35 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.14059422006677		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 1.14059422006677 | validation: 0.9277279659119074]
	TIME [epoch: 1.35 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1197350981853782		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 1.1197350981853782 | validation: 1.0748713923881]
	TIME [epoch: 1.35 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1004376263111082		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 1.1004376263111082 | validation: 0.9393203146224849]
	TIME [epoch: 1.35 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0898396284186989		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 1.0898396284186989 | validation: 1.0464843272579483]
	TIME [epoch: 1.35 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1079653432947763		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 1.1079653432947763 | validation: 0.9210376033503529]
	TIME [epoch: 1.35 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0997848472173044		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 1.0997848472173044 | validation: 1.083286579998695]
	TIME [epoch: 1.35 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.114562494507776		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 1.114562494507776 | validation: 0.9077061116487493]
	TIME [epoch: 1.35 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1200629155009683		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 1.1200629155009683 | validation: 1.0981436865917136]
	TIME [epoch: 1.35 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1078006007775711		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 1.1078006007775711 | validation: 0.8985520104464092]
	TIME [epoch: 1.35 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1158417593613015		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 1.1158417593613015 | validation: 1.085320213921617]
	TIME [epoch: 1.35 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1052928842479353		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 1.1052928842479353 | validation: 0.9203770135858863]
	TIME [epoch: 1.35 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.097189519923781		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 1.097189519923781 | validation: 1.0611091899467486]
	TIME [epoch: 1.35 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1002756394263429		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 1.1002756394263429 | validation: 0.9320204748356837]
	TIME [epoch: 1.35 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.082258217209924		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 1.082258217209924 | validation: 1.0095256807642863]
	TIME [epoch: 1.35 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0747661170930405		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 1.0747661170930405 | validation: 0.9351737357813156]
	TIME [epoch: 1.36 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0803028223383622		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 1.0803028223383622 | validation: 1.0711958601156728]
	TIME [epoch: 1.36 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.098789252729824		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 1.098789252729824 | validation: 0.8907004681594146]
	TIME [epoch: 1.36 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1189368984284647		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 1.1189368984284647 | validation: 1.1335873120241808]
	TIME [epoch: 1.36 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1371406157941686		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 1.1371406157941686 | validation: 0.8925151166498694]
	TIME [epoch: 1.36 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0890792807085785		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 1.0890792807085785 | validation: 1.017329759966852]
	TIME [epoch: 1.36 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.072332086171354		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 1.072332086171354 | validation: 0.9450361092835902]
	TIME [epoch: 1.35 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0682581780182556		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 1.0682581780182556 | validation: 0.98954201636252]
	TIME [epoch: 1.35 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0619742096134361		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 1.0619742096134361 | validation: 0.9616771308941973]
	TIME [epoch: 1.35 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0544072763853918		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 1.0544072763853918 | validation: 1.0330025302797778]
	TIME [epoch: 1.35 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.082782166640453		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 1.082782166640453 | validation: 0.8694947975534412]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_493.pth
	Model improved!!!
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1227490171077		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 1.1227490171077 | validation: 1.170685849492539]
	TIME [epoch: 1.36 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.185640835732021		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 1.185640835732021 | validation: 0.8893872453131544]
	TIME [epoch: 1.36 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.094336781874646		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 1.094336781874646 | validation: 0.9808576186508371]
	TIME [epoch: 1.36 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0543882270743434		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 1.0543882270743434 | validation: 0.9560698786983588]
	TIME [epoch: 1.36 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0593751829625115		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 1.0593751829625115 | validation: 0.943626968294791]
	TIME [epoch: 1.36 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0565117593850486		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 1.0565117593850486 | validation: 0.9766502966180781]
	TIME [epoch: 1.36 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0482780682870634		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 1.0482780682870634 | validation: 0.9257091999095511]
	TIME [epoch: 1.36 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0506508021823893		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 1.0506508021823893 | validation: 1.0318195619348216]
	TIME [epoch: 177 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.066958710831589		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 1.066958710831589 | validation: 0.911500417849028]
	TIME [epoch: 2.68 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1161974464500513		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 1.1161974464500513 | validation: 1.2027938116425585]
	TIME [epoch: 2.68 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1869590034053115		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 1.1869590034053115 | validation: 0.8859756509908833]
	TIME [epoch: 2.68 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0783071348227284		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 1.0783071348227284 | validation: 0.9863302648147368]
	TIME [epoch: 2.67 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0500552227501982		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 1.0500552227501982 | validation: 0.9340426992238937]
	TIME [epoch: 2.68 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0437837107205445		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 1.0437837107205445 | validation: 0.9546799210240086]
	TIME [epoch: 2.67 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0446342568885492		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 1.0446342568885492 | validation: 0.9502391140801578]
	TIME [epoch: 2.68 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0416609366050957		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 1.0416609366050957 | validation: 0.9541523943717574]
	TIME [epoch: 2.68 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0379495640690894		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 1.0379495640690894 | validation: 0.9607821696388037]
	TIME [epoch: 2.69 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0365952179590954		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 1.0365952179590954 | validation: 0.9347868550110445]
	TIME [epoch: 2.68 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0432692654227647		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 1.0432692654227647 | validation: 1.0614526308747008]
	TIME [epoch: 2.67 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0782096681581235		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 1.0782096681581235 | validation: 0.866107786219332]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_513.pth
	Model improved!!!
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2072003358684449		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 1.2072003358684449 | validation: 1.0913100337397]
	TIME [epoch: 2.67 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1143770285474004		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 1.1143770285474004 | validation: 0.8943773695586977]
	TIME [epoch: 2.68 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0401124421677035		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 1.0401124421677035 | validation: 0.9350294068891775]
	TIME [epoch: 2.67 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0289394448503726		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 1.0289394448503726 | validation: 0.9787804723538014]
	TIME [epoch: 2.67 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0430461338522468		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 1.0430461338522468 | validation: 0.9007988753645612]
	TIME [epoch: 2.67 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.047652362409899		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 1.047652362409899 | validation: 1.0582777712683775]
	TIME [epoch: 2.67 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0723271474467846		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 1.0723271474467846 | validation: 0.8817489118071116]
	TIME [epoch: 2.68 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0744593673826381		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 1.0744593673826381 | validation: 1.0184961210644765]
	TIME [epoch: 2.67 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0623985100333295		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 1.0623985100333295 | validation: 0.8880055327926026]
	TIME [epoch: 2.68 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.037101110287684		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 1.037101110287684 | validation: 0.9898916097122972]
	TIME [epoch: 2.67 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.040259920910617		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 1.040259920910617 | validation: 0.9003436073104272]
	TIME [epoch: 2.68 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0343360791063536		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 1.0343360791063536 | validation: 0.9912795949241892]
	TIME [epoch: 2.67 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0335941415848289		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 1.0335941415848289 | validation: 0.8957517725899479]
	TIME [epoch: 2.68 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.044159390961611		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 1.044159390961611 | validation: 1.070706227514274]
	TIME [epoch: 2.67 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.06077597734052		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 1.06077597734052 | validation: 0.8496124255157483]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_528.pth
	Model improved!!!
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0761552228827385		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 1.0761552228827385 | validation: 1.0374668622886227]
	TIME [epoch: 2.68 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.060149817851293		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 1.060149817851293 | validation: 0.8711505334008107]
	TIME [epoch: 2.68 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.048698599439602		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 1.048698599439602 | validation: 0.9881959604636604]
	TIME [epoch: 2.68 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0356074653977485		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 1.0356074653977485 | validation: 0.8773084898473025]
	TIME [epoch: 2.67 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0256338628072146		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 1.0256338628072146 | validation: 0.9989479963975113]
	TIME [epoch: 2.68 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0352009642324138		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 1.0352009642324138 | validation: 0.8818105132277072]
	TIME [epoch: 2.67 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0376593841520103		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 1.0376593841520103 | validation: 1.025733325903266]
	TIME [epoch: 2.68 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.045623075460703		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 1.045623075460703 | validation: 0.8620866617931393]
	TIME [epoch: 2.67 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0555848235229002		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 1.0555848235229002 | validation: 1.0262589797281596]
	TIME [epoch: 2.68 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0471031187638216		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 1.0471031187638216 | validation: 0.8784552174017833]
	TIME [epoch: 2.68 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.02829576412846		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 1.02829576412846 | validation: 0.9692913568738235]
	TIME [epoch: 2.68 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0196289607551683		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 1.0196289607551683 | validation: 0.8809300747963591]
	TIME [epoch: 2.68 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0186901559106047		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 1.0186901559106047 | validation: 1.0238530843353726]
	TIME [epoch: 2.67 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0276667219539588		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 1.0276667219539588 | validation: 0.8335030253017598]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_542.pth
	Model improved!!!
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0524691354385685		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 1.0524691354385685 | validation: 1.0494441083634725]
	TIME [epoch: 2.69 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0661576910348314		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 1.0661576910348314 | validation: 0.8713150943612352]
	TIME [epoch: 2.68 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0371817315085072		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 1.0371817315085072 | validation: 0.9701975821458273]
	TIME [epoch: 2.68 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0201872552512106		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 1.0201872552512106 | validation: 0.8821596594904174]
	TIME [epoch: 2.68 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0047627802007937		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 1.0047627802007937 | validation: 0.9385168388095769]
	TIME [epoch: 2.68 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.00244436396121		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 1.00244436396121 | validation: 0.889419611977475]
	TIME [epoch: 2.68 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.009184685435624		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 1.009184685435624 | validation: 0.9894067508252885]
	TIME [epoch: 2.68 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0171058560271615		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 1.0171058560271615 | validation: 0.8621268905379836]
	TIME [epoch: 2.68 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0345802107547994		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 1.0345802107547994 | validation: 1.076871191536531]
	TIME [epoch: 2.68 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.078784838915782		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 1.078784838915782 | validation: 0.8277315149284846]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_552.pth
	Model improved!!!
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0465994527640647		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 1.0465994527640647 | validation: 0.9632168780630187]
	TIME [epoch: 2.69 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0067753105549146		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 1.0067753105549146 | validation: 0.8655898164582392]
	TIME [epoch: 2.68 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0060349675766527		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 1.0060349675766527 | validation: 0.9812628124103306]
	TIME [epoch: 2.68 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.009959756256251		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 1.009959756256251 | validation: 0.8570195381677149]
	TIME [epoch: 2.68 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0154480383554978		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 1.0154480383554978 | validation: 0.9962134293267124]
	TIME [epoch: 2.68 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0096577254838952		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 1.0096577254838952 | validation: 0.8280268005069337]
	TIME [epoch: 2.67 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0186363748665974		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 1.0186363748665974 | validation: 1.031336611424077]
	TIME [epoch: 2.68 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0252279367635784		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 1.0252279367635784 | validation: 0.8460890844956636]
	TIME [epoch: 2.68 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0221953420099323		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 1.0221953420099323 | validation: 0.9606517727731906]
	TIME [epoch: 2.68 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0051071571172747		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 1.0051071571172747 | validation: 0.8704217900632374]
	TIME [epoch: 2.68 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9953766041587551		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.9953766041587551 | validation: 0.9476222106967822]
	TIME [epoch: 2.68 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9886970179050587		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.9886970179050587 | validation: 0.846801232246661]
	TIME [epoch: 2.68 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9859911909566842		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.9859911909566842 | validation: 0.9891550108928266]
	TIME [epoch: 2.68 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0044517953915664		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 1.0044517953915664 | validation: 0.8318806268514397]
	TIME [epoch: 2.67 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0333678640813744		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 1.0333678640813744 | validation: 1.0413717665120625]
	TIME [epoch: 2.67 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0384012754397804		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 1.0384012754397804 | validation: 0.8414617120727649]
	TIME [epoch: 2.68 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.010762171480699		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 1.010762171480699 | validation: 0.9377459577941206]
	TIME [epoch: 2.68 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9931389042460095		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.9931389042460095 | validation: 0.8511850991702741]
	TIME [epoch: 2.68 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9819631240190694		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.9819631240190694 | validation: 0.9749693302766845]
	TIME [epoch: 2.67 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9914522497007104		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.9914522497007104 | validation: 0.8381848335236536]
	TIME [epoch: 2.68 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0005525806992928		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 1.0005525806992928 | validation: 1.0033767638359683]
	TIME [epoch: 2.68 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0091603043664512		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 1.0091603043664512 | validation: 0.8381533213328032]
	TIME [epoch: 2.68 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9960885266401149		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.9960885266401149 | validation: 0.9602079698681302]
	TIME [epoch: 2.67 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.994096251994772		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.994096251994772 | validation: 0.8612530438394093]
	TIME [epoch: 2.67 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9864239365738056		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.9864239365738056 | validation: 0.9757891570178558]
	TIME [epoch: 2.67 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9880979741489939		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.9880979741489939 | validation: 0.8178747944623226]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_578.pth
	Model improved!!!
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0015695233610598		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 1.0015695233610598 | validation: 0.9885963173099619]
	TIME [epoch: 2.68 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9962558107057689		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.9962558107057689 | validation: 0.8365092785547876]
	TIME [epoch: 2.68 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9815749830115595		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.9815749830115595 | validation: 0.9680669648164208]
	TIME [epoch: 2.69 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9865037350081177		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.9865037350081177 | validation: 0.818610507711508]
	TIME [epoch: 2.68 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9934869115560483		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.9934869115560483 | validation: 0.9846511800967331]
	TIME [epoch: 2.68 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0023801650454442		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 1.0023801650454442 | validation: 0.8297267007392906]
	TIME [epoch: 2.68 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.993581910966605		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.993581910966605 | validation: 0.953592142049541]
	TIME [epoch: 2.68 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9780083250452681		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.9780083250452681 | validation: 0.8538808531530133]
	TIME [epoch: 2.68 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9719275589833696		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.9719275589833696 | validation: 0.9504950800852738]
	TIME [epoch: 2.68 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.957852279167569		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.957852279167569 | validation: 0.8686126651711021]
	TIME [epoch: 2.68 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9675791082486132		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.9675791082486132 | validation: 0.958351770457886]
	TIME [epoch: 2.68 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9701049177837114		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.9701049177837114 | validation: 0.8053725701580107]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_590.pth
	Model improved!!!
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9909729560752658		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.9909729560752658 | validation: 1.0246716000059148]
	TIME [epoch: 2.68 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.021091038234929		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 1.021091038234929 | validation: 0.8144782090168619]
	TIME [epoch: 2.68 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9958832551451877		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.9958832551451877 | validation: 0.912878107392235]
	TIME [epoch: 2.67 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9682966212973732		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.9682966212973732 | validation: 0.8411604717496672]
	TIME [epoch: 2.67 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9602194202538165		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.9602194202538165 | validation: 0.8997222295196186]
	TIME [epoch: 2.67 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9485024549092412		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.9485024549092412 | validation: 0.8353811382468515]
	TIME [epoch: 2.67 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9583534351714397		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.9583534351714397 | validation: 0.9547969956889575]
	TIME [epoch: 2.67 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9618400779570158		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.9618400779570158 | validation: 0.7883569524605223]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_598.pth
	Model improved!!!
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9917581970534352		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.9917581970534352 | validation: 1.0292702985472337]
	TIME [epoch: 2.69 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.014490061212626		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 1.014490061212626 | validation: 0.823371488958164]
	TIME [epoch: 2.68 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9764115809550178		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.9764115809550178 | validation: 0.911404625994167]
	TIME [epoch: 2.69 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.959283188556231		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.959283188556231 | validation: 0.8154305750398649]
	TIME [epoch: 2.68 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9515204149523078		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.9515204149523078 | validation: 0.9412928561715352]
	TIME [epoch: 2.69 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9621482597247504		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.9621482597247504 | validation: 0.7931144778177621]
	TIME [epoch: 2.68 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9683786659362243		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.9683786659362243 | validation: 0.9815119412347277]
	TIME [epoch: 2.68 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9814420518305668		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.9814420518305668 | validation: 0.8176418056052404]
	TIME [epoch: 2.68 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9668106920164726		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.9668106920164726 | validation: 0.9321462639787111]
	TIME [epoch: 2.68 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9606157842332607		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.9606157842332607 | validation: 0.8380779103274325]
	TIME [epoch: 2.68 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9368973701220165		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.9368973701220165 | validation: 0.9314747169523621]
	TIME [epoch: 2.69 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9413986692304246		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.9413986692304246 | validation: 0.8143458224363388]
	TIME [epoch: 2.69 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9612407823876806		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.9612407823876806 | validation: 0.9652707711557732]
	TIME [epoch: 2.68 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.960417112740745		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.960417112740745 | validation: 0.8124965747591191]
	TIME [epoch: 2.69 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9670285020603027		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.9670285020603027 | validation: 0.9316426615844131]
	TIME [epoch: 2.69 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9579014429159068		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.9579014429159068 | validation: 0.7960101929278097]
	TIME [epoch: 2.69 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9529359755904707		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.9529359755904707 | validation: 0.9333198939634662]
	TIME [epoch: 2.69 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9398588556076846		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.9398588556076846 | validation: 0.8104607330461763]
	TIME [epoch: 2.69 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9486845556548835		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.9486845556548835 | validation: 0.960640239135599]
	TIME [epoch: 2.69 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9589146965363671		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.9589146965363671 | validation: 0.8140704131014683]
	TIME [epoch: 2.69 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9431123579193266		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.9431123579193266 | validation: 0.919803836808148]
	TIME [epoch: 2.69 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9446506160142049		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.9446506160142049 | validation: 0.8203306880804558]
	TIME [epoch: 2.68 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9431874061755611		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.9431874061755611 | validation: 0.9665683147052185]
	TIME [epoch: 2.69 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.958393653088362		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.958393653088362 | validation: 0.8087247564657375]
	TIME [epoch: 2.69 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9641546345905976		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.9641546345905976 | validation: 0.9268508979433193]
	TIME [epoch: 2.69 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9492639792461987		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.9492639792461987 | validation: 0.7939428107225863]
	TIME [epoch: 2.68 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9487509520782226		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.9487509520782226 | validation: 0.9336564841483055]
	TIME [epoch: 2.69 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9391764688089508		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.9391764688089508 | validation: 0.7880838156692977]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_626.pth
	Model improved!!!
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9353136360424487		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.9353136360424487 | validation: 0.8816274551895223]
	TIME [epoch: 2.68 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9282488281906681		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.9282488281906681 | validation: 0.7825801589821102]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_628.pth
	Model improved!!!
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9425868154832122		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.9425868154832122 | validation: 0.9375996966128168]
	TIME [epoch: 2.68 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.940861031683767		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.940861031683767 | validation: 0.787468418623047]
	TIME [epoch: 2.67 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9474843479124191		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.9474843479124191 | validation: 0.9319588663657946]
	TIME [epoch: 2.67 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9419767273154466		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.9419767273154466 | validation: 0.8018451476485032]
	TIME [epoch: 2.67 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9315476641774229		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.9315476641774229 | validation: 0.9168953673197988]
	TIME [epoch: 2.67 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9276978211485327		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.9276978211485327 | validation: 0.7933965821627971]
	TIME [epoch: 2.67 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9313313906452312		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.9313313906452312 | validation: 0.9223481451250932]
	TIME [epoch: 2.68 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9287605435407439		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.9287605435407439 | validation: 0.7970178880681726]
	TIME [epoch: 2.67 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9287986987966614		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.9287986987966614 | validation: 0.9192997900578878]
	TIME [epoch: 2.67 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9267431173234512		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.9267431173234512 | validation: 0.7984098284548083]
	TIME [epoch: 2.67 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.933033897216724		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.933033897216724 | validation: 0.9481893272121676]
	TIME [epoch: 2.67 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9382173776851744		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.9382173776851744 | validation: 0.7841473625128537]
	TIME [epoch: 2.67 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9359381745559796		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.9359381745559796 | validation: 0.893517343673933]
	TIME [epoch: 2.67 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9222193096833357		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.9222193096833357 | validation: 0.7596165315482724]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_642.pth
	Model improved!!!
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9357428533817086		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.9357428533817086 | validation: 0.9559122270822299]
	TIME [epoch: 2.68 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9312986153215116		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.9312986153215116 | validation: 0.7839784890542376]
	TIME [epoch: 2.68 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.914563373872584		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.914563373872584 | validation: 0.8903040415074289]
	TIME [epoch: 2.68 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9222638109613985		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.9222638109613985 | validation: 0.8008456881547059]
	TIME [epoch: 2.68 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9265590637874058		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.9265590637874058 | validation: 0.9449596853940938]
	TIME [epoch: 2.67 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9398936965017103		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.9398936965017103 | validation: 0.7875456636657606]
	TIME [epoch: 2.67 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9254928787131622		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.9254928787131622 | validation: 0.9062161978157537]
	TIME [epoch: 2.67 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9168829693925598		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.9168829693925598 | validation: 0.7790921942192273]
	TIME [epoch: 2.67 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9115977278863274		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.9115977278863274 | validation: 0.9152969641919919]
	TIME [epoch: 2.67 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9107357143417818		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.9107357143417818 | validation: 0.7578969240356387]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_652.pth
	Model improved!!!
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9131903499469783		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.9131903499469783 | validation: 0.9090560166812431]
	TIME [epoch: 2.67 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9183649463615495		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.9183649463615495 | validation: 0.7497662817842535]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_654.pth
	Model improved!!!
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9215090336981723		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.9215090336981723 | validation: 0.901808853440811]
	TIME [epoch: 2.68 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9096872286103324		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.9096872286103324 | validation: 0.772144782393088]
	TIME [epoch: 2.67 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9068157707554321		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.9068157707554321 | validation: 0.8927528151349607]
	TIME [epoch: 2.68 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9054573281191407		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.9054573281191407 | validation: 0.7605736310791072]
	TIME [epoch: 2.67 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.915530076045501		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.915530076045501 | validation: 0.9230795700013298]
	TIME [epoch: 2.67 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9110608476259935		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.9110608476259935 | validation: 0.7542272853981155]
	TIME [epoch: 2.67 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9146842528574461		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.9146842528574461 | validation: 0.8991389062306949]
	TIME [epoch: 2.67 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9155287364147314		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.9155287364147314 | validation: 0.7856924556082896]
	TIME [epoch: 2.67 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9081795066760053		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.9081795066760053 | validation: 0.887231067277705]
	TIME [epoch: 2.67 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8958229960061448		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.8958229960061448 | validation: 0.7737118574851339]
	TIME [epoch: 2.68 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9011998569030959		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.9011998569030959 | validation: 0.8893340958932886]
	TIME [epoch: 2.67 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9049477811479036		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.9049477811479036 | validation: 0.7478750500496024]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_666.pth
	Model improved!!!
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9072004678434061		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.9072004678434061 | validation: 0.9096052399897933]
	TIME [epoch: 2.67 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9132903280346448		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.9132903280346448 | validation: 0.7545144015274373]
	TIME [epoch: 2.68 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9111408643388604		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.9111408643388604 | validation: 0.8895919186540815]
	TIME [epoch: 2.67 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9001593856734326		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.9001593856734326 | validation: 0.7558496381379646]
	TIME [epoch: 2.67 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8879706738466089		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.8879706738466089 | validation: 0.8756840211598882]
	TIME [epoch: 2.67 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8896800667810458		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.8896800667810458 | validation: 0.7631699368607698]
	TIME [epoch: 2.67 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8892151195656007		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.8892151195656007 | validation: 0.8986855141494152]
	TIME [epoch: 2.67 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.895524335178374		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.895524335178374 | validation: 0.7260283483884078]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_674.pth
	Model improved!!!
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9120593672488145		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.9120593672488145 | validation: 0.9096893432139679]
	TIME [epoch: 2.68 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9102300122264957		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.9102300122264957 | validation: 0.7710839744779523]
	TIME [epoch: 2.68 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8911417957437476		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.8911417957437476 | validation: 0.8563360729108631]
	TIME [epoch: 2.68 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8864925929186463		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.8864925929186463 | validation: 0.7574958054734534]
	TIME [epoch: 2.68 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8812877018396106		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.8812877018396106 | validation: 0.8335869299919509]
	TIME [epoch: 2.68 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8773600659922274		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.8773600659922274 | validation: 0.7843496178183029]
	TIME [epoch: 2.67 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8751375758523642		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.8751375758523642 | validation: 0.8844310672398917]
	TIME [epoch: 2.68 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8928239655343386		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.8928239655343386 | validation: 0.6960325445957124]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_682.pth
	Model improved!!!
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.927734057375652		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.927734057375652 | validation: 0.9549045567600198]
	TIME [epoch: 2.69 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9217971549059442		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.9217971549059442 | validation: 0.7360183415699852]
	TIME [epoch: 2.69 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8802935406018602		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.8802935406018602 | validation: 0.8339262326963928]
	TIME [epoch: 2.69 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.874985387245306		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.874985387245306 | validation: 0.7794248008106982]
	TIME [epoch: 2.67 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.867069127705592		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.867069127705592 | validation: 0.8254525263977069]
	TIME [epoch: 2.68 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.867972812965383		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.867972812965383 | validation: 0.7585139234382079]
	TIME [epoch: 2.68 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8624320192097278		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.8624320192097278 | validation: 0.8833201347054557]
	TIME [epoch: 2.68 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8981314795424251		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.8981314795424251 | validation: 0.7166389058883423]
	TIME [epoch: 2.67 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9282636440258092		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.9282636440258092 | validation: 0.9109683153870961]
	TIME [epoch: 2.67 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9076925536518831		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.9076925536518831 | validation: 0.7491848976497836]
	TIME [epoch: 2.68 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8744396390667087		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.8744396390667087 | validation: 0.8314460027882948]
	TIME [epoch: 2.68 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8671645091904324		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.8671645091904324 | validation: 0.7670381225367405]
	TIME [epoch: 2.67 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8759681380028878		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.8759681380028878 | validation: 0.8316898544018969]
	TIME [epoch: 2.67 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8757246881671563		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.8757246881671563 | validation: 0.7089823985137198]
	TIME [epoch: 2.67 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8756598723558525		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.8756598723558525 | validation: 0.8955871898001413]
	TIME [epoch: 2.67 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.887045390795335		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.887045390795335 | validation: 0.7126571373227925]
	TIME [epoch: 2.67 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8830561745923666		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.8830561745923666 | validation: 0.8603271845888305]
	TIME [epoch: 2.67 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8782159339565471		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.8782159339565471 | validation: 0.7340889930046719]
	TIME [epoch: 2.68 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8693213964658463		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.8693213964658463 | validation: 0.8558986271299753]
	TIME [epoch: 2.69 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8783054838795991		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.8783054838795991 | validation: 0.7306571674169134]
	TIME [epoch: 2.69 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8672142532833574		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.8672142532833574 | validation: 0.8568898167791628]
	TIME [epoch: 2.69 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.870259839067602		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.870259839067602 | validation: 0.7091818676018163]
	TIME [epoch: 2.69 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8905373019720826		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.8905373019720826 | validation: 0.8870504322637404]
	TIME [epoch: 2.69 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8785811062245361		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.8785811062245361 | validation: 0.7213372415220146]
	TIME [epoch: 2.69 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8737975652448013		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.8737975652448013 | validation: 0.8664959085267656]
	TIME [epoch: 2.69 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8582766887350218		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.8582766887350218 | validation: 0.7327817473422203]
	TIME [epoch: 2.69 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8524687074784493		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.8524687074784493 | validation: 0.8762926665199061]
	TIME [epoch: 2.68 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8580959394844493		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.8580959394844493 | validation: 0.7165900881220062]
	TIME [epoch: 2.69 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8670882960243005		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.8670882960243005 | validation: 0.8515239951898818]
	TIME [epoch: 2.69 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.871503934930484		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.871503934930484 | validation: 0.7051253493103196]
	TIME [epoch: 2.69 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8692072400258212		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.8692072400258212 | validation: 0.8420830215390342]
	TIME [epoch: 2.69 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8626133506194782		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.8626133506194782 | validation: 0.7310480341042803]
	TIME [epoch: 2.69 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8636646080013549		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.8636646080013549 | validation: 0.8657233784787256]
	TIME [epoch: 2.69 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.870064225904153		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.870064225904153 | validation: 0.7220377975696847]
	TIME [epoch: 2.69 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8655039429751249		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.8655039429751249 | validation: 0.8571947043808669]
	TIME [epoch: 2.69 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8620436760650095		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.8620436760650095 | validation: 0.7204818623199231]
	TIME [epoch: 2.69 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.858166872915772		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.858166872915772 | validation: 0.8423467444044569]
	TIME [epoch: 2.69 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8493685561386062		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.8493685561386062 | validation: 0.7192737932168738]
	TIME [epoch: 2.69 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8601359012312902		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.8601359012312902 | validation: 0.8455128630304476]
	TIME [epoch: 2.68 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8565987331863438		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.8565987331863438 | validation: 0.7154585622993883]
	TIME [epoch: 2.69 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8580953323182099		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.8580953323182099 | validation: 0.8719304824623578]
	TIME [epoch: 2.69 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8689857051028693		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.8689857051028693 | validation: 0.6929457060145335]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_724.pth
	Model improved!!!
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8662924706985331		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.8662924706985331 | validation: 0.8497167110932718]
	TIME [epoch: 2.67 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8587611657049635		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.8587611657049635 | validation: 0.7425897760151848]
	TIME [epoch: 2.67 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8482342277745352		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.8482342277745352 | validation: 0.8309862835007152]
	TIME [epoch: 2.67 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8364453617922782		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.8364453617922782 | validation: 0.6995705897447426]
	TIME [epoch: 2.67 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8571307204729536		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.8571307204729536 | validation: 0.895530384726847]
	TIME [epoch: 2.67 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8649351978485537		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.8649351978485537 | validation: 0.7157326550078595]
	TIME [epoch: 2.67 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8460394879622188		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.8460394879622188 | validation: 0.8226845627748269]
	TIME [epoch: 2.67 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8479418894891209		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.8479418894891209 | validation: 0.7163417084775157]
	TIME [epoch: 2.67 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8627798904397888		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.8627798904397888 | validation: 0.8699845384813856]
	TIME [epoch: 2.67 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8624819131586022		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.8624819131586022 | validation: 0.6983500989456478]
	TIME [epoch: 2.69 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8633796168289356		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.8633796168289356 | validation: 0.854670490388346]
	TIME [epoch: 2.68 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8462333179011295		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.8462333179011295 | validation: 0.7099658004288303]
	TIME [epoch: 2.69 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8493080643044855		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.8493080643044855 | validation: 0.8250818431754635]
	TIME [epoch: 2.68 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8360443320793699		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.8360443320793699 | validation: 0.6906896268982012]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_738.pth
	Model improved!!!
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8374605960385176		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.8374605960385176 | validation: 0.8527610742630369]
	TIME [epoch: 2.67 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8567093305287837		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.8567093305287837 | validation: 0.680451188961849]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_740.pth
	Model improved!!!
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8401642246367501		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.8401642246367501 | validation: 0.8460773457625351]
	TIME [epoch: 2.67 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8467574016350701		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.8467574016350701 | validation: 0.7056902680376871]
	TIME [epoch: 2.67 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8392576308799886		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.8392576308799886 | validation: 0.8344669484370331]
	TIME [epoch: 2.67 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.852652326769471		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.852652326769471 | validation: 0.7359848073538129]
	TIME [epoch: 2.68 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8521328381074724		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.8521328381074724 | validation: 0.8176876518728815]
	TIME [epoch: 2.69 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.844673784737409		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.844673784737409 | validation: 0.7039956624927082]
	TIME [epoch: 2.68 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8391788112952145		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.8391788112952145 | validation: 0.8105783587589926]
	TIME [epoch: 2.69 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8394557098025854		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.8394557098025854 | validation: 0.6710481145734367]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_748.pth
	Model improved!!!
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8504972728481527		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.8504972728481527 | validation: 0.8666317367828583]
	TIME [epoch: 2.67 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8505769923190802		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.8505769923190802 | validation: 0.6997047462118247]
	TIME [epoch: 2.67 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8365148711316003		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.8365148711316003 | validation: 0.8217553518643101]
	TIME [epoch: 2.67 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8342896092999718		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.8342896092999718 | validation: 0.6823676395017464]
	TIME [epoch: 2.67 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8440270288085594		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.8440270288085594 | validation: 0.872860266131983]
	TIME [epoch: 2.67 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.849240960021357		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.849240960021357 | validation: 0.7222124954841015]
	TIME [epoch: 2.67 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.840268332611402		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.840268332611402 | validation: 0.8074945115902863]
	TIME [epoch: 2.67 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8351800714856233		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.8351800714856233 | validation: 0.7253323227343846]
	TIME [epoch: 2.69 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8341390193147373		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.8341390193147373 | validation: 0.8203915547622762]
	TIME [epoch: 2.69 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8415009703100681		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.8415009703100681 | validation: 0.7018174581888331]
	TIME [epoch: 2.68 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8262618273437713		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.8262618273437713 | validation: 0.8314079604392923]
	TIME [epoch: 2.69 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8308458866727854		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.8308458866727854 | validation: 0.6930264141867724]
	TIME [epoch: 2.69 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8288019750899994		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.8288019750899994 | validation: 0.8579458346840604]
	TIME [epoch: 2.69 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8347164408396159		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.8347164408396159 | validation: 0.6926938815538707]
	TIME [epoch: 2.69 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8360178941062981		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.8360178941062981 | validation: 0.8366350121603837]
	TIME [epoch: 2.68 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8340668090344696		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.8340668090344696 | validation: 0.7133263048477815]
	TIME [epoch: 2.69 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.845930749932283		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.845930749932283 | validation: 0.802475798275148]
	TIME [epoch: 2.69 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8388895113917265		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.8388895113917265 | validation: 0.7257956531212773]
	TIME [epoch: 2.69 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8265362216955537		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.8265362216955537 | validation: 0.7867745642557185]
	TIME [epoch: 2.68 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8225316972164052		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.8225316972164052 | validation: 0.6907363565992363]
	TIME [epoch: 2.69 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8263207711127393		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.8263207711127393 | validation: 0.846928203875756]
	TIME [epoch: 2.68 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8332609801875185		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.8332609801875185 | validation: 0.6699740100945293]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_770.pth
	Model improved!!!
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8440070055128099		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.8440070055128099 | validation: 0.8367616375404874]
	TIME [epoch: 2.67 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8328964693732592		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.8328964693732592 | validation: 0.6790732334248619]
	TIME [epoch: 2.67 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8248414236394218		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.8248414236394218 | validation: 0.823215025472956]
	TIME [epoch: 2.67 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8289492128063287		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.8289492128063287 | validation: 0.6757232742778636]
	TIME [epoch: 2.67 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8269810745667919		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.8269810745667919 | validation: 0.8370819161203088]
	TIME [epoch: 2.67 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8330586903036011		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.8330586903036011 | validation: 0.682195113651272]
	TIME [epoch: 2.67 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8328521568166707		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.8328521568166707 | validation: 0.8200069040430139]
	TIME [epoch: 2.68 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8308926064406458		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.8308926064406458 | validation: 0.6861178018498361]
	TIME [epoch: 2.67 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8214014356156301		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.8214014356156301 | validation: 0.7853333996090778]
	TIME [epoch: 2.68 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8198935166193438		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.8198935166193438 | validation: 0.7083054692337454]
	TIME [epoch: 2.68 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8133609149776575		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.8133609149776575 | validation: 0.798145044247903]
	TIME [epoch: 2.67 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8245718248398268		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.8245718248398268 | validation: 0.6952800974239991]
	TIME [epoch: 2.69 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8313658629345748		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.8313658629345748 | validation: 0.821448665088425]
	TIME [epoch: 2.68 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8325401944431409		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.8325401944431409 | validation: 0.7002744617783011]
	TIME [epoch: 2.69 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8219866456027642		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.8219866456027642 | validation: 0.8043667662757272]
	TIME [epoch: 2.68 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8242050422071264		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.8242050422071264 | validation: 0.6665215712906312]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_786.pth
	Model improved!!!
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8324492485137609		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.8324492485137609 | validation: 0.8503183195001274]
	TIME [epoch: 2.68 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8225682305231479		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.8225682305231479 | validation: 0.6821614907944225]
	TIME [epoch: 2.67 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8188368682124002		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.8188368682124002 | validation: 0.7824513537375807]
	TIME [epoch: 2.67 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8180570179102858		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.8180570179102858 | validation: 0.71101741737711]
	TIME [epoch: 2.67 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.821402033501279		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.821402033501279 | validation: 0.8050315699453832]
	TIME [epoch: 2.67 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8159184140503325		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.8159184140503325 | validation: 0.6957294625530459]
	TIME [epoch: 2.68 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8212938291098871		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.8212938291098871 | validation: 0.8006083949845746]
	TIME [epoch: 2.67 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8178088945670756		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.8178088945670756 | validation: 0.6665991167421683]
	TIME [epoch: 2.68 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8257835085981856		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.8257835085981856 | validation: 0.843541035599576]
	TIME [epoch: 2.67 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8264266981206783		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.8264266981206783 | validation: 0.6828001163646875]
	TIME [epoch: 2.68 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.806505922193323		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.806505922193323 | validation: 0.7920743805052196]
	TIME [epoch: 2.67 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.806618532505159		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.806618532505159 | validation: 0.691434792847746]
	TIME [epoch: 2.68 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8195132100581977		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.8195132100581977 | validation: 0.8322381843373917]
	TIME [epoch: 2.68 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8264357078066007		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.8264357078066007 | validation: 0.6596031582796427]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_800.pth
	Model improved!!!
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8231155978639023		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.8231155978639023 | validation: 0.8018414036197434]
	TIME [epoch: 2.69 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8152826528563835		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.8152826528563835 | validation: 0.6648170791061047]
	TIME [epoch: 2.69 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8173269501183373		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.8173269501183373 | validation: 0.7716250201232796]
	TIME [epoch: 2.69 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8055510824313931		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.8055510824313931 | validation: 0.6942403753882372]
	TIME [epoch: 2.69 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8088758272778632		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.8088758272778632 | validation: 0.7961193267468278]
	TIME [epoch: 2.67 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8095079306796705		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.8095079306796705 | validation: 0.691371223254286]
	TIME [epoch: 2.67 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8198899671039415		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.8198899671039415 | validation: 0.7903452887300438]
	TIME [epoch: 2.67 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.822773723276523		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.822773723276523 | validation: 0.69439809696648]
	TIME [epoch: 2.67 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8184040625780551		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.8184040625780551 | validation: 0.7698089046145253]
	TIME [epoch: 2.68 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8005943515913945		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.8005943515913945 | validation: 0.6619841651665369]
	TIME [epoch: 2.67 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8129215193726671		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.8129215193726671 | validation: 0.8986001945424601]
	TIME [epoch: 2.67 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8316901241342121		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.8316901241342121 | validation: 0.659048857647336]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_812.pth
	Model improved!!!
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8089037866359096		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.8089037866359096 | validation: 0.7738305305451508]
	TIME [epoch: 2.69 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8064585168933563		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.8064585168933563 | validation: 0.7020371262344087]
	TIME [epoch: 2.69 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8051891197719518		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.8051891197719518 | validation: 0.7858176383817521]
	TIME [epoch: 2.67 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.803149914948678		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.803149914948678 | validation: 0.6666168365961345]
	TIME [epoch: 2.67 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8056636242981621		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.8056636242981621 | validation: 0.8222847298476679]
	TIME [epoch: 2.67 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8241781426783825		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.8241781426783825 | validation: 0.6966464786738037]
	TIME [epoch: 2.67 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8135375360644392		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.8135375360644392 | validation: 0.8063185493283952]
	TIME [epoch: 2.67 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.802629427377472		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.802629427377472 | validation: 0.6736236605175807]
	TIME [epoch: 2.68 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8122252973223184		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.8122252973223184 | validation: 0.8191897073976669]
	TIME [epoch: 2.67 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8150352294045371		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.8150352294045371 | validation: 0.6807277147317592]
	TIME [epoch: 2.67 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7993337090873857		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.7993337090873857 | validation: 0.782263382696771]
	TIME [epoch: 2.67 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7935502525458725		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.7935502525458725 | validation: 0.6684172863833393]
	TIME [epoch: 2.67 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8102036636143933		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.8102036636143933 | validation: 0.8056063782847792]
	TIME [epoch: 2.67 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.808205772461545		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.808205772461545 | validation: 0.6812741863391566]
	TIME [epoch: 2.67 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8136422814680913		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.8136422814680913 | validation: 0.7832706937487585]
	TIME [epoch: 2.67 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.816453109490001		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.816453109490001 | validation: 0.6963604374793664]
	TIME [epoch: 2.67 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.814468546826549		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.814468546826549 | validation: 0.7773624846963599]
	TIME [epoch: 2.67 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.810884822959738		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.810884822959738 | validation: 0.6982364434411225]
	TIME [epoch: 2.67 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8127552911786762		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.8127552911786762 | validation: 0.8231526050818089]
	TIME [epoch: 2.68 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8055627664149992		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.8055627664149992 | validation: 0.6557649236304075]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_832.pth
	Model improved!!!
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8125483902248367		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.8125483902248367 | validation: 0.8490270670356704]
	TIME [epoch: 2.69 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8279884586391214		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.8279884586391214 | validation: 0.6622205016268811]
	TIME [epoch: 2.69 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8021318783772375		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.8021318783772375 | validation: 0.7584296383365544]
	TIME [epoch: 2.69 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7971642981176575		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.7971642981176575 | validation: 0.6835966843131998]
	TIME [epoch: 2.69 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7967646892764293		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.7967646892764293 | validation: 0.8056198020975901]
	TIME [epoch: 2.69 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8113485720899911		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.8113485720899911 | validation: 0.6752727953378553]
	TIME [epoch: 2.69 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8047261742420028		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.8047261742420028 | validation: 0.7753697099248781]
	TIME [epoch: 2.69 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.808030094481622		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.808030094481622 | validation: 0.6875970194539296]
	TIME [epoch: 2.69 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8011065344863273		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.8011065344863273 | validation: 0.7883522521205968]
	TIME [epoch: 2.69 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7995455373886253		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.7995455373886253 | validation: 0.6713690420928065]
	TIME [epoch: 2.69 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8069795691335392		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.8069795691335392 | validation: 0.7925484056080708]
	TIME [epoch: 2.67 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7935741766592115		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.7935741766592115 | validation: 0.6563669304237911]
	TIME [epoch: 2.67 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8061635290325364		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.8061635290325364 | validation: 0.8407675400379635]
	TIME [epoch: 2.67 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8156454482268123		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.8156454482268123 | validation: 0.6459534664963814]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_846.pth
	Model improved!!!
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7991113725721916		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.7991113725721916 | validation: 0.7860747956716687]
	TIME [epoch: 2.68 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.799070669045906		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.799070669045906 | validation: 0.670305801710708]
	TIME [epoch: 2.68 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8056787582022408		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.8056787582022408 | validation: 0.7853061362276116]
	TIME [epoch: 2.68 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7987863476232854		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.7987863476232854 | validation: 0.6639679071485428]
	TIME [epoch: 2.68 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8064124222705209		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.8064124222705209 | validation: 0.7996386174958005]
	TIME [epoch: 2.68 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.804086554852749		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.804086554852749 | validation: 0.654536040683364]
	TIME [epoch: 2.69 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7960763838332313		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.7960763838332313 | validation: 0.7928357312486138]
	TIME [epoch: 2.69 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7903510603621581		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.7903510603621581 | validation: 0.6451922792932652]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_854.pth
	Model improved!!!
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7995362039085583		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.7995362039085583 | validation: 0.802598987118747]
	TIME [epoch: 2.67 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7974138339855859		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.7974138339855859 | validation: 0.6568645730491556]
	TIME [epoch: 2.67 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8051689948423535		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.8051689948423535 | validation: 0.7647239911288024]
	TIME [epoch: 2.67 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7989460857818225		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.7989460857818225 | validation: 0.700915770990262]
	TIME [epoch: 2.67 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8015566355954039		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.8015566355954039 | validation: 0.725188582099922]
	TIME [epoch: 2.67 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7928497987819125		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.7928497987819125 | validation: 0.7076260225985662]
	TIME [epoch: 2.67 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7788682345661242		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.7788682345661242 | validation: 0.7343352937120852]
	TIME [epoch: 2.67 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7927777518282416		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.7927777518282416 | validation: 0.7061511936512843]
	TIME [epoch: 2.67 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7941346536564331		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.7941346536564331 | validation: 0.7771636046120122]
	TIME [epoch: 2.67 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7921388331803293		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.7921388331803293 | validation: 0.6261435751028539]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_864.pth
	Model improved!!!
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8097082271928021		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.8097082271928021 | validation: 0.846567108501006]
	TIME [epoch: 2.67 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8179751643825547		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.8179751643825547 | validation: 0.6744936811608879]
	TIME [epoch: 2.67 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.785915801399557		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.785915801399557 | validation: 0.7245183103385419]
	TIME [epoch: 2.67 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7826810484717447		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.7826810484717447 | validation: 0.7315286903977445]
	TIME [epoch: 2.67 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7912632064896529		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.7912632064896529 | validation: 0.7201596964013784]
	TIME [epoch: 2.67 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7869455176624411		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.7869455176624411 | validation: 0.7410957118505265]
	TIME [epoch: 2.67 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7941965422354903		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.7941965422354903 | validation: 0.6762933702889069]
	TIME [epoch: 2.67 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7900730180692872		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.7900730180692872 | validation: 0.8059876498535623]
	TIME [epoch: 2.67 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8035333859516187		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.8035333859516187 | validation: 0.636162786219082]
	TIME [epoch: 2.67 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8246661365750894		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.8246661365750894 | validation: 0.8263743105071861]
	TIME [epoch: 2.67 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8072188870207636		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.8072188870207636 | validation: 0.6927194876204219]
	TIME [epoch: 2.68 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7901961447761551		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.7901961447761551 | validation: 0.7077251863080254]
	TIME [epoch: 2.67 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7869412692980663		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.7869412692980663 | validation: 0.6988428304705603]
	TIME [epoch: 2.67 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7812841910307725		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.7812841910307725 | validation: 0.7287458736245505]
	TIME [epoch: 2.67 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7831560018967437		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.7831560018967437 | validation: 0.7108453373635014]
	TIME [epoch: 2.67 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7880254350615592		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.7880254350615592 | validation: 0.7658064485071753]
	TIME [epoch: 2.67 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7944902781877068		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.7944902781877068 | validation: 0.641746788937207]
	TIME [epoch: 2.67 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8168598622336924		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.8168598622336924 | validation: 0.8357058835695576]
	TIME [epoch: 2.67 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8197565929669273		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.8197565929669273 | validation: 0.6641240411152183]
	TIME [epoch: 2.67 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7881086943284578		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.7881086943284578 | validation: 0.7686178215326107]
	TIME [epoch: 2.67 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7835405578856889		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.7835405578856889 | validation: 0.6534541007395047]
	TIME [epoch: 2.67 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8018821441956927		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.8018821441956927 | validation: 0.8333221161126197]
	TIME [epoch: 2.68 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8042628317450137		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.8042628317450137 | validation: 0.6462996266176261]
	TIME [epoch: 2.67 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7932888419949347		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.7932888419949347 | validation: 0.7706491127666795]
	TIME [epoch: 2.67 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7823413858404686		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.7823413858404686 | validation: 0.6637167636467882]
	TIME [epoch: 2.67 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.79066548637485		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.79066548637485 | validation: 0.7793817127036223]
	TIME [epoch: 2.67 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7932039868883964		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.7932039868883964 | validation: 0.6688726674394616]
	TIME [epoch: 2.67 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7847978852902543		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.7847978852902543 | validation: 0.769900989104606]
	TIME [epoch: 2.67 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7952170874764939		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.7952170874764939 | validation: 0.6306025248483665]
	TIME [epoch: 2.67 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7948332914012246		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.7948332914012246 | validation: 0.799142798243274]
	TIME [epoch: 2.67 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7936869430274414		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.7936869430274414 | validation: 0.6774895102676599]
	TIME [epoch: 2.67 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7851535951544273		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.7851535951544273 | validation: 0.7596997171703198]
	TIME [epoch: 2.67 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7889463826055492		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.7889463826055492 | validation: 0.6617432299264471]
	TIME [epoch: 2.68 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7904839442066651		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.7904839442066651 | validation: 0.7639410801049219]
	TIME [epoch: 2.67 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7893297638048857		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.7893297638048857 | validation: 0.6852685035986023]
	TIME [epoch: 2.67 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7824856585273591		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.7824856585273591 | validation: 0.7533842212591102]
	TIME [epoch: 2.67 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7908734506206574		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.7908734506206574 | validation: 0.6685034079663916]
	TIME [epoch: 2.67 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7867007393479476		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.7867007393479476 | validation: 0.8049526903552602]
	TIME [epoch: 2.67 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7846108305585924		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.7846108305585924 | validation: 0.6632301342965885]
	TIME [epoch: 2.67 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7910690023218152		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.7910690023218152 | validation: 0.7819913163567721]
	TIME [epoch: 2.67 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7951540051217802		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.7951540051217802 | validation: 0.6549374145734808]
	TIME [epoch: 2.67 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.789832919253105		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.789832919253105 | validation: 0.7563766207946188]
	TIME [epoch: 2.67 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7873723468417698		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.7873723468417698 | validation: 0.6527974130568918]
	TIME [epoch: 2.67 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7808246369823226		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.7808246369823226 | validation: 0.7787722049326342]
	TIME [epoch: 2.67 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7956182492816726		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.7956182492816726 | validation: 0.6639630437193303]
	TIME [epoch: 2.67 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7899254591764259		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.7899254591764259 | validation: 0.7815086908093873]
	TIME [epoch: 2.67 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7832614608027584		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.7832614608027584 | validation: 0.6715087048874999]
	TIME [epoch: 2.69 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7882578962027446		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.7882578962027446 | validation: 0.7438034104698348]
	TIME [epoch: 2.67 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7860439226476231		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.7860439226476231 | validation: 0.7277995457485447]
	TIME [epoch: 2.67 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7748209783555319		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.7748209783555319 | validation: 0.7085372539833938]
	TIME [epoch: 2.67 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7754225451589184		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.7754225451589184 | validation: 0.7056980637795611]
	TIME [epoch: 2.67 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7752448397515795		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.7752448397515795 | validation: 0.7346539109476045]
	TIME [epoch: 2.67 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7775043675300851		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.7775043675300851 | validation: 0.6971910759287435]
	TIME [epoch: 2.67 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7701763420687493		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.7701763420687493 | validation: 0.7687452077428143]
	TIME [epoch: 2.67 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7895310754918932		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.7895310754918932 | validation: 0.617192417768624]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_919.pth
	Model improved!!!
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8008715451906412		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.8008715451906412 | validation: 0.84132907023237]
	TIME [epoch: 2.67 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8096508948473343		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.8096508948473343 | validation: 0.6627965266589895]
	TIME [epoch: 2.67 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7765015264139561		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.7765015264139561 | validation: 0.742090074687928]
	TIME [epoch: 2.67 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7824710040052085		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.7824710040052085 | validation: 0.7067090873734254]
	TIME [epoch: 2.67 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.778044027552272		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.778044027552272 | validation: 0.7325820573414412]
	TIME [epoch: 2.67 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7733760022720152		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.7733760022720152 | validation: 0.6689822269893098]
	TIME [epoch: 2.67 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7868176289471456		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.7868176289471456 | validation: 0.8230158678970615]
	TIME [epoch: 2.67 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7969714863657018		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.7969714863657018 | validation: 0.6495503369269766]
	TIME [epoch: 2.67 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7931749912286655		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.7931749912286655 | validation: 0.7781809315870425]
	TIME [epoch: 2.67 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7811679282816363		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.7811679282816363 | validation: 0.6803382870271331]
	TIME [epoch: 2.67 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7732128196779021		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.7732128196779021 | validation: 0.7275439546377899]
	TIME [epoch: 2.67 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7763677505662279		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.7763677505662279 | validation: 0.6958192505994516]
	TIME [epoch: 2.67 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7748477745172909		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.7748477745172909 | validation: 0.7247154642044313]
	TIME [epoch: 2.67 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7745718297526825		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.7745718297526825 | validation: 0.6408229949901346]
	TIME [epoch: 2.67 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7826746210548184		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.7826746210548184 | validation: 0.8179614610404049]
	TIME [epoch: 2.67 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7960990601196563		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.7960990601196563 | validation: 0.6472222721096169]
	TIME [epoch: 2.67 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7937364134444727		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.7937364134444727 | validation: 0.7749461620721748]
	TIME [epoch: 2.67 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7866223954262753		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.7866223954262753 | validation: 0.6706682317986266]
	TIME [epoch: 2.67 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7883256450028193		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.7883256450028193 | validation: 0.7569044876425546]
	TIME [epoch: 2.67 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7792297380164649		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.7792297380164649 | validation: 0.6979910858480173]
	TIME [epoch: 2.67 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7753530775631344		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.7753530775631344 | validation: 0.7165684515287238]
	TIME [epoch: 2.67 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.766041744706755		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.766041744706755 | validation: 0.690121562286063]
	TIME [epoch: 2.68 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7744483898272947		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.7744483898272947 | validation: 0.755254509269665]
	TIME [epoch: 2.67 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7818189261928133		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.7818189261928133 | validation: 0.6314881804016019]
	TIME [epoch: 2.67 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7854413869361311		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.7854413869361311 | validation: 0.7867264782756426]
	TIME [epoch: 2.67 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7844150939721629		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.7844150939721629 | validation: 0.6720792954494375]
	TIME [epoch: 2.67 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7790107006527468		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.7790107006527468 | validation: 0.7105448216702687]
	TIME [epoch: 2.67 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7758565538253936		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.7758565538253936 | validation: 0.6751752152612345]
	TIME [epoch: 2.67 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7760348098593344		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.7760348098593344 | validation: 0.7770589560092183]
	TIME [epoch: 2.67 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7899538351852543		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.7899538351852543 | validation: 0.6417848302875733]
	TIME [epoch: 2.67 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7942273163543686		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.7942273163543686 | validation: 0.7707882315383229]
	TIME [epoch: 2.67 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.772206848616165		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.772206848616165 | validation: 0.6715086795634656]
	TIME [epoch: 2.67 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7673667410793812		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.7673667410793812 | validation: 0.7214026379765407]
	TIME [epoch: 2.68 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.783099649123173		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.783099649123173 | validation: 0.6925713237954387]
	TIME [epoch: 2.67 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7808604901466742		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.7808604901466742 | validation: 0.7080176985452395]
	TIME [epoch: 2.67 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7804254449989614		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.7804254449989614 | validation: 0.7577340145204117]
	TIME [epoch: 2.67 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7835374223699013		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.7835374223699013 | validation: 0.6404759854122323]
	TIME [epoch: 2.67 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7840980804591972		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.7840980804591972 | validation: 0.783093841754413]
	TIME [epoch: 2.67 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7745392549001795		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.7745392549001795 | validation: 0.675972286031322]
	TIME [epoch: 2.67 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7793263098253052		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.7793263098253052 | validation: 0.747484001140905]
	TIME [epoch: 2.67 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7752817414550979		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.7752817414550979 | validation: 0.6965727098151832]
	TIME [epoch: 2.67 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7658971965831236		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.7658971965831236 | validation: 0.7106788399244244]
	TIME [epoch: 2.67 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7760780742008438		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.7760780742008438 | validation: 0.7108824025254501]
	TIME [epoch: 2.67 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7821002783879017		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.7821002783879017 | validation: 0.7618544624217958]
	TIME [epoch: 2.68 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7774690974787489		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.7774690974787489 | validation: 0.6560526013872953]
	TIME [epoch: 2.67 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.788776039519521		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.788776039519521 | validation: 0.8039136174895067]
	TIME [epoch: 2.67 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7891459464128397		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.7891459464128397 | validation: 0.6408443912661385]
	TIME [epoch: 2.67 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7941872287173481		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.7941872287173481 | validation: 0.7764458047751328]
	TIME [epoch: 2.67 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7786141315050594		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.7786141315050594 | validation: 0.6575230297680107]
	TIME [epoch: 2.67 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7793257612793804		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.7793257612793804 | validation: 0.753971295245317]
	TIME [epoch: 2.67 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7764409765626384		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.7764409765626384 | validation: 0.6841239594994943]
	TIME [epoch: 2.67 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7748097464722556		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.7748097464722556 | validation: 0.749495941724762]
	TIME [epoch: 2.67 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7712648932142628		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.7712648932142628 | validation: 0.6587291837571425]
	TIME [epoch: 2.67 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7806589755483012		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.7806589755483012 | validation: 0.7611561470747132]
	TIME [epoch: 2.67 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7793993725880956		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.7793993725880956 | validation: 0.6859598927125719]
	TIME [epoch: 2.68 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7758602457647843		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.7758602457647843 | validation: 0.7501314131818013]
	TIME [epoch: 2.67 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7695797321120162		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.7695797321120162 | validation: 0.6685369861419121]
	TIME [epoch: 2.67 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7805223945424697		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.7805223945424697 | validation: 0.7334691385443423]
	TIME [epoch: 2.67 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7902923386861588		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.7902923386861588 | validation: 0.6891328468125268]
	TIME [epoch: 2.67 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7703569109779579		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.7703569109779579 | validation: 0.7304135604682545]
	TIME [epoch: 2.67 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7690050137801605		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.7690050137801605 | validation: 0.6938832971617256]
	TIME [epoch: 2.67 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7639603664355672		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.7639603664355672 | validation: 0.7132813596495846]
	TIME [epoch: 2.67 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7630204749173755		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.7630204749173755 | validation: 0.7105631593777321]
	TIME [epoch: 2.67 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7588607981808718		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.7588607981808718 | validation: 0.663970364803127]
	TIME [epoch: 2.67 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7678946170680891		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.7678946170680891 | validation: 0.8102982166122716]
	TIME [epoch: 2.67 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7892630456587679		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.7892630456587679 | validation: 0.644283006937509]
	TIME [epoch: 2.68 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7834919952278654		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.7834919952278654 | validation: 0.7296740220784225]
	TIME [epoch: 2.67 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7709076256781774		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.7709076256781774 | validation: 0.6781354415991436]
	TIME [epoch: 2.67 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7683472655659473		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.7683472655659473 | validation: 0.7074242488891936]
	TIME [epoch: 2.67 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7656916135053482		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.7656916135053482 | validation: 0.6894309243091499]
	TIME [epoch: 2.67 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7679419669119179		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.7679419669119179 | validation: 0.7059963453161764]
	TIME [epoch: 2.68 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7731973526373025		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.7731973526373025 | validation: 0.6328898385309009]
	TIME [epoch: 2.67 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7836414470399339		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.7836414470399339 | validation: 0.7799540432528775]
	TIME [epoch: 2.67 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7844825034419537		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.7844825034419537 | validation: 0.6447741678783391]
	TIME [epoch: 2.67 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7714930600008996		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.7714930600008996 | validation: 0.7338339928459744]
	TIME [epoch: 2.67 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7723087115531806		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.7723087115531806 | validation: 0.7056712060665765]
	TIME [epoch: 2.67 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7757806354860632		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.7757806354860632 | validation: 0.6658402550590936]
	TIME [epoch: 2.67 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7767763359427184		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.7767763359427184 | validation: 0.7572524742992909]
	TIME [epoch: 2.67 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7752937184200391		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.7752937184200391 | validation: 0.6535357051378754]
	TIME [epoch: 2.68 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7762599459125002		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.7762599459125002 | validation: 0.7633326779195997]
	TIME [epoch: 2.67 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7694699191712532		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.7694699191712532 | validation: 0.671983972120437]
	TIME [epoch: 2.67 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7747846503554154		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.7747846503554154 | validation: 0.7401718240827634]
	TIME [epoch: 181 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7759332291021422		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.7759332291021422 | validation: 0.6650691024163834]
	TIME [epoch: 5.73 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7805836397149863		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.7805836397149863 | validation: 0.7825612513648075]
	TIME [epoch: 5.72 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7727265540771118		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.7727265540771118 | validation: 0.6707340784603922]
	TIME [epoch: 5.72 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.778123899113619		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.778123899113619 | validation: 0.7225568036902739]
	TIME [epoch: 5.72 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7721205219678521		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.7721205219678521 | validation: 0.6859678174978103]
	TIME [epoch: 5.72 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7699638354951245		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.7699638354951245 | validation: 0.7065396178719494]
	TIME [epoch: 5.72 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7642957733540274		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.7642957733540274 | validation: 0.6848346815330898]
	TIME [epoch: 5.72 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7684107707253108		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.7684107707253108 | validation: 0.7317683587967889]
	TIME [epoch: 5.72 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7667653957408144		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.7667653957408144 | validation: 0.6556630883185008]
	TIME [epoch: 5.72 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7668473415175905		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.7668473415175905 | validation: 0.7631606183763948]
	TIME [epoch: 5.72 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.778452752441186		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.778452752441186 | validation: 0.6603533238889976]
	TIME [epoch: 5.72 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7663892758607713		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.7663892758607713 | validation: 0.7200504063277826]
	TIME [epoch: 5.72 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7752098941126944		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.7752098941126944 | validation: 0.6728053354549479]
	TIME [epoch: 5.72 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7733637050528586		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.7733637050528586 | validation: 0.7373942211242359]
	TIME [epoch: 5.72 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7831693923962577		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.7831693923962577 | validation: 0.7002237648039665]
	TIME [epoch: 5.72 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7739174855202788		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.7739174855202788 | validation: 0.7325710883641449]
	TIME [epoch: 5.72 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7744076763254247		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.7744076763254247 | validation: 0.6481178986190591]
	TIME [epoch: 5.71 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.783839454756964		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.783839454756964 | validation: 0.7648361091013585]
	TIME [epoch: 5.71 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7730940694172486		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.7730940694172486 | validation: 0.6669935997251226]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_3_v_mmd4_20250516_155132/states/model_phi1_4a_distortion_v2_3_v_mmd4_1020.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 2767.851 seconds.
