Args:
Namespace(name='model_phi1_4a_distortion_v1_1_v_mmd3', outdir='out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_1/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_1/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.04892115, 0.1, 1.0], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3577025037

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.529969928012674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.529969928012674 | validation: 3.5499182983251574]
	TIME [epoch: 163 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4074672422442833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4074672422442833 | validation: 2.850167337875466]
	TIME [epoch: 0.586 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1737609572152268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1737609572152268 | validation: 2.636108121146393]
	TIME [epoch: 0.575 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8949675112930437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8949675112930437 | validation: 2.5515487323592607]
	TIME [epoch: 0.577 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.907708863759258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.907708863759258 | validation: 3.0541111887339447]
	TIME [epoch: 0.577 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.083730189055312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.083730189055312 | validation: 3.0171588256148882]
	TIME [epoch: 0.575 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1140901236565193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1140901236565193 | validation: 2.977653490911376]
	TIME [epoch: 0.575 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0573872735345744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0573872735345744 | validation: 2.842261562483444]
	TIME [epoch: 0.574 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9842578767790133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9842578767790133 | validation: 2.698473351601482]
	TIME [epoch: 0.574 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.880238896962711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.880238896962711 | validation: 2.4192796282337983]
	TIME [epoch: 0.574 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.750166647807989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.750166647807989 | validation: 2.2320016789458057]
	TIME [epoch: 0.577 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.663975287696012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.663975287696012 | validation: 2.3081976567658367]
	TIME [epoch: 0.575 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6503642524819178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6503642524819178 | validation: 2.151597819651002]
	TIME [epoch: 0.574 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.629107486851442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.629107486851442 | validation: 1.9329852858923595]
	TIME [epoch: 0.576 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.514693210635292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.514693210635292 | validation: 2.004958470327059]
	TIME [epoch: 0.577 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4853798641689338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4853798641689338 | validation: 2.0027578486637347]
	TIME [epoch: 0.575 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4597336136280883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4597336136280883 | validation: 2.0024935090811695]
	TIME [epoch: 0.575 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4678436369953625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4678436369953625 | validation: 1.7961587312354341]
	TIME [epoch: 0.575 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.412098837471997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.412098837471997 | validation: 1.8842641882839537]
	TIME [epoch: 0.576 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3748037355312603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3748037355312603 | validation: 1.8501362136051869]
	TIME [epoch: 0.575 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3755074502465483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3755074502465483 | validation: 1.737808840792512]
	TIME [epoch: 0.576 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.340668648573254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.340668648573254 | validation: 1.8249122894035332]
	TIME [epoch: 0.576 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.34681971895944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.34681971895944 | validation: 1.7717277399170035]
	TIME [epoch: 0.573 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.410891960341632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.410891960341632 | validation: 1.6207347669669916]
	TIME [epoch: 0.574 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3001660727871376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3001660727871376 | validation: 1.9426203786768454]
	TIME [epoch: 0.575 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4401027441439758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4401027441439758 | validation: 1.6284375400167725]
	TIME [epoch: 0.573 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2930942576085975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2930942576085975 | validation: 1.61888351665806]
	TIME [epoch: 0.578 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2715598638686454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2715598638686454 | validation: 1.7121736256371147]
	TIME [epoch: 0.574 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2359025484647614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2359025484647614 | validation: 1.666791681078544]
	TIME [epoch: 0.575 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2160390021812577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2160390021812577 | validation: 1.6029205182302413]
	TIME [epoch: 0.574 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2039614707181028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2039614707181028 | validation: 1.6190109125887986]
	TIME [epoch: 0.577 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.182195638628796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.182195638628796 | validation: 1.5748487654263612]
	TIME [epoch: 0.575 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1754445847857697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1754445847857697 | validation: 1.5535743698146731]
	TIME [epoch: 0.574 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1592451265339494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1592451265339494 | validation: 1.5196603831071809]
	TIME [epoch: 0.575 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1433359977322475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1433359977322475 | validation: 1.561991806213996]
	TIME [epoch: 0.574 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1561185162285046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1561185162285046 | validation: 1.7453327523514532]
	TIME [epoch: 0.572 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3128614745716423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3128614745716423 | validation: 1.5047053155614987]
	TIME [epoch: 0.575 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1612571556544364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1612571556544364 | validation: 1.935639455537257]
	TIME [epoch: 0.576 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.401631203358867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.401631203358867 | validation: 1.5177262915752263]
	TIME [epoch: 0.574 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.109191441263686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.109191441263686 | validation: 1.5077042049952611]
	TIME [epoch: 0.575 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1538103574202987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1538103574202987 | validation: 1.500191154034881]
	TIME [epoch: 0.574 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0915265208643636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0915265208643636 | validation: 1.508770303627167]
	TIME [epoch: 0.575 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0921827232803505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0921827232803505 | validation: 1.464305945606195]
	TIME [epoch: 0.575 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.09599793534474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.09599793534474 | validation: 1.4625022003678125]
	TIME [epoch: 0.575 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0669439597483987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0669439597483987 | validation: 1.4645046860830935]
	TIME [epoch: 0.575 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.063519903201435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.063519903201435 | validation: 1.4294727567031449]
	TIME [epoch: 0.573 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0614826767120746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0614826767120746 | validation: 1.4439198663313424]
	TIME [epoch: 0.582 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.048433696747431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.048433696747431 | validation: 1.4092382995657333]
	TIME [epoch: 0.575 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.038313549222806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.038313549222806 | validation: 1.4088772894870385]
	TIME [epoch: 0.577 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0332668496614303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0332668496614303 | validation: 1.429528253794511]
	TIME [epoch: 0.576 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.041268350558041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.041268350558041 | validation: 1.4742977303856835]
	TIME [epoch: 0.573 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0933604696485633		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 2.0933604696485633 | validation: 1.4483541615199909]
	TIME [epoch: 0.573 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0355775947040677		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 2.0355775947040677 | validation: 1.4256056643360848]
	TIME [epoch: 0.574 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0307090026671584		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 2.0307090026671584 | validation: 1.3971142431326002]
	TIME [epoch: 0.573 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.998208334979451		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.998208334979451 | validation: 1.3759058440409466]
	TIME [epoch: 0.573 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9916740833929225		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.9916740833929225 | validation: 1.373107513135217]
	TIME [epoch: 0.574 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9897952460227635		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.9897952460227635 | validation: 1.384234239583081]
	TIME [epoch: 0.573 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9818895637731038		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.9818895637731038 | validation: 1.3610559013747399]
	TIME [epoch: 0.574 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9795189115738043		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.9795189115738043 | validation: 1.3728916121304424]
	TIME [epoch: 0.575 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9713451056713103		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.9713451056713103 | validation: 1.3659871857788202]
	TIME [epoch: 0.575 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.960767025219493		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.960767025219493 | validation: 1.3536865520895476]
	TIME [epoch: 0.574 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9543027111401126		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.9543027111401126 | validation: 1.3569837180136326]
	TIME [epoch: 0.574 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9472726662333668		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.9472726662333668 | validation: 1.3533917817075805]
	TIME [epoch: 0.574 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9412143545925267		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.9412143545925267 | validation: 1.333806943799492]
	TIME [epoch: 0.575 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9286534132208335		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.9286534132208335 | validation: 1.359690905651683]
	TIME [epoch: 0.577 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9098643506196085		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.9098643506196085 | validation: 1.3231854214214551]
	TIME [epoch: 0.575 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.874427757520614		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.874427757520614 | validation: 1.3533946628148628]
	TIME [epoch: 0.574 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8146904758853726		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.8146904758853726 | validation: 1.331895621622505]
	TIME [epoch: 0.575 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7680460904247866		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.7680460904247866 | validation: 1.3337505857153678]
	TIME [epoch: 0.574 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7638453032066332		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.7638453032066332 | validation: 1.4943974813467253]
	TIME [epoch: 0.574 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8306003066559817		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.8306003066559817 | validation: 1.3650194541545102]
	TIME [epoch: 0.577 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.757997236550566		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.757997236550566 | validation: 1.3495164072157952]
	TIME [epoch: 0.575 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7396435246952484		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.7396435246952484 | validation: 1.3668409969698767]
	TIME [epoch: 0.575 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7413387178330368		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.7413387178330368 | validation: 1.3526361635028747]
	TIME [epoch: 0.574 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.734408013101605		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.734408013101605 | validation: 1.3526856675083394]
	TIME [epoch: 0.574 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7367247646899688		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.7367247646899688 | validation: 1.378791515698267]
	TIME [epoch: 0.573 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7341357403283795		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.7341357403283795 | validation: 1.369038359000951]
	TIME [epoch: 0.573 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7331088325294521		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.7331088325294521 | validation: 1.3857327984222687]
	TIME [epoch: 0.572 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.731084291334716		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.731084291334716 | validation: 1.3889633738929548]
	TIME [epoch: 0.573 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.732647185889492		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.732647185889492 | validation: 1.4132166710402236]
	TIME [epoch: 0.574 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7383676533452408		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.7383676533452408 | validation: 1.372178597400304]
	TIME [epoch: 0.573 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7293985322627408		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.7293985322627408 | validation: 1.4195841336953228]
	TIME [epoch: 0.573 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7239041803429218		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.7239041803429218 | validation: 1.3655196719897602]
	TIME [epoch: 0.577 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7149467790335189		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.7149467790335189 | validation: 1.385523369185043]
	TIME [epoch: 0.574 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7123135915658247		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.7123135915658247 | validation: 1.4331554980472927]
	TIME [epoch: 0.573 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7074823682985123		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.7074823682985123 | validation: 1.3965102797993987]
	TIME [epoch: 0.574 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7066214878127528		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.7066214878127528 | validation: 1.4539690671726468]
	TIME [epoch: 0.573 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7178266248088323		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.7178266248088323 | validation: 1.3747793413424672]
	TIME [epoch: 0.573 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7035710515221678		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.7035710515221678 | validation: 1.4396612478719741]
	TIME [epoch: 0.579 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6965098089018187		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.6965098089018187 | validation: 1.4063056647983967]
	TIME [epoch: 0.572 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6836221486869216		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.6836221486869216 | validation: 1.420166083266344]
	TIME [epoch: 0.573 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6888944659051206		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.6888944659051206 | validation: 1.4448937766695868]
	TIME [epoch: 0.575 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6818563536494833		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.6818563536494833 | validation: 1.3981909915815818]
	TIME [epoch: 0.573 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6822845268205187		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.6822845268205187 | validation: 1.4881934453413155]
	TIME [epoch: 0.573 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7021693737828245		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.7021693737828245 | validation: 1.3986123901734304]
	TIME [epoch: 0.574 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6692140375433888		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.6692140375433888 | validation: 1.4279802170148121]
	TIME [epoch: 0.573 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6589511537527641		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.6589511537527641 | validation: 1.4205574836446109]
	TIME [epoch: 0.573 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.65510226823697		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.65510226823697 | validation: 1.425388957122565]
	TIME [epoch: 0.575 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6511974450200149		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.6511974450200149 | validation: 1.4269985531143594]
	TIME [epoch: 0.574 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6490687978976806		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.6490687978976806 | validation: 1.406359279063925]
	TIME [epoch: 0.574 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6498643520704468		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.6498643520704468 | validation: 1.4740269529645247]
	TIME [epoch: 0.575 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.662041118748627		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.662041118748627 | validation: 1.4166094180526019]
	TIME [epoch: 0.574 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.695182807953988		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.695182807953988 | validation: 1.510298643529469]
	TIME [epoch: 0.571 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.677765324985375		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.677765324985375 | validation: 1.4441039232033948]
	TIME [epoch: 0.571 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6416808194708012		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.6416808194708012 | validation: 1.422075639838311]
	TIME [epoch: 0.573 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6572718232693922		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.6572718232693922 | validation: 1.4539251107922155]
	TIME [epoch: 0.572 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6382740500491304		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.6382740500491304 | validation: 1.4403156173761542]
	TIME [epoch: 0.596 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.630178004183323		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.630178004183323 | validation: 1.4462794029230306]
	TIME [epoch: 0.582 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6249598840124386		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.6249598840124386 | validation: 1.4442773490309937]
	TIME [epoch: 0.571 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6246887326738733		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.6246887326738733 | validation: 1.4404259253506435]
	TIME [epoch: 0.57 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6160283051636442		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.6160283051636442 | validation: 1.4821102404894875]
	TIME [epoch: 0.571 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6142155981187694		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.6142155981187694 | validation: 1.4418559195554632]
	TIME [epoch: 0.573 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.632432868768494		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.632432868768494 | validation: 1.5346324217063163]
	TIME [epoch: 0.571 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6213643436513587		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.6213643436513587 | validation: 1.4324397832852611]
	TIME [epoch: 0.57 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5924202763849038		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.5924202763849038 | validation: 1.4567659729134013]
	TIME [epoch: 0.571 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5762060074207227		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.5762060074207227 | validation: 1.5025903939664949]
	TIME [epoch: 0.57 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5621337233389647		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.5621337233389647 | validation: 1.430797322791556]
	TIME [epoch: 0.572 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5347188245244712		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.5347188245244712 | validation: 1.5347968568442578]
	TIME [epoch: 0.572 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5261518956234195		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.5261518956234195 | validation: 1.428536761892377]
	TIME [epoch: 0.571 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.528012207867533		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.528012207867533 | validation: 1.5110168183927408]
	TIME [epoch: 0.572 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3622216531021503		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.3622216531021503 | validation: 1.392621962887236]
	TIME [epoch: 0.572 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.244149524020953		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.244149524020953 | validation: 1.5168786881298633]
	TIME [epoch: 0.571 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.499386979260935		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.499386979260935 | validation: 1.6169792176206674]
	TIME [epoch: 0.571 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.570124258932693		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.570124258932693 | validation: 1.4482867314561674]
	TIME [epoch: 0.572 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3248608909879076		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.3248608909879076 | validation: 1.3556314447146054]
	TIME [epoch: 0.57 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4223545575546541		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.4223545575546541 | validation: 1.3245301760686308]
	TIME [epoch: 0.57 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.207479642302891		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.207479642302891 | validation: 1.3874129850032475]
	TIME [epoch: 0.571 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2705101605195723		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.2705101605195723 | validation: 1.3006256600620862]
	TIME [epoch: 0.572 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2432784840462916		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.2432784840462916 | validation: 1.279391261784287]
	TIME [epoch: 0.574 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1827449710752376		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.1827449710752376 | validation: 1.275935753080952]
	TIME [epoch: 0.576 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1702529767332797		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.1702529767332797 | validation: 1.2598041090603176]
	TIME [epoch: 0.575 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1984711546096427		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.1984711546096427 | validation: 1.2821303347983988]
	TIME [epoch: 0.575 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1903328299346407		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.1903328299346407 | validation: 1.2316359980013207]
	TIME [epoch: 0.574 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.170908230550011		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.170908230550011 | validation: 1.2773676526172364]
	TIME [epoch: 0.577 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1761857917967056		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.1761857917967056 | validation: 1.2283263803781272]
	TIME [epoch: 0.581 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1665787123767986		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.1665787123767986 | validation: 1.258529952976745]
	TIME [epoch: 0.575 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1582234401639935		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.1582234401639935 | validation: 1.1990953365964534]
	TIME [epoch: 0.574 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.153024828295631		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.153024828295631 | validation: 1.2520284273551487]
	TIME [epoch: 0.575 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1632717117892353		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.1632717117892353 | validation: 1.1967674558004393]
	TIME [epoch: 0.574 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_139.pth
	Model improved!!!
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1447970740601467		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.1447970740601467 | validation: 1.2507365821323544]
	TIME [epoch: 0.576 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1489468622747288		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.1489468622747288 | validation: 1.1776339538415264]
	TIME [epoch: 0.575 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1385389667424586		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.1385389667424586 | validation: 1.2343314201487958]
	TIME [epoch: 0.575 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.144061363110543		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.144061363110543 | validation: 1.1480115660867973]
	TIME [epoch: 0.572 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.150246232186834		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.150246232186834 | validation: 1.2371980113090617]
	TIME [epoch: 0.574 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1559086130446319		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.1559086130446319 | validation: 1.1446780983488634]
	TIME [epoch: 0.572 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1182414031119101		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.1182414031119101 | validation: 1.1996716175008306]
	TIME [epoch: 0.58 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1122490613418565		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.1122490613418565 | validation: 1.1605580379393012]
	TIME [epoch: 0.573 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.10877176329054		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.10877176329054 | validation: 1.159827622573535]
	TIME [epoch: 0.573 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0909747725195649		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.0909747725195649 | validation: 1.1559334166924036]
	TIME [epoch: 0.573 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0716439522592216		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.0716439522592216 | validation: 1.1143014278717966]
	TIME [epoch: 0.574 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_150.pth
	Model improved!!!
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0714571120995704		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.0714571120995704 | validation: 1.2127728965993996]
	TIME [epoch: 0.574 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1306776706247892		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.1306776706247892 | validation: 1.135007872391976]
	TIME [epoch: 0.572 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2154680910477134		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.2154680910477134 | validation: 1.2238638432102937]
	TIME [epoch: 0.573 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1449942518036722		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.1449942518036722 | validation: 1.098759074288248]
	TIME [epoch: 0.571 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0419008317715		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.0419008317715 | validation: 1.095322976184017]
	TIME [epoch: 0.573 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0328455149712108		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.0328455149712108 | validation: 1.1149859940017284]
	TIME [epoch: 0.573 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0360971218961865		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.0360971218961865 | validation: 1.0571868280808445]
	TIME [epoch: 0.572 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0455593308236302		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.0455593308236302 | validation: 1.1848326243966696]
	TIME [epoch: 0.574 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0908581073589307		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.0908581073589307 | validation: 1.0551185654554291]
	TIME [epoch: 0.572 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_159.pth
	Model improved!!!
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.137782064167116		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.137782064167116 | validation: 1.1806441132242753]
	TIME [epoch: 0.574 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1259646514621489		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.1259646514621489 | validation: 1.0365745312670018]
	TIME [epoch: 0.572 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_161.pth
	Model improved!!!
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0055507978450091		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.0055507978450091 | validation: 1.0414580099306963]
	TIME [epoch: 0.573 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.002495834957079		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.002495834957079 | validation: 1.0502822962486855]
	TIME [epoch: 0.572 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9956518376230687		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.9956518376230687 | validation: 1.0352484123755654]
	TIME [epoch: 0.573 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.084495750166636		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.084495750166636 | validation: 1.3003090737544047]
	TIME [epoch: 0.573 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1702141526703989		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.1702141526703989 | validation: 1.0365057795403192]
	TIME [epoch: 0.573 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.054404388448722		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.054404388448722 | validation: 1.0496730232852063]
	TIME [epoch: 0.572 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0279855115174548		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.0279855115174548 | validation: 1.087598818976596]
	TIME [epoch: 0.572 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0403556253295208		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.0403556253295208 | validation: 1.1407628468604727]
	TIME [epoch: 0.572 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0607141616169131		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.0607141616169131 | validation: 1.0217045224611787]
	TIME [epoch: 0.572 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0756945127464332		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.0756945127464332 | validation: 1.086505109546828]
	TIME [epoch: 0.575 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0276913142353035		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.0276913142353035 | validation: 0.9881488981364884]
	TIME [epoch: 0.573 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.000692795909835		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.000692795909835 | validation: 1.0038819725036423]
	TIME [epoch: 0.575 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9730125591927012		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.9730125591927012 | validation: 0.9632308825321325]
	TIME [epoch: 0.572 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9613706637881052		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.9613706637881052 | validation: 0.9727393130412608]
	TIME [epoch: 0.576 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9470267412551499		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.9470267412551499 | validation: 0.9372847383349971]
	TIME [epoch: 0.572 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9659296577349717		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.9659296577349717 | validation: 1.0463446348511642]
	TIME [epoch: 0.575 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0206908080071515		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.0206908080071515 | validation: 1.020566994731632]
	TIME [epoch: 0.572 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1287040707692766		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.1287040707692766 | validation: 1.2299970386157202]
	TIME [epoch: 0.572 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1383639994559618		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.1383639994559618 | validation: 0.9279596930559504]
	TIME [epoch: 0.572 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_180.pth
	Model improved!!!
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9661014854557667		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.9661014854557667 | validation: 0.9296103098861629]
	TIME [epoch: 0.574 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9435940958360891		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.9435940958360891 | validation: 0.9746241164402676]
	TIME [epoch: 0.572 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.983261984368411		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.983261984368411 | validation: 0.942048618682393]
	TIME [epoch: 0.572 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9582053853275129		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.9582053853275129 | validation: 0.9939273461533802]
	TIME [epoch: 0.571 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.987622209012128		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.987622209012128 | validation: 1.022076310888994]
	TIME [epoch: 0.571 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9963832338525083		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.9963832338525083 | validation: 0.9453579989047767]
	TIME [epoch: 0.571 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9460236009289357		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.9460236009289357 | validation: 0.8715246244606784]
	TIME [epoch: 0.571 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_187.pth
	Model improved!!!
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9231522372578809		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.9231522372578809 | validation: 0.9781510025780027]
	TIME [epoch: 0.573 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9467071155327216		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.9467071155327216 | validation: 0.8870641315434924]
	TIME [epoch: 0.573 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0053204837836984		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.0053204837836984 | validation: 1.2352223735902936]
	TIME [epoch: 0.573 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1030472817539165		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.1030472817539165 | validation: 0.8655488527015109]
	TIME [epoch: 0.572 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9424805354022853		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.9424805354022853 | validation: 0.9167682513109423]
	TIME [epoch: 0.574 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.925124769907443		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.925124769907443 | validation: 0.946887984165721]
	TIME [epoch: 0.577 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9332225650379334		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.9332225650379334 | validation: 0.8582391054874802]
	TIME [epoch: 0.574 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9050806118057969		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.9050806118057969 | validation: 0.8796633677363499]
	TIME [epoch: 0.577 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8859220494884054		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.8859220494884054 | validation: 0.9220224796704856]
	TIME [epoch: 0.575 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9160353696617562		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.9160353696617562 | validation: 0.9824553092014489]
	TIME [epoch: 0.573 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0054268031668647		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.0054268031668647 | validation: 0.936259329833307]
	TIME [epoch: 0.576 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9761156112054908		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.9761156112054908 | validation: 0.8932755686580436]
	TIME [epoch: 0.574 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9385222446471202		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.9385222446471202 | validation: 0.7958363332702674]
	TIME [epoch: 0.574 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8886488087707405		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.8886488087707405 | validation: 1.021549861590232]
	TIME [epoch: 172 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9653429823364755		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.9653429823364755 | validation: 0.9663225589463299]
	TIME [epoch: 1.13 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1223328389922207		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.1223328389922207 | validation: 1.0744384766708461]
	TIME [epoch: 1.12 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9954322847508931		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.9954322847508931 | validation: 0.9253580434607932]
	TIME [epoch: 1.12 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9421564522419928		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.9421564522419928 | validation: 0.8377028770110088]
	TIME [epoch: 1.12 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9105097570502165		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.9105097570502165 | validation: 0.9103250395047244]
	TIME [epoch: 1.12 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9031321234080635		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.9031321234080635 | validation: 0.8592210189082]
	TIME [epoch: 1.12 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9058039967942202		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.9058039967942202 | validation: 0.8773996018483626]
	TIME [epoch: 1.12 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8911075302479807		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.8911075302479807 | validation: 0.8086342894357209]
	TIME [epoch: 1.12 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8825272034642923		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.8825272034642923 | validation: 0.8645446853816705]
	TIME [epoch: 1.12 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8925990110705544		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.8925990110705544 | validation: 0.784952697162145]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.923022682087633		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.923022682087633 | validation: 0.9776796491320902]
	TIME [epoch: 1.12 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9312189377785173		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.9312189377785173 | validation: 0.8151103409318918]
	TIME [epoch: 1.18 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9489366637979793		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.9489366637979793 | validation: 1.1092665429961532]
	TIME [epoch: 1.13 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.00140614183349		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 1.00140614183349 | validation: 0.8153934080921963]
	TIME [epoch: 1.12 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8855131833935213		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.8855131833935213 | validation: 0.84967245232884]
	TIME [epoch: 1.13 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9006907569585263		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.9006907569585263 | validation: 0.8461419114217033]
	TIME [epoch: 1.12 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8986274677668884		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.8986274677668884 | validation: 0.8210558422715555]
	TIME [epoch: 1.12 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8784405001023906		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.8784405001023906 | validation: 0.7660301461216482]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8851978230101212		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.8851978230101212 | validation: 0.8908465495682837]
	TIME [epoch: 1.12 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8848618115495404		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.8848618115495404 | validation: 0.7381138816937374]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9048758478149183		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.9048758478149183 | validation: 0.9346593393485869]
	TIME [epoch: 1.12 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.896000481738827		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.896000481738827 | validation: 0.8113786424912771]
	TIME [epoch: 1.12 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9487320776793289		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.9487320776793289 | validation: 0.976678448991402]
	TIME [epoch: 1.12 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.952693982715314		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.952693982715314 | validation: 0.7674445274647598]
	TIME [epoch: 1.12 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8553270400241394		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.8553270400241394 | validation: 0.7713893211588322]
	TIME [epoch: 1.12 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8465717115159389		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.8465717115159389 | validation: 0.8383532130686083]
	TIME [epoch: 1.12 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8376912617278629		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.8376912617278629 | validation: 0.746951401632397]
	TIME [epoch: 1.12 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8370628093563198		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.8370628093563198 | validation: 0.8668822372613522]
	TIME [epoch: 1.12 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8654664714842417		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.8654664714842417 | validation: 0.7810601952602617]
	TIME [epoch: 1.12 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8682771198087552		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.8682771198087552 | validation: 0.9269765845247346]
	TIME [epoch: 1.12 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9107600707577669		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.9107600707577669 | validation: 0.7302508909361747]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_232.pth
	Model improved!!!
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8359865247660824		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.8359865247660824 | validation: 0.7237560939962988]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_233.pth
	Model improved!!!
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8233629243277372		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.8233629243277372 | validation: 0.8043132108696555]
	TIME [epoch: 1.12 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8578142409735255		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.8578142409735255 | validation: 0.7447971879454262]
	TIME [epoch: 1.12 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.977104563002335		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.977104563002335 | validation: 0.9884992182746326]
	TIME [epoch: 1.12 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9413205363442768		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.9413205363442768 | validation: 0.7879468847758422]
	TIME [epoch: 1.12 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9385508816281487		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.9385508816281487 | validation: 0.926864515289654]
	TIME [epoch: 1.12 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.916174686101298		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.916174686101298 | validation: 0.7429710650047028]
	TIME [epoch: 1.12 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8293782038019596		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.8293782038019596 | validation: 0.7186500639592497]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8180045605460328		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.8180045605460328 | validation: 0.7374171808004157]
	TIME [epoch: 1.12 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7988852298715392		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.7988852298715392 | validation: 0.6843177286789333]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7926075659946057		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.7926075659946057 | validation: 0.7849625546709325]
	TIME [epoch: 1.12 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8101029972643657		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.8101029972643657 | validation: 0.7396512116165819]
	TIME [epoch: 1.12 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8730973454309512		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.8730973454309512 | validation: 1.1368747501360061]
	TIME [epoch: 1.12 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0030080564431063		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.0030080564431063 | validation: 0.6774594931910014]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8677159405250191		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.8677159405250191 | validation: 0.7638077985340824]
	TIME [epoch: 1.12 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9207320527967197		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.9207320527967197 | validation: 0.7945230450384937]
	TIME [epoch: 1.12 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8290838885767127		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.8290838885767127 | validation: 0.6870021346690208]
	TIME [epoch: 1.12 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.796860206074648		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.796860206074648 | validation: 0.8008372338532865]
	TIME [epoch: 1.12 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8269077809013772		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.8269077809013772 | validation: 0.7442720482315833]
	TIME [epoch: 1.12 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9207248948773881		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.9207248948773881 | validation: 0.911881527038751]
	TIME [epoch: 1.12 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9226901619472415		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.9226901619472415 | validation: 0.6962362019721461]
	TIME [epoch: 1.12 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8218686339453318		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.8218686339453318 | validation: 0.6795737210601897]
	TIME [epoch: 1.12 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7773513971987113		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.7773513971987113 | validation: 0.6854653215766302]
	TIME [epoch: 1.12 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7736389285390675		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.7736389285390675 | validation: 0.6831369807050983]
	TIME [epoch: 1.12 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7746804076877248		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.7746804076877248 | validation: 0.7234744622526428]
	TIME [epoch: 1.12 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8234209493832302		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.8234209493832302 | validation: 0.8052392787292253]
	TIME [epoch: 1.12 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8926002635800049		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.8926002635800049 | validation: 0.8148167588250538]
	TIME [epoch: 1.12 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8086077447548519		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.8086077447548519 | validation: 0.682886311401601]
	TIME [epoch: 1.12 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8888994082782651		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.8888994082782651 | validation: 1.2021514350954368]
	TIME [epoch: 1.12 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1071076867054084		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 1.1071076867054084 | validation: 0.6493204912728755]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_262.pth
	Model improved!!!
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8733369095251436		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.8733369095251436 | validation: 0.7255289309436503]
	TIME [epoch: 1.12 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7954312181940109		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.7954312181940109 | validation: 0.7362823609413843]
	TIME [epoch: 1.12 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8069868112830559		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.8069868112830559 | validation: 0.6677590619155489]
	TIME [epoch: 1.13 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7916865081010102		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.7916865081010102 | validation: 0.7211147196282746]
	TIME [epoch: 1.12 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7827389278334199		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.7827389278334199 | validation: 0.6813553134054313]
	TIME [epoch: 1.12 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.802559080418332		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.802559080418332 | validation: 0.7787534070358785]
	TIME [epoch: 1.12 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8363574218813625		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.8363574218813625 | validation: 0.7309395871144752]
	TIME [epoch: 1.12 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8393687446363618		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.8393687446363618 | validation: 0.7391397208273511]
	TIME [epoch: 1.12 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7966174705407962		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.7966174705407962 | validation: 0.6821039717673996]
	TIME [epoch: 1.12 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.808845474140374		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.808845474140374 | validation: 0.7718756330375257]
	TIME [epoch: 1.12 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8221596451446084		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.8221596451446084 | validation: 0.6607438246609896]
	TIME [epoch: 1.12 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8230738808929536		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.8230738808929536 | validation: 0.6969422739011512]
	TIME [epoch: 1.12 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7661214905561416		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.7661214905561416 | validation: 0.6825662870595823]
	TIME [epoch: 1.12 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7890339819751202		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.7890339819751202 | validation: 0.7184836983248651]
	TIME [epoch: 1.12 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8268941982755681		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.8268941982755681 | validation: 0.6869281255208548]
	TIME [epoch: 1.12 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.806730988699995		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.806730988699995 | validation: 0.7135055473945338]
	TIME [epoch: 1.12 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7893657913691806		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.7893657913691806 | validation: 0.6443002914453249]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_279.pth
	Model improved!!!
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8161072148970012		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.8161072148970012 | validation: 0.9907553942520242]
	TIME [epoch: 1.12 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9199198713349447		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.9199198713349447 | validation: 0.6389012246615959]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_281.pth
	Model improved!!!
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8961182194048545		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.8961182194048545 | validation: 0.9188252527713161]
	TIME [epoch: 1.12 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9827749530283733		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.9827749530283733 | validation: 0.7129100429540837]
	TIME [epoch: 1.12 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.833459401742096		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.833459401742096 | validation: 0.645602625411677]
	TIME [epoch: 1.12 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7502279995188438		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.7502279995188438 | validation: 0.6893807743917262]
	TIME [epoch: 1.12 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7752838780636736		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.7752838780636736 | validation: 0.6510520972311071]
	TIME [epoch: 1.12 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7889985354893517		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.7889985354893517 | validation: 0.7233298072169169]
	TIME [epoch: 1.12 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7822668046353914		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.7822668046353914 | validation: 0.6088144263332144]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_288.pth
	Model improved!!!
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7834481683613143		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.7834481683613143 | validation: 0.850823444849981]
	TIME [epoch: 1.12 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8314365272063657		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.8314365272063657 | validation: 0.6401003521840076]
	TIME [epoch: 1.13 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8511485908662842		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.8511485908662842 | validation: 0.801569567141195]
	TIME [epoch: 1.12 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8253433354726221		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.8253433354726221 | validation: 0.6084038747221525]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_292.pth
	Model improved!!!
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7729298369095434		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.7729298369095434 | validation: 0.6878873787856916]
	TIME [epoch: 1.12 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.763139025504275		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.763139025504275 | validation: 0.6316373610355407]
	TIME [epoch: 1.12 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7618427584590731		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.7618427584590731 | validation: 0.7095182037256788]
	TIME [epoch: 1.12 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7726337910391823		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.7726337910391823 | validation: 0.6509710163689777]
	TIME [epoch: 1.12 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8044436690768713		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.8044436690768713 | validation: 0.7788981177637182]
	TIME [epoch: 1.12 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7970639449869349		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.7970639449869349 | validation: 0.6023991459871072]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_298.pth
	Model improved!!!
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8272365320720784		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.8272365320720784 | validation: 0.9235703516156559]
	TIME [epoch: 1.12 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8868597471811804		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.8868597471811804 | validation: 0.6029816665278996]
	TIME [epoch: 1.12 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8389398049049979		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.8389398049049979 | validation: 0.6481713980519334]
	TIME [epoch: 1.12 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7527992354854756		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.7527992354854756 | validation: 0.7014843136653399]
	TIME [epoch: 1.12 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7726027006997538		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.7726027006997538 | validation: 0.6549252599632581]
	TIME [epoch: 1.12 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.789724081773152		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.789724081773152 | validation: 0.6388385787298024]
	TIME [epoch: 1.12 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7436811847772785		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.7436811847772785 | validation: 0.6781224798377052]
	TIME [epoch: 1.12 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7465706716450152		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.7465706716450152 | validation: 0.6518684793286024]
	TIME [epoch: 1.12 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7984496236342626		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.7984496236342626 | validation: 0.8675155911014759]
	TIME [epoch: 1.12 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8358843950952581		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.8358843950952581 | validation: 0.5871367534721141]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7600160307751586		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.7600160307751586 | validation: 0.6967720763593198]
	TIME [epoch: 1.12 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7535665512571762		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.7535665512571762 | validation: 0.5617943556496031]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_310.pth
	Model improved!!!
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7742443440242989		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.7742443440242989 | validation: 0.7918977877703944]
	TIME [epoch: 1.12 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8399626243364123		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.8399626243364123 | validation: 0.598020941105051]
	TIME [epoch: 1.12 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8625633064827705		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.8625633064827705 | validation: 0.7222742103919512]
	TIME [epoch: 1.12 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7558118145502671		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.7558118145502671 | validation: 0.6667398783249778]
	TIME [epoch: 1.13 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7940597694451139		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.7940597694451139 | validation: 0.6518638315729159]
	TIME [epoch: 1.12 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7562079952004817		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.7562079952004817 | validation: 0.6426573803080553]
	TIME [epoch: 1.12 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7296384665852437		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.7296384665852437 | validation: 0.6056888225493493]
	TIME [epoch: 1.12 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7352302842008658		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.7352302842008658 | validation: 0.6547488213668519]
	TIME [epoch: 1.12 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7564269474727187		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.7564269474727187 | validation: 0.6550057514179801]
	TIME [epoch: 1.12 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7989835341209613		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.7989835341209613 | validation: 0.8325556288894564]
	TIME [epoch: 1.12 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.82864787328099		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.82864787328099 | validation: 0.6339780542549943]
	TIME [epoch: 1.12 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8031477650863199		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.8031477650863199 | validation: 0.8169596730162001]
	TIME [epoch: 1.12 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.833965027367436		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.833965027367436 | validation: 0.5760656945188741]
	TIME [epoch: 1.12 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.873516755043531		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.873516755043531 | validation: 0.6760638078742978]
	TIME [epoch: 1.12 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.738690349429572		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.738690349429572 | validation: 0.6089066216746777]
	TIME [epoch: 1.12 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.733294018339999		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.733294018339999 | validation: 0.6200829969172131]
	TIME [epoch: 1.12 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7356895446734985		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.7356895446734985 | validation: 0.6646153718551856]
	TIME [epoch: 1.12 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7607381921540435		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.7607381921540435 | validation: 0.5995217037127862]
	TIME [epoch: 1.12 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7577508822301803		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.7577508822301803 | validation: 0.6939955353624625]
	TIME [epoch: 1.12 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7516871046412475		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.7516871046412475 | validation: 0.5683650691087804]
	TIME [epoch: 1.12 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.774027514050961		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.774027514050961 | validation: 0.8165018579093535]
	TIME [epoch: 1.12 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8012824365955864		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.8012824365955864 | validation: 0.6090994582568587]
	TIME [epoch: 1.12 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8028052994268731		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.8028052994268731 | validation: 0.8334224493599449]
	TIME [epoch: 1.12 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8580725949287757		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.8580725949287757 | validation: 0.5996957728084239]
	TIME [epoch: 1.12 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7590127931075225		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.7590127931075225 | validation: 0.6243214887738789]
	TIME [epoch: 1.12 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7124389716635683		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.7124389716635683 | validation: 0.6144100442534063]
	TIME [epoch: 1.12 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7133212073530922		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.7133212073530922 | validation: 0.5730935450186251]
	TIME [epoch: 1.12 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7238346676057066		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.7238346676057066 | validation: 0.7154367674145177]
	TIME [epoch: 1.12 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7544271468937674		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.7544271468937674 | validation: 0.5664761959028749]
	TIME [epoch: 1.12 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8393794425171889		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.8393794425171889 | validation: 0.7407196517476846]
	TIME [epoch: 1.12 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7672385225924933		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.7672385225924933 | validation: 0.711878021644001]
	TIME [epoch: 1.13 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8345640138818444		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.8345640138818444 | validation: 0.6032297102657935]
	TIME [epoch: 1.12 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7184552571240389		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.7184552571240389 | validation: 0.6469646530357914]
	TIME [epoch: 1.12 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7453409313330471		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.7453409313330471 | validation: 0.6394109577958992]
	TIME [epoch: 1.12 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7660544014537422		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.7660544014537422 | validation: 0.622374801377946]
	TIME [epoch: 1.12 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7114054599871035		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.7114054599871035 | validation: 0.560733644086506]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_346.pth
	Model improved!!!
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7007396483589315		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.7007396483589315 | validation: 0.7116582049812018]
	TIME [epoch: 1.12 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7557102547832619		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.7557102547832619 | validation: 0.6369714102267396]
	TIME [epoch: 1.12 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8818064210584464		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.8818064210584464 | validation: 0.9324466214316103]
	TIME [epoch: 1.12 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9031094579944435		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.9031094579944435 | validation: 0.7343495887747701]
	TIME [epoch: 1.12 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8591283970941206		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.8591283970941206 | validation: 0.5969353081043082]
	TIME [epoch: 1.12 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7572443572997091		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.7572443572997091 | validation: 0.6808431664756466]
	TIME [epoch: 1.12 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7642685755432364		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.7642685755432364 | validation: 0.6000287344481724]
	TIME [epoch: 1.12 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7074939900919388		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.7074939900919388 | validation: 0.581155221766673]
	TIME [epoch: 1.12 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7192997532178629		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.7192997532178629 | validation: 0.6399597702757851]
	TIME [epoch: 1.12 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7080358971901185		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.7080358971901185 | validation: 0.5805846793128827]
	TIME [epoch: 1.12 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6998020490331516		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.6998020490331516 | validation: 0.5898389301937953]
	TIME [epoch: 1.12 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.700529763722206		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.700529763722206 | validation: 0.601999117914561]
	TIME [epoch: 1.12 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.713320209204702		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.713320209204702 | validation: 0.5482052691715362]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_359.pth
	Model improved!!!
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7363670847145202		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.7363670847145202 | validation: 0.7258724338269686]
	TIME [epoch: 1.12 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.772422748710381		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.772422748710381 | validation: 0.6674161805276836]
	TIME [epoch: 1.12 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.937535009742459		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.937535009742459 | validation: 0.9781796083433344]
	TIME [epoch: 1.12 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8916427625965005		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.8916427625965005 | validation: 0.7423008441226282]
	TIME [epoch: 1.12 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8186687358981231		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.8186687358981231 | validation: 0.6678328547058738]
	TIME [epoch: 1.12 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8583230740449761		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.8583230740449761 | validation: 0.6815045505097537]
	TIME [epoch: 1.12 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7456118622450177		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.7456118622450177 | validation: 0.6321494229874921]
	TIME [epoch: 1.13 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7297184453341079		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.7297184453341079 | validation: 0.5760208967996882]
	TIME [epoch: 1.12 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7247497964630395		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.7247497964630395 | validation: 0.6586747817290997]
	TIME [epoch: 1.12 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7151032534792237		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.7151032534792237 | validation: 0.5876639272132727]
	TIME [epoch: 1.12 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7157762296146257		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.7157762296146257 | validation: 0.6453408750064799]
	TIME [epoch: 1.12 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.720508579597483		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.720508579597483 | validation: 0.5876221115314845]
	TIME [epoch: 1.12 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7182500541561376		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.7182500541561376 | validation: 0.6012421279280157]
	TIME [epoch: 1.12 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6971528921966718		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.6971528921966718 | validation: 0.5431800022181629]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_373.pth
	Model improved!!!
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7030992319004264		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.7030992319004264 | validation: 0.7315805377043332]
	TIME [epoch: 1.12 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7440084170846302		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.7440084170846302 | validation: 0.5906866536970924]
	TIME [epoch: 1.12 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8251911649497459		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.8251911649497459 | validation: 0.8049841928170738]
	TIME [epoch: 1.12 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7810711518522507		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.7810711518522507 | validation: 0.5783672446001147]
	TIME [epoch: 1.12 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7236532887802178		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.7236532887802178 | validation: 0.5828494714937957]
	TIME [epoch: 1.12 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7327189113363121		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.7327189113363121 | validation: 0.6519796893136436]
	TIME [epoch: 1.12 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7006393020607357		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.7006393020607357 | validation: 0.5604699808394001]
	TIME [epoch: 1.12 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7012416410282409		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.7012416410282409 | validation: 0.6503243216299612]
	TIME [epoch: 1.12 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7147363467695348		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.7147363467695348 | validation: 0.5484863874239765]
	TIME [epoch: 1.12 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7298326373602771		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.7298326373602771 | validation: 0.6766052322996537]
	TIME [epoch: 1.12 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7282797896494035		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.7282797896494035 | validation: 0.5385215754192575]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_384.pth
	Model improved!!!
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7244322095978889		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.7244322095978889 | validation: 0.6658602574908445]
	TIME [epoch: 1.12 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7099834947288673		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.7099834947288673 | validation: 0.5462869089854474]
	TIME [epoch: 1.12 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7094198152544547		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.7094198152544547 | validation: 0.6963252110176716]
	TIME [epoch: 1.12 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7162703580782288		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.7162703580782288 | validation: 0.564328138015091]
	TIME [epoch: 1.12 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7171582729609967		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.7171582729609967 | validation: 0.7075128493910245]
	TIME [epoch: 1.12 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7275232524399456		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.7275232524399456 | validation: 0.5362451640865604]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_390.pth
	Model improved!!!
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7247049445387077		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.7247049445387077 | validation: 0.7124696964790024]
	TIME [epoch: 1.13 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7449388196664819		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.7449388196664819 | validation: 0.5670616602670581]
	TIME [epoch: 1.12 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.736190901421704		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.736190901421704 | validation: 0.7003909641866664]
	TIME [epoch: 1.12 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7379373305874601		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.7379373305874601 | validation: 0.6299094222829255]
	TIME [epoch: 1.12 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7566211159781233		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.7566211159781233 | validation: 0.5340686866282897]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_395.pth
	Model improved!!!
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6921975110094684		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.6921975110094684 | validation: 0.6481511824369082]
	TIME [epoch: 1.12 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7028202296401449		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.7028202296401449 | validation: 0.569461809553309]
	TIME [epoch: 1.12 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6995018117167974		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.6995018117167974 | validation: 0.5906791126563947]
	TIME [epoch: 1.12 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6782669766227778		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.6782669766227778 | validation: 0.5841325304437732]
	TIME [epoch: 1.12 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6876413297390467		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.6876413297390467 | validation: 0.5427728988958587]
	TIME [epoch: 1.12 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7101289962025692		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.7101289962025692 | validation: 0.7166913644835066]
	TIME [epoch: 1.12 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7426996357958388		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.7426996357958388 | validation: 0.5704998509652663]
	TIME [epoch: 1.12 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8363363123686199		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.8363363123686199 | validation: 0.8505195189862725]
	TIME [epoch: 1.12 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7897617596896825		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.7897617596896825 | validation: 0.6005206365675667]
	TIME [epoch: 1.12 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7181103598561259		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.7181103598561259 | validation: 0.5242332470552926]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_405.pth
	Model improved!!!
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7164292355422285		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.7164292355422285 | validation: 0.7104104165465241]
	TIME [epoch: 1.12 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7178501941542719		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.7178501941542719 | validation: 0.5548667087644104]
	TIME [epoch: 1.13 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6855710057998539		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.6855710057998539 | validation: 0.5809175638972948]
	TIME [epoch: 1.17 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6671768456100248		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.6671768456100248 | validation: 0.5794120357504541]
	TIME [epoch: 1.12 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6745803654594211		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.6745803654594211 | validation: 0.5607343636665538]
	TIME [epoch: 1.12 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7027840728931274		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.7027840728931274 | validation: 0.6072190765117079]
	TIME [epoch: 1.12 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6823124810153857		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.6823124810153857 | validation: 0.5150527630686741]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_412.pth
	Model improved!!!
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6919306741570774		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.6919306741570774 | validation: 0.6933090860121071]
	TIME [epoch: 1.12 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7331011178669137		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.7331011178669137 | validation: 0.5558628446555386]
	TIME [epoch: 1.12 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8140058376893242		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.8140058376893242 | validation: 0.7898836366038694]
	TIME [epoch: 1.13 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7614453888166185		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.7614453888166185 | validation: 0.6828601552881624]
	TIME [epoch: 1.12 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.77163113264617		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.77163113264617 | validation: 0.5526314452865453]
	TIME [epoch: 1.12 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7340050227227066		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.7340050227227066 | validation: 0.6896288966076566]
	TIME [epoch: 1.12 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7303972152341343		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.7303972152341343 | validation: 0.5729372257844281]
	TIME [epoch: 1.12 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6709413141067111		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.6709413141067111 | validation: 0.5464577565101123]
	TIME [epoch: 1.12 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6830533299248469		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.6830533299248469 | validation: 0.6607527527336557]
	TIME [epoch: 1.12 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6808996389645902		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.6808996389645902 | validation: 0.5374500830657355]
	TIME [epoch: 1.12 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6808062594567262		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.6808062594567262 | validation: 0.670265436385168]
	TIME [epoch: 1.12 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6957506110087656		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.6957506110087656 | validation: 0.535996058829625]
	TIME [epoch: 1.12 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7404981046380258		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.7404981046380258 | validation: 0.6115565414155038]
	TIME [epoch: 1.12 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6676956597577436		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.6676956597577436 | validation: 0.5409606759853679]
	TIME [epoch: 1.12 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6673733190008199		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.6673733190008199 | validation: 0.6223385198878623]
	TIME [epoch: 1.12 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.676489974673097		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.676489974673097 | validation: 0.5841847425856809]
	TIME [epoch: 1.12 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7060169834445995		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.7060169834445995 | validation: 0.5662143599780276]
	TIME [epoch: 1.12 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6792038899079239		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.6792038899079239 | validation: 0.5659710438851769]
	TIME [epoch: 1.12 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6594997682648226		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.6594997682648226 | validation: 0.5338363091843041]
	TIME [epoch: 1.12 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6612364086727901		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.6612364086727901 | validation: 0.6879688038587439]
	TIME [epoch: 1.12 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7043341173806803		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.7043341173806803 | validation: 0.5784955781850636]
	TIME [epoch: 1.12 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8638519595709195		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.8638519595709195 | validation: 0.8476348806095763]
	TIME [epoch: 1.12 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8345072551732904		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.8345072551732904 | validation: 0.6367028879446359]
	TIME [epoch: 1.12 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7571332168170064		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.7571332168170064 | validation: 0.5370358426734659]
	TIME [epoch: 1.12 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7344019012179724		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.7344019012179724 | validation: 0.6708258461377784]
	TIME [epoch: 1.12 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7085693085212563		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.7085693085212563 | validation: 0.5540793109166495]
	TIME [epoch: 1.12 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6733701093653415		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.6733701093653415 | validation: 0.5566180432188519]
	TIME [epoch: 1.12 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6585175306273151		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.6585175306273151 | validation: 0.5792607970274992]
	TIME [epoch: 1.12 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6465914424382748		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.6465914424382748 | validation: 0.5745588875046277]
	TIME [epoch: 1.12 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6590138070354729		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.6590138070354729 | validation: 0.5466464819312093]
	TIME [epoch: 1.13 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.669115879243481		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.669115879243481 | validation: 0.6494793552984106]
	TIME [epoch: 1.12 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6780315716129949		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.6780315716129949 | validation: 0.5183672254711017]
	TIME [epoch: 1.12 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7420708482127785		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.7420708482127785 | validation: 0.7709263452289806]
	TIME [epoch: 1.12 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7266857842551615		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.7266857842551615 | validation: 0.5370762257622473]
	TIME [epoch: 1.12 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.66827795369419		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.66827795369419 | validation: 0.5867029440059668]
	TIME [epoch: 1.12 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6494202807868135		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.6494202807868135 | validation: 0.5579157212694321]
	TIME [epoch: 1.12 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6409462633205039		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.6409462633205039 | validation: 0.5288945693307333]
	TIME [epoch: 1.12 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6543097578773828		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.6543097578773828 | validation: 0.6798708664946179]
	TIME [epoch: 1.12 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6943683238326603		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.6943683238326603 | validation: 0.5420832462190888]
	TIME [epoch: 1.12 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7538838230412389		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.7538838230412389 | validation: 0.6994734307340154]
	TIME [epoch: 1.12 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6910090450466089		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.6910090450466089 | validation: 0.6288031907210304]
	TIME [epoch: 1.12 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7432514449659542		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.7432514449659542 | validation: 0.5080149100158108]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_454.pth
	Model improved!!!
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.657281239530497		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.657281239530497 | validation: 0.6920628392255672]
	TIME [epoch: 1.12 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7003512689797614		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.7003512689797614 | validation: 0.5580802486881165]
	TIME [epoch: 1.12 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6767781572061494		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.6767781572061494 | validation: 0.5462635098140725]
	TIME [epoch: 1.12 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6381427775292516		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.6381427775292516 | validation: 0.6397749406638225]
	TIME [epoch: 1.12 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6533716912543416		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.6533716912543416 | validation: 0.5393794041737169]
	TIME [epoch: 1.12 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6781947373359256		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.6781947373359256 | validation: 0.7096741644374216]
	TIME [epoch: 1.12 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7010464088372794		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.7010464088372794 | validation: 0.5206769930725557]
	TIME [epoch: 1.12 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.75161157589716		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.75161157589716 | validation: 0.5839743123973394]
	TIME [epoch: 1.12 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6606404697440479		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.6606404697440479 | validation: 0.6507284094572521]
	TIME [epoch: 1.12 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7325394399192037		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.7325394399192037 | validation: 0.512556439546881]
	TIME [epoch: 1.12 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6903251664627462		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.6903251664627462 | validation: 0.7446734926475813]
	TIME [epoch: 1.12 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7062838120858033		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.7062838120858033 | validation: 0.5261898979745399]
	TIME [epoch: 1.12 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.649478459747495		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.649478459747495 | validation: 0.5299913644580617]
	TIME [epoch: 1.12 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6389693017768222		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.6389693017768222 | validation: 0.6251216179469685]
	TIME [epoch: 1.13 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6495697949095245		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.6495697949095245 | validation: 0.508140954660718]
	TIME [epoch: 1.12 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6847036161218881		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.6847036161218881 | validation: 0.6762009018940015]
	TIME [epoch: 1.12 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6840014171723019		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.6840014171723019 | validation: 0.5159085737170171]
	TIME [epoch: 1.12 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6940876001450886		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.6940876001450886 | validation: 0.57634559653304]
	TIME [epoch: 1.12 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6333827012281448		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.6333827012281448 | validation: 0.5423514140014708]
	TIME [epoch: 1.12 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6275083143569655		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.6275083143569655 | validation: 0.5412422917763349]
	TIME [epoch: 1.12 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6416923271551703		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.6416923271551703 | validation: 0.572019371788758]
	TIME [epoch: 1.12 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6547435747009923		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.6547435747009923 | validation: 0.5452962988079273]
	TIME [epoch: 1.12 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6705865271227386		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.6705865271227386 | validation: 0.5743109759725084]
	TIME [epoch: 1.12 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6326137284485567		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.6326137284485567 | validation: 0.5142921232710578]
	TIME [epoch: 1.12 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6254988838335122		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.6254988838335122 | validation: 0.6441260536038866]
	TIME [epoch: 1.12 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6516862570429794		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.6516862570429794 | validation: 0.5466875395432226]
	TIME [epoch: 1.12 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7575215200180919		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.7575215200180919 | validation: 0.7874267690726002]
	TIME [epoch: 1.12 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.720238108053627		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.720238108053627 | validation: 0.600426630361044]
	TIME [epoch: 1.12 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7332223995758023		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.7332223995758023 | validation: 0.4964716376408772]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_483.pth
	Model improved!!!
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6694092723646364		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.6694092723646364 | validation: 0.744366863230854]
	TIME [epoch: 1.12 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.69904618076484		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.69904618076484 | validation: 0.5109718151603294]
	TIME [epoch: 1.12 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.639404496382288		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.639404496382288 | validation: 0.5802649165060187]
	TIME [epoch: 1.12 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6473429811626262		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.6473429811626262 | validation: 0.5407589498167145]
	TIME [epoch: 1.12 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6538058777586068		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.6538058777586068 | validation: 0.5732259901749288]
	TIME [epoch: 1.12 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6230199928875892		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.6230199928875892 | validation: 0.5302949012667721]
	TIME [epoch: 1.13 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6195389081716808		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.6195389081716808 | validation: 0.5875357927924417]
	TIME [epoch: 1.13 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6305456399358477		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.6305456399358477 | validation: 0.5092890227207162]
	TIME [epoch: 1.12 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.665151242749131		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.665151242749131 | validation: 0.7193461682641678]
	TIME [epoch: 1.13 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6901140582781516		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.6901140582781516 | validation: 0.5111835222294184]
	TIME [epoch: 1.13 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6924158763452953		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.6924158763452953 | validation: 0.6670432549820586]
	TIME [epoch: 1.13 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6498171242978336		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.6498171242978336 | validation: 0.5209209094308032]
	TIME [epoch: 1.13 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6212864703018321		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.6212864703018321 | validation: 0.5695078004672864]
	TIME [epoch: 1.13 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6158693417395561		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.6158693417395561 | validation: 0.553971372988283]
	TIME [epoch: 1.13 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6167950401727802		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.6167950401727802 | validation: 0.5604036600946672]
	TIME [epoch: 1.13 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6310735895778153		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.6310735895778153 | validation: 0.5557533518915007]
	TIME [epoch: 1.13 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6646794378744969		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.6646794378744969 | validation: 0.5265678327308196]
	TIME [epoch: 1.13 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6289222776118354		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.6289222776118354 | validation: 0.5831078413410978]
	TIME [epoch: 175 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6108427246726176		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.6108427246726176 | validation: 0.505028995207414]
	TIME [epoch: 2.24 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6518497731870668		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.6518497731870668 | validation: 0.7226905854172289]
	TIME [epoch: 2.22 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.692571535860358		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.692571535860358 | validation: 0.5380202108468423]
	TIME [epoch: 2.22 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7183337851824726		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.7183337851824726 | validation: 0.6913394224120151]
	TIME [epoch: 2.22 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6564970176887345		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.6564970176887345 | validation: 0.5877233419498924]
	TIME [epoch: 2.23 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.675595229659692		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.675595229659692 | validation: 0.48506815782053003]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_507.pth
	Model improved!!!
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6480864185226415		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.6480864185226415 | validation: 0.6863631443240946]
	TIME [epoch: 2.22 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6466775620112081		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.6466775620112081 | validation: 0.5226949844849592]
	TIME [epoch: 2.22 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6287373102916722		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.6287373102916722 | validation: 0.5576345549479662]
	TIME [epoch: 2.23 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6068399780480178		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.6068399780480178 | validation: 0.5752605985994875]
	TIME [epoch: 2.22 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6039303984967305		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.6039303984967305 | validation: 0.5105475412833468]
	TIME [epoch: 2.23 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6092237192216267		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.6092237192216267 | validation: 0.6186815265408627]
	TIME [epoch: 2.23 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.610301988339161		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.610301988339161 | validation: 0.49633841283006674]
	TIME [epoch: 2.23 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6477800674067244		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.6477800674067244 | validation: 0.7179836884761245]
	TIME [epoch: 2.23 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6761826295732232		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.6761826295732232 | validation: 0.5046136811327119]
	TIME [epoch: 2.22 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6652057847949672		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.6652057847949672 | validation: 0.5802491939307565]
	TIME [epoch: 2.22 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6007280373428929		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.6007280373428929 | validation: 0.5287786133009225]
	TIME [epoch: 2.22 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5913558410600617		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.5913558410600617 | validation: 0.5422049279628255]
	TIME [epoch: 2.22 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5966369098301789		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.5966369098301789 | validation: 0.5377043911710659]
	TIME [epoch: 2.22 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5941921418590171		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.5941921418590171 | validation: 0.5293357684399946]
	TIME [epoch: 2.22 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6086675962033995		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.6086675962033995 | validation: 0.5915155328658536]
	TIME [epoch: 2.22 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6607914435117413		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.6607914435117413 | validation: 0.536073286252137]
	TIME [epoch: 2.22 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6698791864989877		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.6698791864989877 | validation: 0.7419865806440997]
	TIME [epoch: 2.22 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6840225602175974		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.6840225602175974 | validation: 0.48356700521352836]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_525.pth
	Model improved!!!
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7371187270316554		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.7371187270316554 | validation: 0.5855098618632317]
	TIME [epoch: 2.23 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6434672052335233		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.6434672052335233 | validation: 0.632660548121703]
	TIME [epoch: 2.22 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7014276708635848		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.7014276708635848 | validation: 0.4958816325669281]
	TIME [epoch: 2.22 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6128274946849983		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.6128274946849983 | validation: 0.6362777231282207]
	TIME [epoch: 2.22 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6537801379316155		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.6537801379316155 | validation: 0.515705569503899]
	TIME [epoch: 2.22 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5978887277709205		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.5978887277709205 | validation: 0.5498719784298606]
	TIME [epoch: 2.23 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6026128550707045		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.6026128550707045 | validation: 0.514212244481889]
	TIME [epoch: 2.23 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.612418191206111		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.612418191206111 | validation: 0.6247674129964795]
	TIME [epoch: 2.22 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6184069486562099		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.6184069486562099 | validation: 0.4940517696976919]
	TIME [epoch: 2.24 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6551934924138704		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.6551934924138704 | validation: 0.7359797200215226]
	TIME [epoch: 2.22 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6634745249014635		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.6634745249014635 | validation: 0.5148376371534598]
	TIME [epoch: 2.23 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5941452532628654		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.5941452532628654 | validation: 0.5589892049268488]
	TIME [epoch: 2.24 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5909187286373816		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.5909187286373816 | validation: 0.524882630252049]
	TIME [epoch: 2.24 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5957444967864417		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.5957444967864417 | validation: 0.5615485232771459]
	TIME [epoch: 2.23 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5965762438358764		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.5965762438358764 | validation: 0.5062715195716283]
	TIME [epoch: 2.22 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5916233695247225		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.5916233695247225 | validation: 0.6216425212440196]
	TIME [epoch: 2.23 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6140621122187612		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.6140621122187612 | validation: 0.500578428430012]
	TIME [epoch: 2.22 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6683204555638019		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.6683204555638019 | validation: 0.7249505376582183]
	TIME [epoch: 2.22 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.649355193406595		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.649355193406595 | validation: 0.5098564295588287]
	TIME [epoch: 2.22 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5958007498435507		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.5958007498435507 | validation: 0.5640466940216821]
	TIME [epoch: 2.22 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5785293359500993		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.5785293359500993 | validation: 0.5148140829714661]
	TIME [epoch: 2.22 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5767788426185385		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.5767788426185385 | validation: 0.5342629156864145]
	TIME [epoch: 2.22 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5812936453924479		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.5812936453924479 | validation: 0.5223680291428318]
	TIME [epoch: 2.22 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5974615338192764		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.5974615338192764 | validation: 0.6001013488436592]
	TIME [epoch: 2.22 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6241794074528102		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.6241794074528102 | validation: 0.48936143478060734]
	TIME [epoch: 2.22 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6182115465698662		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.6182115465698662 | validation: 0.7291026416846557]
	TIME [epoch: 2.22 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6446668088605344		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.6446668088605344 | validation: 0.5064396443568334]
	TIME [epoch: 2.23 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6363684337823832		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.6363684337823832 | validation: 0.6590555060810701]
	TIME [epoch: 2.22 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6170424924814968		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.6170424924814968 | validation: 0.5271313358170312]
	TIME [epoch: 2.22 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5973440616748488		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.5973440616748488 | validation: 0.520824902177245]
	TIME [epoch: 2.22 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5882334799413127		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.5882334799413127 | validation: 0.5550632723299769]
	TIME [epoch: 2.22 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5809659364214913		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.5809659364214913 | validation: 0.530818693189414]
	TIME [epoch: 2.22 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5722937580858486		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.5722937580858486 | validation: 0.5208198595769556]
	TIME [epoch: 2.22 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5727828716429497		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.5727828716429497 | validation: 0.5427261102376717]
	TIME [epoch: 2.22 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.580905263387762		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.580905263387762 | validation: 0.49860469784417316]
	TIME [epoch: 2.22 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5818576484614636		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.5818576484614636 | validation: 0.5785364339725417]
	TIME [epoch: 2.22 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5857296850927598		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.5857296850927598 | validation: 0.4749068425989716]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_562.pth
	Model improved!!!
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6173381591162551		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.6173381591162551 | validation: 0.7587460924665186]
	TIME [epoch: 2.22 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6637476340880107		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.6637476340880107 | validation: 0.5082985692661691]
	TIME [epoch: 2.22 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6463423412221649		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.6463423412221649 | validation: 0.5826563540299227]
	TIME [epoch: 2.22 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5638548300411251		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.5638548300411251 | validation: 0.5321291291921441]
	TIME [epoch: 2.22 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5789319029081551		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.5789319029081551 | validation: 0.5066386742812636]
	TIME [epoch: 2.22 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5888350292006044		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.5888350292006044 | validation: 0.5523532499310715]
	TIME [epoch: 2.22 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5675985691727976		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.5675985691727976 | validation: 0.5014677617017788]
	TIME [epoch: 2.24 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5632258110439425		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.5632258110439425 | validation: 0.5747988621300923]
	TIME [epoch: 2.22 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5645543627909744		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.5645543627909744 | validation: 0.4771969958154735]
	TIME [epoch: 2.22 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6198055810785948		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.6198055810785948 | validation: 0.7971302320135149]
	TIME [epoch: 2.22 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.681982483926295		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.681982483926295 | validation: 0.49041749236769533]
	TIME [epoch: 2.22 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5998789143545638		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.5998789143545638 | validation: 0.5351500272527574]
	TIME [epoch: 2.22 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5606001806009457		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.5606001806009457 | validation: 0.5604459665583882]
	TIME [epoch: 2.22 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5635711192481911		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.5635711192481911 | validation: 0.5238714744346045]
	TIME [epoch: 2.22 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5886809551862515		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.5886809551862515 | validation: 0.5517885584650782]
	TIME [epoch: 2.22 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.557106857384955		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.557106857384955 | validation: 0.5064762039202352]
	TIME [epoch: 2.23 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.555799518404771		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.555799518404771 | validation: 0.5339495398655058]
	TIME [epoch: 2.22 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5414779346858837		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.5414779346858837 | validation: 0.5012569667180382]
	TIME [epoch: 2.22 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5470951017631263		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.5470951017631263 | validation: 0.5865345981943654]
	TIME [epoch: 2.22 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5572060900619986		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.5572060900619986 | validation: 0.4678616268612542]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_582.pth
	Model improved!!!
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6121510048168808		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.6121510048168808 | validation: 0.8355867164452718]
	TIME [epoch: 2.22 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6985974690012487		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.6985974690012487 | validation: 0.49171633388164715]
	TIME [epoch: 2.23 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6343927424553026		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.6343927424553026 | validation: 0.47896643702136443]
	TIME [epoch: 2.22 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5597152195786705		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.5597152195786705 | validation: 0.724106196901145]
	TIME [epoch: 2.22 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6290449595226277		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.6290449595226277 | validation: 0.49121527991290004]
	TIME [epoch: 2.22 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6062860698201825		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.6062860698201825 | validation: 0.5182581631692553]
	TIME [epoch: 2.22 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5470021644406385		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.5470021644406385 | validation: 0.5875024657748859]
	TIME [epoch: 2.22 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5646853123499812		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.5646853123499812 | validation: 0.46905895598667996]
	TIME [epoch: 2.22 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5755229388459323		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.5755229388459323 | validation: 0.5690596776565408]
	TIME [epoch: 2.22 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5593914981998966		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.5593914981998966 | validation: 0.4847672241879717]
	TIME [epoch: 2.22 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5634326455510051		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.5634326455510051 | validation: 0.5471306448709164]
	TIME [epoch: 2.22 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5512030700168634		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.5512030700168634 | validation: 0.5012891533348124]
	TIME [epoch: 2.22 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5516258036986477		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.5516258036986477 | validation: 0.5268181505826433]
	TIME [epoch: 2.22 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5612453350180515		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.5612453350180515 | validation: 0.5066763377110486]
	TIME [epoch: 2.22 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5648184217086952		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.5648184217086952 | validation: 0.5119664194839973]
	TIME [epoch: 2.22 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5453751518473686		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.5453751518473686 | validation: 0.533144594531443]
	TIME [epoch: 2.22 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5387105924414469		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.5387105924414469 | validation: 0.48816565841090453]
	TIME [epoch: 2.22 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5432630645512896		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.5432630645512896 | validation: 0.602747753092991]
	TIME [epoch: 2.23 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5654096184161567		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.5654096184161567 | validation: 0.4911024683531093]
	TIME [epoch: 2.22 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6112263666286561		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.6112263666286561 | validation: 0.7080830406079448]
	TIME [epoch: 2.24 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6098597840324209		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.6098597840324209 | validation: 0.4666717089083255]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_603.pth
	Model improved!!!
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6384623805247249		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.6384623805247249 | validation: 0.5314289544917079]
	TIME [epoch: 2.23 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5323164205706327		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.5323164205706327 | validation: 0.5145415958297938]
	TIME [epoch: 2.23 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5366803201506086		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.5366803201506086 | validation: 0.49726625413691866]
	TIME [epoch: 2.22 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5476202736391957		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.5476202736391957 | validation: 0.5449687672537856]
	TIME [epoch: 2.22 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5341334381874527		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.5341334381874527 | validation: 0.46055375625187533]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_608.pth
	Model improved!!!
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5634069015335564		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.5634069015335564 | validation: 0.6715287356265571]
	TIME [epoch: 2.22 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5907664132609819		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.5907664132609819 | validation: 0.47842420911355277]
	TIME [epoch: 2.24 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5977066469549199		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.5977066469549199 | validation: 0.5966416167132053]
	TIME [epoch: 2.22 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5462430338114264		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.5462430338114264 | validation: 0.480761649059726]
	TIME [epoch: 2.22 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5271330264526504		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.5271330264526504 | validation: 0.5278443086559633]
	TIME [epoch: 2.22 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5167068517868691		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.5167068517868691 | validation: 0.48143707123163715]
	TIME [epoch: 2.23 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5180986376094835		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.5180986376094835 | validation: 0.530431000527522]
	TIME [epoch: 2.22 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5182317322298977		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.5182317322298977 | validation: 0.46948361403637345]
	TIME [epoch: 2.23 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5316442978463809		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.5316442978463809 | validation: 0.6301719409774037]
	TIME [epoch: 2.22 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5554084154765254		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.5554084154765254 | validation: 0.47442481563301203]
	TIME [epoch: 2.22 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5964125792183093		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.5964125792183093 | validation: 0.6978563860596064]
	TIME [epoch: 2.22 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5957709886757006		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.5957709886757006 | validation: 0.47618225046465323]
	TIME [epoch: 2.22 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.593673696410553		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.593673696410553 | validation: 0.49001747776223675]
	TIME [epoch: 2.22 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.535523314565359		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.535523314565359 | validation: 0.5495557175537245]
	TIME [epoch: 2.22 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5251945897970063		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.5251945897970063 | validation: 0.45001283666414643]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_623.pth
	Model improved!!!
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5311807045018736		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.5311807045018736 | validation: 0.5779507674981768]
	TIME [epoch: 2.21 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5469903026515263		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.5469903026515263 | validation: 0.47427697470577324]
	TIME [epoch: 2.23 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5940544516866038		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.5940544516866038 | validation: 0.5490207862598392]
	TIME [epoch: 2.21 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5203980142488251		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.5203980142488251 | validation: 0.46981382951261574]
	TIME [epoch: 2.21 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5183458244344042		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.5183458244344042 | validation: 0.5422668071812368]
	TIME [epoch: 2.21 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5180330690989519		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.5180330690989519 | validation: 0.4717149792103877]
	TIME [epoch: 2.23 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5232464623393617		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.5232464623393617 | validation: 0.5622537840782252]
	TIME [epoch: 2.21 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5285557159181365		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.5285557159181365 | validation: 0.4472255562864944]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_631.pth
	Model improved!!!
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5215805221230713		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.5215805221230713 | validation: 0.6000765961706092]
	TIME [epoch: 2.22 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5387565675471621		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.5387565675471621 | validation: 0.4459916597640371]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_633.pth
	Model improved!!!
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5881632902498563		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.5881632902498563 | validation: 0.627546991346223]
	TIME [epoch: 2.22 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5660652981039418		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.5660652981039418 | validation: 0.4644724323713497]
	TIME [epoch: 2.23 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5560655857994965		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.5560655857994965 | validation: 0.5219551240161332]
	TIME [epoch: 2.21 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5082155427814304		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.5082155427814304 | validation: 0.4871562080866408]
	TIME [epoch: 2.22 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4980975012253005		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.4980975012253005 | validation: 0.48696775221281074]
	TIME [epoch: 2.21 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5043505610964135		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.5043505610964135 | validation: 0.49866938016620366]
	TIME [epoch: 2.21 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5014674695308249		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.5014674695308249 | validation: 0.48081699731163474]
	TIME [epoch: 2.22 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5199199603688279		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.5199199603688279 | validation: 0.5584989888813103]
	TIME [epoch: 2.22 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5472676372701936		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.5472676372701936 | validation: 0.42318855430135704]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_642.pth
	Model improved!!!
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5869055472323873		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.5869055472323873 | validation: 0.6971173427347517]
	TIME [epoch: 2.22 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5900807625165487		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.5900807625165487 | validation: 0.500070666545855]
	TIME [epoch: 2.24 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5661608299622652		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.5661608299622652 | validation: 0.45108812266625853]
	TIME [epoch: 2.22 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5020592300462224		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.5020592300462224 | validation: 0.6321875280197903]
	TIME [epoch: 2.22 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5369000946719152		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.5369000946719152 | validation: 0.44184283447438316]
	TIME [epoch: 2.22 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5408140935537997		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.5408140935537997 | validation: 0.5832876771142258]
	TIME [epoch: 2.22 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5326013202288188		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.5326013202288188 | validation: 0.4489217018991326]
	TIME [epoch: 2.22 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5445409926586606		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.5445409926586606 | validation: 0.5032244103943274]
	TIME [epoch: 2.23 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.504685945663715		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.504685945663715 | validation: 0.48816325207698885]
	TIME [epoch: 2.21 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49275775444703107		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.49275775444703107 | validation: 0.4692258361311886]
	TIME [epoch: 2.21 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49018109134723037		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.49018109134723037 | validation: 0.508907553618308]
	TIME [epoch: 2.21 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49921296700663415		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.49921296700663415 | validation: 0.45022599419061526]
	TIME [epoch: 2.22 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5263022243047516		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.5263022243047516 | validation: 0.5826775604155013]
	TIME [epoch: 2.21 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5323270670157194		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.5323270670157194 | validation: 0.4363874823098887]
	TIME [epoch: 2.21 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5758776678558898		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.5758776678558898 | validation: 0.6654906114319833]
	TIME [epoch: 2.23 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5612117703722188		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.5612117703722188 | validation: 0.480986039589806]
	TIME [epoch: 2.22 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5079453549143238		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.5079453549143238 | validation: 0.45661492745622523]
	TIME [epoch: 2.22 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49368696261071743		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.49368696261071743 | validation: 0.5349745453781626]
	TIME [epoch: 2.22 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4972812646289288		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.4972812646289288 | validation: 0.4413945127224899]
	TIME [epoch: 2.21 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5308728235266302		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.5308728235266302 | validation: 0.6100075058199984]
	TIME [epoch: 2.22 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5362132490695655		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.5362132490695655 | validation: 0.44755750457049476]
	TIME [epoch: 2.22 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5249842664548987		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.5249842664548987 | validation: 0.5138362339453311]
	TIME [epoch: 2.22 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49175890416854845		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.49175890416854845 | validation: 0.4808187651176212]
	TIME [epoch: 2.21 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48272755239104764		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.48272755239104764 | validation: 0.4898473866293929]
	TIME [epoch: 2.22 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4986980641097452		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.4986980641097452 | validation: 0.4897548052507805]
	TIME [epoch: 2.22 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.496973671958958		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.496973671958958 | validation: 0.48529394482525545]
	TIME [epoch: 2.21 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.505698359140671		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.505698359140671 | validation: 0.49062314754469044]
	TIME [epoch: 2.21 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4892651487697961		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.4892651487697961 | validation: 0.4676717325219671]
	TIME [epoch: 2.23 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47848264921508127		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.47848264921508127 | validation: 0.4953830172725828]
	TIME [epoch: 2.21 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47828584668737717		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.47828584668737717 | validation: 0.44405593885751815]
	TIME [epoch: 2.21 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4862236498392195		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.4862236498392195 | validation: 0.635615720042425]
	TIME [epoch: 2.21 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5403376786512797		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.5403376786512797 | validation: 0.4463866364033946]
	TIME [epoch: 2.22 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6198682891529655		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.6198682891529655 | validation: 0.5979115048752073]
	TIME [epoch: 2.22 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.520367095703516		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.520367095703516 | validation: 0.4643343755763114]
	TIME [epoch: 2.21 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47166577696631995		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.47166577696631995 | validation: 0.45916926159666205]
	TIME [epoch: 2.21 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47716950411507314		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.47716950411507314 | validation: 0.48320801740625374]
	TIME [epoch: 2.21 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4923994465759786		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.4923994465759786 | validation: 0.49840778154419757]
	TIME [epoch: 2.21 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4986732689031144		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.4986732689031144 | validation: 0.4373463267582416]
	TIME [epoch: 2.21 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5100546030608676		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.5100546030608676 | validation: 0.5860357700667165]
	TIME [epoch: 2.23 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5077202185894868		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.5077202185894868 | validation: 0.43643696596443377]
	TIME [epoch: 2.25 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.519579422535939		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.519579422535939 | validation: 0.5885831270820626]
	TIME [epoch: 2.22 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5021435116955751		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.5021435116955751 | validation: 0.4385124568854166]
	TIME [epoch: 2.21 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5035528518582357		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.5035528518582357 | validation: 0.5124080896701937]
	TIME [epoch: 2.22 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4846886062764396		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.4846886062764396 | validation: 0.4334054005335373]
	TIME [epoch: 2.21 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49012746346130537		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.49012746346130537 | validation: 0.5324323712881004]
	TIME [epoch: 2.21 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48784782006632077		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.48784782006632077 | validation: 0.42579858545457566]
	TIME [epoch: 2.21 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5019034035251211		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.5019034035251211 | validation: 0.5424066866002077]
	TIME [epoch: 2.21 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4838751312384822		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.4838751312384822 | validation: 0.4407926718404749]
	TIME [epoch: 2.21 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4833467716804271		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.4833467716804271 | validation: 0.5807808771336964]
	TIME [epoch: 2.21 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4996362338218927		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.4996362338218927 | validation: 0.4333297856107849]
	TIME [epoch: 2.23 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4883418476843944		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.4883418476843944 | validation: 0.523973425800821]
	TIME [epoch: 2.21 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4769208616112756		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.4769208616112756 | validation: 0.4361536790570689]
	TIME [epoch: 2.22 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4712526487031367		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.4712526487031367 | validation: 0.5180922587675838]
	TIME [epoch: 2.21 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47132232642325844		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.47132232642325844 | validation: 0.41548938802243235]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_696.pth
	Model improved!!!
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.479873427203631		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.479873427203631 | validation: 0.562058723271793]
	TIME [epoch: 2.22 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49518956226538174		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.49518956226538174 | validation: 0.4253725244344703]
	TIME [epoch: 2.23 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5295696511261855		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.5295696511261855 | validation: 0.5514637883006561]
	TIME [epoch: 2.22 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48531260845142493		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.48531260845142493 | validation: 0.41632051301768663]
	TIME [epoch: 2.23 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47625165172064715		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.47625165172064715 | validation: 0.5265596329911304]
	TIME [epoch: 2.21 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47268453950316996		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.47268453950316996 | validation: 0.4229131549673179]
	TIME [epoch: 2.22 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47725943665126863		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.47725943665126863 | validation: 0.5476233238319707]
	TIME [epoch: 2.22 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48293056120798256		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.48293056120798256 | validation: 0.4196802307186947]
	TIME [epoch: 2.22 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4776338813178447		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.4776338813178447 | validation: 0.5783777553001381]
	TIME [epoch: 2.22 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4898689670501743		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.4898689670501743 | validation: 0.4139376878197385]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_706.pth
	Model improved!!!
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4879685755942349		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.4879685755942349 | validation: 0.5404189779943382]
	TIME [epoch: 2.21 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4957284522294026		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.4957284522294026 | validation: 0.42025540240597065]
	TIME [epoch: 2.21 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5029453088712872		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.5029453088712872 | validation: 0.45382953494514644]
	TIME [epoch: 2.21 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4559878562327392		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.4559878562327392 | validation: 0.5039594261915316]
	TIME [epoch: 2.21 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4619263033910738		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.4619263033910738 | validation: 0.41285040890006935]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_711.pth
	Model improved!!!
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4896486022082414		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.4896486022082414 | validation: 0.5514224034185292]
	TIME [epoch: 2.22 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4935960993022041		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.4935960993022041 | validation: 0.4068551581955923]
	TIME [epoch: 2.23 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_713.pth
	Model improved!!!
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47865425332307454		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.47865425332307454 | validation: 0.5275241339335398]
	TIME [epoch: 2.22 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4619815725528567		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.4619815725528567 | validation: 0.41079844227224865]
	TIME [epoch: 2.22 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4623502225493898		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.4623502225493898 | validation: 0.5231214538996628]
	TIME [epoch: 2.22 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.465749027951147		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.465749027951147 | validation: 0.4148953698941138]
	TIME [epoch: 2.22 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47361550020236065		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.47361550020236065 | validation: 0.5266104247192406]
	TIME [epoch: 2.22 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46682883516974505		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.46682883516974505 | validation: 0.40388731779567294]
	TIME [epoch: 2.23 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_719.pth
	Model improved!!!
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47018620208278444		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.47018620208278444 | validation: 0.5048356800002206]
	TIME [epoch: 2.22 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45826929440236824		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.45826929440236824 | validation: 0.4040060840546742]
	TIME [epoch: 2.23 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4572020824734023		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.4572020824734023 | validation: 0.5096527703820047]
	TIME [epoch: 2.22 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4551342910493645		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.4551342910493645 | validation: 0.4072585844550485]
	TIME [epoch: 2.21 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45825131403576236		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.45825131403576236 | validation: 0.5748772060950128]
	TIME [epoch: 2.21 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48069156181517814		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.48069156181517814 | validation: 0.43163269923858344]
	TIME [epoch: 2.23 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48431503093302186		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.48431503093302186 | validation: 0.44731739332175435]
	TIME [epoch: 2.21 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.448188472015371		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.448188472015371 | validation: 0.48523333267621604]
	TIME [epoch: 2.22 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4500413861516573		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.4500413861516573 | validation: 0.4039193992804]
	TIME [epoch: 2.22 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4987074298587916		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.4987074298587916 | validation: 0.6007304725254207]
	TIME [epoch: 2.22 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5001974917661122		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.5001974917661122 | validation: 0.4031372777756903]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_730.pth
	Model improved!!!
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45674261999754295		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.45674261999754295 | validation: 0.469912761650488]
	TIME [epoch: 2.21 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4369047714474765		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.4369047714474765 | validation: 0.4334073068896272]
	TIME [epoch: 2.22 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4382587703055296		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.4382587703055296 | validation: 0.4576985636198315]
	TIME [epoch: 2.21 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44426486379915786		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.44426486379915786 | validation: 0.43546491968697904]
	TIME [epoch: 2.21 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46518739390933955		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.46518739390933955 | validation: 0.4907102847460415]
	TIME [epoch: 2.21 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46241528840160767		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.46241528840160767 | validation: 0.37965307612952226]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_736.pth
	Model improved!!!
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47802483251181005		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.47802483251181005 | validation: 0.6152598111432851]
	TIME [epoch: 2.21 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5044573452577683		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.5044573452577683 | validation: 0.40517875556171784]
	TIME [epoch: 2.21 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4529830940831846		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.4529830940831846 | validation: 0.47857656946122984]
	TIME [epoch: 2.21 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4319682282335955		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.4319682282335955 | validation: 0.4132262261172055]
	TIME [epoch: 2.22 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43241355406813237		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.43241355406813237 | validation: 0.471283708875049]
	TIME [epoch: 2.21 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4387165458079909		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.4387165458079909 | validation: 0.4057541298471179]
	TIME [epoch: 2.23 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4544741681603317		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.4544741681603317 | validation: 0.5181007030090562]
	TIME [epoch: 2.22 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45018158412922843		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.45018158412922843 | validation: 0.39389162640549485]
	TIME [epoch: 2.23 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46513779223388135		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.46513779223388135 | validation: 0.565639524771254]
	TIME [epoch: 2.22 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4814048010954555		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.4814048010954555 | validation: 0.40065328107260295]
	TIME [epoch: 2.21 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45409659474701697		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.45409659474701697 | validation: 0.4872094484759627]
	TIME [epoch: 2.21 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43858912681246964		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.43858912681246964 | validation: 0.41351990502784863]
	TIME [epoch: 2.21 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4270830850170526		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.4270830850170526 | validation: 0.4516599740146305]
	TIME [epoch: 2.21 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4255098139528497		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.4255098139528497 | validation: 0.4211315251504686]
	TIME [epoch: 2.21 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.428947230493271		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.428947230493271 | validation: 0.4973080288541592]
	TIME [epoch: 2.21 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4362482961416873		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.4362482961416873 | validation: 0.37437179564064244]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_752.pth
	Model improved!!!
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4602973319295593		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.4602973319295593 | validation: 0.6185035916119104]
	TIME [epoch: 2.21 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5093778500867205		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.5093778500867205 | validation: 0.4144368316623077]
	TIME [epoch: 2.21 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47012736929190707		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.47012736929190707 | validation: 0.4395277028450628]
	TIME [epoch: 2.21 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4221836638829436		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.4221836638829436 | validation: 0.5055707145689513]
	TIME [epoch: 2.22 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43907555114876795		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.43907555114876795 | validation: 0.38338906819580565]
	TIME [epoch: 2.23 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4420337819921616		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.4420337819921616 | validation: 0.5129457862944312]
	TIME [epoch: 2.21 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4409599068734837		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.4409599068734837 | validation: 0.39268647382872407]
	TIME [epoch: 2.23 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43465632875125026		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.43465632875125026 | validation: 0.4937189913445312]
	TIME [epoch: 2.21 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4356247607976549		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.4356247607976549 | validation: 0.3904637746348284]
	TIME [epoch: 2.23 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4432059442630236		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.4432059442630236 | validation: 0.4698087575560077]
	TIME [epoch: 2.21 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42858345442594603		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.42858345442594603 | validation: 0.4026545521489]
	TIME [epoch: 2.23 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4319526841715143		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.4319526841715143 | validation: 0.4810184091916952]
	TIME [epoch: 2.22 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42659556309057267		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.42659556309057267 | validation: 0.3860135997172897]
	TIME [epoch: 2.21 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43799120532375463		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.43799120532375463 | validation: 0.5505560479230426]
	TIME [epoch: 2.21 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45872820724586233		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.45872820724586233 | validation: 0.3905624995191553]
	TIME [epoch: 2.21 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4459325291603858		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.4459325291603858 | validation: 0.49776643253801983]
	TIME [epoch: 2.21 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4380647325080736		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.4380647325080736 | validation: 0.38886692684809043]
	TIME [epoch: 2.22 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42913633333008777		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.42913633333008777 | validation: 0.47292772599638977]
	TIME [epoch: 2.21 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4241721514477794		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.4241721514477794 | validation: 0.3839446866147056]
	TIME [epoch: 2.21 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42833042639895125		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.42833042639895125 | validation: 0.47340202203062165]
	TIME [epoch: 2.21 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4239662172773581		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.4239662172773581 | validation: 0.3874615624380149]
	TIME [epoch: 2.21 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42527926846626934		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.42527926846626934 | validation: 0.49375780313555506]
	TIME [epoch: 2.22 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4239686587557875		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.4239686587557875 | validation: 0.3770573868013622]
	TIME [epoch: 2.21 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4403523365747371		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.4403523365747371 | validation: 0.5410220763146794]
	TIME [epoch: 2.22 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45116020704974147		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.45116020704974147 | validation: 0.3842879306785337]
	TIME [epoch: 2.21 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4320988121791352		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.4320988121791352 | validation: 0.48404380056897056]
	TIME [epoch: 2.21 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4234030299665662		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.4234030299665662 | validation: 0.39898360910980174]
	TIME [epoch: 2.21 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40623963717336703		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.40623963717336703 | validation: 0.43489185655998064]
	TIME [epoch: 2.21 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40947051620459834		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.40947051620459834 | validation: 0.4164575876772311]
	TIME [epoch: 2.21 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4186609193148735		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.4186609193148735 | validation: 0.46074642275272737]
	TIME [epoch: 2.22 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44313354602046945		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.44313354602046945 | validation: 0.4019472986029583]
	TIME [epoch: 2.21 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41919113751679404		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.41919113751679404 | validation: 0.48378779384762327]
	TIME [epoch: 2.21 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4164077387594548		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.4164077387594548 | validation: 0.3648496466229367]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_785.pth
	Model improved!!!
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4327941214084621		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.4327941214084621 | validation: 0.5457459493535781]
	TIME [epoch: 2.21 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4635894252397877		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.4635894252397877 | validation: 0.3539144989687677]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_787.pth
	Model improved!!!
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4788110509428277		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.4788110509428277 | validation: 0.4713844758401813]
	TIME [epoch: 2.21 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41314370529802846		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.41314370529802846 | validation: 0.42778223091840833]
	TIME [epoch: 2.22 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4172660746190111		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.4172660746190111 | validation: 0.38398176026367653]
	TIME [epoch: 2.22 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4126587168085056		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.4126587168085056 | validation: 0.44932965221303206]
	TIME [epoch: 2.21 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40461927375276563		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.40461927375276563 | validation: 0.4029766964589667]
	TIME [epoch: 2.21 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40671702420389877		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.40671702420389877 | validation: 0.4517824880823604]
	TIME [epoch: 2.21 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4121909317050617		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.4121909317050617 | validation: 0.38324524332055526]
	TIME [epoch: 2.22 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4066256303684753		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.4066256303684753 | validation: 0.49003685130067737]
	TIME [epoch: 2.22 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41701172079448795		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.41701172079448795 | validation: 0.3612287545716501]
	TIME [epoch: 2.23 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44657069024144946		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.44657069024144946 | validation: 0.5150464911300442]
	TIME [epoch: 2.21 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42708908109360566		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.42708908109360566 | validation: 0.3779951497900242]
	TIME [epoch: 2.22 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4072802214950101		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.4072802214950101 | validation: 0.4468058465203836]
	TIME [epoch: 2.21 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39665440880755987		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.39665440880755987 | validation: 0.3991048930783456]
	TIME [epoch: 2.22 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39429306770532685		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.39429306770532685 | validation: 0.4431380377752177]
	TIME [epoch: 2.21 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3975987883601674		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.3975987883601674 | validation: 0.3765489630832519]
	TIME [epoch: 2.22 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4003752050410174		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.4003752050410174 | validation: 0.5220095263316551]
	TIME [epoch: 2.22 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43857829346107224		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.43857829346107224 | validation: 0.3644677097315501]
	TIME [epoch: 2.21 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47532123421883354		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.47532123421883354 | validation: 0.4725770309775095]
	TIME [epoch: 2.21 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4035733749159869		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.4035733749159869 | validation: 0.42261551397390634]
	TIME [epoch: 2.21 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.401204176535413		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.401204176535413 | validation: 0.3783015263588112]
	TIME [epoch: 2.21 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4171336401490434		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.4171336401490434 | validation: 0.4769164936198754]
	TIME [epoch: 2.22 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41295650680278073		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.41295650680278073 | validation: 0.37812984696758517]
	TIME [epoch: 2.22 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4056410647413691		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.4056410647413691 | validation: 0.5028206976935918]
	TIME [epoch: 2.21 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41503445670636957		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.41503445670636957 | validation: 0.37325370546942177]
	TIME [epoch: 2.21 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40014360074065436		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.40014360074065436 | validation: 0.4516367390773439]
	TIME [epoch: 2.22 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40065087243835174		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.40065087243835174 | validation: 0.38719405732780815]
	TIME [epoch: 2.21 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39718304178413916		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.39718304178413916 | validation: 0.44007869162240215]
	TIME [epoch: 2.21 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39224708527399205		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.39224708527399205 | validation: 0.36369434338925927]
	TIME [epoch: 2.21 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4121175138702954		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.4121175138702954 | validation: 0.4789833958267779]
	TIME [epoch: 2.21 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4029812227630283		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.4029812227630283 | validation: 0.3501342707205807]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_817.pth
	Model improved!!!
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4087942747093004		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.4087942747093004 | validation: 0.5008474497953941]
	TIME [epoch: 2.21 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41271379253741886		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.41271379253741886 | validation: 0.3791942218799245]
	TIME [epoch: 2.21 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40629157058438936		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.40629157058438936 | validation: 0.4938265454826521]
	TIME [epoch: 2.24 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4032760824361241		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.4032760824361241 | validation: 0.35843460253403064]
	TIME [epoch: 2.21 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3934313502007328		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.3934313502007328 | validation: 0.45954115338456125]
	TIME [epoch: 2.21 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38982741765998863		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.38982741765998863 | validation: 0.35833425899681814]
	TIME [epoch: 2.21 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4046601671701599		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.4046601671701599 | validation: 0.4562856112189076]
	TIME [epoch: 2.23 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3969446187352406		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.3969446187352406 | validation: 0.37221528957517924]
	TIME [epoch: 2.21 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.396518669267636		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.396518669267636 | validation: 0.45990899508838085]
	TIME [epoch: 2.23 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3888853525977848		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.3888853525977848 | validation: 0.3550713957528956]
	TIME [epoch: 2.21 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40264699044994245		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.40264699044994245 | validation: 0.47481333001219633]
	TIME [epoch: 2.23 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.400066954535136		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.400066954535136 | validation: 0.36427992803566434]
	TIME [epoch: 2.21 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3963812733464103		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.3963812733464103 | validation: 0.47559828448031394]
	TIME [epoch: 2.22 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3934840905633654		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.3934840905633654 | validation: 0.3934431591642495]
	TIME [epoch: 2.22 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39209241266400113		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.39209241266400113 | validation: 0.40222279023557583]
	TIME [epoch: 2.21 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3843048531624114		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.3843048531624114 | validation: 0.4275664204506935]
	TIME [epoch: 2.22 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38721870926312674		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.38721870926312674 | validation: 0.34206580216001903]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_834.pth
	Model improved!!!
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41071306218263076		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.41071306218263076 | validation: 0.520393673563594]
	TIME [epoch: 2.22 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4295185564672734		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.4295185564672734 | validation: 0.35167238790955674]
	TIME [epoch: 2.22 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4033188408563191		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.4033188408563191 | validation: 0.41893447256969907]
	TIME [epoch: 2.22 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3758128808010915		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.3758128808010915 | validation: 0.407168077365676]
	TIME [epoch: 2.22 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37313359308580313		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.37313359308580313 | validation: 0.38029742545811335]
	TIME [epoch: 2.23 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37395183898566187		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.37395183898566187 | validation: 0.4222407884262038]
	TIME [epoch: 2.22 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37802990867482095		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.37802990867482095 | validation: 0.3555324681466533]
	TIME [epoch: 2.22 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38482474875755646		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.38482474875755646 | validation: 0.4578943930564614]
	TIME [epoch: 2.21 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3880770222627827		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.3880770222627827 | validation: 0.34023751889340187]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_843.pth
	Model improved!!!
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40067223160412285		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.40067223160412285 | validation: 0.5172933355030565]
	TIME [epoch: 2.22 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41532809917288155		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.41532809917288155 | validation: 0.3487234767418264]
	TIME [epoch: 2.23 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3959507404859774		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.3959507404859774 | validation: 0.4098799376718265]
	TIME [epoch: 2.22 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3736815331838288		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.3736815331838288 | validation: 0.4049708478660599]
	TIME [epoch: 2.23 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37102692862875564		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.37102692862875564 | validation: 0.364573335356162]
	TIME [epoch: 2.22 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37104079195182027		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.37104079195182027 | validation: 0.4380839563623726]
	TIME [epoch: 2.22 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3809731311179334		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.3809731311179334 | validation: 0.338134292655431]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_850.pth
	Model improved!!!
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39144018388821233		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.39144018388821233 | validation: 0.5194870575910249]
	TIME [epoch: 2.22 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4286105874561827		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.4286105874561827 | validation: 0.34066400139244674]
	TIME [epoch: 2.22 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4005143538590973		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.4005143538590973 | validation: 0.40252798931451805]
	TIME [epoch: 2.22 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3649464820633825		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.3649464820633825 | validation: 0.41602467524830333]
	TIME [epoch: 2.22 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37223229961583626		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.37223229961583626 | validation: 0.344061270000329]
	TIME [epoch: 2.22 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3830754952646755		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.3830754952646755 | validation: 0.4325617418732819]
	TIME [epoch: 2.22 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3791143235880353		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.3791143235880353 | validation: 0.35126518340620905]
	TIME [epoch: 2.23 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37462001398843625		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.37462001398843625 | validation: 0.46399676534915407]
	TIME [epoch: 2.23 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38180152374343423		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.38180152374343423 | validation: 0.34841596915501416]
	TIME [epoch: 2.22 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38453449953804864		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.38453449953804864 | validation: 0.4694741892510723]
	TIME [epoch: 2.23 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3861374871729554		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.3861374871729554 | validation: 0.3541499032982002]
	TIME [epoch: 2.22 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3766227332636624		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.3766227332636624 | validation: 0.4308278613054384]
	TIME [epoch: 2.23 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36865952492689097		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.36865952492689097 | validation: 0.35482486908168687]
	TIME [epoch: 2.22 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37030797204609434		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.37030797204609434 | validation: 0.43827408200455303]
	TIME [epoch: 2.23 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3740811574141474		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.3740811574141474 | validation: 0.3329727086483681]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_865.pth
	Model improved!!!
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3837827975580976		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.3837827975580976 | validation: 0.42464490565852614]
	TIME [epoch: 2.21 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.371677605039994		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.371677605039994 | validation: 0.3481737268527839]
	TIME [epoch: 2.21 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36914195106409625		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.36914195106409625 | validation: 0.44626976858151945]
	TIME [epoch: 2.23 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37352238760460943		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.37352238760460943 | validation: 0.3288018347169337]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_869.pth
	Model improved!!!
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3819632668130467		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.3819632668130467 | validation: 0.4655498400367213]
	TIME [epoch: 2.22 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39170102497402126		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.39170102497402126 | validation: 0.3354900588634232]
	TIME [epoch: 2.21 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3790372853275498		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.3790372853275498 | validation: 0.4017760879730973]
	TIME [epoch: 2.23 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3604097052901885		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.3604097052901885 | validation: 0.38728024294082186]
	TIME [epoch: 2.22 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.354182301630359		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.354182301630359 | validation: 0.3618876774134187]
	TIME [epoch: 2.22 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35775796645868896		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.35775796645868896 | validation: 0.4247671933349011]
	TIME [epoch: 2.21 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3625767645691363		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.3625767645691363 | validation: 0.3440436050277703]
	TIME [epoch: 2.22 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3782487521039883		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.3782487521039883 | validation: 0.46584675985347296]
	TIME [epoch: 2.21 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38745797152291067		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.38745797152291067 | validation: 0.32852289746334773]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_878.pth
	Model improved!!!
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37967815004056177		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.37967815004056177 | validation: 0.4167502336668495]
	TIME [epoch: 2.21 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.357840036688798		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.357840036688798 | validation: 0.3712754181757696]
	TIME [epoch: 2.21 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36228619495984105		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.36228619495984105 | validation: 0.3925891016367071]
	TIME [epoch: 2.21 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.350340192241859		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.350340192241859 | validation: 0.3721719616466683]
	TIME [epoch: 2.21 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3500545906611614		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.3500545906611614 | validation: 0.4014134699560956]
	TIME [epoch: 2.22 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35645309937642167		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.35645309937642167 | validation: 0.3207477629737863]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_884.pth
	Model improved!!!
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3706994629590834		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.3706994629590834 | validation: 0.5058327124700802]
	TIME [epoch: 2.22 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4022380780241297		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.4022380780241297 | validation: 0.3329032256776405]
	TIME [epoch: 2.22 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37994787330978896		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.37994787330978896 | validation: 0.39374618277853674]
	TIME [epoch: 2.22 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35348077520357607		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.35348077520357607 | validation: 0.3851584689635293]
	TIME [epoch: 2.22 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34798450342623		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.34798450342623 | validation: 0.3531599133254071]
	TIME [epoch: 2.23 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3579649299034912		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.3579649299034912 | validation: 0.39899154446162655]
	TIME [epoch: 2.22 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35505318519820295		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.35505318519820295 | validation: 0.3374110036933939]
	TIME [epoch: 2.23 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35411072417324996		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.35411072417324996 | validation: 0.4070964058956957]
	TIME [epoch: 2.22 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35380971032624187		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.35380971032624187 | validation: 0.35052139184568704]
	TIME [epoch: 2.22 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3605792453062132		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.3605792453062132 | validation: 0.4543117168647015]
	TIME [epoch: 2.22 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37295850105724326		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.37295850105724326 | validation: 0.3119860361952701]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_895.pth
	Model improved!!!
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.381276379561321		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.381276379561321 | validation: 0.44392023392153296]
	TIME [epoch: 2.23 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3813056336794841		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.3813056336794841 | validation: 0.33070880973154315]
	TIME [epoch: 2.22 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3738836273318525		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.3738836273318525 | validation: 0.3869615050692925]
	TIME [epoch: 2.22 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3418288894255987		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.3418288894255987 | validation: 0.3798545508432289]
	TIME [epoch: 2.22 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34830752646893404		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.34830752646893404 | validation: 0.3438385322576474]
	TIME [epoch: 2.22 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3513182661319534		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.3513182661319534 | validation: 0.3919601187276004]
	TIME [epoch: 2.21 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3415186831667095		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.3415186831667095 | validation: 0.3846890775717281]
	TIME [epoch: 2.23 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34559887044881893		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.34559887044881893 | validation: 0.3820358270727651]
	TIME [epoch: 2.21 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34328558361247086		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.34328558361247086 | validation: 0.3623179451650594]
	TIME [epoch: 2.21 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3490943955950132		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.3490943955950132 | validation: 0.37961043802472827]
	TIME [epoch: 2.22 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3471395703412709		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.3471395703412709 | validation: 0.3606826768426885]
	TIME [epoch: 2.23 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.342322189091996		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.342322189091996 | validation: 0.4074488220815723]
	TIME [epoch: 2.22 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3454417927737444		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.3454417927737444 | validation: 0.32080004730941863]
	TIME [epoch: 2.24 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3767136627080768		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.3767136627080768 | validation: 0.5101032838059334]
	TIME [epoch: 2.22 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40844286702583465		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.40844286702583465 | validation: 0.3212225365594373]
	TIME [epoch: 2.22 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3648203682916809		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.3648203682916809 | validation: 0.3614297948678883]
	TIME [epoch: 2.22 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34016375128028925		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.34016375128028925 | validation: 0.41747912577358676]
	TIME [epoch: 2.22 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34959062854990736		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.34959062854990736 | validation: 0.3144819147340661]
	TIME [epoch: 2.22 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3617002477300926		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.3617002477300926 | validation: 0.41827877808860214]
	TIME [epoch: 2.22 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3512302484074513		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.3512302484074513 | validation: 0.349283445340254]
	TIME [epoch: 2.22 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3385998556586733		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.3385998556586733 | validation: 0.37991961807562147]
	TIME [epoch: 2.22 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3351192760456205		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.3351192760456205 | validation: 0.36023017134477575]
	TIME [epoch: 2.22 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33155630614171444		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.33155630614171444 | validation: 0.38680145123402354]
	TIME [epoch: 2.21 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34011990049460783		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.34011990049460783 | validation: 0.33216074917616495]
	TIME [epoch: 2.23 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3514633436147848		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.3514633436147848 | validation: 0.4217776196326166]
	TIME [epoch: 2.22 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3546162073490071		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.3546162073490071 | validation: 0.32221407919631506]
	TIME [epoch: 2.22 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35867028200840323		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.35867028200840323 | validation: 0.4546624056241706]
	TIME [epoch: 2.23 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35940029851580796		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.35940029851580796 | validation: 0.32681930846779944]
	TIME [epoch: 2.22 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34428681892902885		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.34428681892902885 | validation: 0.3954155053926467]
	TIME [epoch: 2.22 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3355461841748475		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.3355461841748475 | validation: 0.3535437981615705]
	TIME [epoch: 2.24 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3338499354597003		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.3338499354597003 | validation: 0.35321144458657544]
	TIME [epoch: 2.22 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3325587039664476		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.3325587039664476 | validation: 0.36697919136168866]
	TIME [epoch: 2.22 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3347861803432624		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.3347861803432624 | validation: 0.3314672936582884]
	TIME [epoch: 2.22 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3354559628401081		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.3354559628401081 | validation: 0.4136773969016389]
	TIME [epoch: 2.22 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.353938551715665		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.353938551715665 | validation: 0.30414062999072056]
	TIME [epoch: 2.23 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_930.pth
	Model improved!!!
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37965054135420767		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.37965054135420767 | validation: 0.4143568184998408]
	TIME [epoch: 2.22 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34703832633505427		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.34703832633505427 | validation: 0.34970901420876893]
	TIME [epoch: 2.24 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33725198879730456		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.33725198879730456 | validation: 0.3627961044286296]
	TIME [epoch: 2.22 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32763430911924035		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.32763430911924035 | validation: 0.37141841318646407]
	TIME [epoch: 2.23 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3308913800504308		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.3308913800504308 | validation: 0.3258934194665675]
	TIME [epoch: 2.22 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33668467622962406		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.33668467622962406 | validation: 0.40802117644417674]
	TIME [epoch: 2.22 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3425054578244961		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.3425054578244961 | validation: 0.3093209656167567]
	TIME [epoch: 2.22 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3492111080244525		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.3492111080244525 | validation: 0.4144818623502895]
	TIME [epoch: 2.24 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34630461470000223		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.34630461470000223 | validation: 0.31167635588422365]
	TIME [epoch: 2.22 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3438626870606105		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.3438626870606105 | validation: 0.3797553784552917]
	TIME [epoch: 2.32 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33422959836800537		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.33422959836800537 | validation: 0.3426143984989603]
	TIME [epoch: 2.22 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.327795237068891		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.327795237068891 | validation: 0.3684277365532191]
	TIME [epoch: 2.22 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3281068685025418		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.3281068685025418 | validation: 0.34102477353443555]
	TIME [epoch: 2.22 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32635573929054046		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.32635573929054046 | validation: 0.38096977158347034]
	TIME [epoch: 2.23 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3243567764089055		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.3243567764089055 | validation: 0.3166587133102732]
	TIME [epoch: 2.22 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3323749037021107		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.3323749037021107 | validation: 0.4173225884322005]
	TIME [epoch: 2.23 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34582909552003666		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.34582909552003666 | validation: 0.2897019133000708]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_947.pth
	Model improved!!!
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3599399669558154		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.3599399669558154 | validation: 0.40424859731913304]
	TIME [epoch: 2.22 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3466790631086133		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.3466790631086133 | validation: 0.33405110941310606]
	TIME [epoch: 2.22 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3263841492870108		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.3263841492870108 | validation: 0.3525004663078642]
	TIME [epoch: 2.22 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3251773247094575		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.3251773247094575 | validation: 0.35999601978727097]
	TIME [epoch: 2.22 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3232542610019728		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.3232542610019728 | validation: 0.3292969249132562]
	TIME [epoch: 2.22 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32742767182637006		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.32742767182637006 | validation: 0.37498661910677406]
	TIME [epoch: 2.21 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3194096694328269		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.3194096694328269 | validation: 0.3126761090475313]
	TIME [epoch: 2.22 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3275135770872538		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.3275135770872538 | validation: 0.43443706532364346]
	TIME [epoch: 2.22 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34462919792027635		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.34462919792027635 | validation: 0.29782239150342377]
	TIME [epoch: 2.22 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3481588143229702		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.3481588143229702 | validation: 0.3929897023886817]
	TIME [epoch: 2.22 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.331310392242696		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.331310392242696 | validation: 0.3477061364513132]
	TIME [epoch: 2.22 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3202827467068127		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.3202827467068127 | validation: 0.3231261359850247]
	TIME [epoch: 2.22 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32081565878449203		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.32081565878449203 | validation: 0.37781569135241]
	TIME [epoch: 2.22 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3246417256168987		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.3246417256168987 | validation: 0.31809735865191646]
	TIME [epoch: 2.23 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3267196595172666		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.3267196595172666 | validation: 0.40415027531903847]
	TIME [epoch: 2.22 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3315496832910011		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.3315496832910011 | validation: 0.30183896057773557]
	TIME [epoch: 2.22 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33768953273129293		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.33768953273129293 | validation: 0.41555830080491934]
	TIME [epoch: 2.22 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33248073592720473		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.33248073592720473 | validation: 0.3114260900715042]
	TIME [epoch: 2.22 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32898267044038043		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.32898267044038043 | validation: 0.36576079737419553]
	TIME [epoch: 2.22 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32013809241373087		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.32013809241373087 | validation: 0.3425051900000721]
	TIME [epoch: 2.22 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31289977822961396		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.31289977822961396 | validation: 0.3433103680166887]
	TIME [epoch: 2.22 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32088227893043936		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.32088227893043936 | validation: 0.34478206168710157]
	TIME [epoch: 2.22 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3256748121265792		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.3256748121265792 | validation: 0.3424933551204701]
	TIME [epoch: 2.22 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3130431638817995		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.3130431638817995 | validation: 0.33485507263558417]
	TIME [epoch: 2.22 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31279526622275805		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.31279526622275805 | validation: 0.34749294853733254]
	TIME [epoch: 2.22 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31093671083648017		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.31093671083648017 | validation: 0.33165800363259546]
	TIME [epoch: 2.22 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3139904931028029		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.3139904931028029 | validation: 0.4005635790681951]
	TIME [epoch: 2.22 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3264543736140117		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.3264543736140117 | validation: 0.28218508759931427]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_975.pth
	Model improved!!!
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3401932363210007		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.3401932363210007 | validation: 0.42233439609355766]
	TIME [epoch: 2.22 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34772559878677967		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.34772559878677967 | validation: 0.2988172154536062]
	TIME [epoch: 2.22 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33901848261861955		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.33901848261861955 | validation: 0.3643303534106996]
	TIME [epoch: 2.22 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3134742736507237		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.3134742736507237 | validation: 0.3452341502721588]
	TIME [epoch: 2.21 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3069103285388207		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.3069103285388207 | validation: 0.3259944728469604]
	TIME [epoch: 2.22 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3121770598339602		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.3121770598339602 | validation: 0.3522116175743082]
	TIME [epoch: 2.22 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31256331979212837		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.31256331979212837 | validation: 0.31381588983697295]
	TIME [epoch: 2.22 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3112471131278356		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.3112471131278356 | validation: 0.3819143129860054]
	TIME [epoch: 2.23 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3177955745316185		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.3177955745316185 | validation: 0.2979583099806957]
	TIME [epoch: 2.22 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3203682611707002		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.3203682611707002 | validation: 0.4132852898517059]
	TIME [epoch: 2.24 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33005631073732233		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.33005631073732233 | validation: 0.28552502243853395]
	TIME [epoch: 2.22 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3370983569614811		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.3370983569614811 | validation: 0.3774998462996929]
	TIME [epoch: 2.24 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31042598858536646		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.31042598858536646 | validation: 0.3230507690442435]
	TIME [epoch: 2.22 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3038810109315025		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.3038810109315025 | validation: 0.326431692270358]
	TIME [epoch: 2.22 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30968948172797106		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.30968948172797106 | validation: 0.3490571632803766]
	TIME [epoch: 2.22 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30558184949193984		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.30558184949193984 | validation: 0.32525225949501657]
	TIME [epoch: 2.22 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30394562827338567		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.30394562827338567 | validation: 0.34507308223725536]
	TIME [epoch: 2.22 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3074329501801419		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.3074329501801419 | validation: 0.33159400628887414]
	TIME [epoch: 2.22 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30985930150946817		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.30985930150946817 | validation: 0.36340995983265395]
	TIME [epoch: 2.21 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3096017012355978		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.3096017012355978 | validation: 0.3150045911674988]
	TIME [epoch: 2.22 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3212901252959408		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.3212901252959408 | validation: 0.37967115628816167]
	TIME [epoch: 2.22 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3135722013009593		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.3135722013009593 | validation: 0.3085137726637041]
	TIME [epoch: 2.22 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.307863953476132		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.307863953476132 | validation: 0.37251919605015105]
	TIME [epoch: 2.23 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31021854186281533		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.31021854186281533 | validation: 0.26298929534805765]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_999.pth
	Model improved!!!
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35225227124677394		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.35225227124677394 | validation: 0.3902163572138109]
	TIME [epoch: 2.22 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32035358611602505		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.32035358611602505 | validation: 0.33012389772144335]
	TIME [epoch: 180 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30420810026552914		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.30420810026552914 | validation: 0.3141963747443519]
	TIME [epoch: 4.79 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3062276811366385		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.3062276811366385 | validation: 0.35396137033141345]
	TIME [epoch: 4.79 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30616911427403143		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.30616911427403143 | validation: 0.288392947994478]
	TIME [epoch: 4.78 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3111101285181006		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.3111101285181006 | validation: 0.38885959508093937]
	TIME [epoch: 4.79 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3152290825493273		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.3152290825493273 | validation: 0.2940586224743195]
	TIME [epoch: 4.78 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3119533232747546		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.3119533232747546 | validation: 0.35517312089928144]
	TIME [epoch: 4.79 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3111065100151398		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.3111065100151398 | validation: 0.3083062821734188]
	TIME [epoch: 4.77 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3094657397437088		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.3094657397437088 | validation: 0.36792104730648884]
	TIME [epoch: 4.77 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31964352269174984		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.31964352269174984 | validation: 0.2893733039873636]
	TIME [epoch: 4.77 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31280974789856375		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.31280974789856375 | validation: 0.37298719285924004]
	TIME [epoch: 4.77 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30520472057242953		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.30520472057242953 | validation: 0.31107317594667977]
	TIME [epoch: 4.79 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30424312440277784		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.30424312440277784 | validation: 0.34023948251323793]
	TIME [epoch: 4.77 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2978025630699439		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.2978025630699439 | validation: 0.3227274631176924]
	TIME [epoch: 4.78 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29694606791913386		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.29694606791913386 | validation: 0.3208255992923265]
	TIME [epoch: 4.77 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30006355909047266		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.30006355909047266 | validation: 0.3502060306958526]
	TIME [epoch: 4.78 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29976974441950044		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.29976974441950044 | validation: 0.2969958266669059]
	TIME [epoch: 4.78 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30789389763252833		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.30789389763252833 | validation: 0.3828575785692804]
	TIME [epoch: 4.8 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.314069394914351		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.314069394914351 | validation: 0.27001555117886733]
	TIME [epoch: 4.79 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3173321080169782		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.3173321080169782 | validation: 0.34154696979218474]
	TIME [epoch: 4.79 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29770976194716126		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.29770976194716126 | validation: 0.3445541553802389]
	TIME [epoch: 4.79 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2965092375134414		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.2965092375134414 | validation: 0.2986832231532485]
	TIME [epoch: 4.78 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3026583829169935		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.3026583829169935 | validation: 0.34340052741695426]
	TIME [epoch: 4.79 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3012418149029132		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.3012418149029132 | validation: 0.2791618409314112]
	TIME [epoch: 4.79 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30745367446510985		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.30745367446510985 | validation: 0.40231412630757524]
	TIME [epoch: 4.79 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3204345609068625		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.3204345609068625 | validation: 0.2765366682230838]
	TIME [epoch: 4.77 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3141556676448334		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.3141556676448334 | validation: 0.34847601370242076]
	TIME [epoch: 4.77 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2922396212304896		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.2922396212304896 | validation: 0.3187691054028132]
	TIME [epoch: 4.79 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29645358609703826		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.29645358609703826 | validation: 0.2949270395335299]
	TIME [epoch: 4.78 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29666255435629707		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.29666255435629707 | validation: 0.35604057621180196]
	TIME [epoch: 4.78 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3011299251838495		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.3011299251838495 | validation: 0.29116431586895114]
	TIME [epoch: 4.77 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.303863466129497		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.303863466129497 | validation: 0.35439388253677534]
	TIME [epoch: 4.77 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2979222224427274		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.2979222224427274 | validation: 0.3072247736715063]
	TIME [epoch: 4.78 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29438083908040463		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.29438083908040463 | validation: 0.32726375983983774]
	TIME [epoch: 4.77 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2947382188849619		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.2947382188849619 | validation: 0.3212061229387165]
	TIME [epoch: 4.77 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2895579132991852		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.2895579132991852 | validation: 0.31235817928245896]
	TIME [epoch: 4.78 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28847440249390643		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.28847440249390643 | validation: 0.34243787545465976]
	TIME [epoch: 4.78 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29444912709884624		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.29444912709884624 | validation: 0.2922975292130412]
	TIME [epoch: 4.78 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3051291120819792		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.3051291120819792 | validation: 0.39315188163083126]
	TIME [epoch: 4.78 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3169697319269085		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.3169697319269085 | validation: 0.2784641102633996]
	TIME [epoch: 4.77 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30652318343447266		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.30652318343447266 | validation: 0.35748105964873284]
	TIME [epoch: 4.77 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3030000135197689		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.3030000135197689 | validation: 0.3027029402132172]
	TIME [epoch: 4.78 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2983655143499018		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.2983655143499018 | validation: 0.34395047270555135]
	TIME [epoch: 4.77 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30143057822610725		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.30143057822610725 | validation: 0.29207382300696066]
	TIME [epoch: 4.77 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2967151899295799		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.2967151899295799 | validation: 0.3378916838276862]
	TIME [epoch: 4.77 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29029076975740203		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.29029076975740203 | validation: 0.31453933148605345]
	TIME [epoch: 4.77 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2862719015887598		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.2862719015887598 | validation: 0.3289339060583865]
	TIME [epoch: 4.78 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28925102050648144		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.28925102050648144 | validation: 0.3051448063839002]
	TIME [epoch: 4.78 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2881674901284881		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.2881674901284881 | validation: 0.34396584861267737]
	TIME [epoch: 4.8 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2909040095157655		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.2909040095157655 | validation: 0.28122911744574675]
	TIME [epoch: 4.79 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2917911337792085		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.2917911337792085 | validation: 0.37999447096421407]
	TIME [epoch: 4.78 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3080112411076459		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.3080112411076459 | validation: 0.26212846553189517]
	TIME [epoch: 4.78 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_1052.pth
	Model improved!!!
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3106741032111552		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.3106741032111552 | validation: 0.3429936740747402]
	TIME [epoch: 4.77 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2916336799450632		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.2916336799450632 | validation: 0.3191851884969788]
	TIME [epoch: 4.79 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2871693752341055		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.2871693752341055 | validation: 0.2986139848640785]
	TIME [epoch: 4.78 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2848703487107888		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.2848703487107888 | validation: 0.3139322088517342]
	TIME [epoch: 4.78 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2863392534276898		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.2863392534276898 | validation: 0.328929239256772]
	TIME [epoch: 4.79 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2874321481029975		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.2874321481029975 | validation: 0.3014906764725167]
	TIME [epoch: 4.78 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28347043280682954		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.28347043280682954 | validation: 0.3328760857171764]
	TIME [epoch: 4.79 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28359401898897624		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.28359401898897624 | validation: 0.2873849641366922]
	TIME [epoch: 4.79 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2876474814551658		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.2876474814551658 | validation: 0.34792679903914386]
	TIME [epoch: 4.79 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2883699605196974		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.2883699605196974 | validation: 0.2746002169905843]
	TIME [epoch: 4.79 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3024315632945975		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.3024315632945975 | validation: 0.3763071411312619]
	TIME [epoch: 4.78 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30682587921501897		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.30682587921501897 | validation: 0.2828259559442376]
	TIME [epoch: 4.78 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2980255406450215		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.2980255406450215 | validation: 0.31925418660166544]
	TIME [epoch: 4.79 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28673848578896716		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.28673848578896716 | validation: 0.3080832106828072]
	TIME [epoch: 4.78 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28063334544457014		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.28063334544457014 | validation: 0.2877326243441672]
	TIME [epoch: 4.78 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2819344362752134		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.2819344362752134 | validation: 0.3344667837310731]
	TIME [epoch: 4.78 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28264424179309616		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.28264424179309616 | validation: 0.28590507991908787]
	TIME [epoch: 4.78 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.287716990970289		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.287716990970289 | validation: 0.3389328161754994]
	TIME [epoch: 4.78 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2893669310655744		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.2893669310655744 | validation: 0.2829035530975027]
	TIME [epoch: 4.79 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2953898840524267		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.2953898840524267 | validation: 0.3645056377147919]
	TIME [epoch: 4.79 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2933459771586949		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.2933459771586949 | validation: 0.2940135446639493]
	TIME [epoch: 4.79 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.288663717555461		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.288663717555461 | validation: 0.3294981551840302]
	TIME [epoch: 4.79 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2805528488715391		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.2805528488715391 | validation: 0.2998553022773712]
	TIME [epoch: 4.78 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.282597484419806		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.282597484419806 | validation: 0.3247645253990008]
	TIME [epoch: 4.79 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2775105807332385		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.2775105807332385 | validation: 0.299513584517012]
	TIME [epoch: 4.79 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27953228230259		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.27953228230259 | validation: 0.32114446631863824]
	TIME [epoch: 4.77 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2764731066802042		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.2764731066802042 | validation: 0.30016793324591967]
	TIME [epoch: 4.78 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2836600619027414		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.2836600619027414 | validation: 0.3178313999434124]
	TIME [epoch: 4.78 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28576895939954133		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.28576895939954133 | validation: 0.33173192820819736]
	TIME [epoch: 4.78 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2893007011968385		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.2893007011968385 | validation: 0.2659486325793942]
	TIME [epoch: 4.78 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28979053467195054		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.28979053467195054 | validation: 0.3753670438143608]
	TIME [epoch: 4.79 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30336912442161756		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.30336912442161756 | validation: 0.2786298830638158]
	TIME [epoch: 4.79 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28474431360369123		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.28474431360369123 | validation: 0.30588069153209524]
	TIME [epoch: 4.79 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27794256407844337		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.27794256407844337 | validation: 0.30339789186373284]
	TIME [epoch: 4.78 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27589333568958624		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.27589333568958624 | validation: 0.30224667863005905]
	TIME [epoch: 4.79 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27519286462757814		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.27519286462757814 | validation: 0.3077944046702328]
	TIME [epoch: 4.78 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2720092585373283		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.2720092585373283 | validation: 0.30765660732388533]
	TIME [epoch: 4.8 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27685482946361467		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.27685482946361467 | validation: 0.2922497104893133]
	TIME [epoch: 4.79 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2763974488870862		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.2763974488870862 | validation: 0.3274196853576878]
	TIME [epoch: 4.79 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27417610857775		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.27417610857775 | validation: 0.26010750453370335]
	TIME [epoch: 4.77 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_1092.pth
	Model improved!!!
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29983411401974863		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.29983411401974863 | validation: 0.37086991001857933]
	TIME [epoch: 4.77 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3031015884182653		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.3031015884182653 | validation: 0.2777856301118042]
	TIME [epoch: 4.79 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28028766175781683		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.28028766175781683 | validation: 0.30531974206098506]
	TIME [epoch: 4.79 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27562816444973126		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.27562816444973126 | validation: 0.30546680842510526]
	TIME [epoch: 4.78 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2748064545490456		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.2748064545490456 | validation: 0.2999190055807749]
	TIME [epoch: 4.77 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2720340211356849		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.2720340211356849 | validation: 0.30762014831142614]
	TIME [epoch: 4.77 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2749463166313352		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.2749463166313352 | validation: 0.2927483835601179]
	TIME [epoch: 4.77 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27453784422997946		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.27453784422997946 | validation: 0.2989782411704287]
	TIME [epoch: 4.77 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2720510819731743		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.2720510819731743 | validation: 0.3024495129893094]
	TIME [epoch: 4.77 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2714480153248118		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.2714480153248118 | validation: 0.30044487865620184]
	TIME [epoch: 4.77 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2717508069178517		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.2717508069178517 | validation: 0.27838926492734256]
	TIME [epoch: 4.77 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27362849413130474		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.27362849413130474 | validation: 0.35990386302840705]
	TIME [epoch: 4.77 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3009386985456285		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.3009386985456285 | validation: 0.24714596361876928]
	TIME [epoch: 4.77 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_1105.pth
	Model improved!!!
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3163212934322547		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.3163212934322547 | validation: 0.3037190949589106]
	TIME [epoch: 4.78 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27303188022284475		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.27303188022284475 | validation: 0.3247444703185315]
	TIME [epoch: 4.78 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.279587749008987		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.279587749008987 | validation: 0.2839098213660591]
	TIME [epoch: 4.77 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.278838737534515		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.278838737534515 | validation: 0.3181613915827244]
	TIME [epoch: 4.77 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27026835196274496		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.27026835196274496 | validation: 0.3016608143809678]
	TIME [epoch: 4.77 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27110806109622404		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.27110806109622404 | validation: 0.27910817528018445]
	TIME [epoch: 4.77 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.272262081546188		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.272262081546188 | validation: 0.3304375938564714]
	TIME [epoch: 4.78 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2725031684046249		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.2725031684046249 | validation: 0.289879093828201]
	TIME [epoch: 4.77 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2716438413439361		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.2716438413439361 | validation: 0.31486849642224146]
	TIME [epoch: 4.77 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.265929619641149		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.265929619641149 | validation: 0.2897490495582189]
	TIME [epoch: 4.92 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2685293426699171		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.2685293426699171 | validation: 0.29629050674919244]
	TIME [epoch: 4.77 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26977363098412194		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.26977363098412194 | validation: 0.28570110279852245]
	TIME [epoch: 4.77 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2688194813914367		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.2688194813914367 | validation: 0.322800993494762]
	TIME [epoch: 4.78 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27113615306057687		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.27113615306057687 | validation: 0.27531381689936957]
	TIME [epoch: 4.77 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.272389390269822		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.272389390269822 | validation: 0.35538442375031565]
	TIME [epoch: 4.77 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28647028734376023		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.28647028734376023 | validation: 0.2438661545714844]
	TIME [epoch: 4.77 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_1121.pth
	Model improved!!!
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3036249989546266		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.3036249989546266 | validation: 0.3053768120763706]
	TIME [epoch: 4.77 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2684046061916613		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.2684046061916613 | validation: 0.31982530337619014]
	TIME [epoch: 4.77 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27259126015646545		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.27259126015646545 | validation: 0.2603319171114312]
	TIME [epoch: 4.78 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27314240960499325		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.27314240960499325 | validation: 0.3078573245012664]
	TIME [epoch: 4.77 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26924698365247246		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.26924698365247246 | validation: 0.274565977804519]
	TIME [epoch: 4.77 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2714214736176884		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.2714214736176884 | validation: 0.3254576359404282]
	TIME [epoch: 4.77 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27200569948823633		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.27200569948823633 | validation: 0.2666068449069113]
	TIME [epoch: 4.78 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27142079091190174		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.27142079091190174 | validation: 0.31389090728069746]
	TIME [epoch: 4.77 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26499490673829257		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.26499490673829257 | validation: 0.2887476490241392]
	TIME [epoch: 4.77 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26325977446319393		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.26325977446319393 | validation: 0.27436217367568455]
	TIME [epoch: 4.79 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26769638247188693		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.26769638247188693 | validation: 0.32170646373330125]
	TIME [epoch: 4.78 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27043428976376155		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.27043428976376155 | validation: 0.25429145438592665]
	TIME [epoch: 4.79 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27545506083886717		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.27545506083886717 | validation: 0.3275515183195006]
	TIME [epoch: 4.79 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27732044611893064		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.27732044611893064 | validation: 0.26381489314239887]
	TIME [epoch: 4.78 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2702827898812485		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.2702827898812485 | validation: 0.29317827499954097]
	TIME [epoch: 4.77 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2620935557374787		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.2620935557374787 | validation: 0.2862938837987133]
	TIME [epoch: 4.78 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2611193610577529		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.2611193610577529 | validation: 0.28517027245710225]
	TIME [epoch: 4.79 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26535381775585637		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.26535381775585637 | validation: 0.2997715336629879]
	TIME [epoch: 4.77 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2654745497618221		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.2654745497618221 | validation: 0.27996959015168116]
	TIME [epoch: 4.78 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2612668444154273		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.2612668444154273 | validation: 0.29595974784531665]
	TIME [epoch: 4.77 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2604870687143607		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.2604870687143607 | validation: 0.28950214736403007]
	TIME [epoch: 4.78 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26286804124458935		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.26286804124458935 | validation: 0.2804484971818641]
	TIME [epoch: 4.79 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27010428933846603		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.27010428933846603 | validation: 0.3082176392788527]
	TIME [epoch: 4.79 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2650718048908797		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.2650718048908797 | validation: 0.2569085215433886]
	TIME [epoch: 4.79 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27322742502820696		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.27322742502820696 | validation: 0.335934176822122]
	TIME [epoch: 4.78 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27404286259819344		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.27404286259819344 | validation: 0.2574636935651579]
	TIME [epoch: 4.77 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27102631131871385		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.27102631131871385 | validation: 0.3036978961770139]
	TIME [epoch: 4.78 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2622723006831592		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.2622723006831592 | validation: 0.2837813570881492]
	TIME [epoch: 4.79 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26081605926425777		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.26081605926425777 | validation: 0.2853086033358722]
	TIME [epoch: 4.78 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26378284980174366		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.26378284980174366 | validation: 0.3079189624353289]
	TIME [epoch: 4.77 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2657889607219013		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.2657889607219013 | validation: 0.25744900428189643]
	TIME [epoch: 4.77 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27046617800062533		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.27046617800062533 | validation: 0.32365413529259784]
	TIME [epoch: 4.78 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27137496973755404		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.27137496973755404 | validation: 0.25245413513867376]
	TIME [epoch: 4.79 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26677966759399496		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.26677966759399496 | validation: 0.2883946044385061]
	TIME [epoch: 4.8 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26109553921054945		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.26109553921054945 | validation: 0.2903838340305978]
	TIME [epoch: 4.79 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2607974126269174		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.2607974126269174 | validation: 0.2812072376351895]
	TIME [epoch: 4.79 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26181898049464686		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.26181898049464686 | validation: 0.2849960716560579]
	TIME [epoch: 4.78 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26313547656560315		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.26313547656560315 | validation: 0.28507067742974795]
	TIME [epoch: 4.79 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2603216691702027		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.2603216691702027 | validation: 0.3014882941492414]
	TIME [epoch: 4.79 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2629991840710104		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.2629991840710104 | validation: 0.25295839859615415]
	TIME [epoch: 4.78 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26743520723422876		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.26743520723422876 | validation: 0.34328793234106014]
	TIME [epoch: 4.78 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2742907341287126		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.2742907341287126 | validation: 0.2589137689053413]
	TIME [epoch: 4.77 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2646396085409162		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.2646396085409162 | validation: 0.2854361863431552]
	TIME [epoch: 4.79 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2574083356957734		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.2574083356957734 | validation: 0.2920209069485938]
	TIME [epoch: 4.78 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2569876611108321		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.2569876611108321 | validation: 0.2741942532741628]
	TIME [epoch: 4.77 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2574994249348256		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.2574994249348256 | validation: 0.30158576958735184]
	TIME [epoch: 4.79 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2579029437937309		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.2579029437937309 | validation: 0.26171916845480914]
	TIME [epoch: 4.78 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.257113641175614		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.257113641175614 | validation: 0.29855475948043164]
	TIME [epoch: 4.79 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2568985975960945		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.2568985975960945 | validation: 0.2512581664858791]
	TIME [epoch: 4.78 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2564873903000471		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.2564873903000471 | validation: 0.3131523537410894]
	TIME [epoch: 4.77 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26177160375510206		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.26177160375510206 | validation: 0.27118507832725613]
	TIME [epoch: 4.78 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2586416567835692		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.2586416567835692 | validation: 0.28973708416851063]
	TIME [epoch: 4.79 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2558284223122557		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.2558284223122557 | validation: 0.262644933653136]
	TIME [epoch: 4.79 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2567978915911083		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.2567978915911083 | validation: 0.301063625088843]
	TIME [epoch: 4.79 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2586833771088423		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.2586833771088423 | validation: 0.2755511138834343]
	TIME [epoch: 4.79 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25799235558622763		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.25799235558622763 | validation: 0.2849965085181051]
	TIME [epoch: 4.78 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25635900328691713		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.25635900328691713 | validation: 0.2686852831570374]
	TIME [epoch: 4.77 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25614153628127184		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.25614153628127184 | validation: 0.30901051956888304]
	TIME [epoch: 4.78 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25915601131601546		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.25915601131601546 | validation: 0.25386749559135263]
	TIME [epoch: 4.79 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26118548788240303		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.26118548788240303 | validation: 0.314304206516896]
	TIME [epoch: 4.79 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2605799135035498		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.2605799135035498 | validation: 0.26367529492899977]
	TIME [epoch: 4.79 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2574587977404233		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.2574587977404233 | validation: 0.27981404706167295]
	TIME [epoch: 4.79 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2532180002832075		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.2532180002832075 | validation: 0.28444294793803854]
	TIME [epoch: 4.79 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2529909184496359		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.2529909184496359 | validation: 0.2569079005677321]
	TIME [epoch: 4.77 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25746169164296684		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.25746169164296684 | validation: 0.3011568631622022]
	TIME [epoch: 4.77 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2531389148330815		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.2531389148330815 | validation: 0.23973760964975632]
	TIME [epoch: 4.78 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_1187.pth
	Model improved!!!
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26319257481294317		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.26319257481294317 | validation: 0.3020384438335538]
	TIME [epoch: 4.77 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2589574885943413		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.2589574885943413 | validation: 0.2596361096875288]
	TIME [epoch: 4.77 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.248516419716754		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.248516419716754 | validation: 0.2796002772449608]
	TIME [epoch: 4.78 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.252463144756403		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.252463144756403 | validation: 0.2743284760202663]
	TIME [epoch: 4.77 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2502102987944422		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.2502102987944422 | validation: 0.270484375142042]
	TIME [epoch: 4.77 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25331264443355855		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.25331264443355855 | validation: 0.27391173462970103]
	TIME [epoch: 4.77 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2516847568685037		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.2516847568685037 | validation: 0.28255386687644396]
	TIME [epoch: 4.77 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25133838349822285		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.25133838349822285 | validation: 0.25939112443840723]
	TIME [epoch: 4.77 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2522911477569681		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.2522911477569681 | validation: 0.30739570979523556]
	TIME [epoch: 4.79 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2552232015593543		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.2552232015593543 | validation: 0.23187275217278588]
	TIME [epoch: 4.77 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_1197.pth
	Model improved!!!
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2647573061291039		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.2647573061291039 | validation: 0.30110901598584994]
	TIME [epoch: 4.77 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25899179200242844		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.25899179200242844 | validation: 0.26303100722016165]
	TIME [epoch: 4.77 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25515503559606856		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.25515503559606856 | validation: 0.279487697332726]
	TIME [epoch: 4.79 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2507807774528582		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.2507807774528582 | validation: 0.2862491112131683]
	TIME [epoch: 4.77 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2530522851190871		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.2530522851190871 | validation: 0.26844686712685667]
	TIME [epoch: 4.78 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24891451738920062		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.24891451738920062 | validation: 0.2720461533518838]
	TIME [epoch: 4.77 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25194129412872523		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.25194129412872523 | validation: 0.28019795485745036]
	TIME [epoch: 4.78 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24999184136616845		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.24999184136616845 | validation: 0.26466451570006316]
	TIME [epoch: 4.77 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25362860568705825		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.25362860568705825 | validation: 0.27801970196846054]
	TIME [epoch: 4.77 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2493048957948377		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.2493048957948377 | validation: 0.26710816029802925]
	TIME [epoch: 4.77 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25068684349378445		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.25068684349378445 | validation: 0.2751928020862173]
	TIME [epoch: 4.77 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2515744065418831		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.2515744065418831 | validation: 0.2651340123002303]
	TIME [epoch: 4.78 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24738466684815494		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.24738466684815494 | validation: 0.26247375062105177]
	TIME [epoch: 4.77 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2497962867888673		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.2497962867888673 | validation: 0.29498559905495225]
	TIME [epoch: 4.78 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2528899182695829		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.2528899182695829 | validation: 0.24975389133374348]
	TIME [epoch: 4.77 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2563262057887905		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.2563262057887905 | validation: 0.3130318420455944]
	TIME [epoch: 4.77 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2520114295743102		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.2520114295743102 | validation: 0.24290996362850192]
	TIME [epoch: 4.79 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25179676328193346		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.25179676328193346 | validation: 0.2932935109884106]
	TIME [epoch: 4.79 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2514382561514153		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.2514382561514153 | validation: 0.2584044551219244]
	TIME [epoch: 4.79 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2551813561078379		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.2551813561078379 | validation: 0.2865254001416337]
	TIME [epoch: 4.79 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2585980492796054		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.2585980492796054 | validation: 0.25806006631673567]
	TIME [epoch: 4.79 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2508757853638102		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.2508757853638102 | validation: 0.27254201801655603]
	TIME [epoch: 4.79 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2419751995582394		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.2419751995582394 | validation: 0.2709664098388634]
	TIME [epoch: 4.79 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2446077312188791		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.2446077312188791 | validation: 0.25449885329475086]
	TIME [epoch: 4.79 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24757369737832022		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.24757369737832022 | validation: 0.2864591205343703]
	TIME [epoch: 4.77 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2465689076850413		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.2465689076850413 | validation: 0.2561057057632793]
	TIME [epoch: 4.78 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2442385445006119		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.2442385445006119 | validation: 0.2786696716045824]
	TIME [epoch: 4.78 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24625882104396404		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.24625882104396404 | validation: 0.261315515023977]
	TIME [epoch: 4.77 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24738712920804723		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.24738712920804723 | validation: 0.2942602121270608]
	TIME [epoch: 4.78 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24938358275387157		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.24938358275387157 | validation: 0.2480734255996534]
	TIME [epoch: 4.77 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24763979568721176		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.24763979568721176 | validation: 0.2777154847826439]
	TIME [epoch: 4.77 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24766870479616096		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.24766870479616096 | validation: 0.25114783161931203]
	TIME [epoch: 4.78 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24367640654086778		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.24367640654086778 | validation: 0.28049502556004546]
	TIME [epoch: 4.77 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25005115341666073		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.25005115341666073 | validation: 0.2490595582014124]
	TIME [epoch: 4.79 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24918924925158603		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.24918924925158603 | validation: 0.2761318910753247]
	TIME [epoch: 4.82 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2462577665214589		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.2462577665214589 | validation: 0.25675076405567643]
	TIME [epoch: 4.78 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24222049829584183		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.24222049829584183 | validation: 0.2731038559863146]
	TIME [epoch: 4.79 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24172878806152992		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.24172878806152992 | validation: 0.26407379693947375]
	TIME [epoch: 4.78 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2418892187847371		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.2418892187847371 | validation: 0.26143044018467515]
	TIME [epoch: 4.77 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24281356503562523		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.24281356503562523 | validation: 0.25783271807293856]
	TIME [epoch: 4.77 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.244760687146895		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.244760687146895 | validation: 0.2842069378178644]
	TIME [epoch: 4.8 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24955243315291475		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.24955243315291475 | validation: 0.2295043281705481]
	TIME [epoch: 4.8 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_1239.pth
	Model improved!!!
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2585369876522299		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.2585369876522299 | validation: 0.3062071264465171]
	TIME [epoch: 4.77 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2503140299662779		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.2503140299662779 | validation: 0.26346916406136656]
	TIME [epoch: 4.78 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24056206263786714		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.24056206263786714 | validation: 0.2633022392044748]
	TIME [epoch: 4.77 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24056090846822137		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.24056090846822137 | validation: 0.2725390730138744]
	TIME [epoch: 4.77 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24051910118201464		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.24051910118201464 | validation: 0.2664523713151671]
	TIME [epoch: 4.78 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24223536549089703		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.24223536549089703 | validation: 0.2648725942451995]
	TIME [epoch: 4.77 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23889298564233624		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.23889298564233624 | validation: 0.2692082133238764]
	TIME [epoch: 4.77 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24088004376160474		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.24088004376160474 | validation: 0.2585414898944131]
	TIME [epoch: 4.77 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24033501511455518		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.24033501511455518 | validation: 0.26856174580871084]
	TIME [epoch: 4.77 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24070607191309998		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.24070607191309998 | validation: 0.24537012913104883]
	TIME [epoch: 4.77 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24354700047130431		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.24354700047130431 | validation: 0.29257940160063756]
	TIME [epoch: 4.78 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24729523618889546		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.24729523618889546 | validation: 0.23836434508681534]
	TIME [epoch: 4.77 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24508926216343604		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.24508926216343604 | validation: 0.2805522807740061]
	TIME [epoch: 4.78 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24127508545191703		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.24127508545191703 | validation: 0.2737665389586731]
	TIME [epoch: 4.77 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24163891479314065		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.24163891479314065 | validation: 0.2429921665318452]
	TIME [epoch: 4.79 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24356509921879135		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.24356509921879135 | validation: 0.281043323428656]
	TIME [epoch: 4.77 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2423286235718011		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.2423286235718011 | validation: 0.23818541094875811]
	TIME [epoch: 4.78 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24165949795985156		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.24165949795985156 | validation: 0.28676677590843647]
	TIME [epoch: 4.77 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24796414057640195		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.24796414057640195 | validation: 0.24172064247913957]
	TIME [epoch: 4.77 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24080721404871783		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.24080721404871783 | validation: 0.2678362082451477]
	TIME [epoch: 4.77 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24036941384360794		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.24036941384360794 | validation: 0.25032724724557315]
	TIME [epoch: 4.78 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23906051910800108		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.23906051910800108 | validation: 0.2701886750113602]
	TIME [epoch: 4.78 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23942075497727566		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.23942075497727566 | validation: 0.25084668928604387]
	TIME [epoch: 4.79 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2352567340061383		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.2352567340061383 | validation: 0.26005821141088026]
	TIME [epoch: 4.79 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24215299239069468		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.24215299239069468 | validation: 0.25835231924274443]
	TIME [epoch: 4.77 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23835827731051423		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.23835827731051423 | validation: 0.25288822459240096]
	TIME [epoch: 4.78 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2363647176215551		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.2363647176215551 | validation: 0.2643604111509598]
	TIME [epoch: 4.79 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23824582029284108		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.23824582029284108 | validation: 0.2427569824087378]
	TIME [epoch: 4.78 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2398466184842647		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.2398466184842647 | validation: 0.284463611764394]
	TIME [epoch: 4.79 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24416941064744535		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.24416941064744535 | validation: 0.24230496332562015]
	TIME [epoch: 4.79 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24329924919301035		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.24329924919301035 | validation: 0.2659078378182419]
	TIME [epoch: 4.77 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23791138364435252		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.23791138364435252 | validation: 0.2644551758025691]
	TIME [epoch: 4.78 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23471630739207608		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.23471630739207608 | validation: 0.25670672648705317]
	TIME [epoch: 4.79 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23436858930266924		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.23436858930266924 | validation: 0.247214041416842]
	TIME [epoch: 4.79 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23759274724600232		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.23759274724600232 | validation: 0.2585024209245495]
	TIME [epoch: 4.78 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23634434220389922		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.23634434220389922 | validation: 0.23232545841245733]
	TIME [epoch: 4.79 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24257899991074197		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.24257899991074197 | validation: 0.29503415042227027]
	TIME [epoch: 4.77 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2463945056332328		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.2463945056332328 | validation: 0.23701538814663525]
	TIME [epoch: 4.79 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24645542973711884		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.24645542973711884 | validation: 0.25013327812827113]
	TIME [epoch: 4.79 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2370546783459737		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.2370546783459737 | validation: 0.26190055951176844]
	TIME [epoch: 4.79 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23347380804052173		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.23347380804052173 | validation: 0.24254170106521356]
	TIME [epoch: 4.79 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23748789642116827		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.23748789642116827 | validation: 0.25677980145639245]
	TIME [epoch: 4.79 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23431595744092487		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.23431595744092487 | validation: 0.27028969059179475]
	TIME [epoch: 4.8 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2336437810744744		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.2336437810744744 | validation: 0.25445282514718776]
	TIME [epoch: 4.78 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23706660261116547		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.23706660261116547 | validation: 0.2564619775300182]
	TIME [epoch: 4.77 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23589707320224407		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.23589707320224407 | validation: 0.2617192711076563]
	TIME [epoch: 4.79 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23411400884801775		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.23411400884801775 | validation: 0.2362972249224923]
	TIME [epoch: 4.78 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23919815970730404		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.23919815970730404 | validation: 0.2723381256268564]
	TIME [epoch: 4.78 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2363867389975475		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.2363867389975475 | validation: 0.23658403375452083]
	TIME [epoch: 4.77 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2353497102389281		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.2353497102389281 | validation: 0.25923727866239593]
	TIME [epoch: 4.77 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23574758512417643		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.23574758512417643 | validation: 0.2466421306863883]
	TIME [epoch: 4.79 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23444633850809538		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.23444633850809538 | validation: 0.25356797250856167]
	TIME [epoch: 4.78 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23369693523587523		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.23369693523587523 | validation: 0.24187774695139824]
	TIME [epoch: 4.78 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23491153479528895		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.23491153479528895 | validation: 0.27046006250455273]
	TIME [epoch: 4.78 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.234452883767989		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.234452883767989 | validation: 0.2500512049289917]
	TIME [epoch: 4.79 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23511133578787508		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.23511133578787508 | validation: 0.26099577537936164]
	TIME [epoch: 4.79 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2336131352999249		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.2336131352999249 | validation: 0.2502405738730346]
	TIME [epoch: 4.77 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23258617551438007		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.23258617551438007 | validation: 0.2564502660937655]
	TIME [epoch: 4.77 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23185794650978098		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.23185794650978098 | validation: 0.2278040664888013]
	TIME [epoch: 4.78 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_1298.pth
	Model improved!!!
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24100704254285174		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.24100704254285174 | validation: 0.270730239234878]
	TIME [epoch: 4.77 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23751447668158598		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.23751447668158598 | validation: 0.22486911685830627]
	TIME [epoch: 4.79 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_1300.pth
	Model improved!!!
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23784427952317672		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.23784427952317672 | validation: 0.25882123192142936]
	TIME [epoch: 4.77 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23105923297270942		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.23105923297270942 | validation: 0.2588896574805163]
	TIME [epoch: 4.79 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23209937713685197		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.23209937713685197 | validation: 0.24206211587915566]
	TIME [epoch: 4.78 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23085977060148394		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.23085977060148394 | validation: 0.2526550288210396]
	TIME [epoch: 4.79 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23157748329497538		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.23157748329497538 | validation: 0.2544582231818046]
	TIME [epoch: 4.77 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23094722897553616		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.23094722897553616 | validation: 0.27069278214765635]
	TIME [epoch: 4.78 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2348689579287522		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.2348689579287522 | validation: 0.21562931043417133]
	TIME [epoch: 4.77 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_1307.pth
	Model improved!!!
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24225308567653236		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.24225308567653236 | validation: 0.27918231173569286]
	TIME [epoch: 4.77 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23586347570844698		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.23586347570844698 | validation: 0.23848793328811146]
	TIME [epoch: 4.77 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23412054940649746		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.23412054940649746 | validation: 0.24102244454168842]
	TIME [epoch: 4.79 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23121401004582207		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.23121401004582207 | validation: 0.27117587590459485]
	TIME [epoch: 4.79 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.235493450094451		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.235493450094451 | validation: 0.24522760108586145]
	TIME [epoch: 4.79 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23256880818480063		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.23256880818480063 | validation: 0.2508648024467065]
	TIME [epoch: 4.79 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23073865122432594		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.23073865122432594 | validation: 0.2468817630704072]
	TIME [epoch: 4.79 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22843876779162323		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.22843876779162323 | validation: 0.24943138506509277]
	TIME [epoch: 4.78 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23493314764232723		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.23493314764232723 | validation: 0.26666739886326507]
	TIME [epoch: 4.77 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2340412617344161		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.2340412617344161 | validation: 0.24937805429241308]
	TIME [epoch: 4.78 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23014704041260126		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.23014704041260126 | validation: 0.24561161451140784]
	TIME [epoch: 4.78 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2295966765046126		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.2295966765046126 | validation: 0.25239350471692934]
	TIME [epoch: 4.79 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22756544020255248		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.22756544020255248 | validation: 0.263491042231322]
	TIME [epoch: 4.77 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23054606625925866		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.23054606625925866 | validation: 0.23361247716098946]
	TIME [epoch: 4.77 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23322161612249914		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.23322161612249914 | validation: 0.258866456588954]
	TIME [epoch: 4.79 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2376772556989134		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.2376772556989134 | validation: 0.21922880491519]
	TIME [epoch: 4.78 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23453456822965948		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.23453456822965948 | validation: 0.2547803964651099]
	TIME [epoch: 4.79 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2281855418256437		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.2281855418256437 | validation: 0.24784444647484338]
	TIME [epoch: 4.78 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2269044252141964		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.2269044252141964 | validation: 0.2565521846660425]
	TIME [epoch: 4.77 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22645896046522163		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.22645896046522163 | validation: 0.24455425436158051]
	TIME [epoch: 4.77 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22901091484556615		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.22901091484556615 | validation: 0.23552382601062005]
	TIME [epoch: 4.78 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23000935088665359		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.23000935088665359 | validation: 0.2622538081500205]
	TIME [epoch: 4.77 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23151576785729044		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.23151576785729044 | validation: 0.23984720796685735]
	TIME [epoch: 4.79 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22751263495734353		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.22751263495734353 | validation: 0.24347492876047072]
	TIME [epoch: 4.79 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2277636605738239		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.2277636605738239 | validation: 0.2725776878754063]
	TIME [epoch: 4.78 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23288505878696641		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.23288505878696641 | validation: 0.21985455301919465]
	TIME [epoch: 4.79 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2279650858878312		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.2279650858878312 | validation: 0.2513125392002585]
	TIME [epoch: 4.8 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22706570365730183		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.22706570365730183 | validation: 0.2457036085410886]
	TIME [epoch: 4.77 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22735306350060178		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.22735306350060178 | validation: 0.25408585394323396]
	TIME [epoch: 4.79 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22635035686200908		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.22635035686200908 | validation: 0.2542385297715328]
	TIME [epoch: 4.78 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22727227532813665		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.22727227532813665 | validation: 0.23504743755745383]
	TIME [epoch: 4.79 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22621876950955538		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.22621876950955538 | validation: 0.2440888329513965]
	TIME [epoch: 4.78 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22790534729499012		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.22790534729499012 | validation: 0.23962915633569187]
	TIME [epoch: 4.8 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2292315314367587		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.2292315314367587 | validation: 0.25294636982245394]
	TIME [epoch: 4.77 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22738790381297513		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.22738790381297513 | validation: 0.2454368567918726]
	TIME [epoch: 4.77 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.227537980460001		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.227537980460001 | validation: 0.25522611165184056]
	TIME [epoch: 4.77 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22976610092364572		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.22976610092364572 | validation: 0.22719004612915797]
	TIME [epoch: 4.77 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23529529018956258		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.23529529018956258 | validation: 0.2631490402580782]
	TIME [epoch: 4.77 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22780976610427153		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.22780976610427153 | validation: 0.2358227358187099]
	TIME [epoch: 4.78 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22697851077518252		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.22697851077518252 | validation: 0.24378912176966733]
	TIME [epoch: 4.78 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22262850681029592		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.22262850681029592 | validation: 0.24257010366197929]
	TIME [epoch: 4.78 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2249494233815332		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.2249494233815332 | validation: 0.24662245603791844]
	TIME [epoch: 4.77 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2265035298913029		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.2265035298913029 | validation: 0.25073280225945144]
	TIME [epoch: 4.78 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22540048498288745		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.22540048498288745 | validation: 0.24772549134180233]
	TIME [epoch: 4.77 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22519653330306405		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.22519653330306405 | validation: 0.24638051743481185]
	TIME [epoch: 4.78 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22596165426017456		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.22596165426017456 | validation: 0.24324443337349855]
	TIME [epoch: 4.77 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22824883921995104		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.22824883921995104 | validation: 0.23195653821861947]
	TIME [epoch: 4.77 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23701226081940666		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.23701226081940666 | validation: 0.26496973663482976]
	TIME [epoch: 4.76 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22628127950725044		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.22628127950725044 | validation: 0.25242780211771754]
	TIME [epoch: 4.77 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22503275731551545		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.22503275731551545 | validation: 0.24491280534649335]
	TIME [epoch: 4.77 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2217053702461974		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.2217053702461974 | validation: 0.23768458900029865]
	TIME [epoch: 4.77 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22338512223391582		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.22338512223391582 | validation: 0.2369377229986374]
	TIME [epoch: 4.78 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22507988934936993		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.22507988934936993 | validation: 0.2422116521115379]
	TIME [epoch: 4.77 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2276043228358467		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.2276043228358467 | validation: 0.24181036816567847]
	TIME [epoch: 4.77 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22278773813850103		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.22278773813850103 | validation: 0.2473146729403538]
	TIME [epoch: 4.77 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22311819582244294		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.22311819582244294 | validation: 0.23766168826518733]
	TIME [epoch: 4.77 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2205708871254744		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.2205708871254744 | validation: 0.22722008723373405]
	TIME [epoch: 4.77 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22473767734840938		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.22473767734840938 | validation: 0.2644214912014947]
	TIME [epoch: 4.78 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23374280857643745		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.23374280857643745 | validation: 0.2069249129546097]
	TIME [epoch: 4.77 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_1366.pth
	Model improved!!!
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2309347849133895		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.2309347849133895 | validation: 0.2438818363353096]
	TIME [epoch: 4.77 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22177461148222946		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.22177461148222946 | validation: 0.25214707272143416]
	TIME [epoch: 4.77 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22613512134396072		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.22613512134396072 | validation: 0.2284584800777415]
	TIME [epoch: 4.77 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22299288091417666		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.22299288091417666 | validation: 0.2429011884485237]
	TIME [epoch: 4.77 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2198562150365938		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.2198562150365938 | validation: 0.2396301077233563]
	TIME [epoch: 4.79 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2218314674523413		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.2218314674523413 | validation: 0.2462806681759652]
	TIME [epoch: 4.77 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22077122222251994		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.22077122222251994 | validation: 0.22831574599186052]
	TIME [epoch: 4.78 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2227796491230746		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.2227796491230746 | validation: 0.23656113111106622]
	TIME [epoch: 4.77 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21983651525358824		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.21983651525358824 | validation: 0.24331861312920955]
	TIME [epoch: 4.77 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22528638405811827		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.22528638405811827 | validation: 0.24555196769072204]
	TIME [epoch: 4.77 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22840026729933754		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.22840026729933754 | validation: 0.22276943272497307]
	TIME [epoch: 4.78 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22637656552554322		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.22637656552554322 | validation: 0.2511935565401146]
	TIME [epoch: 4.77 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22492704201385813		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.22492704201385813 | validation: 0.24775605686601768]
	TIME [epoch: 4.77 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22223343060180828		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.22223343060180828 | validation: 0.24499754771966367]
	TIME [epoch: 4.77 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2227826317168953		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.2227826317168953 | validation: 0.23527960151738428]
	TIME [epoch: 4.77 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2219167150389569		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.2219167150389569 | validation: 0.25242700667958273]
	TIME [epoch: 4.77 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21973220040662647		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.21973220040662647 | validation: 0.23331274032657903]
	TIME [epoch: 4.78 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22070439667426633		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.22070439667426633 | validation: 0.22133382499817175]
	TIME [epoch: 4.77 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2221505084441982		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.2221505084441982 | validation: 0.24905462537988649]
	TIME [epoch: 4.77 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22294796085817725		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.22294796085817725 | validation: 0.22187195531956583]
	TIME [epoch: 4.77 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2234229747869123		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.2234229747869123 | validation: 0.24454513011211493]
	TIME [epoch: 4.77 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2202677180808718		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.2202677180808718 | validation: 0.22844267231833518]
	TIME [epoch: 4.77 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22061562700930673		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.22061562700930673 | validation: 0.23336096479405627]
	TIME [epoch: 4.77 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22075461250979836		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.22075461250979836 | validation: 0.2398153702317569]
	TIME [epoch: 4.78 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21988356759756095		[learning rate: 8.6788e-05]
	Learning Rate: 8.67884e-05
	LOSS [training: 0.21988356759756095 | validation: 0.23769699439043676]
	TIME [epoch: 4.78 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2196489111416208		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.2196489111416208 | validation: 0.234250397604638]
	TIME [epoch: 4.77 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21808376302162455		[learning rate: 8.6176e-05]
	Learning Rate: 8.61757e-05
	LOSS [training: 0.21808376302162455 | validation: 0.22992010255156325]
	TIME [epoch: 4.77 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2168399802772111		[learning rate: 8.5871e-05]
	Learning Rate: 8.5871e-05
	LOSS [training: 0.2168399802772111 | validation: 0.23334641054649763]
	TIME [epoch: 4.77 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22257697197674461		[learning rate: 8.5567e-05]
	Learning Rate: 8.55673e-05
	LOSS [training: 0.22257697197674461 | validation: 0.23158151665102322]
	TIME [epoch: 4.77 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2226319162117001		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.2226319162117001 | validation: 0.25370459619139263]
	TIME [epoch: 4.79 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21848307816845564		[learning rate: 8.4963e-05]
	Learning Rate: 8.49632e-05
	LOSS [training: 0.21848307816845564 | validation: 0.21792397397403224]
	TIME [epoch: 4.78 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22043379645114358		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.22043379645114358 | validation: 0.23983823753209832]
	TIME [epoch: 4.77 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2190830575912755		[learning rate: 8.4363e-05]
	Learning Rate: 8.43634e-05
	LOSS [training: 0.2190830575912755 | validation: 0.2299384149429105]
	TIME [epoch: 4.78 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22110632466967722		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.22110632466967722 | validation: 0.23816843123024312]
	TIME [epoch: 4.79 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21956395492372047		[learning rate: 8.3768e-05]
	Learning Rate: 8.37678e-05
	LOSS [training: 0.21956395492372047 | validation: 0.2208666753748193]
	TIME [epoch: 4.77 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22537454607093824		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.22537454607093824 | validation: 0.25798554332850054]
	TIME [epoch: 4.78 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22429404464782288		[learning rate: 8.3176e-05]
	Learning Rate: 8.31764e-05
	LOSS [training: 0.22429404464782288 | validation: 0.24099073078389943]
	TIME [epoch: 4.77 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.219320394333167		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.219320394333167 | validation: 0.21693701081713948]
	TIME [epoch: 4.78 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21986408644943709		[learning rate: 8.2589e-05]
	Learning Rate: 8.25892e-05
	LOSS [training: 0.21986408644943709 | validation: 0.24640780786027722]
	TIME [epoch: 4.77 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21853650561835053		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.21853650561835053 | validation: 0.23586381655977195]
	TIME [epoch: 4.78 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21917648285553945		[learning rate: 8.2006e-05]
	Learning Rate: 8.20061e-05
	LOSS [training: 0.21917648285553945 | validation: 0.23619422813815044]
	TIME [epoch: 4.77 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22058514794449027		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.22058514794449027 | validation: 0.2346409236580399]
	TIME [epoch: 4.78 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2173674790959114		[learning rate: 8.1427e-05]
	Learning Rate: 8.14272e-05
	LOSS [training: 0.2173674790959114 | validation: 0.2280102753500649]
	TIME [epoch: 4.78 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21712609018294948		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.21712609018294948 | validation: 0.24302678526371718]
	TIME [epoch: 4.78 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2149059904299706		[learning rate: 8.0852e-05]
	Learning Rate: 8.08523e-05
	LOSS [training: 0.2149059904299706 | validation: 0.2214545724857398]
	TIME [epoch: 4.77 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2169734924981505		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.2169734924981505 | validation: 0.24445182715528105]
	TIME [epoch: 4.79 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21645879971042742		[learning rate: 8.0281e-05]
	Learning Rate: 8.02815e-05
	LOSS [training: 0.21645879971042742 | validation: 0.23784902556671136]
	TIME [epoch: 4.78 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21765754040420981		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.21765754040420981 | validation: 0.24014619038588259]
	TIME [epoch: 4.8 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21582242001273286		[learning rate: 7.9715e-05]
	Learning Rate: 7.97147e-05
	LOSS [training: 0.21582242001273286 | validation: 0.22494315244528398]
	TIME [epoch: 4.79 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21707662220300286		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.21707662220300286 | validation: 0.23534617908490785]
	TIME [epoch: 4.79 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21606602988666754		[learning rate: 7.9152e-05]
	Learning Rate: 7.9152e-05
	LOSS [training: 0.21606602988666754 | validation: 0.2262067917000904]
	TIME [epoch: 4.78 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2201510113832488		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.2201510113832488 | validation: 0.23291027226591812]
	TIME [epoch: 4.78 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2162760426836827		[learning rate: 7.8593e-05]
	Learning Rate: 7.85931e-05
	LOSS [training: 0.2162760426836827 | validation: 0.21873166292904128]
	TIME [epoch: 4.77 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21443377538708078		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.21443377538708078 | validation: 0.2355063890471394]
	TIME [epoch: 4.79 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21681063472111356		[learning rate: 7.8038e-05]
	Learning Rate: 7.80383e-05
	LOSS [training: 0.21681063472111356 | validation: 0.21463087417920526]
	TIME [epoch: 4.78 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21728290387104918		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.21728290387104918 | validation: 0.24853568665342715]
	TIME [epoch: 4.79 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2190920947101566		[learning rate: 7.7487e-05]
	Learning Rate: 7.74873e-05
	LOSS [training: 0.2190920947101566 | validation: 0.22308412920798013]
	TIME [epoch: 4.78 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2156799925626577		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.2156799925626577 | validation: 0.23067988368851522]
	TIME [epoch: 4.77 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21764004609564036		[learning rate: 7.694e-05]
	Learning Rate: 7.69403e-05
	LOSS [training: 0.21764004609564036 | validation: 0.2243243659341915]
	TIME [epoch: 4.77 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21686655030361215		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.21686655030361215 | validation: 0.2419542825771516]
	TIME [epoch: 4.78 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21474440421605548		[learning rate: 7.6397e-05]
	Learning Rate: 7.63971e-05
	LOSS [training: 0.21474440421605548 | validation: 0.22255666943378768]
	TIME [epoch: 4.77 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21639754648722492		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.21639754648722492 | validation: 0.2231611574330483]
	TIME [epoch: 4.77 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21493485040102792		[learning rate: 7.5858e-05]
	Learning Rate: 7.58578e-05
	LOSS [training: 0.21493485040102792 | validation: 0.2455573013292788]
	TIME [epoch: 4.77 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2134446803873125		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.2134446803873125 | validation: 0.23170037085663842]
	TIME [epoch: 4.77 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21455350776233556		[learning rate: 7.5322e-05]
	Learning Rate: 7.53222e-05
	LOSS [training: 0.21455350776233556 | validation: 0.22949521508774018]
	TIME [epoch: 4.77 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21430149346899133		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.21430149346899133 | validation: 0.23933734753677577]
	TIME [epoch: 4.79 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21765200865790937		[learning rate: 7.479e-05]
	Learning Rate: 7.47905e-05
	LOSS [training: 0.21765200865790937 | validation: 0.20303270715935973]
	TIME [epoch: 4.78 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_1433.pth
	Model improved!!!
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22033355846660618		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.22033355846660618 | validation: 0.23414949468145985]
	TIME [epoch: 4.77 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21727084659363285		[learning rate: 7.4262e-05]
	Learning Rate: 7.42625e-05
	LOSS [training: 0.21727084659363285 | validation: 0.2340618048941356]
	TIME [epoch: 4.79 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21671816320723455		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.21671816320723455 | validation: 0.22051638369958504]
	TIME [epoch: 4.77 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.214924650036372		[learning rate: 7.3738e-05]
	Learning Rate: 7.37382e-05
	LOSS [training: 0.214924650036372 | validation: 0.22469047166234868]
	TIME [epoch: 4.78 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21199297831535788		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.21199297831535788 | validation: 0.2328835485613451]
	TIME [epoch: 4.79 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21214318616364453		[learning rate: 7.3218e-05]
	Learning Rate: 7.32176e-05
	LOSS [training: 0.21214318616364453 | validation: 0.22423579555839798]
	TIME [epoch: 4.79 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21260279721332861		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.21260279721332861 | validation: 0.21668390704259366]
	TIME [epoch: 4.77 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21411179659278895		[learning rate: 7.2701e-05]
	Learning Rate: 7.27007e-05
	LOSS [training: 0.21411179659278895 | validation: 0.24254611631648762]
	TIME [epoch: 4.79 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2167461398050358		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.2167461398050358 | validation: 0.2155110340726235]
	TIME [epoch: 4.77 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21412770570439565		[learning rate: 7.2187e-05]
	Learning Rate: 7.21874e-05
	LOSS [training: 0.21412770570439565 | validation: 0.21979266071594356]
	TIME [epoch: 4.77 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21329426167988252		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.21329426167988252 | validation: 0.23880847832526164]
	TIME [epoch: 4.77 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21606493786871114		[learning rate: 7.1678e-05]
	Learning Rate: 7.16778e-05
	LOSS [training: 0.21606493786871114 | validation: 0.20861511585576886]
	TIME [epoch: 4.78 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21430342136260055		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.21430342136260055 | validation: 0.2204377940976912]
	TIME [epoch: 4.77 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21640855636479248		[learning rate: 7.1172e-05]
	Learning Rate: 7.11718e-05
	LOSS [training: 0.21640855636479248 | validation: 0.24348136468715983]
	TIME [epoch: 4.78 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2177182099272612		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.2177182099272612 | validation: 0.217584721022856]
	TIME [epoch: 4.77 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21274415800600424		[learning rate: 7.0669e-05]
	Learning Rate: 7.06693e-05
	LOSS [training: 0.21274415800600424 | validation: 0.2245205427055472]
	TIME [epoch: 4.79 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21440477077275386		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.21440477077275386 | validation: 0.24249747052085824]
	TIME [epoch: 4.78 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22156171819987144		[learning rate: 7.017e-05]
	Learning Rate: 7.01704e-05
	LOSS [training: 0.22156171819987144 | validation: 0.22479138208097332]
	TIME [epoch: 4.77 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21402957753342658		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.21402957753342658 | validation: 0.21573577710281944]
	TIME [epoch: 4.77 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21154065011221018		[learning rate: 6.9675e-05]
	Learning Rate: 6.9675e-05
	LOSS [training: 0.21154065011221018 | validation: 0.22944762881858738]
	TIME [epoch: 4.77 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21510140311184664		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.21510140311184664 | validation: 0.2227157653131333]
	TIME [epoch: 4.77 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21573060249992715		[learning rate: 6.9183e-05]
	Learning Rate: 6.91831e-05
	LOSS [training: 0.21573060249992715 | validation: 0.2321130963891921]
	TIME [epoch: 4.77 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21125088798166844		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.21125088798166844 | validation: 0.232934471553936]
	TIME [epoch: 4.77 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20874801725471429		[learning rate: 6.8695e-05]
	Learning Rate: 6.86947e-05
	LOSS [training: 0.20874801725471429 | validation: 0.2114338754027119]
	TIME [epoch: 4.77 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21077142769997562		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.21077142769997562 | validation: 0.22525357579617256]
	TIME [epoch: 4.77 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21278567288055122		[learning rate: 6.821e-05]
	Learning Rate: 6.82097e-05
	LOSS [training: 0.21278567288055122 | validation: 0.23497613911136914]
	TIME [epoch: 4.77 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20955238096416323		[learning rate: 6.7969e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.20955238096416323 | validation: 0.2141816337722905]
	TIME [epoch: 4.77 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21583578443172508		[learning rate: 6.7728e-05]
	Learning Rate: 6.77282e-05
	LOSS [training: 0.21583578443172508 | validation: 0.23557777134114014]
	TIME [epoch: 4.79 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21251440626027304		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.21251440626027304 | validation: 0.21765008358371546]
	TIME [epoch: 4.79 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20932370925171667		[learning rate: 6.725e-05]
	Learning Rate: 6.725e-05
	LOSS [training: 0.20932370925171667 | validation: 0.21575023254133524]
	TIME [epoch: 4.77 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21204808706756959		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.21204808706756959 | validation: 0.2359442416492887]
	TIME [epoch: 4.78 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2111604572127714		[learning rate: 6.6775e-05]
	Learning Rate: 6.67752e-05
	LOSS [training: 0.2111604572127714 | validation: 0.21178103638422652]
	TIME [epoch: 4.77 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21369255854830102		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.21369255854830102 | validation: 0.21654808064037498]
	TIME [epoch: 4.78 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20956629572718563		[learning rate: 6.6304e-05]
	Learning Rate: 6.63038e-05
	LOSS [training: 0.20956629572718563 | validation: 0.23424239491699916]
	TIME [epoch: 4.78 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2111691166530719		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.2111691166530719 | validation: 0.20947468215514242]
	TIME [epoch: 4.79 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20902856505154965		[learning rate: 6.5836e-05]
	Learning Rate: 6.58357e-05
	LOSS [training: 0.20902856505154965 | validation: 0.231737111995033]
	TIME [epoch: 4.78 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21156768412368174		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.21156768412368174 | validation: 0.22975064288377403]
	TIME [epoch: 4.78 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2110315663447976		[learning rate: 6.5371e-05]
	Learning Rate: 6.53709e-05
	LOSS [training: 0.2110315663447976 | validation: 0.2099024572879028]
	TIME [epoch: 4.77 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21180941393333208		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.21180941393333208 | validation: 0.23359750969453108]
	TIME [epoch: 4.77 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2124191621972407		[learning rate: 6.4909e-05]
	Learning Rate: 6.49094e-05
	LOSS [training: 0.2124191621972407 | validation: 0.22105512512733824]
	TIME [epoch: 4.78 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20986204925321034		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.20986204925321034 | validation: 0.2277871666260805]
	TIME [epoch: 4.77 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2137840933944094		[learning rate: 6.4451e-05]
	Learning Rate: 6.44512e-05
	LOSS [training: 0.2137840933944094 | validation: 0.21385389184160172]
	TIME [epoch: 4.78 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2109781994430716		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.2109781994430716 | validation: 0.23598305386688653]
	TIME [epoch: 4.78 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2151830536276723		[learning rate: 6.3996e-05]
	Learning Rate: 6.39962e-05
	LOSS [training: 0.2151830536276723 | validation: 0.2156161061932408]
	TIME [epoch: 4.78 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2114880158376971		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.2114880158376971 | validation: 0.20931027371474187]
	TIME [epoch: 4.79 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20729165738272928		[learning rate: 6.3544e-05]
	Learning Rate: 6.35444e-05
	LOSS [training: 0.20729165738272928 | validation: 0.22952509049295353]
	TIME [epoch: 4.78 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21071558004111496		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.21071558004111496 | validation: 0.21904098568837682]
	TIME [epoch: 4.79 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20856077772020812		[learning rate: 6.3096e-05]
	Learning Rate: 6.30958e-05
	LOSS [training: 0.20856077772020812 | validation: 0.21519342043232115]
	TIME [epoch: 4.79 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20921763631905965		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.20921763631905965 | validation: 0.2313767666467101]
	TIME [epoch: 4.79 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21005826467660937		[learning rate: 6.265e-05]
	Learning Rate: 6.26503e-05
	LOSS [training: 0.21005826467660937 | validation: 0.21571475994116787]
	TIME [epoch: 4.77 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2083771685599625		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.2083771685599625 | validation: 0.21883753107858148]
	TIME [epoch: 4.78 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20809212698635426		[learning rate: 6.2208e-05]
	Learning Rate: 6.2208e-05
	LOSS [training: 0.20809212698635426 | validation: 0.2194102842154778]
	TIME [epoch: 4.78 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21205057442841238		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.21205057442841238 | validation: 0.22056613900467276]
	TIME [epoch: 4.77 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20920582671540988		[learning rate: 6.1769e-05]
	Learning Rate: 6.17688e-05
	LOSS [training: 0.20920582671540988 | validation: 0.2284598514490396]
	TIME [epoch: 4.78 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2144597134249109		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.2144597134249109 | validation: 0.20674032771981846]
	TIME [epoch: 4.77 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20808972067746948		[learning rate: 6.1333e-05]
	Learning Rate: 6.13327e-05
	LOSS [training: 0.20808972067746948 | validation: 0.21905540971339327]
	TIME [epoch: 4.77 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21116831239257253		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.21116831239257253 | validation: 0.2242422044331399]
	TIME [epoch: 4.77 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21441278624909121		[learning rate: 6.09e-05]
	Learning Rate: 6.08998e-05
	LOSS [training: 0.21441278624909121 | validation: 0.22732242018010493]
	TIME [epoch: 4.79 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20993776788221227		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.20993776788221227 | validation: 0.21209113213364428]
	TIME [epoch: 4.78 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20826266326274276		[learning rate: 6.047e-05]
	Learning Rate: 6.04698e-05
	LOSS [training: 0.20826266326274276 | validation: 0.22321710484732185]
	TIME [epoch: 4.79 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20656547564264963		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.20656547564264963 | validation: 0.22284914083379104]
	TIME [epoch: 4.79 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20847190137146152		[learning rate: 6.0043e-05]
	Learning Rate: 6.00429e-05
	LOSS [training: 0.20847190137146152 | validation: 0.2218822302183009]
	TIME [epoch: 4.78 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20708359710589377		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.20708359710589377 | validation: 0.21766434262077672]
	TIME [epoch: 4.79 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20518510119807817		[learning rate: 5.9619e-05]
	Learning Rate: 5.9619e-05
	LOSS [training: 0.20518510119807817 | validation: 0.22495715564638108]
	TIME [epoch: 4.78 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20917787811507524		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.20917787811507524 | validation: 0.23237170012886496]
	TIME [epoch: 4.81 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21698786033058792		[learning rate: 5.9198e-05]
	Learning Rate: 5.91981e-05
	LOSS [training: 0.21698786033058792 | validation: 0.2059978377185021]
	TIME [epoch: 4.78 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20977492546706458		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.20977492546706458 | validation: 0.21302294766977756]
	TIME [epoch: 4.77 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21201668688731715		[learning rate: 5.878e-05]
	Learning Rate: 5.87802e-05
	LOSS [training: 0.21201668688731715 | validation: 0.21047998226609332]
	TIME [epoch: 4.76 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2065672479231103		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.2065672479231103 | validation: 0.2180878803299507]
	TIME [epoch: 4.76 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21003663579593593		[learning rate: 5.8365e-05]
	Learning Rate: 5.83652e-05
	LOSS [training: 0.21003663579593593 | validation: 0.2135074524099074]
	TIME [epoch: 4.76 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20904364249678714		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.20904364249678714 | validation: 0.2171347254794741]
	TIME [epoch: 4.78 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2065971678866804		[learning rate: 5.7953e-05]
	Learning Rate: 5.79531e-05
	LOSS [training: 0.2065971678866804 | validation: 0.2144665458388615]
	TIME [epoch: 4.76 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20851796976281944		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.20851796976281944 | validation: 0.22554939006254449]
	TIME [epoch: 4.76 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20834176942784716		[learning rate: 5.7544e-05]
	Learning Rate: 5.7544e-05
	LOSS [training: 0.20834176942784716 | validation: 0.21500031663404207]
	TIME [epoch: 4.77 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20273717395846733		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.20273717395846733 | validation: 0.222804986245118]
	TIME [epoch: 4.78 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20774267900966042		[learning rate: 5.7138e-05]
	Learning Rate: 5.71378e-05
	LOSS [training: 0.20774267900966042 | validation: 0.2153391804494166]
	TIME [epoch: 4.78 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2077467022180935		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.2077467022180935 | validation: 0.22316669164628627]
	TIME [epoch: 4.78 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.209533567542242		[learning rate: 5.6734e-05]
	Learning Rate: 5.67344e-05
	LOSS [training: 0.209533567542242 | validation: 0.2264779718684351]
	TIME [epoch: 4.78 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20599763146491193		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.20599763146491193 | validation: 0.21211196282559613]
	TIME [epoch: 4.77 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21115523803428382		[learning rate: 5.6334e-05]
	Learning Rate: 5.63338e-05
	LOSS [training: 0.21115523803428382 | validation: 0.22463257436246034]
	TIME [epoch: 4.78 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20792027004049515		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.20792027004049515 | validation: 0.21807990926748735]
	TIME [epoch: 4.77 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.207114763003093		[learning rate: 5.5936e-05]
	Learning Rate: 5.59361e-05
	LOSS [training: 0.207114763003093 | validation: 0.21345817574430834]
	TIME [epoch: 4.77 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2048629804070135		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.2048629804070135 | validation: 0.23926334794199489]
	TIME [epoch: 4.76 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21334001732035482		[learning rate: 5.5541e-05]
	Learning Rate: 5.55412e-05
	LOSS [training: 0.21334001732035482 | validation: 0.21199904834151917]
	TIME [epoch: 4.76 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20987336374250448		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.20987336374250448 | validation: 0.2125701895142183]
	TIME [epoch: 4.78 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20636304297621805		[learning rate: 5.5149e-05]
	Learning Rate: 5.51491e-05
	LOSS [training: 0.20636304297621805 | validation: 0.21907330272822684]
	TIME [epoch: 4.77 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20825754121490594		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.20825754121490594 | validation: 0.21586016323874135]
	TIME [epoch: 4.76 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2096224553408954		[learning rate: 5.476e-05]
	Learning Rate: 5.47598e-05
	LOSS [training: 0.2096224553408954 | validation: 0.21098697809448186]
	TIME [epoch: 4.77 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20501601388740107		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.20501601388740107 | validation: 0.23046786146738382]
	TIME [epoch: 4.78 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21173819408341626		[learning rate: 5.4373e-05]
	Learning Rate: 5.43732e-05
	LOSS [training: 0.21173819408341626 | validation: 0.2138586195367887]
	TIME [epoch: 4.77 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20740338471302153		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.20740338471302153 | validation: 0.20318082027218828]
	TIME [epoch: 4.76 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21002585481516103		[learning rate: 5.3989e-05]
	Learning Rate: 5.39893e-05
	LOSS [training: 0.21002585481516103 | validation: 0.23058060097927788]
	TIME [epoch: 4.78 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2077147333750763		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.2077147333750763 | validation: 0.20908038619007452]
	TIME [epoch: 4.77 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2031177670206077		[learning rate: 5.3608e-05]
	Learning Rate: 5.36082e-05
	LOSS [training: 0.2031177670206077 | validation: 0.21873704752296727]
	TIME [epoch: 4.77 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20264559993493625		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.20264559993493625 | validation: 0.22045317115177046]
	TIME [epoch: 4.81 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2043561400526071		[learning rate: 5.323e-05]
	Learning Rate: 5.32297e-05
	LOSS [training: 0.2043561400526071 | validation: 0.21163079275604335]
	TIME [epoch: 4.77 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20312499622917798		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.20312499622917798 | validation: 0.221251711888962]
	TIME [epoch: 4.77 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2056783069265415		[learning rate: 5.2854e-05]
	Learning Rate: 5.28539e-05
	LOSS [training: 0.2056783069265415 | validation: 0.2183761996676492]
	TIME [epoch: 4.77 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20545999350290578		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.20545999350290578 | validation: 0.21089336289972693]
	TIME [epoch: 4.77 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20439662592865585		[learning rate: 5.2481e-05]
	Learning Rate: 5.24807e-05
	LOSS [training: 0.20439662592865585 | validation: 0.2167437543460996]
	TIME [epoch: 4.78 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20127593602769786		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.20127593602769786 | validation: 0.21694997229210058]
	TIME [epoch: 4.77 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_1_v_mmd3_1534.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 5041.919 seconds.
