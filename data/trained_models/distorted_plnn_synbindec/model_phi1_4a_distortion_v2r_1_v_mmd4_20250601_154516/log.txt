Args:
Namespace(name='model_phi1_4a_distortion_v2r_1_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2r_1/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2r_1/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.025831684, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3586735375

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.636884784810174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.636884784810174 | validation: 7.256417415062481]
	TIME [epoch: 169 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.298874627804114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.298874627804114 | validation: 6.817936654837523]
	TIME [epoch: 0.76 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.979804548540255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.979804548540255 | validation: 7.247071563731423]
	TIME [epoch: 0.695 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.254866440879257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.254866440879257 | validation: 5.75243037805409]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.316345896530112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.316345896530112 | validation: 3.156210874087592]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.95772461225219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.95772461225219 | validation: 4.641776567476232]
	TIME [epoch: 0.692 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.792888211212123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.792888211212123 | validation: 5.690126664333807]
	TIME [epoch: 0.693 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.494904113492847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.494904113492847 | validation: 4.471573589342]
	TIME [epoch: 0.694 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008428836651652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.008428836651652 | validation: 3.421236019700463]
	TIME [epoch: 0.727 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3683249749155393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3683249749155393 | validation: 3.419173941713453]
	TIME [epoch: 0.694 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.428187707800487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.428187707800487 | validation: 5.538485440507664]
	TIME [epoch: 0.695 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.4642227074363925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.4642227074363925 | validation: 5.036239755271807]
	TIME [epoch: 0.697 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.005482415433432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.005482415433432 | validation: 3.9443948876749717]
	TIME [epoch: 0.696 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7734623745421874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7734623745421874 | validation: 4.881483414869919]
	TIME [epoch: 0.694 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8278912058480605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8278912058480605 | validation: 2.97980384466497]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7768116419988416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7768116419988416 | validation: 3.62450171631824]
	TIME [epoch: 0.691 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9453493587666486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9453493587666486 | validation: 2.2436218734475775]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.45175076009426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.45175076009426 | validation: 2.4484007733037054]
	TIME [epoch: 0.778 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3661333716793074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3661333716793074 | validation: 1.782525232018633]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.347879126651465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.347879126651465 | validation: 2.76822460921736]
	TIME [epoch: 0.692 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3698008121463143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3698008121463143 | validation: 1.716544933434954]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9440561835866572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9440561835866572 | validation: 1.5328705042098836]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.020726459220895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.020726459220895 | validation: 3.123637739538628]
	TIME [epoch: 0.693 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3955201658923295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3955201658923295 | validation: 1.9126564790037777]
	TIME [epoch: 0.69 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.046763826724088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.046763826724088 | validation: 1.5288378039313484]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9316966820278914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9316966820278914 | validation: 2.748946402680992]
	TIME [epoch: 0.695 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.190820797532224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.190820797532224 | validation: 1.7070805097134802]
	TIME [epoch: 0.731 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3511100471630244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3511100471630244 | validation: 1.28171586979069]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5122031231002393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5122031231002393 | validation: 2.676188621998032]
	TIME [epoch: 0.697 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.462682306324633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.462682306324633 | validation: 1.5598184200076044]
	TIME [epoch: 0.694 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.97035640000825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.97035640000825 | validation: 1.7483896160447507]
	TIME [epoch: 0.694 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5923467320968883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5923467320968883 | validation: 1.5077010516347613]
	TIME [epoch: 0.693 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3481321053702617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3481321053702617 | validation: 1.4023739348124598]
	TIME [epoch: 0.693 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7203154350789422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7203154350789422 | validation: 1.8325202410033896]
	TIME [epoch: 0.694 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5091947397633436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5091947397633436 | validation: 1.0902688489345902]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4950962314416305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4950962314416305 | validation: 1.2976943922716848]
	TIME [epoch: 0.692 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1700594389897245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1700594389897245 | validation: 1.0276582791981885]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.182043567863269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.182043567863269 | validation: 1.8186457964119331]
	TIME [epoch: 0.692 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4676966505158329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4676966505158329 | validation: 1.0498431809821926]
	TIME [epoch: 0.689 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.41941253840828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.41941253840828 | validation: 1.5237950198492696]
	TIME [epoch: 0.69 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2994371138561474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2994371138561474 | validation: 0.9534276192261875]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2333498577473727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2333498577473727 | validation: 1.433339992030886]
	TIME [epoch: 0.69 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2534173127271584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2534173127271584 | validation: 0.953002624595301]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2639786153263695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2639786153263695 | validation: 1.3979317429382019]
	TIME [epoch: 0.692 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2329780932698138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2329780932698138 | validation: 0.9524991052406782]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2066300946571142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2066300946571142 | validation: 1.3745267710635258]
	TIME [epoch: 0.691 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2167535374121892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2167535374121892 | validation: 0.9551765937514207]
	TIME [epoch: 0.689 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2077927815868803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2077927815868803 | validation: 1.4569388695685883]
	TIME [epoch: 0.688 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2649986591604974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2649986591604974 | validation: 1.032196955031609]
	TIME [epoch: 0.687 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2302111708822583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2302111708822583 | validation: 1.393007891942724]
	TIME [epoch: 0.69 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2489099279661844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2489099279661844 | validation: 0.9930261701861195]
	TIME [epoch: 0.688 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1584061355674902		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.1584061355674902 | validation: 1.255347525136325]
	TIME [epoch: 0.69 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1609241105335248		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.1609241105335248 | validation: 0.9632324625992011]
	TIME [epoch: 0.692 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1987606490544898		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.1987606490544898 | validation: 1.4867268957187847]
	TIME [epoch: 0.704 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3002601469669142		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.3002601469669142 | validation: 0.9284941059111376]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.268636628228274		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.268636628228274 | validation: 1.3375462481894276]
	TIME [epoch: 0.694 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2715587612029096		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.2715587612029096 | validation: 1.0184295506005696]
	TIME [epoch: 0.697 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3551606620352363		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.3551606620352363 | validation: 1.1255570199577167]
	TIME [epoch: 0.694 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1067757878231712		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.1067757878231712 | validation: 0.9461589740193745]
	TIME [epoch: 0.689 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1298579828870334		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.1298579828870334 | validation: 1.3338234367496853]
	TIME [epoch: 0.688 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2260470766158948		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.2260470766158948 | validation: 0.9640427728948119]
	TIME [epoch: 0.689 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1914975763797493		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.1914975763797493 | validation: 1.3177381506384007]
	TIME [epoch: 0.688 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2107882272749138		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.2107882272749138 | validation: 0.901941388691871]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1878503253116819		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.1878503253116819 | validation: 1.2743864421536708]
	TIME [epoch: 0.695 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.205170718387045		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.205170718387045 | validation: 0.9602487917149919]
	TIME [epoch: 0.696 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.310207098494622		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.310207098494622 | validation: 1.1945241449659711]
	TIME [epoch: 0.689 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1544237055229152		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.1544237055229152 | validation: 0.8998981774854926]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1421864477265167		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.1421864477265167 | validation: 1.192962527682963]
	TIME [epoch: 0.696 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.162428184238174		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.162428184238174 | validation: 0.8957557247380663]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2304449575256247		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.2304449575256247 | validation: 1.2346826750812898]
	TIME [epoch: 0.695 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1839769611784998		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.1839769611784998 | validation: 0.8882686712868806]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2022391981497487		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.2022391981497487 | validation: 1.2311303405754666]
	TIME [epoch: 0.692 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1583400404407522		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.1583400404407522 | validation: 0.8884494534969472]
	TIME [epoch: 0.691 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1681160806554025		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.1681160806554025 | validation: 1.2078264319032535]
	TIME [epoch: 0.691 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1541360966980057		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.1541360966980057 | validation: 0.9168198128467646]
	TIME [epoch: 0.691 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.164582759591149		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.164582759591149 | validation: 1.3095654202068787]
	TIME [epoch: 0.691 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.207316209375996		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.207316209375996 | validation: 1.0098960668343413]
	TIME [epoch: 1.09 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.227955207919521		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.227955207919521 | validation: 1.391698196852518]
	TIME [epoch: 0.691 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2788201541831483		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.2788201541831483 | validation: 0.9838987758532636]
	TIME [epoch: 0.692 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1032653694055916		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.1032653694055916 | validation: 1.0527302764427207]
	TIME [epoch: 0.705 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0832004366247887		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.0832004366247887 | validation: 0.9272415193920349]
	TIME [epoch: 0.692 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.093814353558829		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.093814353558829 | validation: 1.212434202180974]
	TIME [epoch: 0.692 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.165443728088512		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.165443728088512 | validation: 0.9333479353416]
	TIME [epoch: 0.692 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3824652656662506		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.3824652656662506 | validation: 1.292402438080299]
	TIME [epoch: 0.692 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2111134288898515		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.2111134288898515 | validation: 0.9181754768853733]
	TIME [epoch: 0.692 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1838668718047318		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.1838668718047318 | validation: 1.1414868283509763]
	TIME [epoch: 0.698 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1293757710274706		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.1293757710274706 | validation: 0.9413780207958]
	TIME [epoch: 0.706 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1633848073454407		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.1633848073454407 | validation: 1.167447199255789]
	TIME [epoch: 0.693 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1518902494131156		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.1518902494131156 | validation: 0.8950262507363189]
	TIME [epoch: 0.692 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.191419161303585		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.191419161303585 | validation: 1.2099115651014751]
	TIME [epoch: 0.692 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1551303892278428		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.1551303892278428 | validation: 0.8953597743124624]
	TIME [epoch: 0.693 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1794812729261177		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.1794812729261177 | validation: 1.2729048979321491]
	TIME [epoch: 0.693 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1872653400657247		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.1872653400657247 | validation: 0.973544282251978]
	TIME [epoch: 0.705 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1871587281027498		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.1871587281027498 | validation: 1.3288601488676761]
	TIME [epoch: 0.694 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.23498898734278		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.23498898734278 | validation: 0.9401898852194753]
	TIME [epoch: 0.693 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1139578960861485		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.1139578960861485 | validation: 1.1046621102360903]
	TIME [epoch: 0.693 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1076699020547163		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.1076699020547163 | validation: 0.9033714438034037]
	TIME [epoch: 0.693 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.099874881856731		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.099874881856731 | validation: 1.2340832282785077]
	TIME [epoch: 0.692 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1564770162964202		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.1564770162964202 | validation: 0.8790677091265278]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.210588506440594		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.210588506440594 | validation: 1.31049980309623]
	TIME [epoch: 0.697 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2149239322541212		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.2149239322541212 | validation: 0.9268006533287502]
	TIME [epoch: 0.696 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2107132924476331		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.2107132924476331 | validation: 1.1086215434631916]
	TIME [epoch: 0.697 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1461077554150756		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.1461077554150756 | validation: 0.9440588464005306]
	TIME [epoch: 0.694 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.160553836991323		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.160553836991323 | validation: 1.0622085927594507]
	TIME [epoch: 0.697 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.110259816322722		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.110259816322722 | validation: 0.8968403440177304]
	TIME [epoch: 0.704 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1158725731914867		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.1158725731914867 | validation: 1.1921750505167041]
	TIME [epoch: 0.692 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1426749824006157		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.1426749824006157 | validation: 0.8952695966052453]
	TIME [epoch: 0.693 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2172554408696485		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.2172554408696485 | validation: 1.2741121706883984]
	TIME [epoch: 0.753 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.191458521724298		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.191458521724298 | validation: 0.8930411679169796]
	TIME [epoch: 0.692 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1398824831851988		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.1398824831851988 | validation: 1.208713140726327]
	TIME [epoch: 0.692 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1516072003777313		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.1516072003777313 | validation: 0.9626780107581896]
	TIME [epoch: 0.693 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1494685759477943		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.1494685759477943 | validation: 1.2160952602771684]
	TIME [epoch: 0.696 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1594463086027609		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.1594463086027609 | validation: 0.9417276495700687]
	TIME [epoch: 0.693 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.102024843758435		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.102024843758435 | validation: 1.1299267230548824]
	TIME [epoch: 0.694 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1075019464674074		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.1075019464674074 | validation: 0.8798632842784019]
	TIME [epoch: 0.694 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1523250267357836		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.1523250267357836 | validation: 1.2870868980111887]
	TIME [epoch: 0.694 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2067679867257328		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.2067679867257328 | validation: 0.9160843909839698]
	TIME [epoch: 0.695 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2505732381042882		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.2505732381042882 | validation: 1.0907322256152543]
	TIME [epoch: 0.701 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1086204924219514		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.1086204924219514 | validation: 0.9412989007868966]
	TIME [epoch: 0.694 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.096781342291759		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.096781342291759 | validation: 1.0518277540810037]
	TIME [epoch: 0.693 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.096369170529479		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.096369170529479 | validation: 0.9059711056016178]
	TIME [epoch: 0.693 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1249310958464642		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.1249310958464642 | validation: 1.1828566742297273]
	TIME [epoch: 0.834 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1571589801384048		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.1571589801384048 | validation: 0.8736066061424856]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2115438608201752		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.2115438608201752 | validation: 1.1952607270132851]
	TIME [epoch: 0.698 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1431114763030403		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.1431114763030403 | validation: 0.8803221429849363]
	TIME [epoch: 0.696 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0954256093522279		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.0954256093522279 | validation: 1.1483970409451463]
	TIME [epoch: 0.696 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1179039493088536		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.1179039493088536 | validation: 0.9527026989926874]
	TIME [epoch: 0.696 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1597641352671186		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.1597641352671186 | validation: 1.2650755919313572]
	TIME [epoch: 0.694 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.187797732584381		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.187797732584381 | validation: 0.9043525214351498]
	TIME [epoch: 0.693 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0893067846614395		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.0893067846614395 | validation: 1.1028154297043111]
	TIME [epoch: 0.694 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.086339410524185		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.086339410524185 | validation: 0.8613683740663522]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1375253708072748		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.1375253708072748 | validation: 1.2269806744682519]
	TIME [epoch: 0.696 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1754180127645522		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.1754180127645522 | validation: 0.9056253108678046]
	TIME [epoch: 0.695 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2153782466513232		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.2153782466513232 | validation: 1.0739918644419957]
	TIME [epoch: 0.693 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0814858183814329		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.0814858183814329 | validation: 0.8853928828807602]
	TIME [epoch: 0.692 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0658701721675568		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.0658701721675568 | validation: 1.0804391015189496]
	TIME [epoch: 0.693 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0759917743680882		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.0759917743680882 | validation: 0.8884752926133057]
	TIME [epoch: 0.694 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.136464058119835		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.136464058119835 | validation: 1.2152271851891456]
	TIME [epoch: 0.694 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.160018336954165		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.160018336954165 | validation: 0.8678617032081113]
	TIME [epoch: 0.693 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.15546059988368		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.15546059988368 | validation: 1.0909569733053652]
	TIME [epoch: 0.695 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.097554175179462		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.097554175179462 | validation: 0.8801657507467134]
	TIME [epoch: 0.694 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0775543627205848		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.0775543627205848 | validation: 1.1075021050225367]
	TIME [epoch: 0.693 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0841781666821475		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.0841781666821475 | validation: 0.8492936675479993]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1077358354931806		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.1077358354931806 | validation: 1.1928867405173296]
	TIME [epoch: 0.696 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1375692974404499		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.1375692974404499 | validation: 0.885239693721068]
	TIME [epoch: 0.693 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1729913039329138		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.1729913039329138 | validation: 1.1173150184524003]
	TIME [epoch: 0.692 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1432913565630372		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.1432913565630372 | validation: 0.9861706349135737]
	TIME [epoch: 0.692 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1274354896119025		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.1274354896119025 | validation: 0.9679645803075041]
	TIME [epoch: 0.693 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0511604548155868		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.0511604548155868 | validation: 0.9981528909554058]
	TIME [epoch: 0.703 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0461130207495826		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.0461130207495826 | validation: 0.8853823771367255]
	TIME [epoch: 0.692 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0751118291045816		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.0751118291045816 | validation: 1.3089360838053274]
	TIME [epoch: 0.693 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2010280590637823		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.2010280590637823 | validation: 0.8959064067304134]
	TIME [epoch: 0.694 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1984084620084179		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.1984084620084179 | validation: 1.1806525048379892]
	TIME [epoch: 0.693 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.128765416234347		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.128765416234347 | validation: 0.8956976460304722]
	TIME [epoch: 0.692 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0706569015172571		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.0706569015172571 | validation: 1.0405004446686312]
	TIME [epoch: 0.697 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0580678114726803		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.0580678114726803 | validation: 0.8737892382232918]
	TIME [epoch: 0.693 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0643119733436035		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.0643119733436035 | validation: 1.14051786599397]
	TIME [epoch: 0.692 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1116503949301086		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.1116503949301086 | validation: 0.8669527977662584]
	TIME [epoch: 0.692 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1502532494936717		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.1502532494936717 | validation: 1.2089286271703412]
	TIME [epoch: 0.693 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1329443154085064		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.1329443154085064 | validation: 0.8833729676198294]
	TIME [epoch: 0.703 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0796926284413746		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.0796926284413746 | validation: 1.0566394646268467]
	TIME [epoch: 0.701 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0658861356179516		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.0658861356179516 | validation: 0.8896821160203365]
	TIME [epoch: 0.696 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1085361587180076		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.1085361587180076 | validation: 1.1963595984403264]
	TIME [epoch: 0.693 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.21189115762664		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.21189115762664 | validation: 0.9095420110804355]
	TIME [epoch: 0.694 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1615305965105593		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.1615305965105593 | validation: 1.0351783262211076]
	TIME [epoch: 0.693 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0552377741567913		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.0552377741567913 | validation: 0.9130459985818492]
	TIME [epoch: 0.693 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0635112295557227		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.0635112295557227 | validation: 1.096711466610186]
	TIME [epoch: 0.701 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0930657580168666		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.0930657580168666 | validation: 0.9173707776588804]
	TIME [epoch: 0.692 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0818965182546612		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.0818965182546612 | validation: 1.172810870083803]
	TIME [epoch: 0.692 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1160654233234095		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.1160654233234095 | validation: 0.8517749483399085]
	TIME [epoch: 0.693 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1372751059987605		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.1372751059987605 | validation: 1.1052530593786247]
	TIME [epoch: 0.696 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0908077015524655		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.0908077015524655 | validation: 0.8699507307555354]
	TIME [epoch: 0.694 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0873526913801304		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.0873526913801304 | validation: 1.0554901639863548]
	TIME [epoch: 0.692 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0880330856275813		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.0880330856275813 | validation: 0.8898531450284874]
	TIME [epoch: 0.693 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0990553944058477		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.0990553944058477 | validation: 1.0650852860520423]
	TIME [epoch: 0.693 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0896490014351232		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.0896490014351232 | validation: 0.8594755325955048]
	TIME [epoch: 0.694 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1115448974854867		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.1115448974854867 | validation: 1.1041621066788718]
	TIME [epoch: 0.692 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.097228487871504		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.097228487871504 | validation: 0.858774690257407]
	TIME [epoch: 0.693 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0967721403087427		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.0967721403087427 | validation: 1.1141778752588882]
	TIME [epoch: 0.693 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0850098896855878		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.0850098896855878 | validation: 0.8674181366801588]
	TIME [epoch: 0.694 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0815617919434855		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.0815617919434855 | validation: 1.1125920712924735]
	TIME [epoch: 0.692 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0996126139967037		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.0996126139967037 | validation: 0.883893723488505]
	TIME [epoch: 0.694 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0851651586858577		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.0851651586858577 | validation: 1.1384136908531264]
	TIME [epoch: 0.693 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.112266654600837		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.112266654600837 | validation: 0.8988867927373699]
	TIME [epoch: 0.692 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0914590528009451		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.0914590528009451 | validation: 1.0920275728079936]
	TIME [epoch: 0.692 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0788183549463919		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.0788183549463919 | validation: 0.8669173529532938]
	TIME [epoch: 0.694 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0823090868921303		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.0823090868921303 | validation: 1.1073808739152706]
	TIME [epoch: 0.694 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.089487859321687		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.089487859321687 | validation: 0.8408711094405951]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.088548703073196		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.088548703073196 | validation: 1.1221073458197575]
	TIME [epoch: 0.693 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1056075798032574		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.1056075798032574 | validation: 0.9083582295342612]
	TIME [epoch: 0.692 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1446184721088892		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.1446184721088892 | validation: 1.0606770077004983]
	TIME [epoch: 0.69 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0667935915515958		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.0667935915515958 | validation: 0.8751532365132406]
	TIME [epoch: 0.69 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0589799573634195		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.0589799573634195 | validation: 1.0559947000025605]
	TIME [epoch: 0.691 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.059758743684589		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.059758743684589 | validation: 0.8547746126314504]
	TIME [epoch: 0.69 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0824931690962654		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.0824931690962654 | validation: 1.101377494715103]
	TIME [epoch: 0.691 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.091669155074629		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.091669155074629 | validation: 0.8634421946239931]
	TIME [epoch: 0.69 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0788597173940797		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.0788597173940797 | validation: 1.0685175487223961]
	TIME [epoch: 0.694 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0709079318807713		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.0709079318807713 | validation: 0.8649863440664174]
	TIME [epoch: 0.692 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0703152896335622		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.0703152896335622 | validation: 1.1259762623128744]
	TIME [epoch: 0.692 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1003194479096765		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.1003194479096765 | validation: 0.9165094027818683]
	TIME [epoch: 0.692 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0995111560963589		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.0995111560963589 | validation: 1.1001984982807587]
	TIME [epoch: 179 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0854514971622196		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.0854514971622196 | validation: 0.8963190703237225]
	TIME [epoch: 1.38 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.050565994643235		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.050565994643235 | validation: 0.992152278022455]
	TIME [epoch: 1.36 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.037726358995462		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.037726358995462 | validation: 0.8820995242075717]
	TIME [epoch: 1.36 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0452048022556115		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.0452048022556115 | validation: 1.0997626409023995]
	TIME [epoch: 1.36 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0953862264128982		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.0953862264128982 | validation: 0.8735417593185751]
	TIME [epoch: 1.36 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1922175069925154		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.1922175069925154 | validation: 1.0540840678894061]
	TIME [epoch: 1.36 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0879243891133126		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.0879243891133126 | validation: 0.8699599286150751]
	TIME [epoch: 1.36 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0556278775620245		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.0556278775620245 | validation: 0.981926527212462]
	TIME [epoch: 1.36 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.049807108680715		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.049807108680715 | validation: 0.8855378950010468]
	TIME [epoch: 1.36 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0471668414483901		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.0471668414483901 | validation: 1.0508756381384894]
	TIME [epoch: 1.36 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0730746240001718		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.0730746240001718 | validation: 0.8395916073503694]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1008839285825633		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 1.1008839285825633 | validation: 1.1100529139795874]
	TIME [epoch: 1.36 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0964439397010741		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.0964439397010741 | validation: 0.8607603889484422]
	TIME [epoch: 1.36 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0800451027788278		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 1.0800451027788278 | validation: 1.0540514526575715]
	TIME [epoch: 1.36 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0553011269926211		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.0553011269926211 | validation: 0.8597000498732709]
	TIME [epoch: 1.36 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0597149060145064		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.0597149060145064 | validation: 1.0181300449918718]
	TIME [epoch: 1.36 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0515615408527916		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.0515615408527916 | validation: 0.8601540407254821]
	TIME [epoch: 1.36 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.070431521509646		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.070431521509646 | validation: 1.0872193633585354]
	TIME [epoch: 1.36 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0791739301738357		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.0791739301738357 | validation: 0.8654342653843771]
	TIME [epoch: 1.36 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1039880570197043		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 1.1039880570197043 | validation: 1.0669223348147086]
	TIME [epoch: 1.36 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.112162289978002		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.112162289978002 | validation: 0.9275125385647475]
	TIME [epoch: 1.36 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.068447521013936		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 1.068447521013936 | validation: 0.9655684567979173]
	TIME [epoch: 1.36 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0318190257442714		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.0318190257442714 | validation: 0.9018411604649569]
	TIME [epoch: 1.35 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0338884261763996		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 1.0338884261763996 | validation: 1.0050936850801073]
	TIME [epoch: 1.35 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0487591938339582		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 1.0487591938339582 | validation: 0.8705671644384154]
	TIME [epoch: 1.36 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1003618597889486		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.1003618597889486 | validation: 1.3048267360597592]
	TIME [epoch: 1.36 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.177670013125779		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.177670013125779 | validation: 0.8523367332488955]
	TIME [epoch: 1.36 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0618254383927832		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 1.0618254383927832 | validation: 0.9840682994810964]
	TIME [epoch: 1.35 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0338201452884022		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.0338201452884022 | validation: 0.8887908559147593]
	TIME [epoch: 1.36 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0303686414852447		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 1.0303686414852447 | validation: 0.9585627628137825]
	TIME [epoch: 1.37 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0334131786660232		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.0334131786660232 | validation: 0.867859662549811]
	TIME [epoch: 1.36 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0650182904107792		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 1.0650182904107792 | validation: 1.1194900412533766]
	TIME [epoch: 1.36 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1199657697661685		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.1199657697661685 | validation: 0.8552855884297963]
	TIME [epoch: 1.36 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1336433099826897		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 1.1336433099826897 | validation: 1.0312338957390517]
	TIME [epoch: 1.36 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0402855468797938		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.0402855468797938 | validation: 0.8861362261813863]
	TIME [epoch: 1.36 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0377912715610784		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 1.0377912715610784 | validation: 0.9760535591421129]
	TIME [epoch: 1.36 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0396169893709246		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 1.0396169893709246 | validation: 0.9192274427602188]
	TIME [epoch: 1.37 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.055624535070819		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 1.055624535070819 | validation: 1.0172391683322928]
	TIME [epoch: 1.36 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0621105400386093		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 1.0621105400386093 | validation: 0.8907402436694444]
	TIME [epoch: 1.36 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0640945738297163		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 1.0640945738297163 | validation: 1.151939731733172]
	TIME [epoch: 1.36 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0939805471598119		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 1.0939805471598119 | validation: 0.86418889464816]
	TIME [epoch: 1.36 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0974389355902028		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 1.0974389355902028 | validation: 1.0286377217298388]
	TIME [epoch: 1.36 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0625131890560433		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 1.0625131890560433 | validation: 0.8619035267523926]
	TIME [epoch: 1.36 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0502089202081815		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 1.0502089202081815 | validation: 0.999222535959444]
	TIME [epoch: 1.36 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0470028646934304		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.0470028646934304 | validation: 0.8595042718374372]
	TIME [epoch: 1.36 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0677717109858402		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 1.0677717109858402 | validation: 1.0198718588953073]
	TIME [epoch: 1.36 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0646270897889443		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 1.0646270897889443 | validation: 0.8675954752449446]
	TIME [epoch: 1.36 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0645281777885596		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 1.0645281777885596 | validation: 1.0723299263264874]
	TIME [epoch: 1.36 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.055960592945522		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 1.055960592945522 | validation: 0.8574459976874613]
	TIME [epoch: 1.36 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0581058285064393		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 1.0581058285064393 | validation: 1.0035379487237583]
	TIME [epoch: 1.36 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0536040055966296		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 1.0536040055966296 | validation: 0.8814869834986887]
	TIME [epoch: 1.36 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0420721717980497		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 1.0420721717980497 | validation: 1.0271925730077462]
	TIME [epoch: 1.36 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0448829365704626		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 1.0448829365704626 | validation: 0.8725520687861352]
	TIME [epoch: 1.36 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0530899391667417		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 1.0530899391667417 | validation: 1.0697937823278434]
	TIME [epoch: 1.36 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0724829023844202		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 1.0724829023844202 | validation: 0.8993977260651169]
	TIME [epoch: 1.36 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0784965476991406		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 1.0784965476991406 | validation: 1.0594794963730545]
	TIME [epoch: 1.36 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.067261416329039		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 1.067261416329039 | validation: 0.8713398052641012]
	TIME [epoch: 1.36 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0407468789148364		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 1.0407468789148364 | validation: 1.0144275251549242]
	TIME [epoch: 1.36 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0371653486323222		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 1.0371653486323222 | validation: 0.8612211369247913]
	TIME [epoch: 1.36 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0598819151600274		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 1.0598819151600274 | validation: 1.0922643134908563]
	TIME [epoch: 1.36 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.093258494074099		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 1.093258494074099 | validation: 0.8576898315437962]
	TIME [epoch: 1.36 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1095878230486858		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 1.1095878230486858 | validation: 0.9917942407973714]
	TIME [epoch: 1.36 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.038986914892022		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 1.038986914892022 | validation: 0.8835950214440613]
	TIME [epoch: 1.36 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0339789323928035		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 1.0339789323928035 | validation: 0.9608745402913468]
	TIME [epoch: 1.36 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.018190147161793		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 1.018190147161793 | validation: 0.8594769113014366]
	TIME [epoch: 1.36 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0247336428709148		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 1.0247336428709148 | validation: 0.9661757054643563]
	TIME [epoch: 1.36 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0274715445556846		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 1.0274715445556846 | validation: 0.8450420144489463]
	TIME [epoch: 1.36 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0355857301111417		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 1.0355857301111417 | validation: 1.0809337688592702]
	TIME [epoch: 1.36 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0705535864226574		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 1.0705535864226574 | validation: 0.8331426851610195]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1045031931703444		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 1.1045031931703444 | validation: 1.045868741715478]
	TIME [epoch: 1.36 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0583668489107327		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 1.0583668489107327 | validation: 0.8684180123252908]
	TIME [epoch: 1.36 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.036020799194719		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 1.036020799194719 | validation: 0.9693600811257874]
	TIME [epoch: 1.36 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0443523304994822		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 1.0443523304994822 | validation: 0.9423777265802497]
	TIME [epoch: 1.36 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0671803701168476		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 1.0671803701168476 | validation: 0.9351443138917825]
	TIME [epoch: 1.36 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0524296558149155		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 1.0524296558149155 | validation: 0.9202689518718954]
	TIME [epoch: 1.36 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0217775789314		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 1.0217775789314 | validation: 0.9197885631142184]
	TIME [epoch: 1.36 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.016139519970312		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 1.016139519970312 | validation: 0.9061112900738583]
	TIME [epoch: 1.36 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.017524669108404		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 1.017524669108404 | validation: 0.9406695147705719]
	TIME [epoch: 1.36 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0197429838329133		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 1.0197429838329133 | validation: 0.8701533018557476]
	TIME [epoch: 1.36 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0439991103298092		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 1.0439991103298092 | validation: 1.1727979329230453]
	TIME [epoch: 1.36 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1216000260985837		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 1.1216000260985837 | validation: 0.8596214523383474]
	TIME [epoch: 1.36 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1499605347425752		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 1.1499605347425752 | validation: 1.0265968923691613]
	TIME [epoch: 1.36 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.044487332472343		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 1.044487332472343 | validation: 0.8939935088880784]
	TIME [epoch: 1.36 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0292041767883309		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 1.0292041767883309 | validation: 0.9182446973065009]
	TIME [epoch: 1.41 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0182956847620903		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 1.0182956847620903 | validation: 0.9071029378506884]
	TIME [epoch: 1.36 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0206637313797622		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 1.0206637313797622 | validation: 0.9330990987622949]
	TIME [epoch: 1.36 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0247267804433822		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 1.0247267804433822 | validation: 0.8998095838032892]
	TIME [epoch: 1.36 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0506174455116573		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 1.0506174455116573 | validation: 1.1286460273840369]
	TIME [epoch: 1.36 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1032300993370427		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 1.1032300993370427 | validation: 0.8613735540846683]
	TIME [epoch: 1.36 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0718924468329183		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 1.0718924468329183 | validation: 1.0279073041698819]
	TIME [epoch: 1.36 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0582623150507018		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 1.0582623150507018 | validation: 0.8583336601227354]
	TIME [epoch: 1.36 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0684028546057531		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 1.0684028546057531 | validation: 1.045866523847953]
	TIME [epoch: 1.36 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0695444274049788		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 1.0695444274049788 | validation: 0.8745617340062554]
	TIME [epoch: 1.36 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0540029793950791		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 1.0540029793950791 | validation: 0.9573311269429177]
	TIME [epoch: 1.36 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0331220519184237		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 1.0331220519184237 | validation: 0.8777991596928537]
	TIME [epoch: 1.36 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.027168605202771		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 1.027168605202771 | validation: 0.9892191131890944]
	TIME [epoch: 1.36 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0241416741092264		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 1.0241416741092264 | validation: 0.8791720968942894]
	TIME [epoch: 1.36 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0222917146195685		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 1.0222917146195685 | validation: 1.0129854568144516]
	TIME [epoch: 1.45 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0505960974206696		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 1.0505960974206696 | validation: 0.8583428875126746]
	TIME [epoch: 1.36 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0732759317221736		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 1.0732759317221736 | validation: 1.0665952686988447]
	TIME [epoch: 1.36 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0698283265506447		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 1.0698283265506447 | validation: 0.8709023530127897]
	TIME [epoch: 1.36 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0396556143690825		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 1.0396556143690825 | validation: 0.9611496801566111]
	TIME [epoch: 1.35 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0304097440439681		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 1.0304097440439681 | validation: 0.8782560131641045]
	TIME [epoch: 1.35 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0393488656783112		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 1.0393488656783112 | validation: 1.0048698245039924]
	TIME [epoch: 1.36 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.049582616917651		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 1.049582616917651 | validation: 0.8911830726399664]
	TIME [epoch: 1.36 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0759867482340648		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 1.0759867482340648 | validation: 1.031084303586675]
	TIME [epoch: 1.35 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0487347527516275		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 1.0487347527516275 | validation: 0.8499752939557506]
	TIME [epoch: 1.36 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0540905825999625		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 1.0540905825999625 | validation: 1.0496752625660724]
	TIME [epoch: 1.36 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0465497415849665		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 1.0465497415849665 | validation: 0.8846555772203661]
	TIME [epoch: 1.36 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0375327755209147		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 1.0375327755209147 | validation: 0.9712772324301935]
	TIME [epoch: 1.36 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0355500591331435		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 1.0355500591331435 | validation: 0.871459247322719]
	TIME [epoch: 1.36 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.030678020461375		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 1.030678020461375 | validation: 1.0288334046950922]
	TIME [epoch: 1.36 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0450272838464312		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 1.0450272838464312 | validation: 0.844419920245676]
	TIME [epoch: 1.36 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.051711843183908		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 1.051711843183908 | validation: 1.0236088111114938]
	TIME [epoch: 1.36 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0472030181704743		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 1.0472030181704743 | validation: 0.8599556943624868]
	TIME [epoch: 1.37 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0332309400419748		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 1.0332309400419748 | validation: 1.0055499385900541]
	TIME [epoch: 1.36 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0320004613518554		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 1.0320004613518554 | validation: 0.871455415416717]
	TIME [epoch: 1.36 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0586009963134566		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 1.0586009963134566 | validation: 1.0073664795116095]
	TIME [epoch: 1.36 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0586018704202163		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 1.0586018704202163 | validation: 0.8804533531639231]
	TIME [epoch: 1.36 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0445404521561623		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 1.0445404521561623 | validation: 0.98404371995357]
	TIME [epoch: 1.36 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0214367735212242		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 1.0214367735212242 | validation: 0.8645213212362635]
	TIME [epoch: 1.36 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0230828072944431		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 1.0230828072944431 | validation: 0.9869830009158749]
	TIME [epoch: 1.36 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0474135459192122		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 1.0474135459192122 | validation: 0.8332285456950448]
	TIME [epoch: 1.36 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0388113580695104		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 1.0388113580695104 | validation: 1.0466964777891012]
	TIME [epoch: 1.36 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0522735001731292		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 1.0522735001731292 | validation: 0.8648300628083846]
	TIME [epoch: 1.36 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.034458371085655		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 1.034458371085655 | validation: 1.010370746637775]
	TIME [epoch: 1.36 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0306410283960787		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 1.0306410283960787 | validation: 0.8668136212974306]
	TIME [epoch: 1.36 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0251283191525684		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 1.0251283191525684 | validation: 0.9637391280496413]
	TIME [epoch: 1.36 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.024588427375839		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 1.024588427375839 | validation: 0.8620563783483682]
	TIME [epoch: 1.36 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0262075856027761		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 1.0262075856027761 | validation: 0.9663304896896702]
	TIME [epoch: 1.36 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.01663826272947		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 1.01663826272947 | validation: 0.8741093900658155]
	TIME [epoch: 1.36 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0186177454324516		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 1.0186177454324516 | validation: 0.9902513370865912]
	TIME [epoch: 1.36 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0258750736126898		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 1.0258750736126898 | validation: 0.8654861433274594]
	TIME [epoch: 1.36 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0519083870536912		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 1.0519083870536912 | validation: 1.1052473645416026]
	TIME [epoch: 1.36 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0881171572948207		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 1.0881171572948207 | validation: 0.8776553529316047]
	TIME [epoch: 1.36 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0475834965446564		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 1.0475834965446564 | validation: 0.9885840791097963]
	TIME [epoch: 1.36 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0200119342921106		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 1.0200119342921106 | validation: 0.8879957301153856]
	TIME [epoch: 1.36 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0329721974162969		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 1.0329721974162969 | validation: 0.9998916449214992]
	TIME [epoch: 1.36 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.060567651386827		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 1.060567651386827 | validation: 0.8551967148929873]
	TIME [epoch: 1.35 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.048754998047908		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 1.048754998047908 | validation: 1.0069108076700524]
	TIME [epoch: 1.36 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0376799371178629		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 1.0376799371178629 | validation: 0.8732522931018141]
	TIME [epoch: 1.36 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0263571563581464		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 1.0263571563581464 | validation: 0.9541916121483688]
	TIME [epoch: 1.36 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0261372289668895		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 1.0261372289668895 | validation: 0.8824371900304353]
	TIME [epoch: 1.35 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0162059171173388		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 1.0162059171173388 | validation: 0.9460849524768055]
	TIME [epoch: 1.36 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0106513872580944		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 1.0106513872580944 | validation: 0.8706997520723875]
	TIME [epoch: 1.37 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0285995060861357		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 1.0285995060861357 | validation: 0.9713175884401578]
	TIME [epoch: 1.36 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0237749874909066		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 1.0237749874909066 | validation: 0.8470114418733967]
	TIME [epoch: 1.36 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0471630315063274		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 1.0471630315063274 | validation: 1.059517258335777]
	TIME [epoch: 1.36 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0707015877436414		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 1.0707015877436414 | validation: 0.8649747235129454]
	TIME [epoch: 1.35 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0622653609065982		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 1.0622653609065982 | validation: 0.9459261310216684]
	TIME [epoch: 1.51 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0351245539627585		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 1.0351245539627585 | validation: 0.8957781669434581]
	TIME [epoch: 1.36 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0238001233510543		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 1.0238001233510543 | validation: 0.9024897113086445]
	TIME [epoch: 1.37 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0096772319047518		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 1.0096772319047518 | validation: 0.936033640428591]
	TIME [epoch: 1.36 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.01820031918182		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 1.01820031918182 | validation: 0.9077079485597328]
	TIME [epoch: 1.35 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.010482621470054		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 1.010482621470054 | validation: 0.9387649337852665]
	TIME [epoch: 1.36 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0189395056004642		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 1.0189395056004642 | validation: 0.9113709800631746]
	TIME [epoch: 1.36 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0170862648748726		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 1.0170862648748726 | validation: 0.9265947854867287]
	TIME [epoch: 1.36 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0215740509142401		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 1.0215740509142401 | validation: 0.8711094136417487]
	TIME [epoch: 1.36 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0291458522447596		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 1.0291458522447596 | validation: 1.0409692738066239]
	TIME [epoch: 1.36 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.050198659444951		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 1.050198659444951 | validation: 0.8301463853139666]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_361.pth
	Model improved!!!
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1106525249455457		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 1.1106525249455457 | validation: 1.054538668594121]
	TIME [epoch: 1.35 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0527835347499919		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 1.0527835347499919 | validation: 0.914264427224839]
	TIME [epoch: 1.4 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0243833735224632		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 1.0243833735224632 | validation: 0.8734399656799927]
	TIME [epoch: 1.36 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.019622248802603		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 1.019622248802603 | validation: 0.9744348721152076]
	TIME [epoch: 1.35 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0230449765022902		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 1.0230449765022902 | validation: 0.8851616205629018]
	TIME [epoch: 1.35 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0295880696119832		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 1.0295880696119832 | validation: 0.9703068805439972]
	TIME [epoch: 1.36 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.016777412383533		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 1.016777412383533 | validation: 0.8715198788352652]
	TIME [epoch: 1.36 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0257163905412316		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 1.0257163905412316 | validation: 0.978887780278126]
	TIME [epoch: 1.36 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0303214335799313		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 1.0303214335799313 | validation: 0.8482469079918744]
	TIME [epoch: 1.36 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0352788608131664		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 1.0352788608131664 | validation: 1.0034161457278012]
	TIME [epoch: 1.36 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.043001417457289		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 1.043001417457289 | validation: 0.8629802155087657]
	TIME [epoch: 1.36 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0358082587856816		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 1.0358082587856816 | validation: 0.956047360426259]
	TIME [epoch: 1.36 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.022882974000071		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 1.022882974000071 | validation: 0.9280140066861616]
	TIME [epoch: 1.36 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.026592811601364		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 1.026592811601364 | validation: 0.916465029128652]
	TIME [epoch: 1.35 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0213695289877955		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 1.0213695289877955 | validation: 0.9150620074863959]
	TIME [epoch: 1.35 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0232765780021233		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 1.0232765780021233 | validation: 0.8714031272089499]
	TIME [epoch: 1.35 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0186700835369202		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 1.0186700835369202 | validation: 1.014500157338468]
	TIME [epoch: 1.35 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0362420319157617		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 1.0362420319157617 | validation: 0.8458482782540622]
	TIME [epoch: 1.35 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0609385282088164		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 1.0609385282088164 | validation: 1.0465141672882232]
	TIME [epoch: 1.35 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0488912499387444		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 1.0488912499387444 | validation: 0.8781596435946389]
	TIME [epoch: 1.35 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0228580028609633		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 1.0228580028609633 | validation: 0.932591184886663]
	TIME [epoch: 1.35 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0153719829244559		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 1.0153719829244559 | validation: 0.9087787063255086]
	TIME [epoch: 1.35 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0178947628387287		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 1.0178947628387287 | validation: 0.9233476087268784]
	TIME [epoch: 1.35 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0237939889355594		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 1.0237939889355594 | validation: 0.9094567028184161]
	TIME [epoch: 1.35 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0251277349931462		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 1.0251277349931462 | validation: 0.895725407992956]
	TIME [epoch: 1.35 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.016078585065253		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 1.016078585065253 | validation: 0.9196444125117837]
	TIME [epoch: 1.35 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0155047352067774		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 1.0155047352067774 | validation: 0.8876875593155877]
	TIME [epoch: 1.35 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0124775556217547		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 1.0124775556217547 | validation: 0.9282374886372191]
	TIME [epoch: 1.35 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0127070678147256		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 1.0127070678147256 | validation: 0.8791493738172638]
	TIME [epoch: 1.36 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0158521723737706		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 1.0158521723737706 | validation: 1.0346469801779237]
	TIME [epoch: 1.35 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0436379750944231		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 1.0436379750944231 | validation: 0.8274792958047293]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_392.pth
	Model improved!!!
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.086153280155106		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 1.086153280155106 | validation: 1.0344075463872504]
	TIME [epoch: 1.36 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.040880774534724		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 1.040880774534724 | validation: 0.893540630518152]
	TIME [epoch: 1.36 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0148906257773946		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 1.0148906257773946 | validation: 0.9198045959787375]
	TIME [epoch: 1.37 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.011890976387833		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 1.011890976387833 | validation: 0.9345018253906044]
	TIME [epoch: 1.36 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0272321021219306		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 1.0272321021219306 | validation: 0.9142350855948185]
	TIME [epoch: 1.36 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0235229740388614		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 1.0235229740388614 | validation: 0.9476401635016851]
	TIME [epoch: 1.36 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.024504314009332		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 1.024504314009332 | validation: 0.9054228155127046]
	TIME [epoch: 1.36 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0136351432571507		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 1.0136351432571507 | validation: 0.9171130952495267]
	TIME [epoch: 1.35 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0119011695921665		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 1.0119011695921665 | validation: 0.8822080758620013]
	TIME [epoch: 1.36 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0181459353392333		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 1.0181459353392333 | validation: 0.9611038709181532]
	TIME [epoch: 1.36 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.020460958119614		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 1.020460958119614 | validation: 0.8420417330763798]
	TIME [epoch: 1.37 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0509799734611955		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 1.0509799734611955 | validation: 1.0547727787784784]
	TIME [epoch: 1.36 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0708147782365782		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 1.0708147782365782 | validation: 0.8724347660045144]
	TIME [epoch: 1.36 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0263239750131898		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 1.0263239750131898 | validation: 0.9256036433714616]
	TIME [epoch: 1.36 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0157617027962542		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 1.0157617027962542 | validation: 0.9468550046820113]
	TIME [epoch: 1.36 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0119353191645948		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 1.0119353191645948 | validation: 0.8658457167742329]
	TIME [epoch: 1.36 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0132559844907827		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 1.0132559844907827 | validation: 0.9586197136613382]
	TIME [epoch: 1.36 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.014016279800405		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 1.014016279800405 | validation: 0.8756889441006787]
	TIME [epoch: 1.36 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0242317130165821		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 1.0242317130165821 | validation: 0.9717712107803657]
	TIME [epoch: 1.36 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0285393915119783		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 1.0285393915119783 | validation: 0.8684415208200523]
	TIME [epoch: 1.36 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0224708893682835		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 1.0224708893682835 | validation: 0.9679614841572285]
	TIME [epoch: 1.36 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0214919475150859		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 1.0214919475150859 | validation: 0.8745546144734555]
	TIME [epoch: 1.36 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.018527914851515		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 1.018527914851515 | validation: 0.9714300498898981]
	TIME [epoch: 1.36 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0249594869556418		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 1.0249594869556418 | validation: 0.855716243843631]
	TIME [epoch: 1.36 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0208904625899329		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 1.0208904625899329 | validation: 0.978040545101293]
	TIME [epoch: 1.36 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0243721217727317		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 1.0243721217727317 | validation: 0.8652600237643839]
	TIME [epoch: 1.36 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.040562918177433		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 1.040562918177433 | validation: 0.9755619097187259]
	TIME [epoch: 1.36 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0437233220033315		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 1.0437233220033315 | validation: 0.8956063847727457]
	TIME [epoch: 1.36 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0229197939597248		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 1.0229197939597248 | validation: 0.9482486441243057]
	TIME [epoch: 1.36 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0270669258057623		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 1.0270669258057623 | validation: 0.891825816412175]
	TIME [epoch: 1.39 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.020312854258081		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 1.020312854258081 | validation: 0.956618953452828]
	TIME [epoch: 1.36 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0172225739957281		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 1.0172225739957281 | validation: 0.870484764997931]
	TIME [epoch: 1.36 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0140782818234184		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 1.0140782818234184 | validation: 0.9516358907806595]
	TIME [epoch: 1.36 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.011676700911647		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 1.011676700911647 | validation: 0.8704666133592216]
	TIME [epoch: 1.35 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0234039164476698		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 1.0234039164476698 | validation: 0.9899871455371859]
	TIME [epoch: 1.36 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0178382575916214		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 1.0178382575916214 | validation: 0.8675325720329128]
	TIME [epoch: 1.37 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0221582673791982		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 1.0221582673791982 | validation: 0.9708011233275994]
	TIME [epoch: 1.36 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0249620857713833		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 1.0249620857713833 | validation: 0.876933162186737]
	TIME [epoch: 1.36 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0174402183579203		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 1.0174402183579203 | validation: 0.9640231919291242]
	TIME [epoch: 1.36 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.008649640798731		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 1.008649640798731 | validation: 0.8696633848643798]
	TIME [epoch: 1.36 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0051830559786632		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 1.0051830559786632 | validation: 0.9292121912908936]
	TIME [epoch: 1.36 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0113159561816603		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 1.0113159561816603 | validation: 0.8814456736695171]
	TIME [epoch: 1.36 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.033023066542892		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 1.033023066542892 | validation: 0.9869062448978103]
	TIME [epoch: 1.35 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0271046095585288		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 1.0271046095585288 | validation: 0.8438585169349211]
	TIME [epoch: 1.36 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0269924912517439		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 1.0269924912517439 | validation: 0.9586944539696071]
	TIME [epoch: 1.35 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0173671281410361		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 1.0173671281410361 | validation: 0.8858687385165169]
	TIME [epoch: 1.35 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0095159416227306		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 1.0095159416227306 | validation: 0.9189339092337283]
	TIME [epoch: 1.35 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0139292155213009		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 1.0139292155213009 | validation: 0.8966596169264556]
	TIME [epoch: 1.36 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0014745345355431		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 1.0014745345355431 | validation: 0.9008507418998118]
	TIME [epoch: 1.35 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0117164662541536		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 1.0117164662541536 | validation: 0.9539829543654247]
	TIME [epoch: 1.35 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0104687066534557		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 1.0104687066534557 | validation: 0.8551650939168495]
	TIME [epoch: 1.35 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0233353621897643		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 1.0233353621897643 | validation: 1.0167884457409313]
	TIME [epoch: 1.35 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0551840907223387		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 1.0551840907223387 | validation: 0.8284704035583648]
	TIME [epoch: 1.35 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0403441476459818		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 1.0403441476459818 | validation: 0.947162378766776]
	TIME [epoch: 1.35 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0147923603106839		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 1.0147923603106839 | validation: 0.9005577197231578]
	TIME [epoch: 1.36 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0047535485122854		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 1.0047535485122854 | validation: 0.9116055021145375]
	TIME [epoch: 1.35 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.003566238820324		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 1.003566238820324 | validation: 0.9289859427093594]
	TIME [epoch: 1.36 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0104971590635592		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 1.0104971590635592 | validation: 0.8918361149676779]
	TIME [epoch: 1.36 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0221742582557194		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 1.0221742582557194 | validation: 0.9428097968852969]
	TIME [epoch: 1.35 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0257694207139916		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 1.0257694207139916 | validation: 0.875764999552798]
	TIME [epoch: 1.35 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.018407552838545		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 1.018407552838545 | validation: 0.9353084229564037]
	TIME [epoch: 1.36 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0169522613877529		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 1.0169522613877529 | validation: 0.8758875396339401]
	TIME [epoch: 1.36 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0133936393932703		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 1.0133936393932703 | validation: 0.9795049810824367]
	TIME [epoch: 1.36 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.010113111520341		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 1.010113111520341 | validation: 0.8808760851155043]
	TIME [epoch: 1.35 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0176479448767908		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 1.0176479448767908 | validation: 0.9359353414820643]
	TIME [epoch: 1.36 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0179169941427497		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 1.0179169941427497 | validation: 0.8777270788703891]
	TIME [epoch: 1.36 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0186194742831536		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 1.0186194742831536 | validation: 0.9602616257852641]
	TIME [epoch: 1.36 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0154631386769546		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 1.0154631386769546 | validation: 0.8654747584176443]
	TIME [epoch: 1.36 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.017636246939386		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 1.017636246939386 | validation: 0.9554059978372694]
	TIME [epoch: 1.36 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0115891362392442		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 1.0115891362392442 | validation: 0.8514386280481013]
	TIME [epoch: 1.37 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0238816393330918		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 1.0238816393330918 | validation: 0.942293840421057]
	TIME [epoch: 1.36 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0205368369584962		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 1.0205368369584962 | validation: 0.8833504327753041]
	TIME [epoch: 1.36 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0122658953970973		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 1.0122658953970973 | validation: 0.9413113026241366]
	TIME [epoch: 1.36 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0102551195272818		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 1.0102551195272818 | validation: 0.8641279657413835]
	TIME [epoch: 1.36 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0183960435576314		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 1.0183960435576314 | validation: 0.9662369413312858]
	TIME [epoch: 1.36 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0202938202668372		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 1.0202938202668372 | validation: 0.8636369002001347]
	TIME [epoch: 1.36 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0232813233723672		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 1.0232813233723672 | validation: 0.9655078328450156]
	TIME [epoch: 1.36 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.012997594251863		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 1.012997594251863 | validation: 0.8779994596586018]
	TIME [epoch: 1.36 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.01486842932315		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 1.01486842932315 | validation: 0.9623443011062754]
	TIME [epoch: 1.36 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0173489384268586		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 1.0173489384268586 | validation: 0.8530354407936248]
	TIME [epoch: 1.35 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0060685695285465		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 1.0060685695285465 | validation: 0.9460092615357066]
	TIME [epoch: 1.36 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.006770129771304		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 1.006770129771304 | validation: 0.8765556754622662]
	TIME [epoch: 1.35 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0135233312393073		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 1.0135233312393073 | validation: 0.9715022257039838]
	TIME [epoch: 1.36 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0224329566171446		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 1.0224329566171446 | validation: 0.8729242639067313]
	TIME [epoch: 1.36 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0192001748870214		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 1.0192001748870214 | validation: 0.9335241315339172]
	TIME [epoch: 1.36 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0104374786191312		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 1.0104374786191312 | validation: 0.8989084356849766]
	TIME [epoch: 1.36 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0101592959612227		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 1.0101592959612227 | validation: 0.9174809706457213]
	TIME [epoch: 1.36 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0025319264406318		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 1.0025319264406318 | validation: 0.8888792101178503]
	TIME [epoch: 1.37 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0136478184985833		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 1.0136478184985833 | validation: 0.9132171128427244]
	TIME [epoch: 1.36 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0070237575041927		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 1.0070237575041927 | validation: 0.8800387252304838]
	TIME [epoch: 1.36 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.009353212318692		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 1.009353212318692 | validation: 0.9964679437723301]
	TIME [epoch: 1.37 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0338928832669225		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 1.0338928832669225 | validation: 0.8318092037612126]
	TIME [epoch: 1.36 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0398016191251926		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 1.0398016191251926 | validation: 0.9517137534017488]
	TIME [epoch: 1.36 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.014172989908394		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 1.014172989908394 | validation: 0.8901341068257074]
	TIME [epoch: 1.36 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0057028957813732		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 1.0057028957813732 | validation: 0.9102278578421683]
	TIME [epoch: 1.36 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0086570629834284		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 1.0086570629834284 | validation: 0.9391611362889407]
	TIME [epoch: 1.36 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0072480597807438		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 1.0072480597807438 | validation: 0.8965446750071916]
	TIME [epoch: 1.36 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0003372002095714		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 1.0003372002095714 | validation: 0.898358913277866]
	TIME [epoch: 1.36 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.002338599746751		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 1.002338599746751 | validation: 0.9210227922414008]
	TIME [epoch: 1.36 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0056712398108085		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 1.0056712398108085 | validation: 0.8904494537851053]
	TIME [epoch: 1.36 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0048291032135674		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 1.0048291032135674 | validation: 0.9097174774473538]
	TIME [epoch: 1.36 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0108309849643382		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 1.0108309849643382 | validation: 0.8756684321087088]
	TIME [epoch: 1.36 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0191609271954363		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 1.0191609271954363 | validation: 0.9822613027328537]
	TIME [epoch: 1.36 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0190342778516979		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 1.0190342778516979 | validation: 0.841830102677055]
	TIME [epoch: 1.36 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0321589371122735		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 1.0321589371122735 | validation: 0.9868548417645728]
	TIME [epoch: 1.36 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0227528914204511		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 1.0227528914204511 | validation: 0.8914089236619449]
	TIME [epoch: 1.36 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.003349508609954		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 1.003349508609954 | validation: 0.9121276952586905]
	TIME [epoch: 1.36 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0095179595399022		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 1.0095179595399022 | validation: 0.9361335436648535]
	TIME [epoch: 1.36 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0080043403963856		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 1.0080043403963856 | validation: 0.892796566020842]
	TIME [epoch: 183 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0114624676778174		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 1.0114624676778174 | validation: 0.9567096483150911]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2r_1_v_mmd4_20250601_154516/states/model_phi1_4a_distortion_v2r_1_v_mmd4_502.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 1135.783 seconds.
