Args:
Namespace(name='model_phi1_4a_distortion2_v_mmd1', outdir='out/model_training/model_phi1_4a_distortion2_v_mmd1', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion2/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion2/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1374341522

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.7753768226096245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.7753768226096245 | validation: 5.390978562676058]
	TIME [epoch: 162 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.4674296401794455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.4674296401794455 | validation: 3.8526609738350945]
	TIME [epoch: 1.07 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.154382242368217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.154382242368217 | validation: 5.326879387088917]
	TIME [epoch: 0.693 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.542435364608121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.542435364608121 | validation: 4.965042464459913]
	TIME [epoch: 0.692 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.217644090196426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.217644090196426 | validation: 3.4777135020400958]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.731354943931299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.731354943931299 | validation: 3.891671998675061]
	TIME [epoch: 0.693 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.6722386756656435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6722386756656435 | validation: 3.522788353544648]
	TIME [epoch: 0.69 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.737354679840682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.737354679840682 | validation: 3.7558496490663575]
	TIME [epoch: 0.691 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.407656920007979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.407656920007979 | validation: 2.8216093712123014]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.241830827010731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.241830827010731 | validation: 3.6791071343782864]
	TIME [epoch: 0.691 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.149278504016924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.149278504016924 | validation: 2.0740045416131774]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6908087097486817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6908087097486817 | validation: 2.5069483838823263]
	TIME [epoch: 0.696 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5420881529986836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5420881529986836 | validation: 3.0870146830306386]
	TIME [epoch: 0.694 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.077649357494896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.077649357494896 | validation: 2.82873217069622]
	TIME [epoch: 0.693 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.677031545971686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.677031545971686 | validation: 1.9695350490028223]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2235934603946705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2235934603946705 | validation: 2.125785995110917]
	TIME [epoch: 0.69 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4729353408661066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4729353408661066 | validation: 2.4531398270356717]
	TIME [epoch: 0.688 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4437006916497452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4437006916497452 | validation: 1.6122052981270922]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0542175427613105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0542175427613105 | validation: 1.9601026107284463]
	TIME [epoch: 0.694 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2949846710768225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2949846710768225 | validation: 2.323111938410157]
	TIME [epoch: 0.692 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.365643759760956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.365643759760956 | validation: 1.6191992453878605]
	TIME [epoch: 0.692 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.040700005603528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.040700005603528 | validation: 1.9818418299829759]
	TIME [epoch: 0.691 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2753078929525765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2753078929525765 | validation: 2.0549902119706718]
	TIME [epoch: 0.692 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.223398022655315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.223398022655315 | validation: 1.536704219575011]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9729984559945093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9729984559945093 | validation: 1.729262434473985]
	TIME [epoch: 0.692 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.106610310232172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.106610310232172 | validation: 1.9190515735957363]
	TIME [epoch: 0.692 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1339358448938537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1339358448938537 | validation: 1.478976528171852]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.935836720109983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.935836720109983 | validation: 1.5076289814817079]
	TIME [epoch: 0.695 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9652535355956116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9652535355956116 | validation: 1.7140619073708518]
	TIME [epoch: 0.693 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0208859236285113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0208859236285113 | validation: 1.5221761419765871]
	TIME [epoch: 0.692 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9582105371374787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9582105371374787 | validation: 1.5784708595970143]
	TIME [epoch: 0.691 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9403175799247663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9403175799247663 | validation: 1.4733009114844797]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.917684599394273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.917684599394273 | validation: 1.5556711903473759]
	TIME [epoch: 0.696 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9138638167088433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9138638167088433 | validation: 1.514123706156144]
	TIME [epoch: 0.69 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.918560773208759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.918560773208759 | validation: 1.6917547935032442]
	TIME [epoch: 0.69 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9632122344368526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9632122344368526 | validation: 1.5271555593752564]
	TIME [epoch: 0.691 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.909944997906743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.909944997906743 | validation: 1.6274221655786274]
	TIME [epoch: 0.691 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9157984807798085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9157984807798085 | validation: 1.4842910843912411]
	TIME [epoch: 0.691 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8675521879217287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8675521879217287 | validation: 1.6043281394785334]
	TIME [epoch: 0.692 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8769215784402036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8769215784402036 | validation: 1.469537285083283]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8433719321189197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8433719321189197 | validation: 1.6112157037710249]
	TIME [epoch: 0.693 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.859349055923427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.859349055923427 | validation: 1.4442179207294084]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.813216571602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.813216571602 | validation: 1.5680237053572927]
	TIME [epoch: 0.691 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8161543078814817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8161543078814817 | validation: 1.4246040710006618]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7634891357679954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7634891357679954 | validation: 1.524022124066355]
	TIME [epoch: 0.692 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7633066690657957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7633066690657957 | validation: 1.43081868606345]
	TIME [epoch: 0.692 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.732577008724155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.732577008724155 | validation: 1.5688900072708514]
	TIME [epoch: 0.697 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.765217972339882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.765217972339882 | validation: 1.4663716792445973]
	TIME [epoch: 0.69 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7158701098276743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7158701098276743 | validation: 1.5386349569272377]
	TIME [epoch: 0.692 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.713915162095908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.713915162095908 | validation: 1.3918684326233723]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6277612615092223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6277612615092223 | validation: 1.4105370461145896]
	TIME [epoch: 0.691 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5914438190343345		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 2.5914438190343345 | validation: 1.359832862505766]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.566189181543382		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 2.566189181543382 | validation: 1.5410816015568827]
	TIME [epoch: 0.692 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.63407858651569		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 2.63407858651569 | validation: 2.3838220566170256]
	TIME [epoch: 0.695 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.014503403015242		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 3.014503403015242 | validation: 1.3195264801203974]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.431109019437883		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 2.431109019437883 | validation: 1.4222899310830521]
	TIME [epoch: 0.691 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4974899824945855		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 2.4974899824945855 | validation: 1.3693099370465305]
	TIME [epoch: 0.689 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3932125102008115		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 2.3932125102008115 | validation: 1.3390795993971907]
	TIME [epoch: 0.692 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2553535965163833		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 2.2553535965163833 | validation: 1.2674904328622747]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9071188877969922		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.9071188877969922 | validation: 1.1827225757574862]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8638510642065547		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.8638510642065547 | validation: 2.7084774361105555]
	TIME [epoch: 0.69 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0444267430431546		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 3.0444267430431546 | validation: 1.0375758886943982]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0024883898374934		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 2.0024883898374934 | validation: 0.7091450804234527]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.249717073805174		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.249717073805174 | validation: 1.9618471520372276]
	TIME [epoch: 0.691 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0166193549518825		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 2.0166193549518825 | validation: 0.7046536307619589]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0717370618322055		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.0717370618322055 | validation: 0.6946109727832254]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0089660523623434		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.0089660523623434 | validation: 0.7348621830669235]
	TIME [epoch: 0.691 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8086073611956296		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 0.8086073611956296 | validation: 0.6690279872708781]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7720385989529834		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 0.7720385989529834 | validation: 0.5934921227071955]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7547726868075282		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 0.7547726868075282 | validation: 0.546349471604992]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.734077181980976		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 0.734077181980976 | validation: 0.8121800933085064]
	TIME [epoch: 0.693 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8439573806579508		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 0.8439573806579508 | validation: 0.7263157864456158]
	TIME [epoch: 0.691 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.116284562227205		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.116284562227205 | validation: 0.8116564575898275]
	TIME [epoch: 0.69 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9036921094553171		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 0.9036921094553171 | validation: 0.6164565722799145]
	TIME [epoch: 0.689 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8138362627524715		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 0.8138362627524715 | validation: 0.7607958930912566]
	TIME [epoch: 0.69 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8199197374175538		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 0.8199197374175538 | validation: 0.5884992123924263]
	TIME [epoch: 0.689 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8786829751030476		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 0.8786829751030476 | validation: 0.7558971340940825]
	TIME [epoch: 0.689 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7947105864171746		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 0.7947105864171746 | validation: 0.5800136770214894]
	TIME [epoch: 0.689 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7880551641238932		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 0.7880551641238932 | validation: 0.7034989684862478]
	TIME [epoch: 0.69 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7676861742672678		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 0.7676861742672678 | validation: 0.5569916805366489]
	TIME [epoch: 0.697 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7921249251345799		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 0.7921249251345799 | validation: 0.7357830131701202]
	TIME [epoch: 0.689 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7490276156424313		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 0.7490276156424313 | validation: 0.5453971995892707]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7347774720692817		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 0.7347774720692817 | validation: 0.6855787426743719]
	TIME [epoch: 0.691 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6970255282657329		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 0.6970255282657329 | validation: 0.5284816883013917]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7056764273339112		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 0.7056764273339112 | validation: 0.7146670172451111]
	TIME [epoch: 0.692 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6827681097159833		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 0.6827681097159833 | validation: 0.46249993567102793]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.68547652997083		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 0.68547652997083 | validation: 0.6370178267705582]
	TIME [epoch: 0.69 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6331181157034855		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 0.6331181157034855 | validation: 0.4191087271863409]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5830576837851154		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 0.5830576837851154 | validation: 0.5736572292423713]
	TIME [epoch: 0.69 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6094585471906994		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.6094585471906994 | validation: 0.5729821627981856]
	TIME [epoch: 0.69 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7593862602603094		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 0.7593862602603094 | validation: 0.7421968289251746]
	TIME [epoch: 0.689 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.853384405869196		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.853384405869196 | validation: 0.5156044944930297]
	TIME [epoch: 0.692 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5879452615794365		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.5879452615794365 | validation: 0.479351996366685]
	TIME [epoch: 0.689 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5482877443993901		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 0.5482877443993901 | validation: 0.5121208155947723]
	TIME [epoch: 0.689 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.54216466221053		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 0.54216466221053 | validation: 0.44858425470753416]
	TIME [epoch: 0.689 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5673458640730513		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 0.5673458640730513 | validation: 0.6011543892356201]
	TIME [epoch: 0.69 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.594062378782848		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.594062378782848 | validation: 0.5301850764640605]
	TIME [epoch: 0.689 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6181198609296893		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.6181198609296893 | validation: 0.5269718482658728]
	TIME [epoch: 0.689 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6588104235492668		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.6588104235492668 | validation: 0.6793452853659926]
	TIME [epoch: 0.69 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.756537523496908		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.756537523496908 | validation: 0.5052268692910726]
	TIME [epoch: 0.69 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6993322554016421		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.6993322554016421 | validation: 0.7153980592711209]
	TIME [epoch: 0.69 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6662348774339907		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.6662348774339907 | validation: 0.4593222862169606]
	TIME [epoch: 0.69 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5780793011500721		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.5780793011500721 | validation: 0.5051279646916363]
	TIME [epoch: 0.691 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5402094379691513		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.5402094379691513 | validation: 0.44658475886006177]
	TIME [epoch: 0.691 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5561745423262446		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.5561745423262446 | validation: 0.5102898418581495]
	TIME [epoch: 0.688 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5767083396554166		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 0.5767083396554166 | validation: 0.4853104232830703]
	TIME [epoch: 0.69 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.589451792821508		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.589451792821508 | validation: 0.4666505908019638]
	TIME [epoch: 0.691 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.612499275508201		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.612499275508201 | validation: 0.5925372398285637]
	TIME [epoch: 0.688 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5960681634618895		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.5960681634618895 | validation: 0.4013225236966353]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5195752894020892		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 0.5195752894020892 | validation: 0.41576118492489905]
	TIME [epoch: 0.689 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48922557235264463		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.48922557235264463 | validation: 0.5167067575281735]
	TIME [epoch: 0.687 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5338398921239279		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.5338398921239279 | validation: 0.42083815017558035]
	TIME [epoch: 0.687 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6414684522680064		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.6414684522680064 | validation: 0.5828299237858133]
	TIME [epoch: 0.685 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6051617874733667		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.6051617874733667 | validation: 0.45073954200242944]
	TIME [epoch: 0.687 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5424032925465686		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.5424032925465686 | validation: 0.466119648106604]
	TIME [epoch: 0.687 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48808400299101		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.48808400299101 | validation: 0.4103293061899737]
	TIME [epoch: 0.687 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4677912497807279		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.4677912497807279 | validation: 0.4149304777518139]
	TIME [epoch: 0.688 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4825175495855375		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.4825175495855375 | validation: 0.47780498901380625]
	TIME [epoch: 0.69 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5484357857975827		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.5484357857975827 | validation: 0.4992104162339879]
	TIME [epoch: 0.697 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6584917056537484		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.6584917056537484 | validation: 0.48745251257385985]
	TIME [epoch: 0.69 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.537487435165104		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.537487435165104 | validation: 0.4553389150678513]
	TIME [epoch: 0.688 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4768404361512644		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.4768404361512644 | validation: 0.39428054129147716]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45332073625990743		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.45332073625990743 | validation: 0.44050962430173435]
	TIME [epoch: 0.69 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44617451342448566		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.44617451342448566 | validation: 0.3257071059359826]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45903785805682334		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.45903785805682334 | validation: 0.5020739215135157]
	TIME [epoch: 0.69 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5280064902580781		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.5280064902580781 | validation: 0.463894985454787]
	TIME [epoch: 0.689 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6337096418280529		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.6337096418280529 | validation: 0.5410122168650849]
	TIME [epoch: 0.69 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5643187279755604		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.5643187279755604 | validation: 0.41102596617495557]
	TIME [epoch: 0.691 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46508179184179155		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.46508179184179155 | validation: 0.38155931485271977]
	TIME [epoch: 0.689 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4338414736034906		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.4338414736034906 | validation: 0.5015144223421285]
	TIME [epoch: 0.692 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5207261415004028		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.5207261415004028 | validation: 0.3416438209621169]
	TIME [epoch: 0.693 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4923562947784437		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.4923562947784437 | validation: 0.4339113866108109]
	TIME [epoch: 0.69 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44949820634075127		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.44949820634075127 | validation: 0.39624582081498194]
	TIME [epoch: 0.689 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44337287986409807		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.44337287986409807 | validation: 0.3568796137527168]
	TIME [epoch: 0.69 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4564750769799626		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.4564750769799626 | validation: 0.4448415438653668]
	TIME [epoch: 0.691 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4608485477284239		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.4608485477284239 | validation: 0.374438914092215]
	TIME [epoch: 0.693 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4636031088931709		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.4636031088931709 | validation: 0.4086339804468197]
	TIME [epoch: 0.69 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46725773143279603		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.46725773143279603 | validation: 0.4880773065489412]
	TIME [epoch: 0.69 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48775481842415885		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.48775481842415885 | validation: 0.48491215308239366]
	TIME [epoch: 0.69 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5095790795256234		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.5095790795256234 | validation: 0.40601399412679867]
	TIME [epoch: 0.69 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44176502248749333		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.44176502248749333 | validation: 0.3685468384649544]
	TIME [epoch: 0.69 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40676442411693203		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.40676442411693203 | validation: 0.36104885894487937]
	TIME [epoch: 0.69 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40794885581524654		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.40794885581524654 | validation: 0.38844753310908886]
	TIME [epoch: 0.691 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4329859456929642		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.4329859456929642 | validation: 0.4123306530987291]
	TIME [epoch: 0.691 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4343547977107185		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.4343547977107185 | validation: 0.3437992484273269]
	TIME [epoch: 0.691 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45645739452420486		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.45645739452420486 | validation: 0.433735314908678]
	TIME [epoch: 0.69 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4332012686432901		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.4332012686432901 | validation: 0.30889336823413027]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39057304172594753		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.39057304172594753 | validation: 0.33771393556814994]
	TIME [epoch: 0.692 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36925030245428253		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.36925030245428253 | validation: 0.3487517566485099]
	TIME [epoch: 0.69 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36543357018552136		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.36543357018552136 | validation: 0.3234431562140283]
	TIME [epoch: 0.69 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39769296880454447		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.39769296880454447 | validation: 0.4835567086743002]
	TIME [epoch: 0.69 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47562709580870455		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.47562709580870455 | validation: 0.335858639837754]
	TIME [epoch: 0.691 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48592307012201313		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.48592307012201313 | validation: 0.4143080215434742]
	TIME [epoch: 0.692 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43488493929427097		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.43488493929427097 | validation: 0.5082887818487737]
	TIME [epoch: 0.691 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4620238387358228		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.4620238387358228 | validation: 0.423946047269176]
	TIME [epoch: 0.69 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4197149998236901		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.4197149998236901 | validation: 0.33943984873527233]
	TIME [epoch: 0.693 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36942538812427445		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.36942538812427445 | validation: 0.34997893528932555]
	TIME [epoch: 0.693 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3518954688238151		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.3518954688238151 | validation: 0.33177586533834136]
	TIME [epoch: 0.69 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3449483298184805		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.3449483298184805 | validation: 0.33116379791874784]
	TIME [epoch: 0.698 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34769236644504703		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.34769236644504703 | validation: 0.3857939902480995]
	TIME [epoch: 0.69 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39307187755513734		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.39307187755513734 | validation: 0.3467765577378688]
	TIME [epoch: 0.69 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5153617228524798		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.5153617228524798 | validation: 0.46163802541261206]
	TIME [epoch: 0.69 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4308091259395432		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.4308091259395432 | validation: 0.3486338371126488]
	TIME [epoch: 0.69 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37893652881520273		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.37893652881520273 | validation: 0.2875354648922902]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42978539281777034		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.42978539281777034 | validation: 0.38782011908116076]
	TIME [epoch: 0.692 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40387605910750063		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.40387605910750063 | validation: 0.4030900018131238]
	TIME [epoch: 0.692 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39882891995464564		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.39882891995464564 | validation: 0.33208333541667096]
	TIME [epoch: 0.694 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38159789801666705		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.38159789801666705 | validation: 0.3638239471499196]
	TIME [epoch: 0.689 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3576498126207985		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.3576498126207985 | validation: 0.3418468162793957]
	TIME [epoch: 0.695 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34418792301958434		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.34418792301958434 | validation: 0.3377612048631201]
	TIME [epoch: 0.691 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33928572836237564		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.33928572836237564 | validation: 0.3364028487475717]
	TIME [epoch: 0.69 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35339945897744235		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.35339945897744235 | validation: 0.36281421321148716]
	TIME [epoch: 0.689 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35359695859423995		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.35359695859423995 | validation: 0.32246249706980656]
	TIME [epoch: 0.693 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3605167728576966		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.3605167728576966 | validation: 0.3489293204872155]
	TIME [epoch: 0.69 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35740155265824963		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.35740155265824963 | validation: 0.34054074112236393]
	TIME [epoch: 0.689 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3669466392065678		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.3669466392065678 | validation: 0.3771629283575291]
	TIME [epoch: 0.692 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38301773262243854		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.38301773262243854 | validation: 0.41019430376873744]
	TIME [epoch: 0.69 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.411420133603917		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.411420133603917 | validation: 0.3632367942155381]
	TIME [epoch: 0.689 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3737208193086818		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.3737208193086818 | validation: 0.3309787993995794]
	TIME [epoch: 0.689 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3233741233262426		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.3233741233262426 | validation: 0.29663831718256556]
	TIME [epoch: 0.689 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3015109912402389		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.3015109912402389 | validation: 0.32700673845491524]
	TIME [epoch: 0.691 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2985652256857597		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.2985652256857597 | validation: 0.2756296583162941]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30481258370832615		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.30481258370832615 | validation: 0.36038476011280673]
	TIME [epoch: 0.687 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31230085612861286		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.31230085612861286 | validation: 0.26497881871331613]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_184.pth
	Model improved!!!
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3220899732224757		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.3220899732224757 | validation: 0.35521495439151135]
	TIME [epoch: 0.69 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3308097805961702		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.3308097805961702 | validation: 0.3603452363383852]
	TIME [epoch: 0.69 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35467957139923423		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.35467957139923423 | validation: 0.340005819272149]
	TIME [epoch: 0.691 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3834125977508585		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.3834125977508585 | validation: 0.3777892685260422]
	TIME [epoch: 0.691 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.408986898700109		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.408986898700109 | validation: 0.3875171509533651]
	TIME [epoch: 0.69 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39558364177921596		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.39558364177921596 | validation: 0.4017443823763879]
	TIME [epoch: 0.69 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3919515436680107		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.3919515436680107 | validation: 0.34567726202227256]
	TIME [epoch: 0.69 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32006695621222947		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.32006695621222947 | validation: 0.31242228855691145]
	TIME [epoch: 0.69 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3002105287407693		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.3002105287407693 | validation: 0.3173746028758131]
	TIME [epoch: 0.693 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2989130268657251		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.2989130268657251 | validation: 0.3062846206115953]
	TIME [epoch: 0.693 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30231855704083127		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.30231855704083127 | validation: 0.3337645079059212]
	TIME [epoch: 0.691 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3023694716303259		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.3023694716303259 | validation: 0.2789272213427105]
	TIME [epoch: 0.691 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3139433170235679		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.3139433170235679 | validation: 0.3391020838169756]
	TIME [epoch: 0.691 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3420445156854901		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.3420445156854901 | validation: 0.37060890458740847]
	TIME [epoch: 0.689 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3864341680532036		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.3864341680532036 | validation: 0.34545584658471]
	TIME [epoch: 0.7 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36236298896583063		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.36236298896583063 | validation: 0.3282305041765335]
	TIME [epoch: 0.69 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3665326408845226		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.3665326408845226 | validation: 0.3804739019269812]
	TIME [epoch: 174 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34202556959073876		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.34202556959073876 | validation: 0.2798665526183068]
	TIME [epoch: 1.38 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3975623278416653		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.3975623278416653 | validation: 0.32022608773393624]
	TIME [epoch: 1.35 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3364940250079316		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.3364940250079316 | validation: 0.3344742017885935]
	TIME [epoch: 1.35 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3246402995681436		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.3246402995681436 | validation: 0.28884353202628377]
	TIME [epoch: 1.36 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32993781208210055		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.32993781208210055 | validation: 0.3006050960519087]
	TIME [epoch: 1.35 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3245098892070534		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.3245098892070534 | validation: 0.27047513738106704]
	TIME [epoch: 1.35 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31642627310320287		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.31642627310320287 | validation: 0.2725843825967343]
	TIME [epoch: 1.35 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3255330975259284		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.3255330975259284 | validation: 0.3248616349620566]
	TIME [epoch: 1.35 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3359160834285983		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.3359160834285983 | validation: 0.27741564633546406]
	TIME [epoch: 1.35 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33525365287738135		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.33525365287738135 | validation: 0.28187181495494285]
	TIME [epoch: 1.35 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3042660657265283		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.3042660657265283 | validation: 0.28221255430378384]
	TIME [epoch: 1.35 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29891216019821987		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.29891216019821987 | validation: 0.2638828168388961]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2936799310986886		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.2936799310986886 | validation: 0.3046801121746238]
	TIME [epoch: 1.35 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30086366997233943		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.30086366997233943 | validation: 0.26215028968336335]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_215.pth
	Model improved!!!
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.299407705313883		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.299407705313883 | validation: 0.2812663794703619]
	TIME [epoch: 1.35 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30587196938096717		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.30587196938096717 | validation: 0.30187077094519]
	TIME [epoch: 1.35 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32041547553078287		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.32041547553078287 | validation: 0.28579066147289517]
	TIME [epoch: 1.35 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34230532423155474		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.34230532423155474 | validation: 0.30972316215144674]
	TIME [epoch: 1.35 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3117770410367746		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.3117770410367746 | validation: 0.2775608191316108]
	TIME [epoch: 1.35 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28884608146709007		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.28884608146709007 | validation: 0.2690948027627757]
	TIME [epoch: 1.35 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2762563508408758		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.2762563508408758 | validation: 0.2770149690811916]
	TIME [epoch: 1.35 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2766517815629381		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.2766517815629381 | validation: 0.24105920621305443]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28624536665571454		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.28624536665571454 | validation: 0.28961608704186464]
	TIME [epoch: 1.35 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30185569277573415		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.30185569277573415 | validation: 0.2517147828752491]
	TIME [epoch: 1.36 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28953338954176777		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.28953338954176777 | validation: 0.2686806208181568]
	TIME [epoch: 1.35 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30668457343971023		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.30668457343971023 | validation: 0.3221371391147132]
	TIME [epoch: 1.35 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3332649450149397		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.3332649450149397 | validation: 0.28381996281764227]
	TIME [epoch: 1.35 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35277778041150015		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.35277778041150015 | validation: 0.2961769646722394]
	TIME [epoch: 1.35 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28936950338416695		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.28936950338416695 | validation: 0.24945934437733258]
	TIME [epoch: 1.35 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2565783421009311		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.2565783421009311 | validation: 0.2396948097468764]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25341220051033697		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.25341220051033697 | validation: 0.2514668162048246]
	TIME [epoch: 1.35 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25190237874708343		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.25190237874708343 | validation: 0.22480254757483814]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_233.pth
	Model improved!!!
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2560083433947694		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.2560083433947694 | validation: 0.2579793243317843]
	TIME [epoch: 1.35 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2696494452906076		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.2696494452906076 | validation: 0.23392613423028694]
	TIME [epoch: 1.35 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27998520860442316		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.27998520860442316 | validation: 0.28540790790373477]
	TIME [epoch: 1.35 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3033057444493216		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.3033057444493216 | validation: 0.3358656226895589]
	TIME [epoch: 1.35 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34822391261666286		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.34822391261666286 | validation: 0.31130749848786476]
	TIME [epoch: 1.35 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37721027903197785		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.37721027903197785 | validation: 0.30000968961275165]
	TIME [epoch: 1.35 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28825164791756597		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.28825164791756597 | validation: 0.24287461117645115]
	TIME [epoch: 1.35 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24552743123655812		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.24552743123655812 | validation: 0.21957803613543625]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_241.pth
	Model improved!!!
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24169159955643316		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.24169159955643316 | validation: 0.2467234755467866]
	TIME [epoch: 1.35 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24715465055516106		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.24715465055516106 | validation: 0.22262657902967545]
	TIME [epoch: 1.35 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24331417865919566		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.24331417865919566 | validation: 0.2522043756252081]
	TIME [epoch: 1.35 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25150899134418053		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.25150899134418053 | validation: 0.23053004910833358]
	TIME [epoch: 1.35 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2505790455883193		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.2505790455883193 | validation: 0.2760519273541991]
	TIME [epoch: 1.36 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26691074689723826		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.26691074689723826 | validation: 0.26424271168315894]
	TIME [epoch: 1.35 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29213807437862954		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.29213807437862954 | validation: 0.32149582196289767]
	TIME [epoch: 1.35 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3179323986579105		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.3179323986579105 | validation: 0.3059655741500521]
	TIME [epoch: 1.35 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3297371535541221		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.3297371535541221 | validation: 0.25658401939269526]
	TIME [epoch: 1.35 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30424074578770355		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.30424074578770355 | validation: 0.28382379728364143]
	TIME [epoch: 1.35 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2737719744720287		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.2737719744720287 | validation: 0.21818804090752886]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_252.pth
	Model improved!!!
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2367946971919889		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.2367946971919889 | validation: 0.23209498189929817]
	TIME [epoch: 1.35 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23683635001320796		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.23683635001320796 | validation: 0.23813558679707741]
	TIME [epoch: 1.35 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23924165635526376		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.23924165635526376 | validation: 0.23915998563065705]
	TIME [epoch: 1.35 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23893266887049935		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.23893266887049935 | validation: 0.23273963658548064]
	TIME [epoch: 1.35 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24685372292926025		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.24685372292926025 | validation: 0.2746218866284542]
	TIME [epoch: 1.35 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26631963961080507		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.26631963961080507 | validation: 0.2819847740828429]
	TIME [epoch: 1.35 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.294301110830403		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.294301110830403 | validation: 0.31359562466765567]
	TIME [epoch: 1.35 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29830654480879787		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.29830654480879787 | validation: 0.2478570049467066]
	TIME [epoch: 1.35 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2735450041461599		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.2735450041461599 | validation: 0.24111300640319136]
	TIME [epoch: 1.35 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24553567168579457		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.24553567168579457 | validation: 0.2440675448888884]
	TIME [epoch: 1.35 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23935517199590908		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.23935517199590908 | validation: 0.221893010689302]
	TIME [epoch: 1.35 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23381540411625157		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.23381540411625157 | validation: 0.2626691436452559]
	TIME [epoch: 1.35 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24327438081744643		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.24327438081744643 | validation: 0.21050736576989634]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_265.pth
	Model improved!!!
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2420500202642079		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.2420500202642079 | validation: 0.27532393190612076]
	TIME [epoch: 1.35 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25125540766555043		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.25125540766555043 | validation: 0.20778586873661933]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2506780118223482		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.2506780118223482 | validation: 0.2505862005324748]
	TIME [epoch: 1.34 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24405796887096187		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.24405796887096187 | validation: 0.26808752247672746]
	TIME [epoch: 1.35 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2671005498761313		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.2671005498761313 | validation: 0.30935630200885583]
	TIME [epoch: 1.35 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33664401941433525		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.33664401941433525 | validation: 0.33987625802259397]
	TIME [epoch: 1.35 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32261267112617786		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.32261267112617786 | validation: 0.2424637944467493]
	TIME [epoch: 1.35 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2514741741172011		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.2514741741172011 | validation: 0.24076034089800116]
	TIME [epoch: 1.35 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2237551698582687		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.2237551698582687 | validation: 0.2402721900757607]
	TIME [epoch: 1.35 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22600750108027015		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.22600750108027015 | validation: 0.23276249455991538]
	TIME [epoch: 1.35 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2286647663158304		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.2286647663158304 | validation: 0.2565402679170499]
	TIME [epoch: 1.35 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23330888995841356		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.23330888995841356 | validation: 0.21790056053781878]
	TIME [epoch: 1.35 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23856250412973914		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.23856250412973914 | validation: 0.2643289231107126]
	TIME [epoch: 1.35 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24176329778460023		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.24176329778460023 | validation: 0.22751226926504675]
	TIME [epoch: 1.35 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2460897382239969		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.2460897382239969 | validation: 0.28267398621926865]
	TIME [epoch: 1.35 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2431497531786843		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.2431497531786843 | validation: 0.22857351763534173]
	TIME [epoch: 1.35 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2596577005988589		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.2596577005988589 | validation: 0.29450748298460927]
	TIME [epoch: 1.35 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27583491856716064		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.27583491856716064 | validation: 0.30712895544421753]
	TIME [epoch: 1.35 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3007624723369967		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.3007624723369967 | validation: 0.25891257524934697]
	TIME [epoch: 1.35 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2855837153886167		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.2855837153886167 | validation: 0.26110365704011357]
	TIME [epoch: 1.35 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.241657782984775		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.241657782984775 | validation: 0.22483995473116908]
	TIME [epoch: 1.35 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2222391510779379		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.2222391510779379 | validation: 0.23063641425574782]
	TIME [epoch: 1.35 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22357057113070652		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.22357057113070652 | validation: 0.23724604299583785]
	TIME [epoch: 1.35 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2262876777070836		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.2262876777070836 | validation: 0.24182590033442125]
	TIME [epoch: 1.36 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22953677755461685		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.22953677755461685 | validation: 0.23203283790971663]
	TIME [epoch: 1.35 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2347571272306643		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.2347571272306643 | validation: 0.2600653215989227]
	TIME [epoch: 1.35 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25003462440911794		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.25003462440911794 | validation: 0.26233565033123835]
	TIME [epoch: 1.35 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2752455298792696		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.2752455298792696 | validation: 0.28370488988324105]
	TIME [epoch: 1.35 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2787220271246789		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.2787220271246789 | validation: 0.26113056543489965]
	TIME [epoch: 1.35 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2590717105680354		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.2590717105680354 | validation: 0.2173399048939042]
	TIME [epoch: 1.35 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24602038374407373		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.24602038374407373 | validation: 0.2597986311946109]
	TIME [epoch: 1.35 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2412703484112321		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.2412703484112321 | validation: 0.20707683444699276]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23096109205998322		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.23096109205998322 | validation: 0.23812622040774675]
	TIME [epoch: 1.34 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2254914683937635		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.2254914683937635 | validation: 0.23062192923363375]
	TIME [epoch: 1.34 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22329967082725632		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.22329967082725632 | validation: 0.23733725875345418]
	TIME [epoch: 1.34 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22854189143504042		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.22854189143504042 | validation: 0.23596462701331694]
	TIME [epoch: 1.34 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24180409423376376		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.24180409423376376 | validation: 0.27602176157536057]
	TIME [epoch: 1.34 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2695742110948308		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.2695742110948308 | validation: 0.2792183114312858]
	TIME [epoch: 1.34 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2791410723850705		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.2791410723850705 | validation: 0.2537278713966788]
	TIME [epoch: 1.34 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2533010314701614		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.2533010314701614 | validation: 0.24826288222323864]
	TIME [epoch: 1.34 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22800200818586702		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.22800200818586702 | validation: 0.2268402029505018]
	TIME [epoch: 1.34 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22153675676764856		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.22153675676764856 | validation: 0.24424747048347373]
	TIME [epoch: 1.34 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2260788018009996		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.2260788018009996 | validation: 0.19885368817415006]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23225629794545607		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.23225629794545607 | validation: 0.26836849067993945]
	TIME [epoch: 1.34 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2320673407484793		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.2320673407484793 | validation: 0.20303895377181222]
	TIME [epoch: 1.35 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23193337410835801		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.23193337410835801 | validation: 0.24421851778668593]
	TIME [epoch: 1.34 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22695357853081574		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.22695357853081574 | validation: 0.22129685698027074]
	TIME [epoch: 1.35 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23348183916595092		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.23348183916595092 | validation: 0.24622110781463152]
	TIME [epoch: 1.34 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24785005066136226		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.24785005066136226 | validation: 0.2656091199207499]
	TIME [epoch: 1.34 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2840047462285932		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.2840047462285932 | validation: 0.28356043190384866]
	TIME [epoch: 1.34 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2842879468864071		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.2842879468864071 | validation: 0.26153059008375173]
	TIME [epoch: 1.34 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2449050190843076		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.2449050190843076 | validation: 0.22531221382110714]
	TIME [epoch: 1.34 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2257840157850663		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.2257840157850663 | validation: 0.22880375695324506]
	TIME [epoch: 1.34 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21694278191971758		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.21694278191971758 | validation: 0.24269687374066662]
	TIME [epoch: 1.34 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21664055209411828		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.21664055209411828 | validation: 0.21369220652554688]
	TIME [epoch: 1.34 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22129340426800326		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.22129340426800326 | validation: 0.23563936504932648]
	TIME [epoch: 1.34 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22484461717321072		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.22484461717321072 | validation: 0.228387501780827]
	TIME [epoch: 1.34 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23094675002522952		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.23094675002522952 | validation: 0.24939712162435992]
	TIME [epoch: 1.34 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2416078314148715		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.2416078314148715 | validation: 0.24455177971684386]
	TIME [epoch: 1.34 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2554769761447822		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.2554769761447822 | validation: 0.25515567211747453]
	TIME [epoch: 1.34 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25941655085229903		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.25941655085229903 | validation: 0.25499604854688457]
	TIME [epoch: 1.34 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2448992348099536		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.2448992348099536 | validation: 0.20965166304339988]
	TIME [epoch: 1.34 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23186699569451033		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.23186699569451033 | validation: 0.2544192751652916]
	TIME [epoch: 1.34 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2256844963217664		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.2256844963217664 | validation: 0.20635726999163323]
	TIME [epoch: 1.34 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22703614043868292		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.22703614043868292 | validation: 0.2556643890090909]
	TIME [epoch: 1.34 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22567947933895155		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.22567947933895155 | validation: 0.2121435607780291]
	TIME [epoch: 1.34 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22602314575128327		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.22602314575128327 | validation: 0.23680443534036347]
	TIME [epoch: 1.35 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22443177165593267		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.22443177165593267 | validation: 0.21723942562908763]
	TIME [epoch: 1.34 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22065454766187442		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.22065454766187442 | validation: 0.23119738455621253]
	TIME [epoch: 1.34 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2190394474354325		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.2190394474354325 | validation: 0.23637031720825968]
	TIME [epoch: 1.34 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22895030404161168		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.22895030404161168 | validation: 0.24488754097632534]
	TIME [epoch: 1.34 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24836355379619193		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.24836355379619193 | validation: 0.28843266190560607]
	TIME [epoch: 1.34 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2858975555293864		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.2858975555293864 | validation: 0.2685593315065568]
	TIME [epoch: 1.34 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26369198050780646		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.26369198050780646 | validation: 0.23375073123855827]
	TIME [epoch: 1.34 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22737507352599423		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.22737507352599423 | validation: 0.20560865310361362]
	TIME [epoch: 1.34 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21885778465012848		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.21885778465012848 | validation: 0.21064103156096273]
	TIME [epoch: 1.34 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21706774811852636		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.21706774811852636 | validation: 0.21407608437001616]
	TIME [epoch: 1.34 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22122778863485032		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.22122778863485032 | validation: 0.24015327563185213]
	TIME [epoch: 1.34 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22473872199910866		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.22473872199910866 | validation: 0.22829346936710798]
	TIME [epoch: 1.34 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22862798042816143		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.22862798042816143 | validation: 0.25166121622828846]
	TIME [epoch: 1.34 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23419533080379584		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.23419533080379584 | validation: 0.2114215033338915]
	TIME [epoch: 1.34 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2355518502959396		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.2355518502959396 | validation: 0.2331294351739362]
	TIME [epoch: 1.34 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22359865957108915		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.22359865957108915 | validation: 0.22971684433598202]
	TIME [epoch: 1.34 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22107355375570428		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.22107355375570428 | validation: 0.19893473450035631]
	TIME [epoch: 1.34 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2212079680211155		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.2212079680211155 | validation: 0.2598633530844288]
	TIME [epoch: 1.34 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2400522757083914		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.2400522757083914 | validation: 0.20463509698793772]
	TIME [epoch: 1.34 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24708482095573395		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.24708482095573395 | validation: 0.23223403418268382]
	TIME [epoch: 1.34 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23818136395290235		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.23818136395290235 | validation: 0.23921443607286205]
	TIME [epoch: 1.34 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2231038822123007		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.2231038822123007 | validation: 0.20434106301812627]
	TIME [epoch: 1.35 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23135632830270375		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.23135632830270375 | validation: 0.24930700498458275]
	TIME [epoch: 1.34 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23320203377928103		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.23320203377928103 | validation: 0.21709317157328478]
	TIME [epoch: 1.34 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22712515882457424		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.22712515882457424 | validation: 0.23694060981847428]
	TIME [epoch: 1.34 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22070482227020258		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.22070482227020258 | validation: 0.2420934931503015]
	TIME [epoch: 1.34 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.215623415805363		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.215623415805363 | validation: 0.21031142895378993]
	TIME [epoch: 1.34 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22100695056610542		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.22100695056610542 | validation: 0.2334230606909735]
	TIME [epoch: 1.34 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22464397614085957		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.22464397614085957 | validation: 0.22352951336710902]
	TIME [epoch: 1.34 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22617549740683285		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.22617549740683285 | validation: 0.23538691836952413]
	TIME [epoch: 1.34 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22756781969912537		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.22756781969912537 | validation: 0.23034442852531956]
	TIME [epoch: 1.34 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22933148140175152		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.22933148140175152 | validation: 0.24067886187910195]
	TIME [epoch: 1.34 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23170636918103327		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.23170636918103327 | validation: 0.2166567322970205]
	TIME [epoch: 1.34 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22492332740066207		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.22492332740066207 | validation: 0.2349178404056037]
	TIME [epoch: 1.34 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2211636159720178		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.2211636159720178 | validation: 0.2104136044347023]
	TIME [epoch: 1.34 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2178295209159466		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.2178295209159466 | validation: 0.22217296502852546]
	TIME [epoch: 1.34 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21300335297046236		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.21300335297046236 | validation: 0.21575124379643085]
	TIME [epoch: 1.34 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21490570880480347		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.21490570880480347 | validation: 0.22471868845442897]
	TIME [epoch: 1.34 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21595987238117573		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.21595987238117573 | validation: 0.2107884467386728]
	TIME [epoch: 1.34 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21315303228776686		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.21315303228776686 | validation: 0.2513626234084562]
	TIME [epoch: 1.34 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22481666582630233		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.22481666582630233 | validation: 0.22196611908060776]
	TIME [epoch: 1.34 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24490816413311706		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.24490816413311706 | validation: 0.2682025551746156]
	TIME [epoch: 1.34 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2473077962561228		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.2473077962561228 | validation: 0.2452050635779468]
	TIME [epoch: 1.34 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23809448655574816		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.23809448655574816 | validation: 0.214270878548495]
	TIME [epoch: 1.35 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24300289199058367		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.24300289199058367 | validation: 0.26041125177540564]
	TIME [epoch: 1.34 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2262365274741106		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.2262365274741106 | validation: 0.20502866936000222]
	TIME [epoch: 1.34 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21733089049051738		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.21733089049051738 | validation: 0.22123434034534484]
	TIME [epoch: 1.34 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21338850432875348		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.21338850432875348 | validation: 0.21393479174150565]
	TIME [epoch: 1.34 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21473521260282524		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.21473521260282524 | validation: 0.22300082759452533]
	TIME [epoch: 1.34 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2089013744946427		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.2089013744946427 | validation: 0.2136123058929764]
	TIME [epoch: 1.34 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21621932658885915		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.21621932658885915 | validation: 0.21271912635988866]
	TIME [epoch: 1.34 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2173091799647768		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.2173091799647768 | validation: 0.21901169009029806]
	TIME [epoch: 1.34 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2218067137242466		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.2218067137242466 | validation: 0.223332003155205]
	TIME [epoch: 1.34 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23688821039343566		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.23688821039343566 | validation: 0.23470046667514488]
	TIME [epoch: 1.34 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24403568037723922		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.24403568037723922 | validation: 0.2500406472957177]
	TIME [epoch: 1.34 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2324309100941008		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.2324309100941008 | validation: 0.20542417740579771]
	TIME [epoch: 1.35 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2180585573233179		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.2180585573233179 | validation: 0.22446646122928457]
	TIME [epoch: 1.34 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2081248454637207		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.2081248454637207 | validation: 0.19356706223216316]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_390.pth
	Model improved!!!
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21424346161932342		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.21424346161932342 | validation: 0.25750429013798837]
	TIME [epoch: 1.35 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21628147228367373		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.21628147228367373 | validation: 0.20804342480785032]
	TIME [epoch: 1.35 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21674401112753094		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.21674401112753094 | validation: 0.22484411564958792]
	TIME [epoch: 1.35 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2161894085714362		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.2161894085714362 | validation: 0.21811472635438128]
	TIME [epoch: 1.35 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2215341641759027		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.2215341641759027 | validation: 0.22873056460810615]
	TIME [epoch: 1.35 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22315927933700913		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.22315927933700913 | validation: 0.2067558280981202]
	TIME [epoch: 1.35 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2200887247610208		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.2200887247610208 | validation: 0.2275393390092907]
	TIME [epoch: 1.35 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21536954280944254		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.21536954280944254 | validation: 0.21652551627175282]
	TIME [epoch: 1.36 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21478298767598505		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.21478298767598505 | validation: 0.22373213344828807]
	TIME [epoch: 1.35 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21872567118952788		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.21872567118952788 | validation: 0.24101871239993988]
	TIME [epoch: 1.35 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23086669481268324		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.23086669481268324 | validation: 0.21765929430130712]
	TIME [epoch: 1.36 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23223612680045339		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.23223612680045339 | validation: 0.24709834995133045]
	TIME [epoch: 1.36 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2255874856995898		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.2255874856995898 | validation: 0.208763459403071]
	TIME [epoch: 1.36 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21760385851574582		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.21760385851574582 | validation: 0.23020938425148518]
	TIME [epoch: 1.36 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21082822249769165		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.21082822249769165 | validation: 0.20198784178980211]
	TIME [epoch: 1.36 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20719811739513586		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.20719811739513586 | validation: 0.20648465888671452]
	TIME [epoch: 1.35 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20765798981388167		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.20765798981388167 | validation: 0.2032983977862679]
	TIME [epoch: 1.35 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20744110536686172		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.20744110536686172 | validation: 0.20923423143906447]
	TIME [epoch: 1.35 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20660498248908338		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.20660498248908338 | validation: 0.2183396082951521]
	TIME [epoch: 1.35 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20856052126632596		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.20856052126632596 | validation: 0.22323237374923918]
	TIME [epoch: 1.35 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20966568083332754		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.20966568083332754 | validation: 0.21694095749177747]
	TIME [epoch: 1.35 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22540018849444562		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.22540018849444562 | validation: 0.26542586214056413]
	TIME [epoch: 1.35 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2526056570820889		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.2526056570820889 | validation: 0.21532493544493037]
	TIME [epoch: 1.35 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2421602275514229		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.2421602275514229 | validation: 0.22935505685347402]
	TIME [epoch: 1.35 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21577127764388035		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.21577127764388035 | validation: 0.2258940187998839]
	TIME [epoch: 1.35 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2177521633908353		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.2177521633908353 | validation: 0.20137485960112642]
	TIME [epoch: 1.35 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2139928219475985		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.2139928219475985 | validation: 0.253863357987138]
	TIME [epoch: 1.35 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21423147938854992		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.21423147938854992 | validation: 0.1884681111861868]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_418.pth
	Model improved!!!
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2151003929076712		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.2151003929076712 | validation: 0.23071343503715852]
	TIME [epoch: 1.36 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20797250116453822		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.20797250116453822 | validation: 0.20940318980707706]
	TIME [epoch: 1.36 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20750777650359534		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.20750777650359534 | validation: 0.22044260541256777]
	TIME [epoch: 1.36 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21249513600819842		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.21249513600819842 | validation: 0.21272469187842394]
	TIME [epoch: 1.36 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2165746644323836		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.2165746644323836 | validation: 0.23510276456632098]
	TIME [epoch: 1.35 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21716941301817638		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.21716941301817638 | validation: 0.20995475985637857]
	TIME [epoch: 1.35 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21789451796324763		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.21789451796324763 | validation: 0.2426240972500362]
	TIME [epoch: 1.36 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21691002223482742		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.21691002223482742 | validation: 0.19785923002727446]
	TIME [epoch: 1.36 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2099179626385726		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.2099179626385726 | validation: 0.22831445058347494]
	TIME [epoch: 1.36 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20876886700171163		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.20876886700171163 | validation: 0.1880143539224808]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_428.pth
	Model improved!!!
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20779732778649063		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.20779732778649063 | validation: 0.2281709041693723]
	TIME [epoch: 1.35 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21328795535979148		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.21328795535979148 | validation: 0.19152340205748145]
	TIME [epoch: 1.35 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21617662427070328		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.21617662427070328 | validation: 0.24768750134811449]
	TIME [epoch: 1.35 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21900336809349277		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.21900336809349277 | validation: 0.2050432978346074]
	TIME [epoch: 1.36 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21843715617946427		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.21843715617946427 | validation: 0.21226254681702594]
	TIME [epoch: 1.36 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20894361005255113		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.20894361005255113 | validation: 0.2134867719706008]
	TIME [epoch: 1.36 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2047924674575367		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.2047924674575367 | validation: 0.20133271960348365]
	TIME [epoch: 1.36 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21031241556807104		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.21031241556807104 | validation: 0.2290801027513942]
	TIME [epoch: 1.36 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21256223027622503		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.21256223027622503 | validation: 0.18321772792549756]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_437.pth
	Model improved!!!
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2114033947633493		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.2114033947633493 | validation: 0.22411419170960623]
	TIME [epoch: 1.35 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21138465829571168		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.21138465829571168 | validation: 0.1962828762662088]
	TIME [epoch: 1.35 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21550894062137743		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.21550894062137743 | validation: 0.22185602354744174]
	TIME [epoch: 1.36 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21071058116141736		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.21071058116141736 | validation: 0.2059289353105728]
	TIME [epoch: 1.35 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21293167836854857		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.21293167836854857 | validation: 0.23061517291441522]
	TIME [epoch: 1.35 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2076876601614754		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.2076876601614754 | validation: 0.2314018119317746]
	TIME [epoch: 1.35 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21357530262564928		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.21357530262564928 | validation: 0.19884030599532015]
	TIME [epoch: 1.35 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21848558058063902		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.21848558058063902 | validation: 0.22556172281407397]
	TIME [epoch: 1.35 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21180908273599927		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.21180908273599927 | validation: 0.20029460648386488]
	TIME [epoch: 1.35 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20190080874374322		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.20190080874374322 | validation: 0.21589095314352666]
	TIME [epoch: 1.35 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20362451312797814		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.20362451312797814 | validation: 0.19916732414340885]
	TIME [epoch: 1.35 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20198058559121876		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.20198058559121876 | validation: 0.22514482733935492]
	TIME [epoch: 1.35 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2027511506853148		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.2027511506853148 | validation: 0.2015351528730983]
	TIME [epoch: 1.35 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20005534370634181		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.20005534370634181 | validation: 0.21784337870405018]
	TIME [epoch: 1.35 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20342901758765858		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.20342901758765858 | validation: 0.2039081906004424]
	TIME [epoch: 1.35 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20928198158049305		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.20928198158049305 | validation: 0.21678161726883652]
	TIME [epoch: 1.35 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20905016313418023		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.20905016313418023 | validation: 0.20195311590688295]
	TIME [epoch: 1.35 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21105693614411317		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.21105693614411317 | validation: 0.21392844696629698]
	TIME [epoch: 1.35 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20184650297584925		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.20184650297584925 | validation: 0.21916249769987114]
	TIME [epoch: 1.35 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20395706024903476		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.20395706024903476 | validation: 0.21036684468031566]
	TIME [epoch: 1.35 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22819925036905958		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.22819925036905958 | validation: 0.25187542376252947]
	TIME [epoch: 1.35 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23259988614189095		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.23259988614189095 | validation: 0.23011164576454501]
	TIME [epoch: 1.35 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20629955334489922		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.20629955334489922 | validation: 0.19515562410433995]
	TIME [epoch: 1.35 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21074391806923765		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.21074391806923765 | validation: 0.279818960810444]
	TIME [epoch: 1.35 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22331133371300435		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.22331133371300435 | validation: 0.18721158733805812]
	TIME [epoch: 1.36 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2072882156223984		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.2072882156223984 | validation: 0.19705111458351407]
	TIME [epoch: 1.35 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20317600762920726		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.20317600762920726 | validation: 0.21376538993057276]
	TIME [epoch: 1.35 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20050003609718153		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.20050003609718153 | validation: 0.19797052644603647]
	TIME [epoch: 1.35 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2034474664409983		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.2034474664409983 | validation: 0.20902114944128353]
	TIME [epoch: 1.35 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20218328114528028		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.20218328114528028 | validation: 0.20928208909079543]
	TIME [epoch: 1.35 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19874090700000352		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.19874090700000352 | validation: 0.20284755348356126]
	TIME [epoch: 1.35 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19792764375139157		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.19792764375139157 | validation: 0.20157996100435507]
	TIME [epoch: 1.35 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1948916614293131		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.1948916614293131 | validation: 0.20415649529096236]
	TIME [epoch: 1.35 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19640936261397968		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.19640936261397968 | validation: 0.22490890994332124]
	TIME [epoch: 1.35 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20405299119073		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.20405299119073 | validation: 0.2072989149021474]
	TIME [epoch: 1.35 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21954416612897631		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.21954416612897631 | validation: 0.2647390395236884]
	TIME [epoch: 1.35 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2241114074265191		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.2241114074265191 | validation: 0.19629786738270885]
	TIME [epoch: 1.35 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20566090624236133		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.20566090624236133 | validation: 0.23093505328469688]
	TIME [epoch: 1.35 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20011611986468844		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.20011611986468844 | validation: 0.20169349840502432]
	TIME [epoch: 1.35 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19740520100260617		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.19740520100260617 | validation: 0.20426231962670852]
	TIME [epoch: 1.35 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.202735438607716		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.202735438607716 | validation: 0.21486146628268]
	TIME [epoch: 1.35 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19898419592660155		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.19898419592660155 | validation: 0.20964397631680598]
	TIME [epoch: 1.35 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20183303245147177		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.20183303245147177 | validation: 0.20538065095533864]
	TIME [epoch: 1.35 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19362901526453213		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.19362901526453213 | validation: 0.22638550385946501]
	TIME [epoch: 1.35 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19949096293120647		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.19949096293120647 | validation: 0.19189337530254463]
	TIME [epoch: 1.35 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20014609960768412		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.20014609960768412 | validation: 0.2584532535661274]
	TIME [epoch: 1.35 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21064745991753442		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.21064745991753442 | validation: 0.18889934429131194]
	TIME [epoch: 1.36 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21245334845786545		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.21245334845786545 | validation: 0.24320472540105909]
	TIME [epoch: 1.35 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20990563103402055		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.20990563103402055 | validation: 0.19717742623366244]
	TIME [epoch: 1.35 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19638484763805739		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.19638484763805739 | validation: 0.20114033130602121]
	TIME [epoch: 1.35 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19532257055711238		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.19532257055711238 | validation: 0.22944123667500352]
	TIME [epoch: 1.35 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19982293006979085		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.19982293006979085 | validation: 0.20153108296908162]
	TIME [epoch: 1.35 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1982297785563823		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.1982297785563823 | validation: 0.21466279209930583]
	TIME [epoch: 1.35 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2000199743958872		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.2000199743958872 | validation: 0.20017762518343007]
	TIME [epoch: 1.35 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1953999853503821		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.1953999853503821 | validation: 0.22488293547290664]
	TIME [epoch: 1.35 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19908330547744746		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.19908330547744746 | validation: 0.20306890593815063]
	TIME [epoch: 1.35 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19739398120154705		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.19739398120154705 | validation: 0.20930774876357808]
	TIME [epoch: 1.35 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19934847477429934		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.19934847477429934 | validation: 0.25567607153172994]
	TIME [epoch: 1.35 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20430750684995988		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.20430750684995988 | validation: 0.18705480995257048]
	TIME [epoch: 1.35 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.226080596244022		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.226080596244022 | validation: 0.22730670861844726]
	TIME [epoch: 1.35 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19681535693755345		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.19681535693755345 | validation: 0.20471947114999767]
	TIME [epoch: 1.35 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19029211244646418		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.19029211244646418 | validation: 0.19243413147712382]
	TIME [epoch: 1.35 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1941457497034931		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.1941457497034931 | validation: 0.22429104758819404]
	TIME [epoch: 1.35 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19685649688887302		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.19685649688887302 | validation: 0.19929163197112137]
	TIME [epoch: 179 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19587345341171788		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.19587345341171788 | validation: 0.1984963065349354]
	TIME [epoch: 2.68 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19901265220086076		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.19901265220086076 | validation: 0.20918343229431002]
	TIME [epoch: 2.66 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19166145894588252		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.19166145894588252 | validation: 0.20616141876726354]
	TIME [epoch: 2.67 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19738226986757376		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.19738226986757376 | validation: 0.21078973071048843]
	TIME [epoch: 2.66 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20256739633138907		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.20256739633138907 | validation: 0.1991870719498794]
	TIME [epoch: 2.67 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19584902829223552		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.19584902829223552 | validation: 0.2216960404740691]
	TIME [epoch: 2.66 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19290259174887225		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.19290259174887225 | validation: 0.18826893097577832]
	TIME [epoch: 2.67 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2024678871606102		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.2024678871606102 | validation: 0.2873549432617775]
	TIME [epoch: 2.66 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21416701623928028		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.21416701623928028 | validation: 0.18723850783764745]
	TIME [epoch: 2.67 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20336138228983275		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.20336138228983275 | validation: 0.20420249898836476]
	TIME [epoch: 2.67 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19781182157609373		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.19781182157609373 | validation: 0.22094461895954562]
	TIME [epoch: 2.66 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19277861074522723		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.19277861074522723 | validation: 0.1845627722701394]
	TIME [epoch: 2.66 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18666546061075795		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.18666546061075795 | validation: 0.2199127852623921]
	TIME [epoch: 2.67 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.189104553993375		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.189104553993375 | validation: 0.19742892531917391]
	TIME [epoch: 2.67 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18934550853726584		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.18934550853726584 | validation: 0.20212760097400256]
	TIME [epoch: 2.67 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18464324997911682		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.18464324997911682 | validation: 0.1868559850382825]
	TIME [epoch: 2.66 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1863495128393804		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.1863495128393804 | validation: 0.2088356058657401]
	TIME [epoch: 2.67 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18790789455967377		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.18790789455967377 | validation: 0.1975290955174768]
	TIME [epoch: 2.66 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18946335406151713		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.18946335406151713 | validation: 0.21565934854996766]
	TIME [epoch: 2.66 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1935063782269488		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.1935063782269488 | validation: 0.19984518894170492]
	TIME [epoch: 2.66 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1888207007124278		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.1888207007124278 | validation: 0.21512938059498313]
	TIME [epoch: 2.67 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18513194902906085		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.18513194902906085 | validation: 0.198597221544357]
	TIME [epoch: 2.66 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18460962831676703		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.18460962831676703 | validation: 0.24646533124671088]
	TIME [epoch: 2.67 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19197137342241496		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.19197137342241496 | validation: 0.178717057400587]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_525.pth
	Model improved!!!
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22925149498538183		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.22925149498538183 | validation: 0.27191878253835305]
	TIME [epoch: 2.68 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2106312639487658		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.2106312639487658 | validation: 0.17335726299382923]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_527.pth
	Model improved!!!
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18683298771345033		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.18683298771345033 | validation: 0.1826295288538757]
	TIME [epoch: 2.67 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18189669622722124		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.18189669622722124 | validation: 0.21376027054877428]
	TIME [epoch: 2.67 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18248435131854612		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.18248435131854612 | validation: 0.188951933675503]
	TIME [epoch: 2.67 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17916809206591935		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.17916809206591935 | validation: 0.21229805734370488]
	TIME [epoch: 2.67 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18488015465572916		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.18488015465572916 | validation: 0.18611018353997644]
	TIME [epoch: 2.67 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18366673677157802		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.18366673677157802 | validation: 0.2126890583305765]
	TIME [epoch: 2.67 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17926958694161527		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.17926958694161527 | validation: 0.17435386522388396]
	TIME [epoch: 2.67 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18511915010223667		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.18511915010223667 | validation: 0.23528744967428308]
	TIME [epoch: 2.67 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1906651190706568		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.1906651190706568 | validation: 0.1799408371834169]
	TIME [epoch: 2.67 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19491676818517037		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.19491676818517037 | validation: 0.22574756752293468]
	TIME [epoch: 2.68 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18968792837549836		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.18968792837549836 | validation: 0.17246933067641154]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_538.pth
	Model improved!!!
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19805880495696002		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.19805880495696002 | validation: 0.22330986225838478]
	TIME [epoch: 2.68 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19569136993480055		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.19569136993480055 | validation: 0.18805925490886732]
	TIME [epoch: 2.69 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17775485811697078		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.17775485811697078 | validation: 0.20881637893939434]
	TIME [epoch: 2.68 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18633296847270406		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.18633296847270406 | validation: 0.1906950892271022]
	TIME [epoch: 2.68 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18401850534663364		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.18401850534663364 | validation: 0.2493977615440755]
	TIME [epoch: 2.68 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18450769477454443		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.18450769477454443 | validation: 0.1588683171680442]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_544.pth
	Model improved!!!
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19698368347067294		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.19698368347067294 | validation: 0.2404467851517974]
	TIME [epoch: 2.68 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18501043000854217		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.18501043000854217 | validation: 0.1775581443766821]
	TIME [epoch: 2.68 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1804587073160974		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.1804587073160974 | validation: 0.20152357136202106]
	TIME [epoch: 2.68 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1763856630338418		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.1763856630338418 | validation: 0.1942345498976987]
	TIME [epoch: 2.69 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1751438744327031		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.1751438744327031 | validation: 0.19963818075431655]
	TIME [epoch: 2.68 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17046273556913544		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.17046273556913544 | validation: 0.18996082610160797]
	TIME [epoch: 2.68 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17207947825744135		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.17207947825744135 | validation: 0.19433228677214573]
	TIME [epoch: 2.68 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1693555926091517		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.1693555926091517 | validation: 0.1959010719115689]
	TIME [epoch: 2.68 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17221838602888284		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.17221838602888284 | validation: 0.18501154589049773]
	TIME [epoch: 2.68 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17362478381861915		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.17362478381861915 | validation: 0.2375975332367896]
	TIME [epoch: 2.68 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1876993891643263		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.1876993891643263 | validation: 0.18214220218661292]
	TIME [epoch: 2.68 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21144098853514187		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.21144098853514187 | validation: 0.24826012965449312]
	TIME [epoch: 2.68 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18350090468515748		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.18350090468515748 | validation: 0.17969265472221285]
	TIME [epoch: 2.68 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1779107333817032		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.1779107333817032 | validation: 0.17710268605373947]
	TIME [epoch: 2.68 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17016651462243101		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.17016651462243101 | validation: 0.2171527353849252]
	TIME [epoch: 2.68 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17347386392109904		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.17347386392109904 | validation: 0.17771021415259028]
	TIME [epoch: 2.68 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1785558242187568		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.1785558242187568 | validation: 0.21731450610488814]
	TIME [epoch: 2.68 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1716109512562838		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.1716109512562838 | validation: 0.17462871186452913]
	TIME [epoch: 2.68 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17386398697291663		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.17386398697291663 | validation: 0.203180749400152]
	TIME [epoch: 2.68 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16884074423372197		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.16884074423372197 | validation: 0.17678215765596128]
	TIME [epoch: 2.68 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1687642949326279		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.1687642949326279 | validation: 0.20207678964190512]
	TIME [epoch: 2.68 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1637857693094147		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.1637857693094147 | validation: 0.15630310631715574]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_566.pth
	Model improved!!!
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17196069669899325		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.17196069669899325 | validation: 0.23604294703300677]
	TIME [epoch: 2.65 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18039489401273454		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.18039489401273454 | validation: 0.1603039253293703]
	TIME [epoch: 2.66 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18801717257787862		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.18801717257787862 | validation: 0.23426340218895195]
	TIME [epoch: 2.68 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17627409474244146		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.17627409474244146 | validation: 0.16595251003071573]
	TIME [epoch: 2.69 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16889639898006045		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.16889639898006045 | validation: 0.1926497462417438]
	TIME [epoch: 2.68 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16151086346878565		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.16151086346878565 | validation: 0.190418696620517]
	TIME [epoch: 2.68 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1618856151938486		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.1618856151938486 | validation: 0.1928087263796774]
	TIME [epoch: 2.68 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15705044186959566		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.15705044186959566 | validation: 0.19544016921734753]
	TIME [epoch: 2.68 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15603734604343253		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.15603734604343253 | validation: 0.16848454011710923]
	TIME [epoch: 2.68 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16669695119778433		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.16669695119778433 | validation: 0.2695854638464678]
	TIME [epoch: 2.68 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19529402582895153		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.19529402582895153 | validation: 0.16544739263020916]
	TIME [epoch: 2.68 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20494355914979992		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.20494355914979992 | validation: 0.17623975195137187]
	TIME [epoch: 2.68 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16574177564243253		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.16574177564243253 | validation: 0.22056050986962727]
	TIME [epoch: 2.68 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16852963215013086		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.16852963215013086 | validation: 0.14649485551110966]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_580.pth
	Model improved!!!
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17507130634055953		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.17507130634055953 | validation: 0.20301329748090577]
	TIME [epoch: 2.66 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1585131262500399		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.1585131262500399 | validation: 0.17361265320491379]
	TIME [epoch: 2.65 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16024159555153505		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.16024159555153505 | validation: 0.18179452783399427]
	TIME [epoch: 2.66 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1592896207478602		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.1592896207478602 | validation: 0.19908919245021917]
	TIME [epoch: 2.66 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1608034880806551		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.1608034880806551 | validation: 0.1882617525775635]
	TIME [epoch: 2.66 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15774242688688786		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.15774242688688786 | validation: 0.18864556392415577]
	TIME [epoch: 2.66 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1518003172881442		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.1518003172881442 | validation: 0.16586823457755243]
	TIME [epoch: 2.65 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1537774450679313		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.1537774450679313 | validation: 0.20967783362979855]
	TIME [epoch: 2.66 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15688463621265952		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.15688463621265952 | validation: 0.15174367432577093]
	TIME [epoch: 2.65 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18518613887313046		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.18518613887313046 | validation: 0.2758790791387174]
	TIME [epoch: 2.66 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18799113533779405		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.18799113533779405 | validation: 0.1401735764094912]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_591.pth
	Model improved!!!
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17347874210812403		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.17347874210812403 | validation: 0.17245558660229438]
	TIME [epoch: 2.67 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1530875157231535		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.1530875157231535 | validation: 0.19254969955025125]
	TIME [epoch: 2.66 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16261035881857383		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.16261035881857383 | validation: 0.1680316356418998]
	TIME [epoch: 2.67 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16724409110002103		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.16724409110002103 | validation: 0.18178015190129695]
	TIME [epoch: 2.66 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15166616634737526		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.15166616634737526 | validation: 0.16508759191441993]
	TIME [epoch: 2.66 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1471037776583789		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.1471037776583789 | validation: 0.17883995628016072]
	TIME [epoch: 2.66 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1501782686966006		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.1501782686966006 | validation: 0.1767448008422149]
	TIME [epoch: 2.66 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14727071808501954		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.14727071808501954 | validation: 0.16131279938456256]
	TIME [epoch: 2.66 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14792076937239046		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.14792076937239046 | validation: 0.21045531191753863]
	TIME [epoch: 2.66 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15305764449546325		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.15305764449546325 | validation: 0.14426118889779438]
	TIME [epoch: 2.67 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17628409806810616		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.17628409806810616 | validation: 0.2638764808305644]
	TIME [epoch: 2.67 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17885958548232359		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.17885958548232359 | validation: 0.1415105895723021]
	TIME [epoch: 2.67 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16575949227396794		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.16575949227396794 | validation: 0.1741445811332495]
	TIME [epoch: 2.66 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15148975689143915		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.15148975689143915 | validation: 0.18935193188230104]
	TIME [epoch: 2.67 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1458911914312997		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.1458911914312997 | validation: 0.154746874974303]
	TIME [epoch: 2.66 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15714417227326702		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.15714417227326702 | validation: 0.20961107293116588]
	TIME [epoch: 2.66 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15577568252303065		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.15577568252303065 | validation: 0.15879578355653778]
	TIME [epoch: 2.67 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14739694456961233		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.14739694456961233 | validation: 0.17910805649609102]
	TIME [epoch: 2.67 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14241479285724748		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.14241479285724748 | validation: 0.16120558107655525]
	TIME [epoch: 2.67 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14456555047636885		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.14456555047636885 | validation: 0.1967560790450981]
	TIME [epoch: 2.67 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14880920573349113		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.14880920573349113 | validation: 0.15877244493834622]
	TIME [epoch: 2.66 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1626679448302881		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.1626679448302881 | validation: 0.23261476908002174]
	TIME [epoch: 2.66 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.162796506618689		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.162796506618689 | validation: 0.14787563854059402]
	TIME [epoch: 2.67 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16426883476895815		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.16426883476895815 | validation: 0.17945699704187024]
	TIME [epoch: 2.66 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14038840315862083		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.14038840315862083 | validation: 0.15764068699892633]
	TIME [epoch: 2.66 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13611496887833213		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.13611496887833213 | validation: 0.1628393540553819]
	TIME [epoch: 2.66 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14389533985243		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.14389533985243 | validation: 0.16715675192994475]
	TIME [epoch: 2.66 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.143013292574386		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.143013292574386 | validation: 0.1735534509489869]
	TIME [epoch: 2.66 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.142725451836529		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.142725451836529 | validation: 0.18603796123274177]
	TIME [epoch: 2.66 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14009604495242736		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.14009604495242736 | validation: 0.15443567692509866]
	TIME [epoch: 2.66 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14688283094401705		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.14688283094401705 | validation: 0.22969774419786815]
	TIME [epoch: 2.66 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15173100934080322		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.15173100934080322 | validation: 0.1443972056853748]
	TIME [epoch: 2.66 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16685034911612		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.16685034911612 | validation: 0.19427699673180174]
	TIME [epoch: 2.66 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14270304628551309		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.14270304628551309 | validation: 0.15649492527957504]
	TIME [epoch: 2.67 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1365320253267378		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.1365320253267378 | validation: 0.15402431951050213]
	TIME [epoch: 2.67 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13706956724802083		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.13706956724802083 | validation: 0.1882923087607038]
	TIME [epoch: 2.67 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1376878664553375		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.1376878664553375 | validation: 0.15299886770032203]
	TIME [epoch: 2.67 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1485815748771117		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.1485815748771117 | validation: 0.21222820969800696]
	TIME [epoch: 2.68 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15141773893254626		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.15141773893254626 | validation: 0.15049145107477627]
	TIME [epoch: 2.67 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16195287924329135		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.16195287924329135 | validation: 0.17409053897946847]
	TIME [epoch: 2.68 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13925017283980176		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.13925017283980176 | validation: 0.14620356554351996]
	TIME [epoch: 2.67 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12894602131157204		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.12894602131157204 | validation: 0.1740079320567141]
	TIME [epoch: 2.67 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13125900349572153		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.13125900349572153 | validation: 0.1445582121633705]
	TIME [epoch: 2.67 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13795134264039813		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.13795134264039813 | validation: 0.18424800852168005]
	TIME [epoch: 2.67 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13571983518691855		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.13571983518691855 | validation: 0.15593653459644]
	TIME [epoch: 2.68 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13352010252178156		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.13352010252178156 | validation: 0.1567845062627541]
	TIME [epoch: 2.67 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13292973686488788		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.13292973686488788 | validation: 0.20073756219995148]
	TIME [epoch: 2.68 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1399566134892807		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.1399566134892807 | validation: 0.13652201153329258]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_639.pth
	Model improved!!!
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18333274104990185		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.18333274104990185 | validation: 0.22634660273555884]
	TIME [epoch: 2.65 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15035255402326342		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.15035255402326342 | validation: 0.1376713681719622]
	TIME [epoch: 2.65 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13178203708312886		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.13178203708312886 | validation: 0.1531765343076569]
	TIME [epoch: 2.65 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13065344967000733		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.13065344967000733 | validation: 0.1847224983845385]
	TIME [epoch: 2.65 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13434312530863818		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.13434312530863818 | validation: 0.1493570897161725]
	TIME [epoch: 2.66 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14345097527629108		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.14345097527629108 | validation: 0.18413014875587877]
	TIME [epoch: 2.66 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13068015751788914		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.13068015751788914 | validation: 0.1331850745548969]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_646.pth
	Model improved!!!
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13002177772432613		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.13002177772432613 | validation: 0.1589427328318562]
	TIME [epoch: 2.69 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12508070452062128		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.12508070452062128 | validation: 0.14612649822163554]
	TIME [epoch: 2.68 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1260069827885523		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.1260069827885523 | validation: 0.15616608144036676]
	TIME [epoch: 2.68 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12434393117593667		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.12434393117593667 | validation: 0.15471448951045805]
	TIME [epoch: 2.65 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12225201244961109		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.12225201244961109 | validation: 0.15867505051070038]
	TIME [epoch: 2.65 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1315881396008344		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.1315881396008344 | validation: 0.14750725049578792]
	TIME [epoch: 2.65 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15058855719641606		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.15058855719641606 | validation: 0.242428924969904]
	TIME [epoch: 2.65 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16410061109416035		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.16410061109416035 | validation: 0.13624711635877856]
	TIME [epoch: 2.65 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15600207002357647		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.15600207002357647 | validation: 0.16145704289366494]
	TIME [epoch: 2.65 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1204636804557432		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.1204636804557432 | validation: 0.17531293168519413]
	TIME [epoch: 2.65 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13415305903182692		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.13415305903182692 | validation: 0.13776677700069703]
	TIME [epoch: 2.65 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1436099488925232		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.1436099488925232 | validation: 0.17170953417863632]
	TIME [epoch: 2.66 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1260692763289836		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.1260692763289836 | validation: 0.14971664698968815]
	TIME [epoch: 2.65 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12062759213262524		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.12062759213262524 | validation: 0.14919894504383804]
	TIME [epoch: 2.65 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12482968711351905		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.12482968711351905 | validation: 0.16283991064251302]
	TIME [epoch: 2.66 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1228114702190038		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.1228114702190038 | validation: 0.14805118535662448]
	TIME [epoch: 2.66 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12335840812997767		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.12335840812997767 | validation: 0.1542008284612273]
	TIME [epoch: 2.66 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1210749701847928		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.1210749701847928 | validation: 0.14512037087235605]
	TIME [epoch: 2.66 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11971321081441655		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.11971321081441655 | validation: 0.16610127622838694]
	TIME [epoch: 2.66 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1255878209089926		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.1255878209089926 | validation: 0.13762689564173622]
	TIME [epoch: 2.66 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14024929789221044		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.14024929789221044 | validation: 0.223042517998846]
	TIME [epoch: 2.65 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14511221863664667		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.14511221863664667 | validation: 0.12259921392419143]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_668.pth
	Model improved!!!
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1375216973644163		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.1375216973644163 | validation: 0.16174278373060288]
	TIME [epoch: 2.69 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.115364080882429		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.115364080882429 | validation: 0.1412239073055941]
	TIME [epoch: 2.68 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11233479984799363		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.11233479984799363 | validation: 0.13633482892993656]
	TIME [epoch: 2.68 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11523134944392288		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.11523134944392288 | validation: 0.17092945925085168]
	TIME [epoch: 2.68 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11992562589323537		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.11992562589323537 | validation: 0.13703898549444976]
	TIME [epoch: 2.68 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1260633566728362		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.1260633566728362 | validation: 0.1922132829648274]
	TIME [epoch: 2.68 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1312672567528644		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.1312672567528644 | validation: 0.13119083332620793]
	TIME [epoch: 2.68 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14162210629006744		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.14162210629006744 | validation: 0.17900839671234461]
	TIME [epoch: 2.68 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1267978056525769		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.1267978056525769 | validation: 0.13874727799742256]
	TIME [epoch: 2.68 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11625679893780454		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.11625679893780454 | validation: 0.16029516120635393]
	TIME [epoch: 2.68 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1143559927053273		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.1143559927053273 | validation: 0.14318617486531246]
	TIME [epoch: 2.68 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11515041957925654		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.11515041957925654 | validation: 0.15439716295668282]
	TIME [epoch: 2.69 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11413364734268776		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.11413364734268776 | validation: 0.132192099809588]
	TIME [epoch: 2.68 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11591854851059377		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.11591854851059377 | validation: 0.16898220682795026]
	TIME [epoch: 2.68 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12194393507335914		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.12194393507335914 | validation: 0.13430266294960683]
	TIME [epoch: 2.68 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1349787882873406		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.1349787882873406 | validation: 0.1829178060918919]
	TIME [epoch: 2.68 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13171229620244265		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.13171229620244265 | validation: 0.12673305474069727]
	TIME [epoch: 2.68 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12593745532525724		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.12593745532525724 | validation: 0.15607748854172598]
	TIME [epoch: 2.68 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11163334022710954		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.11163334022710954 | validation: 0.1428505531669426]
	TIME [epoch: 2.68 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11144849354216922		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.11144849354216922 | validation: 0.1443322721980639]
	TIME [epoch: 2.68 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11492895113368148		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.11492895113368148 | validation: 0.1517040367752291]
	TIME [epoch: 2.68 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11715197989837642		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.11715197989837642 | validation: 0.12605910594650824]
	TIME [epoch: 2.68 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12159046870699491		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.12159046870699491 | validation: 0.19615944670722607]
	TIME [epoch: 2.69 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13169364067716918		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.13169364067716918 | validation: 0.12439589555186337]
	TIME [epoch: 2.68 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13791712455328195		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.13791712455328195 | validation: 0.1633544367462025]
	TIME [epoch: 2.68 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11276817785171905		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.11276817785171905 | validation: 0.1276843204602753]
	TIME [epoch: 2.68 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10790037603933503		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.10790037603933503 | validation: 0.13367993621579824]
	TIME [epoch: 2.68 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10547228158303515		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.10547228158303515 | validation: 0.14458424209315177]
	TIME [epoch: 2.68 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10794511861939367		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.10794511861939367 | validation: 0.14471901581426497]
	TIME [epoch: 2.68 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1120518658401492		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.1120518658401492 | validation: 0.1517420687067237]
	TIME [epoch: 2.68 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11061325848650039		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.11061325848650039 | validation: 0.13813538466889316]
	TIME [epoch: 2.68 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11218862152014182		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.11218862152014182 | validation: 0.1613684688021608]
	TIME [epoch: 2.68 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11805700133193564		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.11805700133193564 | validation: 0.13119566196872157]
	TIME [epoch: 2.65 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14033036424543233		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.14033036424543233 | validation: 0.17733975309673644]
	TIME [epoch: 2.66 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11967411017493323		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.11967411017493323 | validation: 0.11191505227961757]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_703.pth
	Model improved!!!
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11392406533562077		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.11392406533562077 | validation: 0.14894036363442603]
	TIME [epoch: 2.65 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10539732068012626		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.10539732068012626 | validation: 0.13254978441274085]
	TIME [epoch: 2.65 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10746115175384176		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.10746115175384176 | validation: 0.1411374352209994]
	TIME [epoch: 2.65 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10278421679465825		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.10278421679465825 | validation: 0.12697233613725373]
	TIME [epoch: 2.65 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10572676429103244		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.10572676429103244 | validation: 0.14292240966979305]
	TIME [epoch: 2.65 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10537238436070342		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.10537238436070342 | validation: 0.12553015502867138]
	TIME [epoch: 2.65 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10912635297476404		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.10912635297476404 | validation: 0.18476218954490908]
	TIME [epoch: 2.65 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12322310204168474		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.12322310204168474 | validation: 0.13452151574321322]
	TIME [epoch: 2.67 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15344429344644994		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.15344429344644994 | validation: 0.15232516241814076]
	TIME [epoch: 2.67 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11536890409788363		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.11536890409788363 | validation: 0.1362952612986992]
	TIME [epoch: 2.67 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10632604612191554		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.10632604612191554 | validation: 0.11136375587333909]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_714.pth
	Model improved!!!
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11011625265784564		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.11011625265784564 | validation: 0.14316026809081933]
	TIME [epoch: 2.67 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10317093277515799		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.10317093277515799 | validation: 0.13219552911372348]
	TIME [epoch: 2.67 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09755352810195078		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.09755352810195078 | validation: 0.12356133949792372]
	TIME [epoch: 2.67 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10384160935199323		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.10384160935199323 | validation: 0.1537514951545932]
	TIME [epoch: 2.67 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10854453772240107		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.10854453772240107 | validation: 0.1178448744941587]
	TIME [epoch: 2.67 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11522192664852732		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.11522192664852732 | validation: 0.17488585940763623]
	TIME [epoch: 2.67 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12067284117790034		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.12067284117790034 | validation: 0.12621065401822779]
	TIME [epoch: 2.67 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13064522701579365		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.13064522701579365 | validation: 0.13855836518143655]
	TIME [epoch: 2.67 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10466091274256915		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.10466091274256915 | validation: 0.12867835355436555]
	TIME [epoch: 2.67 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10008601925013913		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.10008601925013913 | validation: 0.1292872544073274]
	TIME [epoch: 2.68 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10305749793449584		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.10305749793449584 | validation: 0.12363170866414704]
	TIME [epoch: 2.67 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09872454065753442		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.09872454065753442 | validation: 0.13763721489345118]
	TIME [epoch: 2.67 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1040012586335602		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.1040012586335602 | validation: 0.13360265074813568]
	TIME [epoch: 2.67 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10470421798476011		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.10470421798476011 | validation: 0.12531460741548492]
	TIME [epoch: 2.67 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10503792211679262		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.10503792211679262 | validation: 0.18389526391882988]
	TIME [epoch: 2.67 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12218710280358928		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.12218710280358928 | validation: 0.12250064960073864]
	TIME [epoch: 2.67 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1317046804791331		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.1317046804791331 | validation: 0.15515943210439614]
	TIME [epoch: 2.67 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10575258360848577		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.10575258360848577 | validation: 0.12656394543734586]
	TIME [epoch: 2.67 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09703009047267495		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.09703009047267495 | validation: 0.1254703613909296]
	TIME [epoch: 2.67 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10067424121548026		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.10067424121548026 | validation: 0.14740717092741704]
	TIME [epoch: 2.67 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09904452672895996		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.09904452672895996 | validation: 0.11899475223478584]
	TIME [epoch: 2.68 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10450893298538581		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.10450893298538581 | validation: 0.16123620447640888]
	TIME [epoch: 2.67 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10723202319730547		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.10723202319730547 | validation: 0.1205753083889643]
	TIME [epoch: 2.67 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11317918641923293		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.11317918641923293 | validation: 0.15028374036764391]
	TIME [epoch: 2.67 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10766263784666513		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.10766263784666513 | validation: 0.12407649720189876]
	TIME [epoch: 2.67 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10698396234236367		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.10698396234236367 | validation: 0.12652013535864284]
	TIME [epoch: 2.67 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09686326167707336		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.09686326167707336 | validation: 0.126476681086849]
	TIME [epoch: 2.67 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09407270013333253		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.09407270013333253 | validation: 0.12769946706084556]
	TIME [epoch: 2.67 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09461265428438725		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.09461265428438725 | validation: 0.12118586098192075]
	TIME [epoch: 2.67 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09492344455339115		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.09492344455339115 | validation: 0.1487266366460691]
	TIME [epoch: 2.67 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1000267894342628		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.1000267894342628 | validation: 0.11549115390548743]
	TIME [epoch: 2.67 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11423117961721815		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.11423117961721815 | validation: 0.1741109454281352]
	TIME [epoch: 2.68 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1240163352949984		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.1240163352949984 | validation: 0.11715952656104518]
	TIME [epoch: 2.67 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1101562245050395		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.1101562245050395 | validation: 0.12672091533895014]
	TIME [epoch: 2.67 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09530102323039824		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.09530102323039824 | validation: 0.13086287356647297]
	TIME [epoch: 2.67 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09217515699822328		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.09217515699822328 | validation: 0.11885994098210116]
	TIME [epoch: 2.67 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.094094662717553		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.094094662717553 | validation: 0.15341814966378464]
	TIME [epoch: 2.67 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09750277210919894		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.09750277210919894 | validation: 0.10764837254011833]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_752.pth
	Model improved!!!
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10086779064481123		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.10086779064481123 | validation: 0.1476724037587421]
	TIME [epoch: 2.67 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09830673367462697		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.09830673367462697 | validation: 0.1136765358355367]
	TIME [epoch: 2.67 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09719750910293558		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.09719750910293558 | validation: 0.14167262153146143]
	TIME [epoch: 2.67 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09451024482287458		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.09451024482287458 | validation: 0.11588617946589744]
	TIME [epoch: 2.67 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10446473589687988		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.10446473589687988 | validation: 0.14610130470925736]
	TIME [epoch: 2.68 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10744091789546531		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.10744091789546531 | validation: 0.11681486514886151]
	TIME [epoch: 2.67 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11691919873438768		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.11691919873438768 | validation: 0.15062898964460114]
	TIME [epoch: 2.67 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09999740904529897		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.09999740904529897 | validation: 0.10696776190021887]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_760.pth
	Model improved!!!
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09507032033894731		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.09507032033894731 | validation: 0.1228681913032581]
	TIME [epoch: 2.67 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09225264527260858		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.09225264527260858 | validation: 0.11860846365340896]
	TIME [epoch: 2.67 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09438580934694049		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.09438580934694049 | validation: 0.13523546212750145]
	TIME [epoch: 2.67 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0927275333575307		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.0927275333575307 | validation: 0.10765345990703407]
	TIME [epoch: 2.67 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09582592356132304		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.09582592356132304 | validation: 0.15538424448741456]
	TIME [epoch: 2.67 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10173363086927426		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.10173363086927426 | validation: 0.10978407617860034]
	TIME [epoch: 2.67 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10908825122408454		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.10908825122408454 | validation: 0.14253664224055945]
	TIME [epoch: 2.67 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10153437950804371		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.10153437950804371 | validation: 0.11532162786252612]
	TIME [epoch: 2.68 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09881560946563879		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.09881560946563879 | validation: 0.12146302368393261]
	TIME [epoch: 2.67 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09631933708310954		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.09631933708310954 | validation: 0.12687200957981218]
	TIME [epoch: 2.67 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09357628963380492		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.09357628963380492 | validation: 0.11389128932835245]
	TIME [epoch: 2.67 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08773201172930271		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.08773201172930271 | validation: 0.11424918966474361]
	TIME [epoch: 2.67 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08759560219663115		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.08759560219663115 | validation: 0.11686449233776633]
	TIME [epoch: 2.67 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09001953532989529		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.09001953532989529 | validation: 0.11697748717321965]
	TIME [epoch: 2.67 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0895041310454977		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.0895041310454977 | validation: 0.11584130026810309]
	TIME [epoch: 2.67 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09137829005171652		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.09137829005171652 | validation: 0.15854054373180607]
	TIME [epoch: 2.67 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11201867286167902		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.11201867286167902 | validation: 0.11853392052976304]
	TIME [epoch: 2.67 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13534504956037774		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.13534504956037774 | validation: 0.1493652898186261]
	TIME [epoch: 2.67 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0981234543343248		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.0981234543343248 | validation: 0.11183007449105437]
	TIME [epoch: 2.68 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09036081504327306		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.09036081504327306 | validation: 0.10659822034771889]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_780.pth
	Model improved!!!
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09621092926770385		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.09621092926770385 | validation: 0.1376371895397833]
	TIME [epoch: 2.67 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09372006805100803		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.09372006805100803 | validation: 0.1054513563138074]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_782.pth
	Model improved!!!
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09252425908707335		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.09252425908707335 | validation: 0.12286761925045249]
	TIME [epoch: 2.67 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09186348735143586		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.09186348735143586 | validation: 0.11462635344491048]
	TIME [epoch: 2.67 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09066397232114148		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.09066397232114148 | validation: 0.12364747152691456]
	TIME [epoch: 2.67 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08823971977142762		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.08823971977142762 | validation: 0.11355623471770085]
	TIME [epoch: 2.67 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08615752589239539		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.08615752589239539 | validation: 0.12260981562721368]
	TIME [epoch: 2.67 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08570393603290119		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.08570393603290119 | validation: 0.11082888944166513]
	TIME [epoch: 2.67 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08714624033278337		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.08714624033278337 | validation: 0.11780870015075942]
	TIME [epoch: 2.68 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09360746386240701		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.09360746386240701 | validation: 0.12284451541330915]
	TIME [epoch: 2.67 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11187008620986401		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.11187008620986401 | validation: 0.15186527620296567]
	TIME [epoch: 2.67 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10324944532621422		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.10324944532621422 | validation: 0.09855678893358488]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_792.pth
	Model improved!!!
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09796536173544378		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.09796536173544378 | validation: 0.13343398253739128]
	TIME [epoch: 2.67 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09033450268720426		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.09033450268720426 | validation: 0.11520356800824878]
	TIME [epoch: 2.67 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08623632904295764		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.08623632904295764 | validation: 0.11285321361257239]
	TIME [epoch: 2.67 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08722328176599163		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.08722328176599163 | validation: 0.1319087959378769]
	TIME [epoch: 2.67 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09170381453460272		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.09170381453460272 | validation: 0.10248596968314078]
	TIME [epoch: 2.67 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0973976331646985		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.0973976331646985 | validation: 0.14955686531594584]
	TIME [epoch: 2.67 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09946809988795476		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.09946809988795476 | validation: 0.11428577697565859]
	TIME [epoch: 2.67 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10155608991847986		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.10155608991847986 | validation: 0.11397116450656691]
	TIME [epoch: 2.68 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08803198500669962		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.08803198500669962 | validation: 0.11783624199086512]
	TIME [epoch: 2.68 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08581562199157243		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.08581562199157243 | validation: 0.10434166649785524]
	TIME [epoch: 2.68 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08282698685942994		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.08282698685942994 | validation: 0.11711199846820755]
	TIME [epoch: 2.68 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08619052762283876		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.08619052762283876 | validation: 0.10923822340027828]
	TIME [epoch: 2.68 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08429369348950737		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.08429369348950737 | validation: 0.11028310636636687]
	TIME [epoch: 2.67 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08401378121563376		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.08401378121563376 | validation: 0.11127460368339825]
	TIME [epoch: 2.67 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08550848294659652		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.08550848294659652 | validation: 0.12424711441168418]
	TIME [epoch: 2.67 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09796541484550464		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.09796541484550464 | validation: 0.12016521303516506]
	TIME [epoch: 2.67 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09899729814895791		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.09899729814895791 | validation: 0.09847969895999427]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_809.pth
	Model improved!!!
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10601211264281396		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.10601211264281396 | validation: 0.16011196807009556]
	TIME [epoch: 2.68 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1020543727564684		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.1020543727564684 | validation: 0.09801489943464642]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_811.pth
	Model improved!!!
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08564262817113845		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.08564262817113845 | validation: 0.10398965664883211]
	TIME [epoch: 2.68 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08327286694722755		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.08327286694722755 | validation: 0.12279496229768348]
	TIME [epoch: 2.68 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08468257499808314		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.08468257499808314 | validation: 0.09563822449105153]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_814.pth
	Model improved!!!
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08299640971688187		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.08299640971688187 | validation: 0.11587539678691527]
	TIME [epoch: 2.68 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08481963342169553		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.08481963342169553 | validation: 0.10943630929723075]
	TIME [epoch: 2.68 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0858585310366243		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.0858585310366243 | validation: 0.11542827369914083]
	TIME [epoch: 2.68 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08704339521157543		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.08704339521157543 | validation: 0.1115536605091631]
	TIME [epoch: 2.68 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08944434622450188		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.08944434622450188 | validation: 0.1274514987350245]
	TIME [epoch: 2.68 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09021072149880009		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.09021072149880009 | validation: 0.1018353736319142]
	TIME [epoch: 2.68 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09247516371205272		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.09247516371205272 | validation: 0.12116802756333511]
	TIME [epoch: 2.69 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08835275026038567		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.08835275026038567 | validation: 0.10488808847615137]
	TIME [epoch: 2.68 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09179559460781192		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.09179559460781192 | validation: 0.11309348180874666]
	TIME [epoch: 2.68 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08385868353994548		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.08385868353994548 | validation: 0.1055578856081358]
	TIME [epoch: 2.68 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08248705143146938		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.08248705143146938 | validation: 0.10508044074181391]
	TIME [epoch: 2.68 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08242067715202898		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.08242067715202898 | validation: 0.10319076234983714]
	TIME [epoch: 2.68 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08508613775806315		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.08508613775806315 | validation: 0.12043522320598905]
	TIME [epoch: 2.68 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08289436716620785		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.08289436716620785 | validation: 0.0982151094863817]
	TIME [epoch: 2.68 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08732593289358008		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.08732593289358008 | validation: 0.13114951890733106]
	TIME [epoch: 2.68 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09235443723114856		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.09235443723114856 | validation: 0.10537993579598481]
	TIME [epoch: 2.68 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09170341583549796		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.09170341583549796 | validation: 0.11295474286893517]
	TIME [epoch: 2.68 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0870310812253262		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.0870310812253262 | validation: 0.09469705913898217]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_832.pth
	Model improved!!!
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08576577091725575		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.08576577091725575 | validation: 0.13354381381567632]
	TIME [epoch: 2.65 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09003093751933165		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.09003093751933165 | validation: 0.09277966225730395]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_834.pth
	Model improved!!!
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08923025753418352		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.08923025753418352 | validation: 0.12559969898928355]
	TIME [epoch: 2.65 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08141397407369343		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.08141397407369343 | validation: 0.09833203929470676]
	TIME [epoch: 2.65 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07972873289470639		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.07972873289470639 | validation: 0.10065888732155269]
	TIME [epoch: 2.65 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08033500637583735		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.08033500637583735 | validation: 0.110182144547253]
	TIME [epoch: 2.65 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07808039310828495		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.07808039310828495 | validation: 0.09218292216796126]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_839.pth
	Model improved!!!
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.078614340730827		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.078614340730827 | validation: 0.11260389234565374]
	TIME [epoch: 2.67 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07793660680853394		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.07793660680853394 | validation: 0.09189485625956934]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_841.pth
	Model improved!!!
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07874192925135612		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.07874192925135612 | validation: 0.11471692317036052]
	TIME [epoch: 2.68 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07905036707350455		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.07905036707350455 | validation: 0.0957654439609186]
	TIME [epoch: 2.68 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08129761154833723		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.08129761154833723 | validation: 0.15616596681676764]
	TIME [epoch: 2.68 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10339792229401784		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.10339792229401784 | validation: 0.12139077410004849]
	TIME [epoch: 2.68 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13230602301643357		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.13230602301643357 | validation: 0.09691635989544249]
	TIME [epoch: 2.68 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08037454577243806		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.08037454577243806 | validation: 0.13273430056774124]
	TIME [epoch: 2.68 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09410931503142851		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.09410931503142851 | validation: 0.10305799866881922]
	TIME [epoch: 2.68 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09456585053443316		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.09456585053443316 | validation: 0.09010209863397786]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_849.pth
	Model improved!!!
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07788226764697048		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.07788226764697048 | validation: 0.11923719866748149]
	TIME [epoch: 2.65 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08348733699816531		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.08348733699816531 | validation: 0.10243163268244074]
	TIME [epoch: 2.65 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07808365053327929		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.07808365053327929 | validation: 0.09225468832279841]
	TIME [epoch: 2.65 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07748085917731116		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.07748085917731116 | validation: 0.1055568878038875]
	TIME [epoch: 2.65 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07924619555826236		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.07924619555826236 | validation: 0.10042725178425714]
	TIME [epoch: 2.66 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07883215886340812		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.07883215886340812 | validation: 0.1124241038314171]
	TIME [epoch: 2.65 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07933815341141816		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.07933815341141816 | validation: 0.09442496356382792]
	TIME [epoch: 2.65 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08068267829488601		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.08068267829488601 | validation: 0.13709195900941998]
	TIME [epoch: 2.65 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09294595808314035		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.09294595808314035 | validation: 0.10384331845224877]
	TIME [epoch: 2.65 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0849247503447571		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.0849247503447571 | validation: 0.10197894235791978]
	TIME [epoch: 2.65 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07896789147543935		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.07896789147543935 | validation: 0.1080173516074775]
	TIME [epoch: 2.65 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07975179867942414		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.07975179867942414 | validation: 0.09910890297277236]
	TIME [epoch: 2.67 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07643695963576198		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.07643695963576198 | validation: 0.10043509933439335]
	TIME [epoch: 2.67 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07622135283281548		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.07622135283281548 | validation: 0.10419849666934296]
	TIME [epoch: 2.66 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07471485678379358		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.07471485678379358 | validation: 0.09366869142728661]
	TIME [epoch: 2.67 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07534796528155717		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.07534796528155717 | validation: 0.11618853380826964]
	TIME [epoch: 2.67 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07875918544180797		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.07875918544180797 | validation: 0.0860362775585774]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_866.pth
	Model improved!!!
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08160981231185627		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.08160981231185627 | validation: 0.11516550192292874]
	TIME [epoch: 2.68 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08092535983595658		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.08092535983595658 | validation: 0.1191406988122563]
	TIME [epoch: 2.68 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08834244101041712		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.08834244101041712 | validation: 0.10764572913340738]
	TIME [epoch: 2.68 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0954243219984351		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.0954243219984351 | validation: 0.1136482728415986]
	TIME [epoch: 2.68 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08052178714423672		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.08052178714423672 | validation: 0.1028917446182754]
	TIME [epoch: 2.68 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.075980733207942		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.075980733207942 | validation: 0.10326661290782271]
	TIME [epoch: 2.66 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07554817662186639		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.07554817662186639 | validation: 0.09891533544375138]
	TIME [epoch: 2.66 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07527851527362049		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.07527851527362049 | validation: 0.0958716752361431]
	TIME [epoch: 2.66 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07565958045589982		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.07565958045589982 | validation: 0.1022951442016802]
	TIME [epoch: 2.66 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0765435309807042		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.0765435309807042 | validation: 0.09426209343719492]
	TIME [epoch: 2.67 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07554074322774566		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.07554074322774566 | validation: 0.1003300209708764]
	TIME [epoch: 2.66 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07434060939137085		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.07434060939137085 | validation: 0.1020583013587244]
	TIME [epoch: 2.66 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07468191076565878		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.07468191076565878 | validation: 0.08199717110097152]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_879.pth
	Model improved!!!
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07816935033191531		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.07816935033191531 | validation: 0.1276321902410051]
	TIME [epoch: 2.67 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08205998053032999		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.08205998053032999 | validation: 0.09700875892485483]
	TIME [epoch: 2.67 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10062737091024383		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.10062737091024383 | validation: 0.10785990453952295]
	TIME [epoch: 2.67 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08388440350683644		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.08388440350683644 | validation: 0.10798797209533668]
	TIME [epoch: 2.66 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07862691637601289		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.07862691637601289 | validation: 0.0880150667312327]
	TIME [epoch: 2.67 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07314513590202244		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.07314513590202244 | validation: 0.09642247640007089]
	TIME [epoch: 2.67 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.072462276057862		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.072462276057862 | validation: 0.09611723029136221]
	TIME [epoch: 2.66 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07422966982415447		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.07422966982415447 | validation: 0.09894556783623328]
	TIME [epoch: 2.67 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0752051818571377		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.0752051818571377 | validation: 0.09419350434418779]
	TIME [epoch: 2.66 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07538732110985884		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.07538732110985884 | validation: 0.09670717427689962]
	TIME [epoch: 2.67 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07548113865272445		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.07548113865272445 | validation: 0.129456390041578]
	TIME [epoch: 2.66 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08410246264351898		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.08410246264351898 | validation: 0.0929402945286012]
	TIME [epoch: 2.67 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09089929643428912		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.09089929643428912 | validation: 0.10604455858376596]
	TIME [epoch: 2.66 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0764421548452335		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.0764421548452335 | validation: 0.09984186121220126]
	TIME [epoch: 2.66 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07347722204420082		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.07347722204420082 | validation: 0.0906925764058718]
	TIME [epoch: 2.66 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07193565981062675		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.07193565981062675 | validation: 0.09431925364274445]
	TIME [epoch: 2.66 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0743587689653678		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.0743587689653678 | validation: 0.09577998459151649]
	TIME [epoch: 2.66 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07357585919832076		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.07357585919832076 | validation: 0.09682691447187528]
	TIME [epoch: 2.66 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07773592899101928		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.07773592899101928 | validation: 0.10811419359396762]
	TIME [epoch: 2.67 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08841759490292782		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.08841759490292782 | validation: 0.11276237226879547]
	TIME [epoch: 2.67 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08101631471126947		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.08101631471126947 | validation: 0.08990484984925623]
	TIME [epoch: 2.66 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07301839680440132		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.07301839680440132 | validation: 0.09058792300736607]
	TIME [epoch: 2.67 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.073533085996877		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.073533085996877 | validation: 0.09266838203914957]
	TIME [epoch: 2.67 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07388527020112093		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.07388527020112093 | validation: 0.08901870086335223]
	TIME [epoch: 2.67 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07601111859327		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.07601111859327 | validation: 0.10064168025704788]
	TIME [epoch: 2.67 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0742202272985279		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.0742202272985279 | validation: 0.08827270854982852]
	TIME [epoch: 2.67 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07389609026092739		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.07389609026092739 | validation: 0.1178416348145247]
	TIME [epoch: 2.67 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07589646529510903		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.07589646529510903 | validation: 0.0900277213195328]
	TIME [epoch: 2.67 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08176450698184048		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.08176450698184048 | validation: 0.10124905639275444]
	TIME [epoch: 2.67 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0745789474617066		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.0745789474617066 | validation: 0.09088799578326559]
	TIME [epoch: 2.67 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07321525266163545		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.07321525266163545 | validation: 0.09865379639595571]
	TIME [epoch: 2.67 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07340719577011456		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.07340719577011456 | validation: 0.0859936386276581]
	TIME [epoch: 2.67 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07409358839303173		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.07409358839303173 | validation: 0.09592309121186088]
	TIME [epoch: 2.66 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0739291822466464		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.0739291822466464 | validation: 0.08708406634338937]
	TIME [epoch: 2.66 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07294029339783796		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.07294029339783796 | validation: 0.08567442242805913]
	TIME [epoch: 2.66 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07205801787810032		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.07205801787810032 | validation: 0.10052290457989227]
	TIME [epoch: 2.66 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07370295110530785		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.07370295110530785 | validation: 0.08219532270290791]
	TIME [epoch: 2.66 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07766522404252423		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.07766522404252423 | validation: 0.12182885997426307]
	TIME [epoch: 2.66 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08341384199508876		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.08341384199508876 | validation: 0.093330065715231]
	TIME [epoch: 2.66 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08573026752231762		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.08573026752231762 | validation: 0.09461728855543868]
	TIME [epoch: 2.66 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07653351678759818		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.07653351678759818 | validation: 0.09478057404847948]
	TIME [epoch: 2.67 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07145355429822574		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.07145355429822574 | validation: 0.09273183832072415]
	TIME [epoch: 2.66 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07099922012363946		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.07099922012363946 | validation: 0.08637539479546384]
	TIME [epoch: 2.66 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07063717755261642		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.07063717755261642 | validation: 0.09844842694093915]
	TIME [epoch: 2.66 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07146481515619911		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.07146481515619911 | validation: 0.08028738216630488]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_924.pth
	Model improved!!!
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07103346369002722		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.07103346369002722 | validation: 0.10748704538302484]
	TIME [epoch: 2.68 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07297940272352992		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.07297940272352992 | validation: 0.08756133085840913]
	TIME [epoch: 2.65 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07459010691957754		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.07459010691957754 | validation: 0.10594614006894228]
	TIME [epoch: 2.65 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07913176690724649		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.07913176690724649 | validation: 0.09686955853604132]
	TIME [epoch: 2.65 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08357041472608316		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.08357041472608316 | validation: 0.1005285542677476]
	TIME [epoch: 2.64 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0823023299309843		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.0823023299309843 | validation: 0.09294551937120814]
	TIME [epoch: 2.65 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07376275781766238		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.07376275781766238 | validation: 0.09183081631078319]
	TIME [epoch: 2.65 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07112072370696365		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.07112072370696365 | validation: 0.11379518697932255]
	TIME [epoch: 2.65 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07531656100823168		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.07531656100823168 | validation: 0.085803722460261]
	TIME [epoch: 2.64 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07530036530305872		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.07530036530305872 | validation: 0.09124638834985407]
	TIME [epoch: 2.65 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0700224437197004		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.0700224437197004 | validation: 0.09468039041323272]
	TIME [epoch: 2.64 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.068741337329235		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.068741337329235 | validation: 0.08939314576683688]
	TIME [epoch: 2.64 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07128334428235453		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.07128334428235453 | validation: 0.08483559598234702]
	TIME [epoch: 2.64 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0702239674765692		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.0702239674765692 | validation: 0.08939771921600448]
	TIME [epoch: 2.64 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06981780930645563		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.06981780930645563 | validation: 0.09301790982935178]
	TIME [epoch: 2.65 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07428902090912884		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.07428902090912884 | validation: 0.1071706554449488]
	TIME [epoch: 2.65 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07828235060440207		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.07828235060440207 | validation: 0.0897303419757987]
	TIME [epoch: 2.64 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07857157426282264		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.07857157426282264 | validation: 0.10176004146459912]
	TIME [epoch: 2.65 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07319599434970689		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.07319599434970689 | validation: 0.08290635387136808]
	TIME [epoch: 2.64 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06931120625931227		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.06931120625931227 | validation: 0.08222941062080802]
	TIME [epoch: 2.64 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06906779168460676		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.06906779168460676 | validation: 0.09246713526531995]
	TIME [epoch: 2.64 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06966230386982296		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.06966230386982296 | validation: 0.0791369182440207]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_946.pth
	Model improved!!!
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07308475412138261		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.07308475412138261 | validation: 0.10430106886020987]
	TIME [epoch: 2.64 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06908805868747274		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.06908805868747274 | validation: 0.08105682247825302]
	TIME [epoch: 2.65 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0703212287262368		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.0703212287262368 | validation: 0.10277278438312054]
	TIME [epoch: 2.64 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07163329584247698		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.07163329584247698 | validation: 0.0898131231854335]
	TIME [epoch: 2.65 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06817236684636639		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.06817236684636639 | validation: 0.09135294510722891]
	TIME [epoch: 2.64 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06725403350074806		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.06725403350074806 | validation: 0.08257388542001619]
	TIME [epoch: 2.65 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0698649469008095		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.0698649469008095 | validation: 0.10053881074219889]
	TIME [epoch: 2.65 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07211248827716045		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.07211248827716045 | validation: 0.08793096264377064]
	TIME [epoch: 2.64 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0815414591496597		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.0815414591496597 | validation: 0.08682886393243769]
	TIME [epoch: 2.65 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08490749934100109		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.08490749934100109 | validation: 0.12376618100372667]
	TIME [epoch: 2.65 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07903731289592007		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.07903731289592007 | validation: 0.0848337876459062]
	TIME [epoch: 2.65 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06974044344842147		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.06974044344842147 | validation: 0.08123633682741815]
	TIME [epoch: 2.64 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06851917209554896		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.06851917209554896 | validation: 0.09852473022473682]
	TIME [epoch: 2.64 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06848979465933931		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.06848979465933931 | validation: 0.08294299846321104]
	TIME [epoch: 2.65 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06855841084394658		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.06855841084394658 | validation: 0.0977963264100169]
	TIME [epoch: 2.64 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06914224783635288		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.06914224783635288 | validation: 0.08971482897048423]
	TIME [epoch: 2.64 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07076826185810103		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.07076826185810103 | validation: 0.1011368234563666]
	TIME [epoch: 2.65 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07070392351619702		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.07070392351619702 | validation: 0.09001694120711545]
	TIME [epoch: 2.65 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07061222894206177		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.07061222894206177 | validation: 0.09164083271701101]
	TIME [epoch: 2.65 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06450133465822949		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.06450133465822949 | validation: 0.09237722065189266]
	TIME [epoch: 2.65 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0687429379048588		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.0687429379048588 | validation: 0.08320511914132292]
	TIME [epoch: 2.65 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06870677287891801		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.06870677287891801 | validation: 0.09613981633382616]
	TIME [epoch: 2.64 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06979758586373558		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.06979758586373558 | validation: 0.09038858029139411]
	TIME [epoch: 2.64 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07527767396902729		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.07527767396902729 | validation: 0.0961645094995186]
	TIME [epoch: 2.64 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0780167798695236		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.0780167798695236 | validation: 0.08252629312396097]
	TIME [epoch: 2.65 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07356484426384392		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.07356484426384392 | validation: 0.09430538701386434]
	TIME [epoch: 2.64 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06752331986559472		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.06752331986559472 | validation: 0.08449819229287747]
	TIME [epoch: 2.64 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06843503621443331		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.06843503621443331 | validation: 0.0899470248867759]
	TIME [epoch: 2.64 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06653537509769368		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.06653537509769368 | validation: 0.09251005335740063]
	TIME [epoch: 2.64 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06773116711921297		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.06773116711921297 | validation: 0.07626403984831831]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_976.pth
	Model improved!!!
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07502701777082932		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.07502701777082932 | validation: 0.10104468474289159]
	TIME [epoch: 2.65 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07453512562423355		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.07453512562423355 | validation: 0.08486233485103242]
	TIME [epoch: 2.65 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07195758379143359		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.07195758379143359 | validation: 0.10050157062005981]
	TIME [epoch: 2.65 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0694984562235349		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.0694984562235349 | validation: 0.0895358746706967]
	TIME [epoch: 2.64 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07017576472066792		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.07017576472066792 | validation: 0.08047406263558332]
	TIME [epoch: 2.65 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0677662034827564		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.0677662034827564 | validation: 0.09062726290865358]
	TIME [epoch: 2.65 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0661231904065007		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.0661231904065007 | validation: 0.07989559140685543]
	TIME [epoch: 2.64 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06853928363421605		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.06853928363421605 | validation: 0.09290718616587558]
	TIME [epoch: 2.64 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06638914860014233		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.06638914860014233 | validation: 0.08728952376924552]
	TIME [epoch: 2.65 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07139707016741768		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.07139707016741768 | validation: 0.0808389164596864]
	TIME [epoch: 2.65 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07447885350073634		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.07447885350073634 | validation: 0.10375611735427555]
	TIME [epoch: 2.65 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07214105791237962		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.07214105791237962 | validation: 0.0843799720884626]
	TIME [epoch: 2.64 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06703387240855		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.06703387240855 | validation: 0.08369204923826781]
	TIME [epoch: 2.65 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06719315284704834		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.06719315284704834 | validation: 0.09324731256625163]
	TIME [epoch: 2.64 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0674156889697087		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.0674156889697087 | validation: 0.0788702321679109]
	TIME [epoch: 2.65 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06791015370803645		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.06791015370803645 | validation: 0.09266422445286084]
	TIME [epoch: 2.64 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06791694493673861		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.06791694493673861 | validation: 0.08397261031545172]
	TIME [epoch: 2.65 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06729262240183788		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.06729262240183788 | validation: 0.08796089316266777]
	TIME [epoch: 2.64 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06561528314413675		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.06561528314413675 | validation: 0.08175636115777554]
	TIME [epoch: 2.64 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06714663400859021		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.06714663400859021 | validation: 0.09235630720779664]
	TIME [epoch: 2.64 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06934802520753361		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.06934802520753361 | validation: 0.08653742953470052]
	TIME [epoch: 2.66 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07406941444390734		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.07406941444390734 | validation: 0.1120987649603459]
	TIME [epoch: 2.64 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07667708577021239		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.07667708577021239 | validation: 0.08666199434644724]
	TIME [epoch: 2.65 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06891402140338539		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.06891402140338539 | validation: 0.0810350571352429]
	TIME [epoch: 2.65 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06790443602747419		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.06790443602747419 | validation: 0.08390225430232048]
	TIME [epoch: 172 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06531256454687544		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.06531256454687544 | validation: 0.08327839495676131]
	TIME [epoch: 5.71 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06549678328704828		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.06549678328704828 | validation: 0.08175124175276903]
	TIME [epoch: 5.7 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0638351057526553		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.0638351057526553 | validation: 0.08859224078833766]
	TIME [epoch: 5.7 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06641346651846612		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.06641346651846612 | validation: 0.08386155898639025]
	TIME [epoch: 5.7 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06483346522030599		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.06483346522030599 | validation: 0.10301573517797083]
	TIME [epoch: 5.71 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06769436470491817		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.06769436470491817 | validation: 0.08715962186189498]
	TIME [epoch: 5.7 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07756177726940247		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.07756177726940247 | validation: 0.09042588706676213]
	TIME [epoch: 5.7 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07134259216673287		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.07134259216673287 | validation: 0.09322925558578425]
	TIME [epoch: 5.7 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06840403933470093		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.06840403933470093 | validation: 0.0856554519141145]
	TIME [epoch: 5.7 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06633530737939602		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.06633530737939602 | validation: 0.09534114966859442]
	TIME [epoch: 5.7 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06908666063628184		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.06908666063628184 | validation: 0.07824192787173233]
	TIME [epoch: 5.71 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06727917769259173		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.06727917769259173 | validation: 0.07941983096247189]
	TIME [epoch: 5.71 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06466826641428243		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.06466826641428243 | validation: 0.08527163648974753]
	TIME [epoch: 5.7 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06511565505333945		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.06511565505333945 | validation: 0.07592819626243913]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_1015.pth
	Model improved!!!
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06548071180424844		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.06548071180424844 | validation: 0.08658736573787526]
	TIME [epoch: 5.7 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06855836218472397		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.06855836218472397 | validation: 0.08105480930576328]
	TIME [epoch: 5.7 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06651045588185796		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.06651045588185796 | validation: 0.08105597000023654]
	TIME [epoch: 5.7 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0642843951640164		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.0642843951640164 | validation: 0.09203299411883936]
	TIME [epoch: 5.7 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06740301810261153		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.06740301810261153 | validation: 0.07295943518687005]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_1020.pth
	Model improved!!!
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07071500396845858		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.07071500396845858 | validation: 0.11800164563172127]
	TIME [epoch: 5.7 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07409569602627143		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.07409569602627143 | validation: 0.07592099140187666]
	TIME [epoch: 5.7 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06703438930769164		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.06703438930769164 | validation: 0.08388186435513768]
	TIME [epoch: 5.7 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06639768772989614		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.06639768772989614 | validation: 0.08620121964680712]
	TIME [epoch: 5.7 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06888101959238337		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.06888101959238337 | validation: 0.09770502962150782]
	TIME [epoch: 5.7 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07382179715942806		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.07382179715942806 | validation: 0.08110276897098052]
	TIME [epoch: 5.7 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06866475055439857		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.06866475055439857 | validation: 0.0924806109198334]
	TIME [epoch: 5.71 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06563227910567627		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.06563227910567627 | validation: 0.07963826136556892]
	TIME [epoch: 5.7 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06438492574992426		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.06438492574992426 | validation: 0.08847544773727864]
	TIME [epoch: 5.7 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06420744278746891		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.06420744278746891 | validation: 0.08287286513860881]
	TIME [epoch: 5.71 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0646570672281667		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.0646570672281667 | validation: 0.07724906100573688]
	TIME [epoch: 5.7 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06461980927725197		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.06461980927725197 | validation: 0.08134894918163399]
	TIME [epoch: 5.71 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06165566557181963		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.06165566557181963 | validation: 0.0869271680917367]
	TIME [epoch: 5.7 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06240900001436887		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.06240900001436887 | validation: 0.08514003537017319]
	TIME [epoch: 5.7 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06469413115898244		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.06469413115898244 | validation: 0.08129976040027953]
	TIME [epoch: 5.7 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06411282003754443		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.06411282003754443 | validation: 0.08501898524382459]
	TIME [epoch: 5.7 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0642732410003999		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.0642732410003999 | validation: 0.07782399733804526]
	TIME [epoch: 5.71 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06405076626171378		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.06405076626171378 | validation: 0.07728547944550435]
	TIME [epoch: 5.7 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0623182491280415		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.0623182491280415 | validation: 0.08388906180475802]
	TIME [epoch: 5.7 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06516934910012426		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.06516934910012426 | validation: 0.07564347243390468]
	TIME [epoch: 5.7 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06368520314966185		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.06368520314966185 | validation: 0.08985050280599059]
	TIME [epoch: 5.7 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0691493883025488		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.0691493883025488 | validation: 0.09616335328028441]
	TIME [epoch: 5.7 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07267979653514385		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.07267979653514385 | validation: 0.0814007694646488]
	TIME [epoch: 5.71 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07416802233251879		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.07416802233251879 | validation: 0.09063894876446671]
	TIME [epoch: 5.7 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0645923135194016		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.0645923135194016 | validation: 0.08198907971913298]
	TIME [epoch: 5.7 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06209131788276442		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.06209131788276442 | validation: 0.09388627210152238]
	TIME [epoch: 5.7 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06478860692527706		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.06478860692527706 | validation: 0.07567798149275387]
	TIME [epoch: 5.7 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06163989176733109		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.06163989176733109 | validation: 0.0719217882397809]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_1048.pth
	Model improved!!!
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06445942234886959		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.06445942234886959 | validation: 0.08923456653604628]
	TIME [epoch: 5.7 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0668622883264061		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.0668622883264061 | validation: 0.08365542830395815]
	TIME [epoch: 5.7 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06442776246529108		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.06442776246529108 | validation: 0.07357094634286065]
	TIME [epoch: 5.7 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06459313492544597		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.06459313492544597 | validation: 0.09087413474345662]
	TIME [epoch: 5.7 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06630050317412661		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.06630050317412661 | validation: 0.07562391123553924]
	TIME [epoch: 5.71 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0675967405268853		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.0675967405268853 | validation: 0.07361443992576755]
	TIME [epoch: 5.7 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061427351919370134		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.061427351919370134 | validation: 0.08004661364556687]
	TIME [epoch: 5.7 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06507246959691901		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.06507246959691901 | validation: 0.07924522089510865]
	TIME [epoch: 5.7 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06442319653785379		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.06442319653785379 | validation: 0.08192469429252924]
	TIME [epoch: 5.69 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06756791566390287		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.06756791566390287 | validation: 0.082983021007842]
	TIME [epoch: 5.7 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06408582361767802		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.06408582361767802 | validation: 0.077452640054015]
	TIME [epoch: 5.7 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06319195850950629		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.06319195850950629 | validation: 0.07551503856264469]
	TIME [epoch: 5.7 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06243648761611582		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.06243648761611582 | validation: 0.08700195913576875]
	TIME [epoch: 5.7 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06531816214764569		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.06531816214764569 | validation: 0.08426076522361012]
	TIME [epoch: 5.7 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06742785983760292		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.06742785983760292 | validation: 0.08625768675866483]
	TIME [epoch: 5.7 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06764282828659585		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.06764282828659585 | validation: 0.07808989798588277]
	TIME [epoch: 5.71 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062144562555880654		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.062144562555880654 | validation: 0.08456871149437933]
	TIME [epoch: 5.7 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06157467488800422		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.06157467488800422 | validation: 0.07515404478698906]
	TIME [epoch: 5.7 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06392855201383092		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.06392855201383092 | validation: 0.0876109091631756]
	TIME [epoch: 5.69 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06279730514154078		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.06279730514154078 | validation: 0.08489525859181302]
	TIME [epoch: 5.7 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06835052224660906		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.06835052224660906 | validation: 0.07362119111859385]
	TIME [epoch: 5.7 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06748332656066945		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.06748332656066945 | validation: 0.08520156668959035]
	TIME [epoch: 5.72 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06720624970964145		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.06720624970964145 | validation: 0.07430896224102175]
	TIME [epoch: 5.7 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061912359134252325		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.061912359134252325 | validation: 0.08035896852104928]
	TIME [epoch: 5.71 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0635789715777665		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.0635789715777665 | validation: 0.07697959729295384]
	TIME [epoch: 5.7 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06225723091385554		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.06225723091385554 | validation: 0.07164696806006009]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_1074.pth
	Model improved!!!
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060947228633175934		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.060947228633175934 | validation: 0.08445915156142647]
	TIME [epoch: 5.7 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06237589989496311		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.06237589989496311 | validation: 0.07745325933411346]
	TIME [epoch: 5.7 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06263951770254235		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.06263951770254235 | validation: 0.08434220548666539]
	TIME [epoch: 5.69 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06452450888395476		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.06452450888395476 | validation: 0.08267200645130271]
	TIME [epoch: 5.7 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06557858282101817		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.06557858282101817 | validation: 0.0797119328012562]
	TIME [epoch: 5.7 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06401042822483868		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.06401042822483868 | validation: 0.08006976732509462]
	TIME [epoch: 5.7 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06286082946179776		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.06286082946179776 | validation: 0.07393011786471847]
	TIME [epoch: 5.69 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07145537389047854		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.07145537389047854 | validation: 0.091388819681288]
	TIME [epoch: 5.69 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06519782716416189		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.06519782716416189 | validation: 0.08256684693192894]
	TIME [epoch: 5.7 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06468414014555711		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.06468414014555711 | validation: 0.07154902575692473]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_1084.pth
	Model improved!!!
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0623653152822283		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.0623653152822283 | validation: 0.0814728405394969]
	TIME [epoch: 5.7 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06169446215906456		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.06169446215906456 | validation: 0.08344451332054996]
	TIME [epoch: 5.7 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06256431423244224		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.06256431423244224 | validation: 0.06865658396507499]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_1087.pth
	Model improved!!!
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.065400294520057		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.065400294520057 | validation: 0.08637165014174236]
	TIME [epoch: 5.7 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061138406324650625		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.061138406324650625 | validation: 0.07852634429391275]
	TIME [epoch: 5.7 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06333127492992764		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.06333127492992764 | validation: 0.08212227275709356]
	TIME [epoch: 5.7 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06243449340424387		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.06243449340424387 | validation: 0.08473563958668669]
	TIME [epoch: 5.7 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0678676790509931		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.0678676790509931 | validation: 0.08784457035316887]
	TIME [epoch: 5.7 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06864338606194821		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.06864338606194821 | validation: 0.08358915096859537]
	TIME [epoch: 5.69 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06575126052710704		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.06575126052710704 | validation: 0.08257062184522486]
	TIME [epoch: 5.69 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06342981112624137		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.06342981112624137 | validation: 0.08865434622380858]
	TIME [epoch: 5.71 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06204143323820369		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.06204143323820369 | validation: 0.07123229443060088]
	TIME [epoch: 5.7 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06352361891017579		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.06352361891017579 | validation: 0.07579851983609931]
	TIME [epoch: 5.69 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062184780960537436		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.062184780960537436 | validation: 0.08476691157171687]
	TIME [epoch: 5.69 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06258868550888684		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.06258868550888684 | validation: 0.07978010221082582]
	TIME [epoch: 5.69 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06149327653380689		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.06149327653380689 | validation: 0.07835625471009738]
	TIME [epoch: 5.7 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06330317088953559		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.06330317088953559 | validation: 0.07444821972100672]
	TIME [epoch: 5.69 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06060169701277598		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.06060169701277598 | validation: 0.08206258285651785]
	TIME [epoch: 5.7 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06324325464523224		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.06324325464523224 | validation: 0.08772996147466094]
	TIME [epoch: 5.7 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0612133521928774		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.0612133521928774 | validation: 0.07531411337741913]
	TIME [epoch: 5.7 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06263532997050587		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.06263532997050587 | validation: 0.0818446863939879]
	TIME [epoch: 5.7 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06143115844310249		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.06143115844310249 | validation: 0.08091674648506192]
	TIME [epoch: 5.7 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06495173337214946		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.06495173337214946 | validation: 0.07237879044373287]
	TIME [epoch: 5.7 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06278552115706901		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.06278552115706901 | validation: 0.08370501510114632]
	TIME [epoch: 5.7 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06464446055338507		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.06464446055338507 | validation: 0.07850123736417317]
	TIME [epoch: 5.7 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061476684440344		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.061476684440344 | validation: 0.07832974016237498]
	TIME [epoch: 5.7 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06477005176372759		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.06477005176372759 | validation: 0.08042357306579952]
	TIME [epoch: 5.7 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06407884608729045		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.06407884608729045 | validation: 0.077789200989772]
	TIME [epoch: 5.69 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05998357968243962		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.05998357968243962 | validation: 0.07700227007989917]
	TIME [epoch: 5.7 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06144817152477109		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.06144817152477109 | validation: 0.07771762632395862]
	TIME [epoch: 5.69 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0615078679486154		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.0615078679486154 | validation: 0.09412693544531608]
	TIME [epoch: 5.71 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06462130202760412		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.06462130202760412 | validation: 0.07128366885734816]
	TIME [epoch: 5.7 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06289382817076343		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.06289382817076343 | validation: 0.07489933476889525]
	TIME [epoch: 5.7 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060810641697920116		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.060810641697920116 | validation: 0.09260165990306624]
	TIME [epoch: 5.69 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06435850840040752		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.06435850840040752 | validation: 0.07764494018975102]
	TIME [epoch: 5.7 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06838970153393821		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.06838970153393821 | validation: 0.08149150133577188]
	TIME [epoch: 5.7 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06263366851194588		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.06263366851194588 | validation: 0.07662519957957642]
	TIME [epoch: 5.71 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06377539164907824		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.06377539164907824 | validation: 0.08285860919902888]
	TIME [epoch: 6.53 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061699007526411304		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.061699007526411304 | validation: 0.07901070431028709]
	TIME [epoch: 5.7 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06269996617126701		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.06269996617126701 | validation: 0.07421972518665758]
	TIME [epoch: 5.7 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06160726836299304		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.06160726836299304 | validation: 0.07575271992985012]
	TIME [epoch: 5.69 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06332476777742943		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.06332476777742943 | validation: 0.0836905961295851]
	TIME [epoch: 5.71 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06459120978825929		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.06459120978825929 | validation: 0.07772797167924546]
	TIME [epoch: 5.7 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06176270789089661		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.06176270789089661 | validation: 0.07476928438343106]
	TIME [epoch: 5.7 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06163609365813625		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.06163609365813625 | validation: 0.07505888021023971]
	TIME [epoch: 5.7 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06020193248794831		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.06020193248794831 | validation: 0.08626375621917472]
	TIME [epoch: 5.69 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06156917510657163		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.06156917510657163 | validation: 0.07219498036420714]
	TIME [epoch: 5.7 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06083987942994936		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.06083987942994936 | validation: 0.08167009566326537]
	TIME [epoch: 5.69 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06071473249179336		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.06071473249179336 | validation: 0.07451602287947424]
	TIME [epoch: 5.7 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06236633618610087		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.06236633618610087 | validation: 0.07090856045896195]
	TIME [epoch: 5.69 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0593058580339883		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.0593058580339883 | validation: 0.08162474264755276]
	TIME [epoch: 5.69 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0594266606844697		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.0594266606844697 | validation: 0.07639854640567459]
	TIME [epoch: 5.7 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06060326384287071		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.06060326384287071 | validation: 0.07324095031077887]
	TIME [epoch: 5.7 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06262308648419584		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.06262308648419584 | validation: 0.08689001971824337]
	TIME [epoch: 5.7 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06144511045566169		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.06144511045566169 | validation: 0.06882821881140382]
	TIME [epoch: 5.69 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059245350809202735		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.059245350809202735 | validation: 0.08445508089833415]
	TIME [epoch: 5.7 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06196905107801679		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.06196905107801679 | validation: 0.07699887061489182]
	TIME [epoch: 5.7 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06788722310016153		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.06788722310016153 | validation: 0.08641399529595721]
	TIME [epoch: 5.7 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06251652024261313		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.06251652024261313 | validation: 0.0825378413771427]
	TIME [epoch: 5.7 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06055753773439058		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.06055753773439058 | validation: 0.06914231550851767]
	TIME [epoch: 5.7 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06222227435689561		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.06222227435689561 | validation: 0.07391832479416956]
	TIME [epoch: 5.69 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05869238405173485		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.05869238405173485 | validation: 0.07669291407276277]
	TIME [epoch: 5.7 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061445330337328165		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.061445330337328165 | validation: 0.07332878030188854]
	TIME [epoch: 5.7 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05994271302526702		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.05994271302526702 | validation: 0.06829371249271558]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_1148.pth
	Model improved!!!
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06026362787639517		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.06026362787639517 | validation: 0.07800624262860506]
	TIME [epoch: 5.69 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059465926817004924		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.059465926817004924 | validation: 0.07833451069217276]
	TIME [epoch: 5.69 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06880803707288059		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.06880803707288059 | validation: 0.08184064617285579]
	TIME [epoch: 5.7 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06340264742422706		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.06340264742422706 | validation: 0.07498256984838306]
	TIME [epoch: 5.7 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06035064169698167		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.06035064169698167 | validation: 0.08372559941727302]
	TIME [epoch: 5.69 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06112620019588324		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.06112620019588324 | validation: 0.0777812059173281]
	TIME [epoch: 5.69 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06067134852774553		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.06067134852774553 | validation: 0.07587121400991803]
	TIME [epoch: 5.7 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05950684101097041		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.05950684101097041 | validation: 0.07051006953249105]
	TIME [epoch: 5.7 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05898197671268776		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.05898197671268776 | validation: 0.07746117049133328]
	TIME [epoch: 5.7 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05953482079314204		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.05953482079314204 | validation: 0.0787496773245201]
	TIME [epoch: 5.7 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06089107836155705		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.06089107836155705 | validation: 0.07270808991656096]
	TIME [epoch: 5.7 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05975132138175702		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.05975132138175702 | validation: 0.06981462090583392]
	TIME [epoch: 5.7 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05949440664841246		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.05949440664841246 | validation: 0.0766327504508812]
	TIME [epoch: 5.7 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05940878018483607		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.05940878018483607 | validation: 0.07653003726770918]
	TIME [epoch: 5.71 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059883662832501704		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.059883662832501704 | validation: 0.07703957069692337]
	TIME [epoch: 5.7 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05914588120518069		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.05914588120518069 | validation: 0.07314350607049762]
	TIME [epoch: 5.7 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059975511061773165		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.059975511061773165 | validation: 0.06853467395813373]
	TIME [epoch: 5.7 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060338634648896945		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.060338634648896945 | validation: 0.084101249153675]
	TIME [epoch: 5.69 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06060110105405617		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.06060110105405617 | validation: 0.07453332902009918]
	TIME [epoch: 5.7 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05987996318721235		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.05987996318721235 | validation: 0.07928103494871429]
	TIME [epoch: 5.7 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06463576440552618		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.06463576440552618 | validation: 0.07657548923977393]
	TIME [epoch: 5.7 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06158933585586009		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.06158933585586009 | validation: 0.06890186142200376]
	TIME [epoch: 5.7 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059659072017474515		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.059659072017474515 | validation: 0.0758652536652826]
	TIME [epoch: 5.7 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06238564418898594		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.06238564418898594 | validation: 0.07405019942339901]
	TIME [epoch: 5.7 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059707107145936356		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.059707107145936356 | validation: 0.07981132359656135]
	TIME [epoch: 5.71 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061410064439404034		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.061410064439404034 | validation: 0.07084993840878538]
	TIME [epoch: 5.7 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06287311748723286		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.06287311748723286 | validation: 0.07649188419589614]
	TIME [epoch: 5.7 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06029559659678377		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.06029559659678377 | validation: 0.07401992269777889]
	TIME [epoch: 5.7 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05887549678580888		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.05887549678580888 | validation: 0.0709024657091477]
	TIME [epoch: 5.7 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057667872303355		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.057667872303355 | validation: 0.06761650578989328]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_1178.pth
	Model improved!!!
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059151451284615034		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.059151451284615034 | validation: 0.06899007536700068]
	TIME [epoch: 5.71 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05815868241948558		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.05815868241948558 | validation: 0.06643931297099083]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_1180.pth
	Model improved!!!
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06097604152304926		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.06097604152304926 | validation: 0.07715676870029259]
	TIME [epoch: 5.7 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06039377915819598		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.06039377915819598 | validation: 0.07346654982736818]
	TIME [epoch: 5.71 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0583553520899046		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.0583553520899046 | validation: 0.0667320064097174]
	TIME [epoch: 5.7 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05853971368824446		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.05853971368824446 | validation: 0.08035889004706041]
	TIME [epoch: 5.7 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05828790830211441		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.05828790830211441 | validation: 0.07787149023066939]
	TIME [epoch: 5.7 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0597647918913342		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.0597647918913342 | validation: 0.07834724774870379]
	TIME [epoch: 5.7 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05811887248894654		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.05811887248894654 | validation: 0.07457201829924245]
	TIME [epoch: 5.7 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06064587639202145		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.06064587639202145 | validation: 0.08245262088277498]
	TIME [epoch: 5.7 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06008871080286581		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.06008871080286581 | validation: 0.0837844786172982]
	TIME [epoch: 5.7 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06014728931991897		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.06014728931991897 | validation: 0.07312500664700372]
	TIME [epoch: 5.71 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06321428863621323		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.06321428863621323 | validation: 0.06963104603893754]
	TIME [epoch: 5.7 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05876780328209314		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.05876780328209314 | validation: 0.07750255735761895]
	TIME [epoch: 5.71 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05881554708248195		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.05881554708248195 | validation: 0.06838780437232574]
	TIME [epoch: 5.7 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061809093531353096		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.061809093531353096 | validation: 0.07741081986749888]
	TIME [epoch: 5.7 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05798445552913645		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.05798445552913645 | validation: 0.07433885111499396]
	TIME [epoch: 5.7 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05754903173652342		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.05754903173652342 | validation: 0.07096416118682433]
	TIME [epoch: 5.7 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05788217313731231		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.05788217313731231 | validation: 0.07240252682264171]
	TIME [epoch: 5.7 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057361896100101666		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.057361896100101666 | validation: 0.0681448436755929]
	TIME [epoch: 5.71 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058259651840497355		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.058259651840497355 | validation: 0.08211604846308153]
	TIME [epoch: 5.7 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059496505096374		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.059496505096374 | validation: 0.07517546332767867]
	TIME [epoch: 5.7 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06100326021964882		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.06100326021964882 | validation: 0.07348619514871728]
	TIME [epoch: 5.69 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0630538444182978		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.0630538444182978 | validation: 0.07448307171121368]
	TIME [epoch: 5.69 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059905175615459924		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.059905175615459924 | validation: 0.08087055957480932]
	TIME [epoch: 5.69 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06135465097064014		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.06135465097064014 | validation: 0.07914298582611151]
	TIME [epoch: 5.7 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05970501351982027		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.05970501351982027 | validation: 0.07067867687122524]
	TIME [epoch: 5.7 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0603132867131969		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.0603132867131969 | validation: 0.07347752954191306]
	TIME [epoch: 5.7 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05838302466958252		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.05838302466958252 | validation: 0.07619435625931796]
	TIME [epoch: 5.7 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060578664162603		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.060578664162603 | validation: 0.07682097636963403]
	TIME [epoch: 5.69 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057565675865699585		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.057565675865699585 | validation: 0.07204140615469595]
	TIME [epoch: 5.7 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05715761600584652		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.05715761600584652 | validation: 0.07506424471619139]
	TIME [epoch: 5.7 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0590345157511587		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.0590345157511587 | validation: 0.07655711795417425]
	TIME [epoch: 5.7 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06051555669080985		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.06051555669080985 | validation: 0.06850236143861449]
	TIME [epoch: 5.7 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05695341251481423		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.05695341251481423 | validation: 0.08232817327846496]
	TIME [epoch: 5.69 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06069036552828901		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.06069036552828901 | validation: 0.06855242560990125]
	TIME [epoch: 5.7 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06020410537234772		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.06020410537234772 | validation: 0.07755902484159884]
	TIME [epoch: 5.69 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05854388047809555		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.05854388047809555 | validation: 0.0736179002876001]
	TIME [epoch: 5.69 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05921145278399781		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.05921145278399781 | validation: 0.07177143459930113]
	TIME [epoch: 5.69 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057852400732236235		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.057852400732236235 | validation: 0.08318850260253274]
	TIME [epoch: 5.69 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06046572512013253		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.06046572512013253 | validation: 0.06806695295587309]
	TIME [epoch: 5.7 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05782627831816076		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.05782627831816076 | validation: 0.06473571093269402]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_1220.pth
	Model improved!!!
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05848914409701691		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.05848914409701691 | validation: 0.07251439220002746]
	TIME [epoch: 5.69 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057006171590529016		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.057006171590529016 | validation: 0.0745442098004219]
	TIME [epoch: 5.69 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05677223775493		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.05677223775493 | validation: 0.07916468476229484]
	TIME [epoch: 5.7 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05956677020125199		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.05956677020125199 | validation: 0.0741725405822684]
	TIME [epoch: 5.7 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05902891980739623		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.05902891980739623 | validation: 0.0709501710388051]
	TIME [epoch: 5.7 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0571709281046782		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.0571709281046782 | validation: 0.07296136545143878]
	TIME [epoch: 5.69 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05930651766513379		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.05930651766513379 | validation: 0.07423301216959484]
	TIME [epoch: 5.7 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05794886295331991		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.05794886295331991 | validation: 0.07282807762066007]
	TIME [epoch: 5.69 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058277417088056054		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.058277417088056054 | validation: 0.07544505676268448]
	TIME [epoch: 5.69 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05705333426624714		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.05705333426624714 | validation: 0.07324141833431659]
	TIME [epoch: 5.7 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05609208173675666		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.05609208173675666 | validation: 0.06698493457117853]
	TIME [epoch: 5.7 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05920820066384295		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.05920820066384295 | validation: 0.0787265378882524]
	TIME [epoch: 5.69 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05936257006828769		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.05936257006828769 | validation: 0.07616048469074206]
	TIME [epoch: 5.7 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05947901779558617		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.05947901779558617 | validation: 0.06747785333313988]
	TIME [epoch: 5.69 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05702746471350861		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.05702746471350861 | validation: 0.0730688492835022]
	TIME [epoch: 5.7 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06125150989943874		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.06125150989943874 | validation: 0.07227503127804417]
	TIME [epoch: 5.69 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058696404370246785		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.058696404370246785 | validation: 0.08146728512929258]
	TIME [epoch: 5.7 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06164105008360554		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.06164105008360554 | validation: 0.0731213123972842]
	TIME [epoch: 5.69 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05899216775965373		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.05899216775965373 | validation: 0.0687134166474855]
	TIME [epoch: 5.69 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05733821269920671		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.05733821269920671 | validation: 0.07327787615819843]
	TIME [epoch: 5.7 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05611462130544128		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.05611462130544128 | validation: 0.0797496000709228]
	TIME [epoch: 5.7 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05717699340228975		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.05717699340228975 | validation: 0.07530234250778259]
	TIME [epoch: 5.7 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056682558578539155		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.056682558578539155 | validation: 0.06976697335758374]
	TIME [epoch: 5.7 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0580993740898693		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.0580993740898693 | validation: 0.07640919256620621]
	TIME [epoch: 5.69 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058144316238104815		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.058144316238104815 | validation: 0.06645064343356609]
	TIME [epoch: 5.71 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056958360536521424		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.056958360536521424 | validation: 0.06965609297539448]
	TIME [epoch: 5.69 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05887942603831421		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.05887942603831421 | validation: 0.07460549828711051]
	TIME [epoch: 5.7 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05687204277298207		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.05687204277298207 | validation: 0.06699468432236676]
	TIME [epoch: 5.69 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05707134873396056		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.05707134873396056 | validation: 0.06466397249553789]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_1249.pth
	Model improved!!!
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05656608867673601		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.05656608867673601 | validation: 0.07945689100049612]
	TIME [epoch: 5.7 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055691588570182214		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.055691588570182214 | validation: 0.06849787539684192]
	TIME [epoch: 5.7 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057506206710860694		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.057506206710860694 | validation: 0.07531460154648284]
	TIME [epoch: 5.7 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05641176106575765		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.05641176106575765 | validation: 0.06813833311934382]
	TIME [epoch: 5.7 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057201383019081786		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.057201383019081786 | validation: 0.07149067330195628]
	TIME [epoch: 5.69 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057703056960499474		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.057703056960499474 | validation: 0.08662070262569939]
	TIME [epoch: 5.69 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06472350254263909		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.06472350254263909 | validation: 0.0698132571985175]
	TIME [epoch: 5.7 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058580873129478336		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.058580873129478336 | validation: 0.06867928116895534]
	TIME [epoch: 5.69 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0601125322724985		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.0601125322724985 | validation: 0.07406207300289157]
	TIME [epoch: 5.7 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058206075482869844		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.058206075482869844 | validation: 0.07501801679574202]
	TIME [epoch: 5.7 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05512060552298445		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.05512060552298445 | validation: 0.07031131482992262]
	TIME [epoch: 5.69 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05832079122467476		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.05832079122467476 | validation: 0.07770168406543068]
	TIME [epoch: 5.7 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05602341518003906		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.05602341518003906 | validation: 0.07245579248317562]
	TIME [epoch: 5.7 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05729374554030505		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.05729374554030505 | validation: 0.07071338534357567]
	TIME [epoch: 5.7 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05794962738302314		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.05794962738302314 | validation: 0.07931407835152596]
	TIME [epoch: 5.69 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059532322316520156		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.059532322316520156 | validation: 0.06801360505494276]
	TIME [epoch: 5.7 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05675913003142348		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.05675913003142348 | validation: 0.06925906141045568]
	TIME [epoch: 5.71 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056823019191175814		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.056823019191175814 | validation: 0.06903169659959872]
	TIME [epoch: 5.7 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0577644522703746		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.0577644522703746 | validation: 0.06893996160628206]
	TIME [epoch: 5.69 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058825723800321854		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.058825723800321854 | validation: 0.0730430889059406]
	TIME [epoch: 5.7 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057331343296724895		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.057331343296724895 | validation: 0.06835849756701581]
	TIME [epoch: 5.69 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05718962885031708		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.05718962885031708 | validation: 0.07638804770200715]
	TIME [epoch: 5.7 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05869203407085996		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.05869203407085996 | validation: 0.07694345745752343]
	TIME [epoch: 5.7 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056249812151016786		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.056249812151016786 | validation: 0.06919941235596666]
	TIME [epoch: 5.7 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058655456280457194		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.058655456280457194 | validation: 0.07985151801060802]
	TIME [epoch: 5.7 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05741051731522918		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.05741051731522918 | validation: 0.07895449385571257]
	TIME [epoch: 5.7 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05624452100296807		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.05624452100296807 | validation: 0.06637225485934388]
	TIME [epoch: 5.7 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05750343537574221		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.05750343537574221 | validation: 0.07269436902699826]
	TIME [epoch: 5.7 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059070017460109064		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.059070017460109064 | validation: 0.0699718921998487]
	TIME [epoch: 5.69 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05627563369178736		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.05627563369178736 | validation: 0.06757946817383709]
	TIME [epoch: 5.7 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055653536052875215		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.055653536052875215 | validation: 0.0725429561993494]
	TIME [epoch: 5.69 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05821845137168964		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.05821845137168964 | validation: 0.07113876480250227]
	TIME [epoch: 5.7 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05773614137558912		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.05773614137558912 | validation: 0.07867551706906956]
	TIME [epoch: 5.7 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05846513322250301		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.05846513322250301 | validation: 0.0682079926190702]
	TIME [epoch: 5.7 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05711031573488905		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.05711031573488905 | validation: 0.07156727875138658]
	TIME [epoch: 5.69 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05794349174765124		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.05794349174765124 | validation: 0.07410753196283656]
	TIME [epoch: 5.69 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05780615382376434		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.05780615382376434 | validation: 0.06640613578517943]
	TIME [epoch: 5.69 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05762937891586365		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.05762937891586365 | validation: 0.07277611664856198]
	TIME [epoch: 5.7 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05663876492973241		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.05663876492973241 | validation: 0.06970000479057685]
	TIME [epoch: 5.7 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05605234626486719		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.05605234626486719 | validation: 0.07453763880779862]
	TIME [epoch: 5.69 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059271093470670916		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.059271093470670916 | validation: 0.07326006052553108]
	TIME [epoch: 5.69 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05532055087711374		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.05532055087711374 | validation: 0.06649074231979017]
	TIME [epoch: 5.7 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05748789920764424		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.05748789920764424 | validation: 0.07495404235399508]
	TIME [epoch: 5.7 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05696987646943679		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.05696987646943679 | validation: 0.07916778200664383]
	TIME [epoch: 5.7 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05919097014064295		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.05919097014064295 | validation: 0.06548985325021199]
	TIME [epoch: 5.69 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0559685093984362		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.0559685093984362 | validation: 0.07064304173552226]
	TIME [epoch: 5.7 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05489965434303809		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.05489965434303809 | validation: 0.07072110686099232]
	TIME [epoch: 5.69 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05748451576206769		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.05748451576206769 | validation: 0.0668891266935701]
	TIME [epoch: 5.7 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05530522629106349		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.05530522629106349 | validation: 0.07590408204105385]
	TIME [epoch: 5.69 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06033071451193266		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.06033071451193266 | validation: 0.0712961863502759]
	TIME [epoch: 5.7 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05740337030444322		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.05740337030444322 | validation: 0.06541997804207669]
	TIME [epoch: 5.69 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058853390482590556		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.058853390482590556 | validation: 0.07110474642030322]
	TIME [epoch: 5.72 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057246219348289794		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.057246219348289794 | validation: 0.0725476999844283]
	TIME [epoch: 5.72 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056534486654565654		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.056534486654565654 | validation: 0.06818893761366113]
	TIME [epoch: 5.73 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05715474215784933		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.05715474215784933 | validation: 0.0680103728577736]
	TIME [epoch: 5.72 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05654533286279765		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.05654533286279765 | validation: 0.07203805602247428]
	TIME [epoch: 5.72 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057170952343959025		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.057170952343959025 | validation: 0.07199381955563547]
	TIME [epoch: 5.69 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057612967112946945		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.057612967112946945 | validation: 0.06800380588317481]
	TIME [epoch: 5.69 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057580760642736754		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.057580760642736754 | validation: 0.07139812243792766]
	TIME [epoch: 5.7 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05733369910306579		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.05733369910306579 | validation: 0.0835467297462063]
	TIME [epoch: 5.69 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060439396432268144		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.060439396432268144 | validation: 0.07059258206682911]
	TIME [epoch: 5.7 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05643314890724245		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.05643314890724245 | validation: 0.06975063175920145]
	TIME [epoch: 5.69 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05808441834149807		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.05808441834149807 | validation: 0.07247389199609265]
	TIME [epoch: 5.69 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05733380712204131		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.05733380712204131 | validation: 0.07299120849188123]
	TIME [epoch: 5.7 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055348130641833894		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.055348130641833894 | validation: 0.0721209811068983]
	TIME [epoch: 5.69 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05834701227374662		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.05834701227374662 | validation: 0.07186681935427335]
	TIME [epoch: 5.7 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05577796780377887		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.05577796780377887 | validation: 0.06534656125826532]
	TIME [epoch: 5.69 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05863441479272637		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.05863441479272637 | validation: 0.07043351611618223]
	TIME [epoch: 5.7 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05558986426842731		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.05558986426842731 | validation: 0.067487686288281]
	TIME [epoch: 5.7 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0556484220278095		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.0556484220278095 | validation: 0.06966446155801834]
	TIME [epoch: 5.7 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05732273803041274		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.05732273803041274 | validation: 0.0730836537408893]
	TIME [epoch: 5.7 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05835431891645655		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.05835431891645655 | validation: 0.06926908675647639]
	TIME [epoch: 5.69 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05737305051560262		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.05737305051560262 | validation: 0.07362981183926047]
	TIME [epoch: 5.69 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056163474320791745		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.056163474320791745 | validation: 0.06506972724265349]
	TIME [epoch: 5.69 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05532955033129203		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.05532955033129203 | validation: 0.06990389168690184]
	TIME [epoch: 5.7 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05662791188126791		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.05662791188126791 | validation: 0.0661461580458606]
	TIME [epoch: 5.71 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05551541369198011		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.05551541369198011 | validation: 0.07042303319785032]
	TIME [epoch: 5.69 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058343067884959916		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.058343067884959916 | validation: 0.0721750405822]
	TIME [epoch: 5.7 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059390203870504865		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.059390203870504865 | validation: 0.06856853549651208]
	TIME [epoch: 5.69 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057430811734754075		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.057430811734754075 | validation: 0.07829653201781614]
	TIME [epoch: 5.7 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05638644263241654		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.05638644263241654 | validation: 0.07175931698346512]
	TIME [epoch: 5.7 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05694707848680894		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.05694707848680894 | validation: 0.06934249067551408]
	TIME [epoch: 5.69 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05551203341969308		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.05551203341969308 | validation: 0.06768267530320958]
	TIME [epoch: 5.7 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05521308984344542		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.05521308984344542 | validation: 0.07092220568808973]
	TIME [epoch: 5.69 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055626960046221544		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.055626960046221544 | validation: 0.06669098326638999]
	TIME [epoch: 5.7 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056601740642880344		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.056601740642880344 | validation: 0.06245620249965433]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_1335.pth
	Model improved!!!
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05633084136431597		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.05633084136431597 | validation: 0.07209792349747782]
	TIME [epoch: 5.7 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058290850473776724		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.058290850473776724 | validation: 0.06926700256220954]
	TIME [epoch: 5.7 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05655221233783696		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.05655221233783696 | validation: 0.06620212753899533]
	TIME [epoch: 5.7 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055430934635320325		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.055430934635320325 | validation: 0.06826333827696335]
	TIME [epoch: 5.7 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056749521127025825		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.056749521127025825 | validation: 0.07160635552943255]
	TIME [epoch: 5.69 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05597447510845458		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.05597447510845458 | validation: 0.06815133592705183]
	TIME [epoch: 5.7 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05563780757325165		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.05563780757325165 | validation: 0.07042399088979091]
	TIME [epoch: 5.7 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056176772983678014		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.056176772983678014 | validation: 0.061085569309687206]
	TIME [epoch: 5.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_1343.pth
	Model improved!!!
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05450426903843358		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.05450426903843358 | validation: 0.07338442152210202]
	TIME [epoch: 5.7 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05608912092925872		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.05608912092925872 | validation: 0.07250457750302258]
	TIME [epoch: 5.69 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057960263213742966		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.057960263213742966 | validation: 0.07184139865955216]
	TIME [epoch: 5.69 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05880892862806087		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.05880892862806087 | validation: 0.06331944510910274]
	TIME [epoch: 5.68 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05648952746584248		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.05648952746584248 | validation: 0.06826908742199685]
	TIME [epoch: 5.69 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05880054978400472		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.05880054978400472 | validation: 0.07117228297079865]
	TIME [epoch: 5.69 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057609247593841194		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.057609247593841194 | validation: 0.07059041064405414]
	TIME [epoch: 5.69 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05660474981318791		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.05660474981318791 | validation: 0.06702873442879592]
	TIME [epoch: 5.69 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055688669259778445		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.055688669259778445 | validation: 0.07493364425637117]
	TIME [epoch: 5.69 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05512713101596282		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.05512713101596282 | validation: 0.07219944211518473]
	TIME [epoch: 5.69 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05570205112673074		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.05570205112673074 | validation: 0.06517538278751779]
	TIME [epoch: 5.7 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056979114933784526		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.056979114933784526 | validation: 0.07176409334014774]
	TIME [epoch: 5.71 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05622960562541421		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.05622960562541421 | validation: 0.0675834496381961]
	TIME [epoch: 5.7 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05679260644930443		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.05679260644930443 | validation: 0.07598708441646558]
	TIME [epoch: 5.7 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05739346666201262		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.05739346666201262 | validation: 0.07141056069815718]
	TIME [epoch: 5.7 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05692817183942741		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.05692817183942741 | validation: 0.07418162625627427]
	TIME [epoch: 5.7 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056840086049468566		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.056840086049468566 | validation: 0.06475385144238599]
	TIME [epoch: 5.66 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0558302755582657		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.0558302755582657 | validation: 0.06660293250464137]
	TIME [epoch: 5.66 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05591357156345656		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.05591357156345656 | validation: 0.06984786564026957]
	TIME [epoch: 5.67 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05567081603854199		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.05567081603854199 | validation: 0.06765243460112062]
	TIME [epoch: 5.66 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0554571273153586		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.0554571273153586 | validation: 0.06843924313925655]
	TIME [epoch: 5.66 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05544933634614025		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.05544933634614025 | validation: 0.07872294701423029]
	TIME [epoch: 5.66 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05710247547198552		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.05710247547198552 | validation: 0.06476920189697104]
	TIME [epoch: 5.66 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054225186256748284		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.054225186256748284 | validation: 0.06326736087348395]
	TIME [epoch: 5.66 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05733529865310234		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.05733529865310234 | validation: 0.0665893030604401]
	TIME [epoch: 5.65 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05541742313079807		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.05541742313079807 | validation: 0.07228834356400422]
	TIME [epoch: 5.66 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05595371053446503		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.05595371053446503 | validation: 0.06489957510420305]
	TIME [epoch: 5.66 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055282327841099196		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.055282327841099196 | validation: 0.07343672872356724]
	TIME [epoch: 5.66 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055464633805814734		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.055464633805814734 | validation: 0.06907140731140782]
	TIME [epoch: 5.65 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054601169061727885		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.054601169061727885 | validation: 0.06686067593321514]
	TIME [epoch: 5.66 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056351583739734645		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.056351583739734645 | validation: 0.07169479942258959]
	TIME [epoch: 5.65 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054912368807269914		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.054912368807269914 | validation: 0.07500694573867848]
	TIME [epoch: 5.66 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056856538687563116		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.056856538687563116 | validation: 0.06997593200095314]
	TIME [epoch: 5.66 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05537721510822369		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.05537721510822369 | validation: 0.07708665661408035]
	TIME [epoch: 5.66 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057726274679075706		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.057726274679075706 | validation: 0.06828484737731014]
	TIME [epoch: 5.66 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05765974241268285		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.05765974241268285 | validation: 0.06779989869866639]
	TIME [epoch: 5.66 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05572285333367953		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.05572285333367953 | validation: 0.06510135081497623]
	TIME [epoch: 5.68 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05730933329440109		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.05730933329440109 | validation: 0.06922589038459676]
	TIME [epoch: 5.69 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0539221397195026		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.0539221397195026 | validation: 0.07477557405176895]
	TIME [epoch: 5.69 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05610705041501528		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.05610705041501528 | validation: 0.07021276906214029]
	TIME [epoch: 5.69 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05490235656297029		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.05490235656297029 | validation: 0.06841179451142564]
	TIME [epoch: 5.68 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05528174244211961		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.05528174244211961 | validation: 0.06729691635080691]
	TIME [epoch: 5.69 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055587433324198264		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.055587433324198264 | validation: 0.06941606732674396]
	TIME [epoch: 5.69 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05635427914403758		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.05635427914403758 | validation: 0.06847099564524783]
	TIME [epoch: 5.7 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05520042836934902		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.05520042836934902 | validation: 0.06260392728270124]
	TIME [epoch: 5.69 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057717534631238215		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.057717534631238215 | validation: 0.06866568831182367]
	TIME [epoch: 5.69 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05420909285579531		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.05420909285579531 | validation: 0.06754662337303671]
	TIME [epoch: 5.68 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05516316767013068		[learning rate: 8.6788e-05]
	Learning Rate: 8.67884e-05
	LOSS [training: 0.05516316767013068 | validation: 0.06400067272845382]
	TIME [epoch: 5.69 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05618192940190198		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.05618192940190198 | validation: 0.06757903666364219]
	TIME [epoch: 5.69 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054265854132293415		[learning rate: 8.6176e-05]
	Learning Rate: 8.61757e-05
	LOSS [training: 0.054265854132293415 | validation: 0.07139901253474439]
	TIME [epoch: 5.69 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05729137284891676		[learning rate: 8.5871e-05]
	Learning Rate: 8.5871e-05
	LOSS [training: 0.05729137284891676 | validation: 0.07136697097375307]
	TIME [epoch: 5.68 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054520954090143015		[learning rate: 8.5567e-05]
	Learning Rate: 8.55673e-05
	LOSS [training: 0.054520954090143015 | validation: 0.07090286916158915]
	TIME [epoch: 5.69 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055453005441298726		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.055453005441298726 | validation: 0.06616426352988779]
	TIME [epoch: 5.69 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05522213856460276		[learning rate: 8.4963e-05]
	Learning Rate: 8.49632e-05
	LOSS [training: 0.05522213856460276 | validation: 0.07253572458339382]
	TIME [epoch: 5.69 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054565098725313614		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.054565098725313614 | validation: 0.0692196594357837]
	TIME [epoch: 5.69 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05459535832262082		[learning rate: 8.4363e-05]
	Learning Rate: 8.43634e-05
	LOSS [training: 0.05459535832262082 | validation: 0.06740776534963182]
	TIME [epoch: 5.69 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056711695802035306		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.056711695802035306 | validation: 0.07475020761895791]
	TIME [epoch: 5.69 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05465101077460451		[learning rate: 8.3768e-05]
	Learning Rate: 8.37678e-05
	LOSS [training: 0.05465101077460451 | validation: 0.07753974699073013]
	TIME [epoch: 5.7 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055370126636089374		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.055370126636089374 | validation: 0.06679179630522775]
	TIME [epoch: 5.7 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057002114573535254		[learning rate: 8.3176e-05]
	Learning Rate: 8.31764e-05
	LOSS [training: 0.057002114573535254 | validation: 0.06429533536175157]
	TIME [epoch: 5.69 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0550931989183532		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.0550931989183532 | validation: 0.07260471027500774]
	TIME [epoch: 5.69 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056474415680784984		[learning rate: 8.2589e-05]
	Learning Rate: 8.25892e-05
	LOSS [training: 0.056474415680784984 | validation: 0.0711690926008682]
	TIME [epoch: 5.69 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056814741386066146		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.056814741386066146 | validation: 0.06509174288921919]
	TIME [epoch: 5.66 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054669805547375286		[learning rate: 8.2006e-05]
	Learning Rate: 8.20061e-05
	LOSS [training: 0.054669805547375286 | validation: 0.06652732021074208]
	TIME [epoch: 5.7 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05682837018741038		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.05682837018741038 | validation: 0.07272655545932212]
	TIME [epoch: 5.7 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05598533700280171		[learning rate: 8.1427e-05]
	Learning Rate: 8.14272e-05
	LOSS [training: 0.05598533700280171 | validation: 0.06507596217245658]
	TIME [epoch: 5.69 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05627646581658316		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.05627646581658316 | validation: 0.06984474848162513]
	TIME [epoch: 5.69 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05384008389276031		[learning rate: 8.0852e-05]
	Learning Rate: 8.08523e-05
	LOSS [training: 0.05384008389276031 | validation: 0.0761617290447795]
	TIME [epoch: 5.69 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055131281232512847		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.055131281232512847 | validation: 0.06827693475174657]
	TIME [epoch: 5.69 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05404816718213709		[learning rate: 8.0281e-05]
	Learning Rate: 8.02815e-05
	LOSS [training: 0.05404816718213709 | validation: 0.06919700690133766]
	TIME [epoch: 5.69 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054642490842128154		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.054642490842128154 | validation: 0.07083474790147118]
	TIME [epoch: 5.69 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053996349904994946		[learning rate: 7.9715e-05]
	Learning Rate: 7.97147e-05
	LOSS [training: 0.053996349904994946 | validation: 0.06798992692314212]
	TIME [epoch: 5.69 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05490355447761076		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.05490355447761076 | validation: 0.07432874214818158]
	TIME [epoch: 5.69 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056739493376277164		[learning rate: 7.9152e-05]
	Learning Rate: 7.9152e-05
	LOSS [training: 0.056739493376277164 | validation: 0.06325179149566489]
	TIME [epoch: 5.69 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05528119212840065		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.05528119212840065 | validation: 0.06567309025277984]
	TIME [epoch: 5.69 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05472565044274268		[learning rate: 7.8593e-05]
	Learning Rate: 7.85931e-05
	LOSS [training: 0.05472565044274268 | validation: 0.06694933241116763]
	TIME [epoch: 5.69 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05493171626291192		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.05493171626291192 | validation: 0.07305991697402882]
	TIME [epoch: 5.69 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056246367335777626		[learning rate: 7.8038e-05]
	Learning Rate: 7.80383e-05
	LOSS [training: 0.056246367335777626 | validation: 0.0708484580204759]
	TIME [epoch: 5.69 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056175066913716876		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.056175066913716876 | validation: 0.06342642930932756]
	TIME [epoch: 5.69 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05458709578620744		[learning rate: 7.7487e-05]
	Learning Rate: 7.74873e-05
	LOSS [training: 0.05458709578620744 | validation: 0.06871713311908696]
	TIME [epoch: 5.69 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053908915268797104		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.053908915268797104 | validation: 0.07699405227269518]
	TIME [epoch: 5.69 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05403266142859764		[learning rate: 7.694e-05]
	Learning Rate: 7.69403e-05
	LOSS [training: 0.05403266142859764 | validation: 0.06443394438121056]
	TIME [epoch: 5.68 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05594915751070062		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.05594915751070062 | validation: 0.06748002277630692]
	TIME [epoch: 5.69 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05412354553860322		[learning rate: 7.6397e-05]
	Learning Rate: 7.63971e-05
	LOSS [training: 0.05412354553860322 | validation: 0.07262462196379003]
	TIME [epoch: 5.68 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05602941564642383		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.05602941564642383 | validation: 0.06974159872749384]
	TIME [epoch: 5.69 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05494182013414352		[learning rate: 7.5858e-05]
	Learning Rate: 7.58578e-05
	LOSS [training: 0.05494182013414352 | validation: 0.07017808393740016]
	TIME [epoch: 5.69 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05667590850116239		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.05667590850116239 | validation: 0.06254058339756455]
	TIME [epoch: 5.69 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05515810586381438		[learning rate: 7.5322e-05]
	Learning Rate: 7.53222e-05
	LOSS [training: 0.05515810586381438 | validation: 0.07010115183594534]
	TIME [epoch: 5.69 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054325376222085386		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.054325376222085386 | validation: 0.06434624096641893]
	TIME [epoch: 5.69 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05491946458705409		[learning rate: 7.479e-05]
	Learning Rate: 7.47905e-05
	LOSS [training: 0.05491946458705409 | validation: 0.06558789949534724]
	TIME [epoch: 5.69 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056073161905790994		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.056073161905790994 | validation: 0.06843700203293245]
	TIME [epoch: 5.68 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056675849027659826		[learning rate: 7.4262e-05]
	Learning Rate: 7.42625e-05
	LOSS [training: 0.056675849027659826 | validation: 0.07111246469235954]
	TIME [epoch: 5.69 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054299352247741885		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.054299352247741885 | validation: 0.07011649161750548]
	TIME [epoch: 5.68 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05395887069962635		[learning rate: 7.3738e-05]
	Learning Rate: 7.37382e-05
	LOSS [training: 0.05395887069962635 | validation: 0.06480760280478111]
	TIME [epoch: 5.69 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05435673100847849		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.05435673100847849 | validation: 0.07353257281978805]
	TIME [epoch: 5.69 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053726666950488194		[learning rate: 7.3218e-05]
	Learning Rate: 7.32176e-05
	LOSS [training: 0.053726666950488194 | validation: 0.07104876215573411]
	TIME [epoch: 5.68 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055916718528717484		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.055916718528717484 | validation: 0.07338998353297767]
	TIME [epoch: 5.69 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055109524295410384		[learning rate: 7.2701e-05]
	Learning Rate: 7.27007e-05
	LOSS [training: 0.055109524295410384 | validation: 0.06849396454938489]
	TIME [epoch: 5.69 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053385697941363566		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.053385697941363566 | validation: 0.06850643633543606]
	TIME [epoch: 5.69 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05388067486752779		[learning rate: 7.2187e-05]
	Learning Rate: 7.21874e-05
	LOSS [training: 0.05388067486752779 | validation: 0.07008270111850622]
	TIME [epoch: 5.68 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05457959071617943		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.05457959071617943 | validation: 0.07201809592122424]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion2_v_mmd1_20250430_112623/states/model_phi1_4a_distortion2_v_mmd1_1444.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 5189.735 seconds.
