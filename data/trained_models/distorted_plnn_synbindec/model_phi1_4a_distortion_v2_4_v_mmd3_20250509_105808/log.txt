Args:
Namespace(name='model_phi1_4a_distortion_v2_4_v_mmd3', outdir='out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_4/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_4/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.019716218, 0.1, 1.0], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2663491989

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2765084340437967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2765084340437967 | validation: 3.547429053532637]
	TIME [epoch: 122 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2291138730627713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2291138730627713 | validation: 3.549527170654571]
	TIME [epoch: 0.485 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.051672772898553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.051672772898553 | validation: 3.3008399890406688]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3483262795034925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3483262795034925 | validation: 3.2199722656080456]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7848606528566187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7848606528566187 | validation: 2.902820072084116]
	TIME [epoch: 0.478 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6899343641927547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6899343641927547 | validation: 2.510623409659125]
	TIME [epoch: 0.473 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7383395521391765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7383395521391765 | validation: 2.9296960906085436]
	TIME [epoch: 0.472 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7126757601402955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7126757601402955 | validation: 2.4553338022385827]
	TIME [epoch: 0.472 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3477818973265148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3477818973265148 | validation: 2.2520532568121547]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.397648031127316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.397648031127316 | validation: 2.523586174680137]
	TIME [epoch: 0.472 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3696727676144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3696727676144 | validation: 2.1437571152661263]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0979151963609297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0979151963609297 | validation: 2.0540095001430645]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9887939345111403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9887939345111403 | validation: 1.864422116287746]
	TIME [epoch: 0.473 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8355084643435198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8355084643435198 | validation: 2.493915247160525]
	TIME [epoch: 0.474 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4302655088874894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4302655088874894 | validation: 1.6076584641054978]
	TIME [epoch: 0.472 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9506933310700947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9506933310700947 | validation: 2.464234431362263]
	TIME [epoch: 0.472 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.359650329787502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.359650329787502 | validation: 2.0156241721581822]
	TIME [epoch: 0.471 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9239457580455064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9239457580455064 | validation: 2.2629995230569904]
	TIME [epoch: 0.474 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5930624104311306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5930624104311306 | validation: 1.706409970083258]
	TIME [epoch: 0.473 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.069518053281968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.069518053281968 | validation: 2.1100564136063666]
	TIME [epoch: 0.472 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9820126559803344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9820126559803344 | validation: 2.0185285516116496]
	TIME [epoch: 0.472 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8957842783169612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8957842783169612 | validation: 1.77245552902154]
	TIME [epoch: 0.472 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8427887315871436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8427887315871436 | validation: 1.7693826442847163]
	TIME [epoch: 0.474 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7216815142927147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7216815142927147 | validation: 1.7416807074057212]
	TIME [epoch: 0.472 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.680771360296322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.680771360296322 | validation: 1.5109284872133069]
	TIME [epoch: 0.471 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7042851719676315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7042851719676315 | validation: 1.9786803116727958]
	TIME [epoch: 0.472 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9537338530269739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9537338530269739 | validation: 1.5519344788641711]
	TIME [epoch: 0.473 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6391576847547964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6391576847547964 | validation: 1.4558567406979486]
	TIME [epoch: 0.472 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.66928610103609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.66928610103609 | validation: 1.9481265572817517]
	TIME [epoch: 0.475 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9034705219367114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9034705219367114 | validation: 1.5431284902529754]
	TIME [epoch: 0.474 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.61143116507459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.61143116507459 | validation: 1.4734638986115296]
	TIME [epoch: 0.477 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6340369269186559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6340369269186559 | validation: 1.9208913918700448]
	TIME [epoch: 0.472 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8840660536743037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8840660536743037 | validation: 1.5689515666502125]
	TIME [epoch: 0.471 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5895189889835228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5895189889835228 | validation: 1.4682575859529015]
	TIME [epoch: 0.471 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7276063607952952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7276063607952952 | validation: 1.894745304676215]
	TIME [epoch: 0.472 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.84351215600359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.84351215600359 | validation: 1.6574472409672811]
	TIME [epoch: 0.472 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6101607288335509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6101607288335509 | validation: 1.4871070310188612]
	TIME [epoch: 0.472 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8068748660475962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8068748660475962 | validation: 1.6268543036881]
	TIME [epoch: 0.48 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6018915348713199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6018915348713199 | validation: 1.5659703431785421]
	TIME [epoch: 0.472 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5764059359433964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5764059359433964 | validation: 1.4124850070798252]
	TIME [epoch: 0.472 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5803596775865345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5803596775865345 | validation: 1.5660436448014659]
	TIME [epoch: 0.472 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5859643900466902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5859643900466902 | validation: 1.3677387802472067]
	TIME [epoch: 0.471 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5618131684590844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5618131684590844 | validation: 1.547467603702981]
	TIME [epoch: 0.473 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5969644516968233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5969644516968233 | validation: 1.3850133336917612]
	TIME [epoch: 0.472 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5668921263156859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5668921263156859 | validation: 1.5183172328118273]
	TIME [epoch: 0.472 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.557111727135142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.557111727135142 | validation: 1.3788303521167908]
	TIME [epoch: 0.475 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5172565931750506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5172565931750506 | validation: 1.3912743978746007]
	TIME [epoch: 0.472 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5114267659196026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5114267659196026 | validation: 1.3359095014332079]
	TIME [epoch: 0.472 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4971030210337466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4971030210337466 | validation: 1.4032346760585395]
	TIME [epoch: 0.472 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5139878767745953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5139878767745953 | validation: 1.3890564942299812]
	TIME [epoch: 0.472 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5734646157296703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5734646157296703 | validation: 1.5856796647297622]
	TIME [epoch: 0.472 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6256427281715942		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.6256427281715942 | validation: 1.303782495018301]
	TIME [epoch: 0.473 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4837788520718709		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.4837788520718709 | validation: 1.3293020335325956]
	TIME [epoch: 0.472 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.470223618063435		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.470223618063435 | validation: 1.2928263802390174]
	TIME [epoch: 0.473 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4685529810443245		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.4685529810443245 | validation: 1.2862217921184695]
	TIME [epoch: 0.473 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4689623770125133		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.4689623770125133 | validation: 1.3162439069879384]
	TIME [epoch: 0.473 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4891617222183475		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.4891617222183475 | validation: 1.3726014572236893]
	TIME [epoch: 0.472 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5645989862715726		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.5645989862715726 | validation: 1.5432223837963017]
	TIME [epoch: 0.472 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5836926974248866		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.5836926974248866 | validation: 1.3100816550552103]
	TIME [epoch: 0.473 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4530909971646486		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.4530909971646486 | validation: 1.2510295609233948]
	TIME [epoch: 0.472 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.469400850121155		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.469400850121155 | validation: 1.4183975115640697]
	TIME [epoch: 0.472 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5218225624303354		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.5218225624303354 | validation: 1.3413194316223143]
	TIME [epoch: 0.472 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4793839640638158		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.4793839640638158 | validation: 1.3071569350733407]
	TIME [epoch: 0.473 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.456665182914441		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.456665182914441 | validation: 1.2383133473335564]
	TIME [epoch: 0.472 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4460370838090697		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.4460370838090697 | validation: 1.2770175789649614]
	TIME [epoch: 0.472 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.440939680471125		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.440939680471125 | validation: 1.2793496283161048]
	TIME [epoch: 0.472 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4477781113242654		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.4477781113242654 | validation: 1.2623292645189832]
	TIME [epoch: 0.473 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4479842132514393		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.4479842132514393 | validation: 1.2388436122762596]
	TIME [epoch: 0.472 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4605509318935679		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.4605509318935679 | validation: 1.3538937034312193]
	TIME [epoch: 0.471 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4683720860701117		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.4683720860701117 | validation: 1.2689801814822095]
	TIME [epoch: 0.471 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.466845602754906		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.466845602754906 | validation: 1.3344414016086767]
	TIME [epoch: 0.471 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4616314417160976		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.4616314417160976 | validation: 1.2683560226127035]
	TIME [epoch: 0.471 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4410959626341497		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.4410959626341497 | validation: 1.2621767684668652]
	TIME [epoch: 0.472 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4288857204640157		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.4288857204640157 | validation: 1.231771653734933]
	TIME [epoch: 0.471 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4221636088082465		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.4221636088082465 | validation: 1.2510632872074527]
	TIME [epoch: 0.473 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4166565799203699		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.4166565799203699 | validation: 1.2461693277417876]
	TIME [epoch: 0.471 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4225722480161032		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.4225722480161032 | validation: 1.2179626120154947]
	TIME [epoch: 0.471 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4204021955925294		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.4204021955925294 | validation: 1.253727930001764]
	TIME [epoch: 0.478 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.421330156438022		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.421330156438022 | validation: 1.2360857878498566]
	TIME [epoch: 0.474 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.448734468674247		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.448734468674247 | validation: 1.4223662853616252]
	TIME [epoch: 0.472 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5116094528819235		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.5116094528819235 | validation: 1.3105230234432494]
	TIME [epoch: 0.471 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4592574053048475		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.4592574053048475 | validation: 1.3061684522858126]
	TIME [epoch: 0.471 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4311311746308781		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.4311311746308781 | validation: 1.194130241408308]
	TIME [epoch: 0.472 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4200595182211213		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.4200595182211213 | validation: 1.2670561612779225]
	TIME [epoch: 0.478 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4141857258768682		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.4141857258768682 | validation: 1.2364430966858064]
	TIME [epoch: 0.472 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.409199244597607		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.409199244597607 | validation: 1.2357624971247985]
	TIME [epoch: 0.472 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4125369732158821		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.4125369732158821 | validation: 1.2487165625044492]
	TIME [epoch: 0.472 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4091389702518717		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.4091389702518717 | validation: 1.2371256275890838]
	TIME [epoch: 0.473 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4105713234116253		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.4105713234116253 | validation: 1.230291156628592]
	TIME [epoch: 0.471 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4122285209645447		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.4122285209645447 | validation: 1.2583929077437899]
	TIME [epoch: 0.471 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4342112434295424		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.4342112434295424 | validation: 1.2949747201030062]
	TIME [epoch: 0.472 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4820703115647058		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.4820703115647058 | validation: 1.499315610616285]
	TIME [epoch: 0.472 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5293915969633674		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.5293915969633674 | validation: 1.2596225649990735]
	TIME [epoch: 0.472 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4081568691068744		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.4081568691068744 | validation: 1.205163711150405]
	TIME [epoch: 0.472 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4442516391276203		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.4442516391276203 | validation: 1.3166526745818292]
	TIME [epoch: 0.471 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.442136984248155		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.442136984248155 | validation: 1.289540850916448]
	TIME [epoch: 0.471 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.41301157997935		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.41301157997935 | validation: 1.2333749077177496]
	TIME [epoch: 0.472 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4024916878497253		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.4024916878497253 | validation: 1.2076425823199188]
	TIME [epoch: 0.472 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4103160987969		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.4103160987969 | validation: 1.2335052336635106]
	TIME [epoch: 0.472 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.408506148089577		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.408506148089577 | validation: 1.2536676857069406]
	TIME [epoch: 0.472 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4014900561910912		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.4014900561910912 | validation: 1.231154288555191]
	TIME [epoch: 0.473 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4009848464741816		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.4009848464741816 | validation: 1.230658608967942]
	TIME [epoch: 0.473 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.398752589144691		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.398752589144691 | validation: 1.2032833534259044]
	TIME [epoch: 0.472 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4012241953379883		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.4012241953379883 | validation: 1.218029387759529]
	TIME [epoch: 0.472 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4112743896356823		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.4112743896356823 | validation: 1.3656821491911852]
	TIME [epoch: 0.472 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.444173683159362		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.444173683159362 | validation: 1.2919066371282015]
	TIME [epoch: 0.473 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4748313294423536		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.4748313294423536 | validation: 1.3515557334361075]
	TIME [epoch: 0.472 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4437905056674194		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.4437905056674194 | validation: 1.2435798789082064]
	TIME [epoch: 0.472 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3988006132223212		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.3988006132223212 | validation: 1.2362222434877763]
	TIME [epoch: 0.472 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.402466909581421		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.402466909581421 | validation: 1.2653167889912582]
	TIME [epoch: 0.472 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4028122804086962		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.4028122804086962 | validation: 1.2151355886721833]
	TIME [epoch: 0.472 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4017447417827034		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.4017447417827034 | validation: 1.230246817092199]
	TIME [epoch: 0.472 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3941619241665513		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.3941619241665513 | validation: 1.205382633429236]
	TIME [epoch: 0.471 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3925266017573739		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.3925266017573739 | validation: 1.203655209087024]
	TIME [epoch: 0.471 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3953860048550837		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.3953860048550837 | validation: 1.2231173769680677]
	TIME [epoch: 0.472 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3965513689129854		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.3965513689129854 | validation: 1.1949252297665052]
	TIME [epoch: 0.472 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.395267314067213		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.395267314067213 | validation: 1.3118782235022721]
	TIME [epoch: 0.472 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4396214225184705		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.4396214225184705 | validation: 1.2742984727808289]
	TIME [epoch: 0.471 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5039951252644743		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.5039951252644743 | validation: 1.3640120776137086]
	TIME [epoch: 0.472 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.451996328784531		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.451996328784531 | validation: 1.2679225408903385]
	TIME [epoch: 0.472 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3938101813518973		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.3938101813518973 | validation: 1.2177162556902545]
	TIME [epoch: 0.472 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4073154045666143		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.4073154045666143 | validation: 1.2654969960604543]
	TIME [epoch: 0.471 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.402691859570676		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.402691859570676 | validation: 1.24801007412196]
	TIME [epoch: 0.471 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3952747804251204		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.3952747804251204 | validation: 1.2128284462935572]
	TIME [epoch: 0.472 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3882676082464405		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.3882676082464405 | validation: 1.2233427737971636]
	TIME [epoch: 0.472 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3926672040407977		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.3926672040407977 | validation: 1.211437672170143]
	TIME [epoch: 0.471 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3870170923889926		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.3870170923889926 | validation: 1.2213013763428604]
	TIME [epoch: 0.472 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3829335928357531		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.3829335928357531 | validation: 1.2062033750851648]
	TIME [epoch: 0.472 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3895105310188445		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.3895105310188445 | validation: 1.226285517024586]
	TIME [epoch: 0.472 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3921746688059378		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.3921746688059378 | validation: 1.2126162169832155]
	TIME [epoch: 0.472 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3960117763435937		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.3960117763435937 | validation: 1.3283231552133299]
	TIME [epoch: 0.471 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4459646342239134		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.4459646342239134 | validation: 1.1928724308357357]
	TIME [epoch: 0.471 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4575455324438316		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.4575455324438316 | validation: 1.3167921178401267]
	TIME [epoch: 0.475 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.418571941333327		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.418571941333327 | validation: 1.2497066320267]
	TIME [epoch: 0.473 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3900490398171461		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.3900490398171461 | validation: 1.1973387957662873]
	TIME [epoch: 0.473 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4083666776545567		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.4083666776545567 | validation: 1.234373013022636]
	TIME [epoch: 0.472 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.395096874217814		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.395096874217814 | validation: 1.2251016392750476]
	TIME [epoch: 0.472 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3816398876879692		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.3816398876879692 | validation: 1.1885268314240454]
	TIME [epoch: 0.472 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3810285168300453		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.3810285168300453 | validation: 1.2089638329798689]
	TIME [epoch: 0.473 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3809927451779243		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.3809927451779243 | validation: 1.2202197901477405]
	TIME [epoch: 0.472 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3774628364306634		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.3774628364306634 | validation: 1.185051565456635]
	TIME [epoch: 0.471 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3762842010677587		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.3762842010677587 | validation: 1.209778454362223]
	TIME [epoch: 0.473 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.378104023351811		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.378104023351811 | validation: 1.2134686578615488]
	TIME [epoch: 0.473 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3970013326566093		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.3970013326566093 | validation: 1.2117627463588692]
	TIME [epoch: 0.477 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.466637983963737		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.466637983963737 | validation: 1.4135974021545044]
	TIME [epoch: 0.472 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4710109968804985		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.4710109968804985 | validation: 1.2409467468293327]
	TIME [epoch: 0.473 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.389149069019195		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.389149069019195 | validation: 1.1969830648896687]
	TIME [epoch: 0.472 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.406193637688016		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.406193637688016 | validation: 1.2449541098119472]
	TIME [epoch: 0.472 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3904221509952934		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.3904221509952934 | validation: 1.2571551426034298]
	TIME [epoch: 0.471 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.384699161599568		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.384699161599568 | validation: 1.1928526044663372]
	TIME [epoch: 0.472 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.389394121819576		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.389394121819576 | validation: 1.2100793507224776]
	TIME [epoch: 0.473 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3816810523300456		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.3816810523300456 | validation: 1.203170440575915]
	TIME [epoch: 0.471 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.374692245879095		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.374692245879095 | validation: 1.1935942393919974]
	TIME [epoch: 0.471 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3771757965297318		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.3771757965297318 | validation: 1.2111337122407926]
	TIME [epoch: 0.471 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3713182954403862		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.3713182954403862 | validation: 1.1724728796094432]
	TIME [epoch: 0.471 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3821640151446537		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.3821640151446537 | validation: 1.221018615844772]
	TIME [epoch: 0.472 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3859438559406676		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.3859438559406676 | validation: 1.1870304642246148]
	TIME [epoch: 0.471 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3958217526043943		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.3958217526043943 | validation: 1.314113343456642]
	TIME [epoch: 0.471 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4161386888391438		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.4161386888391438 | validation: 1.1736196649997204]
	TIME [epoch: 0.472 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3822694761201604		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.3822694761201604 | validation: 1.1741904027139751]
	TIME [epoch: 0.471 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.376974943000386		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.376974943000386 | validation: 1.209696948922466]
	TIME [epoch: 0.471 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3734516181186813		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.3734516181186813 | validation: 1.1639827592220706]
	TIME [epoch: 0.471 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3848892773603743		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.3848892773603743 | validation: 1.204268576471625]
	TIME [epoch: 0.475 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3814680341169987		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.3814680341169987 | validation: 1.1812092549178395]
	TIME [epoch: 0.472 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3829359215504076		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.3829359215504076 | validation: 1.2808714914379435]
	TIME [epoch: 0.471 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3981181838957246		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.3981181838957246 | validation: 1.1260968357925016]
	TIME [epoch: 0.471 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4065512840998236		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.4065512840998236 | validation: 1.2758147267459539]
	TIME [epoch: 0.474 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3943604871897153		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.3943604871897153 | validation: 1.1933321889397361]
	TIME [epoch: 0.472 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3703348814256406		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.3703348814256406 | validation: 1.1555106747285542]
	TIME [epoch: 0.472 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3704874679639187		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.3704874679639187 | validation: 1.2038012943887866]
	TIME [epoch: 0.471 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.36928174604469		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.36928174604469 | validation: 1.184963911106143]
	TIME [epoch: 0.472 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.370217848362193		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.370217848362193 | validation: 1.1915341171258158]
	TIME [epoch: 0.474 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3751736056015613		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.3751736056015613 | validation: 1.1656066783013423]
	TIME [epoch: 0.471 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3747767286366122		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.3747767286366122 | validation: 1.3063700402624479]
	TIME [epoch: 0.471 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.403255975427668		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.403255975427668 | validation: 1.1340245228098973]
	TIME [epoch: 0.471 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3977575582852853		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.3977575582852853 | validation: 1.218622585940027]
	TIME [epoch: 0.471 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3832123280241306		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.3832123280241306 | validation: 1.1928271360551648]
	TIME [epoch: 0.471 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3659579648728661		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.3659579648728661 | validation: 1.1581587363920216]
	TIME [epoch: 0.471 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3634842612562803		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.3634842612562803 | validation: 1.1918712686488224]
	TIME [epoch: 0.471 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3713440728663733		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.3713440728663733 | validation: 1.1229888254031817]
	TIME [epoch: 0.471 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_180.pth
	Model improved!!!
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3806458792318006		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.3806458792318006 | validation: 1.2919389860244348]
	TIME [epoch: 0.474 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.396434301572237		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.396434301572237 | validation: 1.137501405248119]
	TIME [epoch: 0.473 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3910260366018292		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.3910260366018292 | validation: 1.2269842971572478]
	TIME [epoch: 0.471 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3773620784104343		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.3773620784104343 | validation: 1.2109855785310444]
	TIME [epoch: 0.471 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3639073866415317		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.3639073866415317 | validation: 1.1222744050286118]
	TIME [epoch: 0.471 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.370200442095564		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.370200442095564 | validation: 1.1870843672907476]
	TIME [epoch: 0.476 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3672990970265073		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.3672990970265073 | validation: 1.1501951849988756]
	TIME [epoch: 0.473 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3681298351896738		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.3681298351896738 | validation: 1.223746587185276]
	TIME [epoch: 0.472 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3728548669126996		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.3728548669126996 | validation: 1.134474145953991]
	TIME [epoch: 0.474 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3725488831704484		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.3725488831704484 | validation: 1.2692743778585294]
	TIME [epoch: 0.472 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3923998652723186		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.3923998652723186 | validation: 1.122104799571676]
	TIME [epoch: 0.472 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3685494628555928		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.3685494628555928 | validation: 1.1411510935610014]
	TIME [epoch: 0.473 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3586105371781825		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.3586105371781825 | validation: 1.223092638163789]
	TIME [epoch: 0.472 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3660789871246817		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.3660789871246817 | validation: 1.1422620328103004]
	TIME [epoch: 0.472 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3583238830652777		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.3583238830652777 | validation: 1.1268065076881937]
	TIME [epoch: 0.472 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3667441445379864		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.3667441445379864 | validation: 1.1533782776315313]
	TIME [epoch: 0.472 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3748806335224795		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.3748806335224795 | validation: 1.3354836249593838]
	TIME [epoch: 0.473 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4169755866795826		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.4169755866795826 | validation: 1.1351539154898256]
	TIME [epoch: 0.472 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3756406572435285		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.3756406572435285 | validation: 1.1911694851884658]
	TIME [epoch: 0.471 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3642196013362304		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.3642196013362304 | validation: 1.1823880324462543]
	TIME [epoch: 0.471 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3519053956197808		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.3519053956197808 | validation: 1.1427833734973198]
	TIME [epoch: 128 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3577978630075742		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.3577978630075742 | validation: 1.1777994865218016]
	TIME [epoch: 0.938 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.359675474757301		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.359675474757301 | validation: 1.1577176450085729]
	TIME [epoch: 0.927 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3537112678236454		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.3537112678236454 | validation: 1.129717204308503]
	TIME [epoch: 0.927 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3561309425873878		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.3561309425873878 | validation: 1.1578379703200186]
	TIME [epoch: 0.927 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3509939986544222		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.3509939986544222 | validation: 1.1655388541047371]
	TIME [epoch: 0.925 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3548168007865722		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.3548168007865722 | validation: 1.1391823469969677]
	TIME [epoch: 0.926 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3638290371849078		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.3638290371849078 | validation: 1.2774147738852033]
	TIME [epoch: 0.926 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4057598898604682		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.4057598898604682 | validation: 1.1021133852711986]
	TIME [epoch: 0.927 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_209.pth
	Model improved!!!
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4267284239063345		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.4267284239063345 | validation: 1.2432276324935847]
	TIME [epoch: 0.927 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3774313913344816		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.3774313913344816 | validation: 1.1614508994482333]
	TIME [epoch: 0.927 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3486479175378343		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.3486479175378343 | validation: 1.1410702195518974]
	TIME [epoch: 0.926 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3625603629734762		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 1.3625603629734762 | validation: 1.1596625425694915]
	TIME [epoch: 0.926 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.352551875454742		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.352551875454742 | validation: 1.1517727570448073]
	TIME [epoch: 0.925 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.357743925357571		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 1.357743925357571 | validation: 1.1658640045346853]
	TIME [epoch: 0.925 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3519166877385749		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.3519166877385749 | validation: 1.1095785065411132]
	TIME [epoch: 0.926 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3565948293985657		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.3565948293985657 | validation: 1.2076763596667797]
	TIME [epoch: 0.925 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3553984983058178		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.3553984983058178 | validation: 1.195123080505409]
	TIME [epoch: 0.928 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3546984131332818		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.3546984131332818 | validation: 1.0704227288644301]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3663503252111582		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.3663503252111582 | validation: 1.2445341810116615]
	TIME [epoch: 0.927 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3712077992899294		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 1.3712077992899294 | validation: 1.1261245028425635]
	TIME [epoch: 0.926 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3902211962557278		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.3902211962557278 | validation: 1.259659182832676]
	TIME [epoch: 0.927 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.388102938310908		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 1.388102938310908 | validation: 1.1492641197035507]
	TIME [epoch: 0.926 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3526100274116868		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.3526100274116868 | validation: 1.138418712881376]
	TIME [epoch: 0.93 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.349269388951806		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 1.349269388951806 | validation: 1.180158250300787]
	TIME [epoch: 0.925 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3635497277874566		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 1.3635497277874566 | validation: 1.111994349007008]
	TIME [epoch: 0.926 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.355698902134704		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.355698902134704 | validation: 1.2054389474901415]
	TIME [epoch: 0.925 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3579608116188575		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.3579608116188575 | validation: 1.1102217872936098]
	TIME [epoch: 0.926 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3584914564553097		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 1.3584914564553097 | validation: 1.1488079789867542]
	TIME [epoch: 0.926 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3559303233672548		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.3559303233672548 | validation: 1.1528206799941862]
	TIME [epoch: 0.924 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3544362760364572		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 1.3544362760364572 | validation: 1.1030386601229925]
	TIME [epoch: 0.926 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.349488792325102		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.349488792325102 | validation: 1.193655048235276]
	TIME [epoch: 0.926 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3476214371951696		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 1.3476214371951696 | validation: 1.1000983938587414]
	TIME [epoch: 0.926 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.356416731086736		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.356416731086736 | validation: 1.2877264480447561]
	TIME [epoch: 0.926 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.397014043703933		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 1.397014043703933 | validation: 1.1319855868270552]
	TIME [epoch: 0.926 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3931122739307322		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.3931122739307322 | validation: 1.2139781196141444]
	TIME [epoch: 0.926 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3611898575609522		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 1.3611898575609522 | validation: 1.1351290941763128]
	TIME [epoch: 0.926 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3532560473016144		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 1.3532560473016144 | validation: 1.1139546640996318]
	TIME [epoch: 0.925 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3517233188752584		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 1.3517233188752584 | validation: 1.171886341291845]
	TIME [epoch: 0.926 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3489367044913434		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 1.3489367044913434 | validation: 1.129688797422365]
	TIME [epoch: 0.925 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3479719825989398		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 1.3479719825989398 | validation: 1.1392360855423789]
	TIME [epoch: 0.925 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.341479003753062		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 1.341479003753062 | validation: 1.1246843864919118]
	TIME [epoch: 0.925 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3429738250099035		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 1.3429738250099035 | validation: 1.138321211200198]
	TIME [epoch: 0.926 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.346686396983697		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 1.346686396983697 | validation: 1.1032934439432882]
	TIME [epoch: 0.925 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3516162009903985		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 1.3516162009903985 | validation: 1.1885277430905437]
	TIME [epoch: 0.926 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3658912111087247		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.3658912111087247 | validation: 1.081609258129351]
	TIME [epoch: 0.925 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3813816994273571		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 1.3813816994273571 | validation: 1.2461681423917463]
	TIME [epoch: 0.925 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3727658205803892		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 1.3727658205803892 | validation: 1.090845806665296]
	TIME [epoch: 0.925 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3520276291045357		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 1.3520276291045357 | validation: 1.1173233888941139]
	TIME [epoch: 0.924 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3435199095977648		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 1.3435199095977648 | validation: 1.1855620275069292]
	TIME [epoch: 0.926 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3520725649099274		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 1.3520725649099274 | validation: 1.0828349174905727]
	TIME [epoch: 0.926 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.354589075402227		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 1.354589075402227 | validation: 1.1606471668873415]
	TIME [epoch: 0.926 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.349028208014446		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 1.349028208014446 | validation: 1.1570078016720515]
	TIME [epoch: 0.925 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3473266520690457		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 1.3473266520690457 | validation: 1.1737739549682031]
	TIME [epoch: 0.925 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.350316884269018		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 1.350316884269018 | validation: 1.0829719752363425]
	TIME [epoch: 0.925 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3571165885905407		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 1.3571165885905407 | validation: 1.2296260787625766]
	TIME [epoch: 0.931 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3635321372822182		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 1.3635321372822182 | validation: 1.088003480571603]
	TIME [epoch: 0.925 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.361747004413995		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 1.361747004413995 | validation: 1.1545400706269184]
	TIME [epoch: 0.925 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3506578019969266		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 1.3506578019969266 | validation: 1.1118070544853158]
	TIME [epoch: 0.925 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3433122262422221		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 1.3433122262422221 | validation: 1.142498958415891]
	TIME [epoch: 0.925 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3413187026457813		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 1.3413187026457813 | validation: 1.1243760258941986]
	TIME [epoch: 0.925 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3439780287588041		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 1.3439780287588041 | validation: 1.1305800198290397]
	TIME [epoch: 0.925 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3373353810765969		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 1.3373353810765969 | validation: 1.0924603214973367]
	TIME [epoch: 0.925 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3397208477019364		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 1.3397208477019364 | validation: 1.2218460940472562]
	TIME [epoch: 0.927 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3556189205438847		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 1.3556189205438847 | validation: 1.0149573011983828]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_265.pth
	Model improved!!!
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.37786185459117		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 1.37786185459117 | validation: 1.2329792240547044]
	TIME [epoch: 0.928 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3795218532953077		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 1.3795218532953077 | validation: 1.1927985528185379]
	TIME [epoch: 0.927 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3614253833991847		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 1.3614253833991847 | validation: 1.1022856782489336]
	TIME [epoch: 0.926 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3443277464178744		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 1.3443277464178744 | validation: 1.1567609689209715]
	TIME [epoch: 0.926 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3511610498152533		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 1.3511610498152533 | validation: 1.1183980842005643]
	TIME [epoch: 0.926 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3478747766871484		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 1.3478747766871484 | validation: 1.128536048845507]
	TIME [epoch: 0.926 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.338328040394434		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 1.338328040394434 | validation: 1.1613048013965954]
	TIME [epoch: 0.925 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3403127507458368		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 1.3403127507458368 | validation: 1.1151715355058773]
	TIME [epoch: 0.925 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3465373984208215		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 1.3465373984208215 | validation: 1.1733829461191976]
	TIME [epoch: 0.927 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3473234212322591		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 1.3473234212322591 | validation: 1.1025942196781089]
	TIME [epoch: 0.925 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3445569516336517		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 1.3445569516336517 | validation: 1.1684947698358898]
	TIME [epoch: 0.926 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3401381674467285		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 1.3401381674467285 | validation: 1.0885953806651707]
	TIME [epoch: 0.925 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3440692939115682		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 1.3440692939115682 | validation: 1.1211360720208625]
	TIME [epoch: 0.925 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.339669705194213		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 1.339669705194213 | validation: 1.1233320065313046]
	TIME [epoch: 0.926 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.332814310336118		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 1.332814310336118 | validation: 1.1268311172999055]
	TIME [epoch: 0.925 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3391774179829907		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 1.3391774179829907 | validation: 1.1374454249113053]
	TIME [epoch: 0.925 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3433384090620308		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 1.3433384090620308 | validation: 1.1806085213615802]
	TIME [epoch: 0.925 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3545458885896502		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 1.3545458885896502 | validation: 1.0323438543066261]
	TIME [epoch: 0.927 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.388534851430145		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 1.388534851430145 | validation: 1.2673579625580205]
	TIME [epoch: 0.926 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3784849658083442		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 1.3784849658083442 | validation: 1.1065685238941476]
	TIME [epoch: 0.926 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3386231801255064		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 1.3386231801255064 | validation: 1.077390663777489]
	TIME [epoch: 0.925 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3382869110763438		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 1.3382869110763438 | validation: 1.1825037993995708]
	TIME [epoch: 0.925 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.342729877493776		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 1.342729877493776 | validation: 1.1212437722360318]
	TIME [epoch: 0.93 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3341712800958976		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 1.3341712800958976 | validation: 1.102935556121576]
	TIME [epoch: 0.925 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3374356565248957		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 1.3374356565248957 | validation: 1.1746090124432476]
	TIME [epoch: 0.925 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3405417760224372		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 1.3405417760224372 | validation: 1.0886811832154766]
	TIME [epoch: 0.925 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.339330816449982		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 1.339330816449982 | validation: 1.1401581077454044]
	TIME [epoch: 0.924 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3380738644739443		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 1.3380738644739443 | validation: 1.1867791544961828]
	TIME [epoch: 0.925 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.345083602263271		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 1.345083602263271 | validation: 1.028173509218449]
	TIME [epoch: 0.925 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3516376290095329		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 1.3516376290095329 | validation: 1.1719962869447496]
	TIME [epoch: 0.924 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3393010483207264		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 1.3393010483207264 | validation: 1.0878875807043173]
	TIME [epoch: 0.925 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3596356519797512		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 1.3596356519797512 | validation: 1.17266114805792]
	TIME [epoch: 0.926 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3514625395165378		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 1.3514625395165378 | validation: 1.092748651653977]
	TIME [epoch: 0.925 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3387161994498125		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 1.3387161994498125 | validation: 1.1674764110063782]
	TIME [epoch: 0.925 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3392858907093796		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 1.3392858907093796 | validation: 1.072457276072335]
	TIME [epoch: 0.925 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3372832234527223		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 1.3372832234527223 | validation: 1.1323735201483616]
	TIME [epoch: 0.927 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3342904551166703		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 1.3342904551166703 | validation: 1.1007867471099615]
	TIME [epoch: 0.927 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3328514249380703		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 1.3328514249380703 | validation: 1.0963109965106081]
	TIME [epoch: 0.925 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3314017057614342		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 1.3314017057614342 | validation: 1.0830576983312772]
	TIME [epoch: 0.927 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3327796012562465		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 1.3327796012562465 | validation: 1.1932170093904215]
	TIME [epoch: 0.925 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3447827350196755		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 1.3447827350196755 | validation: 1.1081798898469661]
	TIME [epoch: 0.925 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3603733101741418		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 1.3603733101741418 | validation: 1.2062610147868007]
	TIME [epoch: 0.925 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3625271648974087		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 1.3625271648974087 | validation: 1.0467196534018848]
	TIME [epoch: 0.924 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3492061707424825		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 1.3492061707424825 | validation: 1.179084246273337]
	TIME [epoch: 0.926 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3413175718787875		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 1.3413175718787875 | validation: 1.1244012441257767]
	TIME [epoch: 0.924 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3330973992296282		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 1.3330973992296282 | validation: 1.0809276131407775]
	TIME [epoch: 0.925 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3319844277612385		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 1.3319844277612385 | validation: 1.1480669582195984]
	TIME [epoch: 0.925 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3415434219704314		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 1.3415434219704314 | validation: 1.0632817966586896]
	TIME [epoch: 0.928 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3444438076503877		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 1.3444438076503877 | validation: 1.1906248313109806]
	TIME [epoch: 0.926 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3475738699627315		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 1.3475738699627315 | validation: 1.0973159747365435]
	TIME [epoch: 0.925 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.330777929777739		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 1.330777929777739 | validation: 1.1065825323618321]
	TIME [epoch: 0.924 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.332949922051668		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 1.332949922051668 | validation: 1.1031752717779904]
	TIME [epoch: 0.925 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3341635240946592		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 1.3341635240946592 | validation: 1.1554453675825733]
	TIME [epoch: 0.925 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.339151929527278		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 1.339151929527278 | validation: 1.0887225146578488]
	TIME [epoch: 0.926 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3355931288174459		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 1.3355931288174459 | validation: 1.148369696277495]
	TIME [epoch: 0.93 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3382857732026314		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 1.3382857732026314 | validation: 1.0824675967015425]
	TIME [epoch: 0.925 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3496744160052783		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 1.3496744160052783 | validation: 1.2148929784630427]
	TIME [epoch: 0.924 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3581119086815474		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 1.3581119086815474 | validation: 1.1070325951844968]
	TIME [epoch: 0.925 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3347667839818302		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 1.3347667839818302 | validation: 1.0913865012149229]
	TIME [epoch: 0.925 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3285467689656898		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 1.3285467689656898 | validation: 1.152574872555184]
	TIME [epoch: 0.924 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3287004386947672		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 1.3287004386947672 | validation: 1.0663994034081261]
	TIME [epoch: 0.925 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3289842302777397		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 1.3289842302777397 | validation: 1.1681731322312376]
	TIME [epoch: 0.925 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3370316776647035		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 1.3370316776647035 | validation: 1.039526358270984]
	TIME [epoch: 0.925 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3404548848088809		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 1.3404548848088809 | validation: 1.172265519856513]
	TIME [epoch: 0.925 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.339810891996226		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 1.339810891996226 | validation: 1.0933457971457745]
	TIME [epoch: 0.925 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3334421277571056		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 1.3334421277571056 | validation: 1.166558784363938]
	TIME [epoch: 0.925 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3469029476898198		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 1.3469029476898198 | validation: 1.1206179849745126]
	TIME [epoch: 0.925 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3428287545204733		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 1.3428287545204733 | validation: 1.1294755452478693]
	TIME [epoch: 0.926 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3315208931402174		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 1.3315208931402174 | validation: 1.0449338019898982]
	TIME [epoch: 0.925 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3306517519432732		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 1.3306517519432732 | validation: 1.1481437708110491]
	TIME [epoch: 0.925 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3316044732456254		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 1.3316044732456254 | validation: 1.106386548124419]
	TIME [epoch: 0.925 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3257035656125227		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 1.3257035656125227 | validation: 1.1344953739175387]
	TIME [epoch: 0.925 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3281669507284908		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 1.3281669507284908 | validation: 1.0534228504711083]
	TIME [epoch: 0.924 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3277171959383238		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 1.3277171959383238 | validation: 1.1563548174760072]
	TIME [epoch: 0.925 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3419307979033397		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 1.3419307979033397 | validation: 1.034739627902022]
	TIME [epoch: 0.925 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3572033639257233		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 1.3572033639257233 | validation: 1.2344091488607711]
	TIME [epoch: 0.925 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3518803300921471		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 1.3518803300921471 | validation: 1.0689804550107203]
	TIME [epoch: 0.926 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3319783942785584		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 1.3319783942785584 | validation: 1.0887798046470678]
	TIME [epoch: 0.925 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.324207344019073		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 1.324207344019073 | validation: 1.1093368602317308]
	TIME [epoch: 0.925 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3260397498839245		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 1.3260397498839245 | validation: 1.1283946921562644]
	TIME [epoch: 0.924 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3319106958834004		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 1.3319106958834004 | validation: 1.0824437060852474]
	TIME [epoch: 0.925 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3258270718225333		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 1.3258270718225333 | validation: 1.1043153432213286]
	TIME [epoch: 0.926 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3248430445798909		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 1.3248430445798909 | validation: 1.0454400668050179]
	TIME [epoch: 0.927 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3270722262719556		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 1.3270722262719556 | validation: 1.2068687947897265]
	TIME [epoch: 0.925 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3398664265859004		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 1.3398664265859004 | validation: 1.0362023925321502]
	TIME [epoch: 0.926 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3467729673551037		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 1.3467729673551037 | validation: 1.1719974690974422]
	TIME [epoch: 0.924 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3367498251178795		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 1.3367498251178795 | validation: 1.1182378211337156]
	TIME [epoch: 0.93 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.324132293633902		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 1.324132293633902 | validation: 1.0441604190374625]
	TIME [epoch: 0.924 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.333153724867941		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 1.333153724867941 | validation: 1.126176050452988]
	TIME [epoch: 0.924 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3267505138388658		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 1.3267505138388658 | validation: 1.1397589978151463]
	TIME [epoch: 0.926 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3242088475060496		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 1.3242088475060496 | validation: 1.102337320727394]
	TIME [epoch: 0.925 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3266919418720686		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 1.3266919418720686 | validation: 1.0952421093151747]
	TIME [epoch: 0.925 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3251417496400129		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 1.3251417496400129 | validation: 1.0821438852305882]
	TIME [epoch: 0.926 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3218260901564776		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 1.3218260901564776 | validation: 1.0570844013095255]
	TIME [epoch: 0.924 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3229067049884686		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 1.3229067049884686 | validation: 1.114252826667449]
	TIME [epoch: 0.926 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.330015845934932		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 1.330015845934932 | validation: 1.0509548983365773]
	TIME [epoch: 0.925 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3329244333262238		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 1.3329244333262238 | validation: 1.2286182418635212]
	TIME [epoch: 0.925 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3649093460913375		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 1.3649093460913375 | validation: 1.0415142458154132]
	TIME [epoch: 0.925 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3397072737315834		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 1.3397072737315834 | validation: 1.099098589752688]
	TIME [epoch: 0.925 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.327729936142603		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 1.327729936142603 | validation: 1.1409000501140927]
	TIME [epoch: 0.925 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3292014423226208		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 1.3292014423226208 | validation: 1.0561595721857113]
	TIME [epoch: 0.925 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3143500571401696		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 1.3143500571401696 | validation: 1.08247944550564]
	TIME [epoch: 0.926 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3246096239773804		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 1.3246096239773804 | validation: 1.1461636028123412]
	TIME [epoch: 0.926 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.336123410237782		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 1.336123410237782 | validation: 1.0429672164876809]
	TIME [epoch: 0.926 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3338731214659272		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 1.3338731214659272 | validation: 1.1436679361821127]
	TIME [epoch: 0.927 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3245831021984702		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 1.3245831021984702 | validation: 1.07448875914137]
	TIME [epoch: 0.926 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3226304363610053		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 1.3226304363610053 | validation: 1.089788198643911]
	TIME [epoch: 0.927 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.322188821341258		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 1.322188821341258 | validation: 1.0741335235196023]
	TIME [epoch: 0.926 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3237768418418367		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 1.3237768418418367 | validation: 1.1255155251436715]
	TIME [epoch: 0.927 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3348572415301296		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 1.3348572415301296 | validation: 1.1071592821222223]
	TIME [epoch: 0.926 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.334495164793032		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 1.334495164793032 | validation: 1.1469408238318697]
	TIME [epoch: 0.927 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3285815658975082		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 1.3285815658975082 | validation: 1.0363136959488657]
	TIME [epoch: 0.928 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3272030580034142		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 1.3272030580034142 | validation: 1.183446448415662]
	TIME [epoch: 0.928 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.331071428892259		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 1.331071428892259 | validation: 1.0475234350931404]
	TIME [epoch: 0.928 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3249893666237844		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 1.3249893666237844 | validation: 1.1208488003523998]
	TIME [epoch: 0.927 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3240043449450034		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 1.3240043449450034 | validation: 1.061469385857465]
	TIME [epoch: 0.927 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3191211750915117		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 1.3191211750915117 | validation: 1.1453161927785076]
	TIME [epoch: 0.928 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3293344919669614		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 1.3293344919669614 | validation: 1.0397569326964151]
	TIME [epoch: 0.926 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.320271074836799		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 1.320271074836799 | validation: 1.0820490925168191]
	TIME [epoch: 0.931 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3179554565347404		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 1.3179554565347404 | validation: 1.0686971616328342]
	TIME [epoch: 0.926 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3154638201662108		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 1.3154638201662108 | validation: 1.1204382213088537]
	TIME [epoch: 0.926 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3246365485408305		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 1.3246365485408305 | validation: 1.0273817741689086]
	TIME [epoch: 0.927 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3217144709075423		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 1.3217144709075423 | validation: 1.1535418944544895]
	TIME [epoch: 0.926 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3218319040611686		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 1.3218319040611686 | validation: 1.016839629230324]
	TIME [epoch: 0.927 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3202347362335274		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 1.3202347362335274 | validation: 1.1258862571811776]
	TIME [epoch: 0.927 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3215750988988135		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 1.3215750988988135 | validation: 1.0624582222627736]
	TIME [epoch: 0.925 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3217310328329586		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 1.3217310328329586 | validation: 1.11071677080539]
	TIME [epoch: 0.927 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3250701819475783		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 1.3250701819475783 | validation: 1.0909656341487524]
	TIME [epoch: 0.927 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3328565670218353		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 1.3328565670218353 | validation: 1.0776678463639928]
	TIME [epoch: 1.31 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3179592995266842		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 1.3179592995266842 | validation: 1.0872414269751427]
	TIME [epoch: 0.929 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3194142073426212		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 1.3194142073426212 | validation: 1.0390244662007408]
	TIME [epoch: 0.926 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3186310440142865		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 1.3186310440142865 | validation: 1.1709567015979907]
	TIME [epoch: 0.925 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3233003219223054		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 1.3233003219223054 | validation: 1.0591843028436554]
	TIME [epoch: 0.925 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3226564441432027		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 1.3226564441432027 | validation: 1.0967141222811183]
	TIME [epoch: 0.926 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.320840689039016		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 1.320840689039016 | validation: 1.0539083008603185]
	TIME [epoch: 0.924 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3185052868916416		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 1.3185052868916416 | validation: 1.113582626436205]
	TIME [epoch: 0.927 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3279135494191037		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 1.3279135494191037 | validation: 1.0962648480155235]
	TIME [epoch: 0.925 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.319243557266228		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 1.319243557266228 | validation: 1.0912716446974116]
	TIME [epoch: 0.926 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3251404538545564		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 1.3251404538545564 | validation: 1.0593668960980007]
	TIME [epoch: 0.925 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3245608008411358		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 1.3245608008411358 | validation: 1.156801360559031]
	TIME [epoch: 0.925 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3311145516228569		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 1.3311145516228569 | validation: 0.9841883494914829]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_406.pth
	Model improved!!!
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3340429208613946		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 1.3340429208613946 | validation: 1.1422631533660794]
	TIME [epoch: 0.927 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3199502861008228		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 1.3199502861008228 | validation: 1.050365816817982]
	TIME [epoch: 0.925 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3159185664505129		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 1.3159185664505129 | validation: 1.081931957243753]
	TIME [epoch: 0.926 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3153964953822697		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 1.3153964953822697 | validation: 1.0975975100706916]
	TIME [epoch: 0.926 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3136505674498158		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 1.3136505674498158 | validation: 1.078043986466782]
	TIME [epoch: 0.927 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3145664358155649		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 1.3145664358155649 | validation: 1.1026551530835371]
	TIME [epoch: 0.925 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3140624595085686		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 1.3140624595085686 | validation: 1.0833954556214132]
	TIME [epoch: 0.925 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.31062172831474		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 1.31062172831474 | validation: 1.0848332020932803]
	TIME [epoch: 0.925 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3173062248272651		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 1.3173062248272651 | validation: 1.054258044952166]
	TIME [epoch: 0.93 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3209828832530068		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 1.3209828832530068 | validation: 1.184733915402689]
	TIME [epoch: 0.925 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3430453923897399		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 1.3430453923897399 | validation: 1.0403324004759178]
	TIME [epoch: 0.925 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3225873769729048		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 1.3225873769729048 | validation: 1.0745422221371748]
	TIME [epoch: 0.925 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3110023981008132		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 1.3110023981008132 | validation: 1.102091796706348]
	TIME [epoch: 0.925 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3145980242612003		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 1.3145980242612003 | validation: 1.1174464703316622]
	TIME [epoch: 0.925 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3192211111896535		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 1.3192211111896535 | validation: 1.0296770803799162]
	TIME [epoch: 0.924 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3308915104647434		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 1.3308915104647434 | validation: 1.1486866827990483]
	TIME [epoch: 0.926 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3239901961074874		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 1.3239901961074874 | validation: 1.0376584143539263]
	TIME [epoch: 0.926 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.314883908926596		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 1.314883908926596 | validation: 1.0502958667246318]
	TIME [epoch: 0.925 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3137025975884924		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 1.3137025975884924 | validation: 1.132356317844289]
	TIME [epoch: 0.925 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3202458212979795		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 1.3202458212979795 | validation: 1.0345771985017227]
	TIME [epoch: 0.925 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3159967719695704		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 1.3159967719695704 | validation: 1.0273712677260252]
	TIME [epoch: 0.925 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3100243468097506		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 1.3100243468097506 | validation: 1.1637138344646392]
	TIME [epoch: 0.925 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3270854524322568		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 1.3270854524322568 | validation: 1.01727451303857]
	TIME [epoch: 0.925 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3116997580169192		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 1.3116997580169192 | validation: 1.089343796351963]
	TIME [epoch: 0.924 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3221384501690778		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 1.3221384501690778 | validation: 1.0934802151901992]
	TIME [epoch: 0.925 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3186435484108512		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 1.3186435484108512 | validation: 1.0553050142131484]
	TIME [epoch: 0.925 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3187445774223006		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 1.3187445774223006 | validation: 1.0795435355022145]
	TIME [epoch: 0.925 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3123161193742732		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 1.3123161193742732 | validation: 1.154946986265607]
	TIME [epoch: 0.925 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3234542376563156		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 1.3234542376563156 | validation: 0.9649880393562711]
	TIME [epoch: 0.924 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_435.pth
	Model improved!!!
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3422141155042295		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 1.3422141155042295 | validation: 1.0930865593841463]
	TIME [epoch: 0.928 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3098024313146843		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 1.3098024313146843 | validation: 1.1431454772493923]
	TIME [epoch: 0.926 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3143864118431094		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 1.3143864118431094 | validation: 1.0254838737475551]
	TIME [epoch: 0.927 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3080005215366872		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 1.3080005215366872 | validation: 1.117478730168498]
	TIME [epoch: 0.924 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3172287021378		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 1.3172287021378 | validation: 1.081291312768282]
	TIME [epoch: 0.942 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3162038345417582		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 1.3162038345417582 | validation: 1.078232110604742]
	TIME [epoch: 0.924 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.301946299101836		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 1.301946299101836 | validation: 1.0415580999079777]
	TIME [epoch: 0.925 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3102321349065518		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 1.3102321349065518 | validation: 1.0956161241810276]
	TIME [epoch: 0.924 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3162878832639948		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 1.3162878832639948 | validation: 1.0919847317661784]
	TIME [epoch: 0.925 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3204544507118354		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 1.3204544507118354 | validation: 1.0298655559112029]
	TIME [epoch: 0.925 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3130946051586068		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 1.3130946051586068 | validation: 1.0573268470419042]
	TIME [epoch: 0.926 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3077047226868974		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 1.3077047226868974 | validation: 1.1281339021119492]
	TIME [epoch: 0.929 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3184672277869305		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 1.3184672277869305 | validation: 1.0367660802203056]
	TIME [epoch: 0.925 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.315136106511198		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 1.315136106511198 | validation: 1.1084098938747535]
	TIME [epoch: 0.924 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3118472222119484		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 1.3118472222119484 | validation: 1.0163392837654435]
	TIME [epoch: 0.925 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3122063438258564		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 1.3122063438258564 | validation: 1.1341629784947325]
	TIME [epoch: 0.925 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3115027157532806		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 1.3115027157532806 | validation: 1.0347530222121457]
	TIME [epoch: 0.925 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3121003457661748		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 1.3121003457661748 | validation: 1.092735213427588]
	TIME [epoch: 0.924 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3116556252992786		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 1.3116556252992786 | validation: 1.108652028088552]
	TIME [epoch: 0.925 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3160584974895237		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 1.3160584974895237 | validation: 1.000663106427369]
	TIME [epoch: 0.925 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3184603906949437		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 1.3184603906949437 | validation: 1.1077363563650142]
	TIME [epoch: 0.924 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3106612076770137		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 1.3106612076770137 | validation: 1.0319404023892624]
	TIME [epoch: 0.965 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3090858390458322		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 1.3090858390458322 | validation: 1.0710321209209164]
	TIME [epoch: 0.926 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.311483887589378		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 1.311483887589378 | validation: 1.0165771977233482]
	TIME [epoch: 0.925 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3150530539958498		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 1.3150530539958498 | validation: 1.1630669467718886]
	TIME [epoch: 0.924 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3214121646825607		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 1.3214121646825607 | validation: 1.0316466459500675]
	TIME [epoch: 0.926 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3174723589803983		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 1.3174723589803983 | validation: 1.0509355499354056]
	TIME [epoch: 0.925 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.30968794641096		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 1.30968794641096 | validation: 1.0961308265694238]
	TIME [epoch: 0.925 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3071862753063617		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 1.3071862753063617 | validation: 1.0638357441134703]
	TIME [epoch: 0.924 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3032607998492234		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 1.3032607998492234 | validation: 1.078617203516154]
	TIME [epoch: 0.926 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3032026235903542		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 1.3032026235903542 | validation: 1.0358427590920822]
	TIME [epoch: 0.924 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3079539325101093		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 1.3079539325101093 | validation: 1.069467601236516]
	TIME [epoch: 0.925 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3106208666116124		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 1.3106208666116124 | validation: 1.049934268474116]
	TIME [epoch: 0.924 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.313965775900899		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 1.313965775900899 | validation: 1.1366648575169926]
	TIME [epoch: 0.926 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3243431679672855		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 1.3243431679672855 | validation: 1.0005978023230668]
	TIME [epoch: 0.925 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3237768319676053		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 1.3237768319676053 | validation: 1.1334939907879604]
	TIME [epoch: 0.925 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.31207451698939		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 1.31207451698939 | validation: 1.096340918729479]
	TIME [epoch: 0.926 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3090556537173956		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 1.3090556537173956 | validation: 0.948197404388364]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_473.pth
	Model improved!!!
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3412285721082093		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 1.3412285721082093 | validation: 1.0679396447730038]
	TIME [epoch: 0.927 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3050367686110813		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 1.3050367686110813 | validation: 1.0678269333331023]
	TIME [epoch: 0.937 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3006127349609093		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 1.3006127349609093 | validation: 1.0616354631898255]
	TIME [epoch: 0.925 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3082066895829132		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 1.3082066895829132 | validation: 1.0387876617353506]
	TIME [epoch: 0.925 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3088571762952972		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 1.3088571762952972 | validation: 1.0651020997967195]
	TIME [epoch: 0.924 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3139204040135963		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 1.3139204040135963 | validation: 1.059174004185904]
	TIME [epoch: 0.929 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3189836526961773		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 1.3189836526961773 | validation: 1.1120002212229634]
	TIME [epoch: 0.924 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3123591828528087		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 1.3123591828528087 | validation: 1.0415945521285657]
	TIME [epoch: 0.923 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3036076246460724		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 1.3036076246460724 | validation: 1.0329981114231015]
	TIME [epoch: 0.923 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.308611488817105		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 1.308611488817105 | validation: 1.1286972102833273]
	TIME [epoch: 0.923 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3106396403328364		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 1.3106396403328364 | validation: 0.9777694170559924]
	TIME [epoch: 0.924 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3123210205300422		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 1.3123210205300422 | validation: 1.1101357095059523]
	TIME [epoch: 0.924 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3100142834064372		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 1.3100142834064372 | validation: 1.0157912497139285]
	TIME [epoch: 0.924 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2993762110612455		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 1.2993762110612455 | validation: 1.0171887799153152]
	TIME [epoch: 0.924 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3028299306874616		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 1.3028299306874616 | validation: 1.0580963102680898]
	TIME [epoch: 0.925 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3025188701036978		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 1.3025188701036978 | validation: 1.0943870268215423]
	TIME [epoch: 0.924 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3065960628342503		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 1.3065960628342503 | validation: 1.021723826411846]
	TIME [epoch: 0.923 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3232325286014923		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 1.3232325286014923 | validation: 1.101052791203795]
	TIME [epoch: 0.926 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.307633926048998		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 1.307633926048998 | validation: 1.023681979495556]
	TIME [epoch: 0.923 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2993834671912288		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 1.2993834671912288 | validation: 0.97768425302558]
	TIME [epoch: 0.924 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3174278508072916		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 1.3174278508072916 | validation: 1.1242263725227977]
	TIME [epoch: 0.924 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.308006152417546		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 1.308006152417546 | validation: 1.0674909966246513]
	TIME [epoch: 0.925 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3046990367386457		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 1.3046990367386457 | validation: 0.9982753639418803]
	TIME [epoch: 0.924 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3093391807217807		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 1.3093391807217807 | validation: 1.091910757333469]
	TIME [epoch: 0.924 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.310236685741247		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 1.310236685741247 | validation: 1.017886269838147]
	TIME [epoch: 0.924 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3020017932003163		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 1.3020017932003163 | validation: 0.9824648324898008]
	TIME [epoch: 0.924 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3104982219036878		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 1.3104982219036878 | validation: 1.0795804886775022]
	TIME [epoch: 0.924 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3062645738796639		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 1.3062645738796639 | validation: 1.0464047231886393]
	TIME [epoch: 132 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3137335099525496		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 1.3137335099525496 | validation: 1.1170280658697942]
	TIME [epoch: 1.85 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3119453133658983		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 1.3119453133658983 | validation: 1.0391874281977853]
	TIME [epoch: 1.84 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3073963137978755		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 1.3073963137978755 | validation: 1.1223647923816291]
	TIME [epoch: 1.83 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3151426445375187		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 1.3151426445375187 | validation: 1.045446080509522]
	TIME [epoch: 1.83 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3045219506721675		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 1.3045219506721675 | validation: 1.0681365770294]
	TIME [epoch: 1.83 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3043682048878258		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 1.3043682048878258 | validation: 1.0517617769107968]
	TIME [epoch: 1.83 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3052931100183218		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 1.3052931100183218 | validation: 1.102810635933363]
	TIME [epoch: 1.84 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3146338487381442		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 1.3146338487381442 | validation: 1.045950744929095]
	TIME [epoch: 1.83 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3111924693549712		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 1.3111924693549712 | validation: 1.1003022420758493]
	TIME [epoch: 1.83 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3127640201188058		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 1.3127640201188058 | validation: 0.9754128071621184]
	TIME [epoch: 1.83 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3211652919573424		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 1.3211652919573424 | validation: 1.104188234209026]
	TIME [epoch: 1.83 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3131519992945937		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 1.3131519992945937 | validation: 1.0908625778468235]
	TIME [epoch: 1.83 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3095835697608151		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 1.3095835697608151 | validation: 1.0698767712229764]
	TIME [epoch: 1.83 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.315263520152651		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 1.315263520152651 | validation: 1.0298979259462018]
	TIME [epoch: 1.83 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3169649440646038		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 1.3169649440646038 | validation: 1.1252520950063112]
	TIME [epoch: 1.84 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3142305237086347		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 1.3142305237086347 | validation: 1.0442110572528018]
	TIME [epoch: 1.83 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3053700611166843		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 1.3053700611166843 | validation: 1.0515616599233673]
	TIME [epoch: 1.83 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.311349717621541		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 1.311349717621541 | validation: 1.1097949255897461]
	TIME [epoch: 1.83 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3163867909026987		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 1.3163867909026987 | validation: 0.9738924964669927]
	TIME [epoch: 1.83 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.308176771641928		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 1.308176771641928 | validation: 1.1444660930690214]
	TIME [epoch: 1.83 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3189731654832715		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 1.3189731654832715 | validation: 1.0452502592717008]
	TIME [epoch: 1.83 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3105131564720716		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 1.3105131564720716 | validation: 0.992580346639155]
	TIME [epoch: 1.83 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3185094979319687		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 1.3185094979319687 | validation: 1.1123491240018286]
	TIME [epoch: 1.83 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3173088232201926		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 1.3173088232201926 | validation: 1.0602302327288502]
	TIME [epoch: 1.84 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3059061948461252		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 1.3059061948461252 | validation: 1.0007332034424552]
	TIME [epoch: 1.83 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3098600437472565		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 1.3098600437472565 | validation: 1.0679106629997246]
	TIME [epoch: 1.85 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3050197377276807		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 1.3050197377276807 | validation: 1.0734470244509284]
	TIME [epoch: 1.83 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3013823950386731		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 1.3013823950386731 | validation: 1.010579556595223]
	TIME [epoch: 1.83 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.307892092297696		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 1.307892092297696 | validation: 1.0981400709569973]
	TIME [epoch: 1.83 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.314730841880231		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 1.314730841880231 | validation: 0.9939879495513386]
	TIME [epoch: 1.83 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3111523817301447		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 1.3111523817301447 | validation: 1.070262215348257]
	TIME [epoch: 1.84 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.307091645352765		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 1.307091645352765 | validation: 1.0254784068268454]
	TIME [epoch: 1.83 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3023410148693189		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 1.3023410148693189 | validation: 1.0687829815655125]
	TIME [epoch: 1.83 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3054004104556804		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 1.3054004104556804 | validation: 1.036641324605486]
	TIME [epoch: 1.83 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3041668710595509		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 1.3041668710595509 | validation: 1.0583470723125197]
	TIME [epoch: 1.83 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2990599467018915		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 1.2990599467018915 | validation: 0.9826767255106268]
	TIME [epoch: 1.83 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3120870255480386		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 1.3120870255480386 | validation: 1.115369386999274]
	TIME [epoch: 1.83 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.312456484362246		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 1.312456484362246 | validation: 1.0420301506466723]
	TIME [epoch: 1.83 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3003658320000022		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 1.3003658320000022 | validation: 1.0808939521267915]
	TIME [epoch: 1.83 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3031842895056878		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 1.3031842895056878 | validation: 0.9597601303725286]
	TIME [epoch: 1.84 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3195598469418541		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 1.3195598469418541 | validation: 1.1477257963064404]
	TIME [epoch: 1.83 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3191901293886565		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 1.3191901293886565 | validation: 1.0555299964918614]
	TIME [epoch: 1.83 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3020893663262558		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 1.3020893663262558 | validation: 0.9689187571643647]
	TIME [epoch: 1.83 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3095445374040628		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 1.3095445374040628 | validation: 1.0800676525787338]
	TIME [epoch: 1.84 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3098627379221126		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 1.3098627379221126 | validation: 1.095931872774791]
	TIME [epoch: 1.83 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3128259885755038		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 1.3128259885755038 | validation: 0.9915333260869024]
	TIME [epoch: 1.83 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3009612649041082		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 1.3009612649041082 | validation: 1.0787273860338604]
	TIME [epoch: 1.84 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.306124378302701		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 1.306124378302701 | validation: 1.0477116748554476]
	TIME [epoch: 1.83 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2988929877416753		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 1.2988929877416753 | validation: 1.03495243026386]
	TIME [epoch: 1.83 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3024102006808471		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 1.3024102006808471 | validation: 0.9929675716844113]
	TIME [epoch: 1.83 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3035775780799292		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 1.3035775780799292 | validation: 1.1287243551422446]
	TIME [epoch: 1.83 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3183129580898467		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 1.3183129580898467 | validation: 0.9886923041741981]
	TIME [epoch: 1.83 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3104533894705788		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 1.3104533894705788 | validation: 1.0372036274143062]
	TIME [epoch: 1.83 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3015851391348765		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 1.3015851391348765 | validation: 1.0422227545886646]
	TIME [epoch: 1.83 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3041308615689762		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 1.3041308615689762 | validation: 1.0574692950894449]
	TIME [epoch: 1.84 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3022884768211338		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 1.3022884768211338 | validation: 1.0227745767448688]
	TIME [epoch: 1.83 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3025148351605338		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 1.3025148351605338 | validation: 1.1004037597043084]
	TIME [epoch: 1.83 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.302308156166101		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 1.302308156166101 | validation: 1.0024581346204209]
	TIME [epoch: 1.83 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.30007752797449		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 1.30007752797449 | validation: 1.0281352605098466]
	TIME [epoch: 1.83 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2965294865265986		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 1.2965294865265986 | validation: 1.0380555339281374]
	TIME [epoch: 1.83 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2985039965882448		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 1.2985039965882448 | validation: 1.0485883820533162]
	TIME [epoch: 1.83 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3053333353481975		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 1.3053333353481975 | validation: 0.9740792936462559]
	TIME [epoch: 1.83 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3102670727098624		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 1.3102670727098624 | validation: 1.118103881267589]
	TIME [epoch: 1.83 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3180362434848543		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 1.3180362434848543 | validation: 0.9784053639195543]
	TIME [epoch: 1.84 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3158517337537277		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 1.3158517337537277 | validation: 1.0651788147595114]
	TIME [epoch: 1.83 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2988499609409965		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 1.2988499609409965 | validation: 1.0093704048351506]
	TIME [epoch: 1.83 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3025315954348642		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 1.3025315954348642 | validation: 1.040835773153107]
	TIME [epoch: 1.83 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2943969343355193		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 1.2943969343355193 | validation: 1.0158565880973014]
	TIME [epoch: 1.83 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2951877875200135		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 1.2951877875200135 | validation: 1.0504096547994377]
	TIME [epoch: 1.83 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2984313568153325		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 1.2984313568153325 | validation: 0.9962310845434917]
	TIME [epoch: 1.83 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3071572489953622		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 1.3071572489953622 | validation: 1.0419600857081155]
	TIME [epoch: 1.83 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2982391293242956		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 1.2982391293242956 | validation: 1.0295563457056527]
	TIME [epoch: 1.84 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2985287490861939		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 1.2985287490861939 | validation: 1.0335694573977605]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd3_20250509_105808/states/model_phi1_4a_distortion_v2_4_v_mmd3_574.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 911.241 seconds.
